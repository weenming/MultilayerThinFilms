{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./../../designer/script/')\n",
    "sys.path.append('./../')\n",
    "from design import Design\n",
    "from film import FilmSimple\n",
    "from spectrum import Spectrum\n",
    "import pickle\n",
    "\n",
    "def make_reflection_design(init_film: FilmSimple):\n",
    "    inc_ang = 0.\n",
    "    wls = np.linspace(400, 1000, 500) # when wls = 50, ~100 min\n",
    "    target_spec = [Spectrum(inc_ang, wls, np.ones(wls.shape[0], dtype='float'))]\n",
    "    \n",
    "    design = Design(target_spec, init_film)\n",
    "    return design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration, loss: 0.9545036783633585, 5 gd steps\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "0-th iteration recorded\n",
      "1-th iteration, loss: 0.6598180968910518, 37 gd steps\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "1-th iteration recorded\n",
      "2-th iteration, loss: 0.5253695015681243, 93 gd steps\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "2-th iteration recorded\n",
      "3-th iteration, loss: 0.41619380061309225, 71 gd steps\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "3-th iteration recorded\n",
      "4-th iteration, loss: 0.3399751099045045, 132 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "4-th iteration recorded\n",
      "5-th iteration, loss: 0.27394380057786455, 147 gd steps\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "5-th iteration recorded\n",
      "6-th iteration, loss: 0.2613972336879183, 999 gd steps\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "6-th iteration recorded\n",
      "7-th iteration, loss: 0.2271451822063708, 999 gd steps\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "7-th iteration recorded\n",
      "8-th iteration, loss: 0.21352233163004458, 999 gd steps\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "8-th iteration recorded\n",
      "9-th iteration, loss: 0.15674515669410252, 679 gd steps\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "9-th iteration recorded\n",
      "10-th iteration, loss: 0.14833037030804022, 589 gd steps\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "10-th iteration recorded\n",
      "11-th iteration, loss: 0.12078837626675515, 611 gd steps\n",
      "11-th iteration, new layer inserted. now 25 layers\n",
      "11-th iteration recorded\n",
      "12-th iteration, loss: 0.09686333376776855, 196 gd steps\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "12-th iteration recorded\n",
      "13-th iteration, loss: 0.06209958473956688, 999 gd steps\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "13-th iteration recorded\n",
      "14-th iteration, loss: 0.06036229920985235, 999 gd steps\n",
      "14-th iteration, new layer inserted. now 29 layers\n",
      "14-th iteration recorded\n",
      "15-th iteration, loss: 0.05799605605239107, 999 gd steps\n",
      "15-th iteration, new layer inserted. now 31 layers\n",
      "15-th iteration recorded\n",
      "16-th iteration, loss: 0.055857540613321766, 999 gd steps\n",
      "16-th iteration, new layer inserted. now 33 layers\n",
      "16-th iteration recorded\n",
      "17-th iteration, loss: 0.05499412641050284, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "17-th iteration, new layer inserted. now 35 layers\n",
      "17-th iteration recorded\n",
      "18-th iteration, loss: 0.05372381172004992, 999 gd steps\n",
      "18-th iteration, new layer inserted. now 35 layers\n",
      "18-th iteration recorded\n",
      "19-th iteration, loss: 0.043908640799635835, 869 gd steps\n",
      "19-th iteration, new layer inserted. now 35 layers\n",
      "19-th iteration recorded\n",
      "20-th iteration, loss: 0.03706419642378569, 999 gd steps\n",
      "20-th iteration, new layer inserted. now 37 layers\n",
      "20-th iteration recorded\n",
      "21-th iteration, loss: 0.03389112217708216, 999 gd steps\n",
      "21-th iteration, new layer inserted. now 37 layers\n",
      "21-th iteration recorded\n",
      "22-th iteration, loss: 0.032747766081630214, 999 gd steps\n",
      "22-th iteration, new layer inserted. now 39 layers\n",
      "22-th iteration recorded\n",
      "23-th iteration, loss: 0.02847013829464784, 999 gd steps\n",
      "23-th iteration, new layer inserted. now 41 layers\n",
      "23-th iteration recorded\n",
      "24-th iteration, loss: 0.027398040035990476, 999 gd steps\n",
      "24-th iteration, new layer inserted. now 43 layers\n",
      "24-th iteration recorded\n",
      "25-th iteration, loss: 0.02663471615986367, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "25-th iteration, new layer inserted. now 45 layers\n",
      "25-th iteration recorded\n",
      "26-th iteration, loss: 0.025841947509045744, 999 gd steps\n",
      "26-th iteration, new layer inserted. now 47 layers\n",
      "26-th iteration recorded\n",
      "27-th iteration, loss: 0.025570236474584798, 999 gd steps\n",
      "27-th iteration, new layer inserted. now 49 layers\n",
      "27-th iteration recorded\n",
      "28-th iteration, loss: 0.021433878479469162, 999 gd steps\n",
      "28-th iteration, new layer inserted. now 51 layers\n",
      "28-th iteration recorded\n",
      "29-th iteration, loss: 0.018600753240037668, 999 gd steps\n",
      "29-th iteration, new layer inserted. now 51 layers\n",
      "29-th iteration recorded\n",
      "30-th iteration, loss: 0.01846816741702799, 999 gd steps\n",
      "30-th iteration, new layer inserted. now 51 layers\n",
      "30-th iteration recorded\n",
      "31-th iteration, loss: 0.018399296319591852, 999 gd steps\n",
      "31-th iteration, new layer inserted. now 53 layers\n",
      "31-th iteration recorded\n",
      "32-th iteration, loss: 0.01832925345669548, 999 gd steps\n",
      "32-th iteration, new layer inserted. now 53 layers\n",
      "32-th iteration recorded\n",
      "33-th iteration, loss: 0.01827801178014242, 999 gd steps\n",
      "33-th iteration, new layer inserted. now 53 layers\n",
      "33-th iteration recorded\n",
      "34-th iteration, loss: 0.01822577122635517, 999 gd steps\n",
      "34-th iteration, new layer inserted. now 55 layers\n",
      "34-th iteration recorded\n",
      "35-th iteration, loss: 0.018077227542326872, 999 gd steps\n",
      "35-th iteration, new layer inserted. now 55 layers\n",
      "35-th iteration recorded\n",
      "36-th iteration, loss: 0.018045314696116616, 999 gd steps\n",
      "36-th iteration, new layer inserted. now 55 layers\n",
      "36-th iteration recorded\n",
      "37-th iteration, loss: 0.018013484409580106, 999 gd steps\n",
      "37-th iteration, new layer inserted. now 55 layers\n",
      "37-th iteration recorded\n",
      "38-th iteration, loss: 0.017968673127319833, 999 gd steps\n",
      "38-th iteration, new layer inserted. now 55 layers\n",
      "38-th iteration recorded\n",
      "39-th iteration, loss: 0.017933158499556543, 999 gd steps\n",
      "39-th iteration, new layer inserted. now 55 layers\n",
      "39-th iteration recorded\n",
      "40-th iteration, loss: 0.017900035300164773, 999 gd steps\n",
      "40-th iteration, new layer inserted. now 57 layers\n",
      "40-th iteration recorded\n",
      "41-th iteration, loss: 0.017846737464434496, 999 gd steps\n",
      "41-th iteration, new layer inserted. now 55 layers\n",
      "41-th iteration recorded\n",
      "42-th iteration, loss: 0.01780467946366468, 999 gd steps\n",
      "42-th iteration, new layer inserted. now 55 layers\n",
      "42-th iteration recorded\n",
      "43-th iteration, loss: 0.01774262799684307, 999 gd steps\n",
      "43-th iteration, new layer inserted. now 55 layers\n",
      "43-th iteration recorded\n",
      "44-th iteration, loss: 0.017633008760517294, 999 gd steps\n",
      "44-th iteration, new layer inserted. now 55 layers\n",
      "44-th iteration recorded\n",
      "45-th iteration, loss: 0.017425224956019975, 999 gd steps\n",
      "45-th iteration, new layer inserted. now 57 layers\n",
      "45-th iteration recorded\n",
      "46-th iteration, loss: 0.017274251261675325, 999 gd steps\n",
      "46-th iteration, new layer inserted. now 57 layers\n",
      "46-th iteration recorded\n",
      "47-th iteration, loss: 0.016367623259697664, 999 gd steps\n",
      "47-th iteration, new layer inserted. now 53 layers\n",
      "47-th iteration recorded\n",
      "48-th iteration, loss: 0.014102935699938098, 999 gd steps\n",
      "48-th iteration, new layer inserted. now 53 layers\n",
      "48-th iteration recorded\n",
      "49-th iteration, loss: 0.01392907272361815, 136 gd steps\n",
      "49-th iteration, new layer inserted. now 53 layers\n",
      "49-th iteration recorded\n",
      "50-th iteration, loss: 0.013809593415665575, 999 gd steps\n",
      "50-th iteration, new layer inserted. now 51 layers\n",
      "50-th iteration recorded\n",
      "51-th iteration, loss: 0.013629308334211667, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "51-th iteration, new layer inserted. now 51 layers\n",
      "51-th iteration recorded\n",
      "52-th iteration, loss: 0.01357466624536891, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "52-th iteration, new layer inserted. now 53 layers\n",
      "52-th iteration recorded\n",
      "53-th iteration, loss: 0.013404071944773532, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "53-th iteration, new layer inserted. now 51 layers\n",
      "53-th iteration recorded\n",
      "54-th iteration, loss: 0.013367540740178882, 999 gd steps\n",
      "54-th iteration, new layer inserted. now 51 layers\n",
      "54-th iteration recorded\n",
      "55-th iteration, loss: 0.013337873097750374, 999 gd steps\n",
      "55-th iteration, new layer inserted. now 53 layers\n",
      "55-th iteration recorded\n",
      "56-th iteration, loss: 0.013308539100966724, 999 gd steps\n",
      "56-th iteration, new layer inserted. now 55 layers\n",
      "56-th iteration recorded\n",
      "57-th iteration, loss: 0.01313225611295179, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "57-th iteration, new layer inserted. now 55 layers\n",
      "57-th iteration recorded\n",
      "58-th iteration, loss: 0.013104505254066187, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "58-th iteration, new layer inserted. now 57 layers\n",
      "58-th iteration recorded\n",
      "59-th iteration, loss: 0.013068937946643358, 999 gd steps\n",
      "59-th iteration, new layer inserted. now 59 layers\n",
      "59-th iteration recorded\n",
      "60-th iteration, loss: 0.01260655461931254, 999 gd steps\n",
      "60-th iteration, new layer inserted. now 57 layers\n",
      "60-th iteration recorded\n",
      "61-th iteration, loss: 0.012517349821827955, 999 gd steps\n",
      "61-th iteration, new layer inserted. now 59 layers\n",
      "61-th iteration recorded\n",
      "62-th iteration, loss: 0.01203762954581663, 999 gd steps\n",
      "62-th iteration, new layer inserted. now 55 layers\n",
      "62-th iteration recorded\n",
      "63-th iteration, loss: 0.01179960796066661, 999 gd steps\n",
      "63-th iteration, new layer inserted. now 55 layers\n",
      "63-th iteration recorded\n",
      "64-th iteration, loss: 0.01157312499518399, 999 gd steps\n",
      "64-th iteration, new layer inserted. now 53 layers\n",
      "64-th iteration recorded\n",
      "65-th iteration, loss: 0.011512280386459168, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "65-th iteration, new layer inserted. now 55 layers\n",
      "65-th iteration recorded\n",
      "66-th iteration, loss: 0.011437535392452583, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "66-th iteration, new layer inserted. now 57 layers\n",
      "66-th iteration recorded\n",
      "67-th iteration, loss: 0.011353551427049649, 999 gd steps\n",
      "67-th iteration, new layer inserted. now 59 layers\n",
      "67-th iteration recorded\n",
      "68-th iteration, loss: 0.011196734915509382, 999 gd steps\n",
      "68-th iteration, new layer inserted. now 59 layers\n",
      "68-th iteration recorded\n",
      "69-th iteration, loss: 0.011002960634518395, 999 gd steps\n",
      "69-th iteration, new layer inserted. now 59 layers\n",
      "69-th iteration recorded\n",
      "70-th iteration, loss: 0.010943117582028148, 999 gd steps\n",
      "70-th iteration, new layer inserted. now 59 layers\n",
      "70-th iteration recorded\n",
      "71-th iteration, loss: 0.010874907628721482, 999 gd steps\n",
      "71-th iteration, new layer inserted. now 59 layers\n",
      "71-th iteration recorded\n",
      "72-th iteration, loss: 0.010750904657638906, 999 gd steps\n",
      "72-th iteration, new layer inserted. now 59 layers\n",
      "72-th iteration recorded\n",
      "73-th iteration, loss: 0.010588754114285935, 999 gd steps\n",
      "73-th iteration, new layer inserted. now 55 layers\n",
      "73-th iteration recorded\n",
      "74-th iteration, loss: 0.010487903825246041, 999 gd steps\n",
      "74-th iteration, new layer inserted. now 53 layers\n",
      "74-th iteration recorded\n",
      "75-th iteration, loss: 0.010459255589365096, 999 gd steps\n",
      "75-th iteration, new layer inserted. now 53 layers\n",
      "75-th iteration recorded\n",
      "76-th iteration, loss: 0.010432930480626481, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "76-th iteration, new layer inserted. now 55 layers\n",
      "76-th iteration recorded\n",
      "77-th iteration, loss: 0.010421950247879602, 999 gd steps\n",
      "77-th iteration, new layer inserted. now 55 layers\n",
      "77-th iteration recorded\n",
      "78-th iteration, loss: 0.010411195424553571, 999 gd steps\n",
      "78-th iteration, new layer inserted. now 57 layers\n",
      "78-th iteration recorded\n",
      "79-th iteration, loss: 0.010393261300868487, 999 gd steps\n",
      "79-th iteration, new layer inserted. now 57 layers\n",
      "79-th iteration recorded\n",
      "80-th iteration, loss: 0.010361327073593529, 999 gd steps\n",
      "80-th iteration, new layer inserted. now 55 layers\n",
      "80-th iteration recorded\n",
      "81-th iteration, loss: 0.010351682072425582, 999 gd steps\n",
      "81-th iteration, new layer inserted. now 55 layers\n",
      "81-th iteration recorded\n",
      "82-th iteration, loss: 0.010342190809371451, 999 gd steps\n",
      "82-th iteration, new layer inserted. now 55 layers\n",
      "82-th iteration recorded\n",
      "83-th iteration, loss: 0.010332843143051511, 999 gd steps\n",
      "83-th iteration, new layer inserted. now 55 layers\n",
      "83-th iteration recorded\n",
      "84-th iteration, loss: 0.010323654536796246, 999 gd steps\n",
      "84-th iteration, new layer inserted. now 57 layers\n",
      "84-th iteration recorded\n",
      "85-th iteration, loss: 0.010312352553716797, 999 gd steps\n",
      "85-th iteration, new layer inserted. now 59 layers\n",
      "85-th iteration recorded\n",
      "86-th iteration, loss: 0.010300453258004324, 999 gd steps\n",
      "86-th iteration, new layer inserted. now 57 layers\n",
      "86-th iteration recorded\n",
      "87-th iteration, loss: 0.010270176517061705, 999 gd steps\n",
      "87-th iteration, new layer inserted. now 53 layers\n",
      "87-th iteration recorded\n",
      "88-th iteration, loss: 0.010261421956314961, 999 gd steps\n",
      "88-th iteration, new layer inserted. now 55 layers\n",
      "88-th iteration recorded\n",
      "89-th iteration, loss: 0.0102517100125466, 999 gd steps\n",
      "89-th iteration, new layer inserted. now 53 layers\n",
      "89-th iteration recorded\n",
      "90-th iteration, loss: 0.010232290103314619, 999 gd steps\n",
      "90-th iteration, new layer inserted. now 53 layers\n",
      "90-th iteration recorded\n",
      "91-th iteration, loss: 0.010224153610580414, 999 gd steps\n",
      "91-th iteration, new layer inserted. now 55 layers\n",
      "91-th iteration recorded\n",
      "92-th iteration, loss: 0.01021504650279645, 999 gd steps\n",
      "92-th iteration, new layer inserted. now 55 layers\n",
      "92-th iteration recorded\n",
      "93-th iteration, loss: 0.010206170472169712, 999 gd steps\n",
      "93-th iteration, new layer inserted. now 53 layers\n",
      "93-th iteration recorded\n",
      "94-th iteration, loss: 0.010198418634339465, 999 gd steps\n",
      "94-th iteration, new layer inserted. now 53 layers\n",
      "94-th iteration recorded\n",
      "95-th iteration, loss: 0.010189868940279226, 999 gd steps\n",
      "95-th iteration, new layer inserted. now 55 layers\n",
      "95-th iteration recorded\n",
      "96-th iteration, loss: 0.010181405424460003, 999 gd steps\n",
      "96-th iteration, new layer inserted. now 53 layers\n",
      "96-th iteration recorded\n",
      "97-th iteration, loss: 0.010173056292100954, 999 gd steps\n",
      "97-th iteration, new layer inserted. now 53 layers\n",
      "97-th iteration recorded\n",
      "98-th iteration, loss: 0.010165781003991032, 999 gd steps\n",
      "98-th iteration, new layer inserted. now 55 layers\n",
      "98-th iteration recorded\n",
      "99-th iteration, loss: 0.01015768975791242, 999 gd steps\n",
      "99-th iteration, new layer inserted. now 53 layers\n",
      "99-th iteration recorded\n",
      "100-th iteration, loss: 0.010150625046719942, 999 gd steps\n",
      "100-th iteration, new layer inserted. now 55 layers\n",
      "100-th iteration recorded\n",
      "101-th iteration, loss: 0.01014279150931207, 999 gd steps\n",
      "101-th iteration, new layer inserted. now 55 layers\n",
      "101-th iteration recorded\n",
      "102-th iteration, loss: 0.010135032130817685, 999 gd steps\n",
      "102-th iteration, new layer inserted. now 55 layers\n",
      "102-th iteration recorded\n",
      "103-th iteration, loss: 0.01012740648563933, 999 gd steps\n",
      "103-th iteration, new layer inserted. now 55 layers\n",
      "103-th iteration recorded\n",
      "104-th iteration, loss: 0.010119977682588302, 999 gd steps\n",
      "104-th iteration, new layer inserted. now 53 layers\n",
      "104-th iteration recorded\n",
      "105-th iteration, loss: 0.010113376111274578, 999 gd steps\n",
      "105-th iteration, new layer inserted. now 53 layers\n",
      "105-th iteration recorded\n",
      "106-th iteration, loss: 0.010106886293379842, 999 gd steps\n",
      "106-th iteration, new layer inserted. now 55 layers\n",
      "106-th iteration recorded\n",
      "107-th iteration, loss: 0.010099741313417862, 999 gd steps\n",
      "107-th iteration, new layer inserted. now 53 layers\n",
      "107-th iteration recorded\n",
      "108-th iteration, loss: 0.010091824369872611, 999 gd steps\n",
      "108-th iteration, new layer inserted. now 53 layers\n",
      "108-th iteration recorded\n",
      "109-th iteration, loss: 0.010085631485716505, 999 gd steps\n",
      "109-th iteration, new layer inserted. now 53 layers\n",
      "109-th iteration recorded\n",
      "110-th iteration, loss: 0.01007952589003505, 999 gd steps\n",
      "110-th iteration, new layer inserted. now 53 layers\n",
      "110-th iteration recorded\n",
      "111-th iteration, loss: 0.010073503715623716, 999 gd steps\n",
      "111-th iteration, new layer inserted. now 55 layers\n",
      "111-th iteration recorded\n",
      "112-th iteration, loss: 0.010053750794845353, 999 gd steps\n",
      "112-th iteration, new layer inserted. now 55 layers\n",
      "112-th iteration recorded\n",
      "113-th iteration, loss: 0.01004728973188884, 999 gd steps\n",
      "113-th iteration, new layer inserted. now 53 layers\n",
      "113-th iteration recorded\n",
      "114-th iteration, loss: 0.01004130507474653, 999 gd steps\n",
      "114-th iteration, new layer inserted. now 53 layers\n",
      "114-th iteration recorded\n",
      "115-th iteration, loss: 0.01003572492059415, 999 gd steps\n",
      "115-th iteration, new layer inserted. now 53 layers\n",
      "115-th iteration recorded\n",
      "116-th iteration, loss: 0.010030187349890092, 999 gd steps\n",
      "116-th iteration, new layer inserted. now 53 layers\n",
      "116-th iteration recorded\n",
      "117-th iteration, loss: 0.010024716652506118, 999 gd steps\n",
      "117-th iteration, new layer inserted. now 53 layers\n",
      "117-th iteration recorded\n",
      "118-th iteration, loss: 0.010019241422121796, 999 gd steps\n",
      "118-th iteration, new layer inserted. now 53 layers\n",
      "118-th iteration recorded\n",
      "119-th iteration, loss: 0.010013860578784088, 999 gd steps\n",
      "119-th iteration, new layer inserted. now 53 layers\n",
      "119-th iteration recorded\n",
      "120-th iteration, loss: 0.010008518632176324, 999 gd steps\n",
      "120-th iteration, new layer inserted. now 55 layers\n",
      "120-th iteration recorded\n",
      "121-th iteration, loss: 0.010002447442169913, 999 gd steps\n",
      "121-th iteration, new layer inserted. now 53 layers\n",
      "121-th iteration recorded\n",
      "122-th iteration, loss: 0.009996257347547817, 999 gd steps\n",
      "122-th iteration, new layer inserted. now 53 layers\n",
      "122-th iteration recorded\n",
      "123-th iteration, loss: 0.009991008465269874, 999 gd steps\n",
      "123-th iteration, new layer inserted. now 53 layers\n",
      "123-th iteration recorded\n",
      "124-th iteration, loss: 0.009984923948880937, 999 gd steps\n",
      "124-th iteration, new layer inserted. now 53 layers\n",
      "124-th iteration recorded\n",
      "125-th iteration, loss: 0.009979696661897217, 999 gd steps\n",
      "125-th iteration, new layer inserted. now 53 layers\n",
      "125-th iteration recorded\n",
      "126-th iteration, loss: 0.009974447308150997, 999 gd steps\n",
      "126-th iteration, new layer inserted. now 53 layers\n",
      "126-th iteration recorded\n",
      "127-th iteration, loss: 0.009969213713039857, 999 gd steps\n",
      "127-th iteration, new layer inserted. now 55 layers\n",
      "127-th iteration recorded\n",
      "128-th iteration, loss: 0.009964214576212995, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "128-th iteration, new layer inserted. now 57 layers\n",
      "128-th iteration recorded\n",
      "129-th iteration, loss: 0.009958002187110408, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "129-th iteration, new layer inserted. now 59 layers\n",
      "129-th iteration recorded\n",
      "130-th iteration, loss: 0.009953901297660703, 999 gd steps\n",
      "130-th iteration, new layer inserted. now 59 layers\n",
      "130-th iteration recorded\n",
      "131-th iteration, loss: 0.009949670568651085, 999 gd steps\n",
      "131-th iteration, new layer inserted. now 61 layers\n",
      "131-th iteration recorded\n",
      "132-th iteration, loss: 0.009944755747140547, 999 gd steps\n",
      "132-th iteration, new layer inserted. now 61 layers\n",
      "132-th iteration recorded\n",
      "133-th iteration, loss: 0.009940015030442758, 999 gd steps\n",
      "133-th iteration, new layer inserted. now 63 layers\n",
      "133-th iteration recorded\n",
      "134-th iteration, loss: 0.009935587652314955, 999 gd steps\n",
      "134-th iteration, new layer inserted. now 63 layers\n",
      "134-th iteration recorded\n",
      "135-th iteration, loss: 0.009931102844533635, 999 gd steps\n",
      "135-th iteration, new layer inserted. now 61 layers\n",
      "135-th iteration recorded\n",
      "136-th iteration, loss: 0.009927068961192727, 999 gd steps\n",
      "136-th iteration, new layer inserted. now 63 layers\n",
      "136-th iteration recorded\n",
      "137-th iteration, loss: 0.009921559162607157, 999 gd steps\n",
      "137-th iteration, new layer inserted. now 65 layers\n",
      "137-th iteration recorded\n",
      "138-th iteration, loss: 0.009917286995530641, 999 gd steps\n",
      "138-th iteration, new layer inserted. now 61 layers\n",
      "138-th iteration recorded\n",
      "139-th iteration, loss: 0.009913391655723747, 999 gd steps\n",
      "139-th iteration, new layer inserted. now 63 layers\n",
      "139-th iteration recorded\n",
      "140-th iteration, loss: 0.009909482473226561, 999 gd steps\n",
      "140-th iteration, new layer inserted. now 65 layers\n",
      "140-th iteration recorded\n",
      "141-th iteration, loss: 0.009905594883706337, 999 gd steps\n",
      "141-th iteration, new layer inserted. now 67 layers\n",
      "141-th iteration recorded\n",
      "142-th iteration, loss: 0.009901149493131478, 999 gd steps\n",
      "142-th iteration, new layer inserted. now 65 layers\n",
      "142-th iteration recorded\n",
      "143-th iteration, loss: 0.009896883496774029, 999 gd steps\n",
      "143-th iteration, new layer inserted. now 65 layers\n",
      "143-th iteration recorded\n",
      "144-th iteration, loss: 0.00989208420848373, 999 gd steps\n",
      "144-th iteration, new layer inserted. now 65 layers\n",
      "144-th iteration recorded\n",
      "145-th iteration, loss: 0.00988432701314747, 999 gd steps\n",
      "145-th iteration, new layer inserted. now 67 layers\n",
      "145-th iteration recorded\n",
      "146-th iteration, loss: 0.009878154222766587, 999 gd steps\n",
      "146-th iteration, new layer inserted. now 65 layers\n",
      "146-th iteration recorded\n",
      "147-th iteration, loss: 0.009854557160047874, 999 gd steps\n",
      "147-th iteration, new layer inserted. now 63 layers\n",
      "147-th iteration recorded\n",
      "148-th iteration, loss: 0.009848821476058115, 999 gd steps\n",
      "148-th iteration, new layer inserted. now 65 layers\n",
      "148-th iteration recorded\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "0-th iteration recorded\n",
      "1-th iteration, loss: 0.659818096891053, 36 gd steps\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "1-th iteration recorded\n",
      "2-th iteration, loss: 0.5253695015681193, 103 gd steps\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "2-th iteration recorded\n",
      "3-th iteration, loss: 0.4161938006130996, 72 gd steps\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "3-th iteration recorded\n",
      "4-th iteration, loss: 0.3399751099045046, 132 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "4-th iteration recorded\n",
      "5-th iteration, loss: 0.27394380057782575, 151 gd steps\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "5-th iteration recorded\n",
      "6-th iteration, loss: 0.26141216031216474, 999 gd steps\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "6-th iteration recorded\n",
      "7-th iteration, loss: 0.2271451847616023, 999 gd steps\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "7-th iteration recorded\n",
      "8-th iteration, loss: 0.21352233162940804, 999 gd steps\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "8-th iteration recorded\n",
      "9-th iteration, loss: 0.15674515669394637, 670 gd steps\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "9-th iteration recorded\n",
      "10-th iteration, loss: 0.1483303703078698, 597 gd steps\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "10-th iteration recorded\n",
      "11-th iteration, loss: 0.12078837626675516, 611 gd steps\n",
      "11-th iteration, new layer inserted. now 25 layers\n",
      "11-th iteration recorded\n",
      "12-th iteration, loss: 0.0968632784498307, 196 gd steps\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "12-th iteration recorded\n",
      "13-th iteration, loss: 0.062099574784600986, 999 gd steps\n",
      "13-th iteration, new layer inserted. now 26 layers\n",
      "13-th iteration recorded\n",
      "14-th iteration, loss: 0.060361705542731806, 999 gd steps\n",
      "14-th iteration, new layer inserted. now 28 layers\n",
      "14-th iteration recorded\n",
      "15-th iteration, loss: 0.05033551742003641, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 30 layers\n",
      "15-th iteration recorded\n",
      "16-th iteration, loss: 0.045675715023285234, 999 gd steps\n",
      "16-th iteration, new layer inserted. now 32 layers\n",
      "16-th iteration recorded\n",
      "17-th iteration, loss: 0.03975007133994194, 375 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "17-th iteration, new layer inserted. now 33 layers\n",
      "17-th iteration recorded\n",
      "18-th iteration, loss: 0.028347294498181335, 145 gd steps\n",
      "18-th iteration, new layer inserted. now 33 layers\n",
      "18-th iteration recorded\n",
      "19-th iteration, loss: 0.02501511250335462, 999 gd steps\n",
      "19-th iteration, new layer inserted. now 35 layers\n",
      "19-th iteration recorded\n",
      "20-th iteration, loss: 0.02315738807096611, 486 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 37 layers\n",
      "20-th iteration recorded\n",
      "21-th iteration, loss: 0.02196969774284139, 999 gd steps\n",
      "21-th iteration, new layer inserted. now 37 layers\n",
      "21-th iteration recorded\n",
      "22-th iteration, loss: 0.01901219116072142, 999 gd steps\n",
      "22-th iteration, new layer inserted. now 39 layers\n",
      "22-th iteration recorded\n",
      "23-th iteration, loss: 0.017841466260566593, 999 gd steps\n",
      "23-th iteration, new layer inserted. now 41 layers\n",
      "23-th iteration recorded\n",
      "24-th iteration, loss: 0.016257764816463797, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 43 layers\n",
      "24-th iteration recorded\n",
      "25-th iteration, loss: 0.01598645697240439, 999 gd steps\n",
      "25-th iteration, new layer inserted. now 45 layers\n",
      "25-th iteration recorded\n",
      "26-th iteration, loss: 0.015893985052174826, 999 gd steps\n",
      "26-th iteration, new layer inserted. now 45 layers\n",
      "26-th iteration recorded\n",
      "27-th iteration, loss: 0.01582748355619032, 999 gd steps\n",
      "27-th iteration, new layer inserted. now 47 layers\n",
      "27-th iteration recorded\n",
      "28-th iteration, loss: 0.015802510763421485, 999 gd steps\n",
      "28-th iteration, new layer inserted. now 47 layers\n",
      "28-th iteration recorded\n",
      "29-th iteration, loss: 0.015780030782453807, 999 gd steps\n",
      "29-th iteration, new layer inserted. now 47 layers\n",
      "29-th iteration recorded\n",
      "30-th iteration, loss: 0.015759342441585285, 999 gd steps\n",
      "30-th iteration, new layer inserted. now 47 layers\n",
      "30-th iteration recorded\n",
      "31-th iteration, loss: 0.015739620488982243, 999 gd steps\n",
      "31-th iteration, new layer inserted. now 47 layers\n",
      "31-th iteration recorded\n",
      "32-th iteration, loss: 0.01571499366407387, 999 gd steps\n",
      "32-th iteration, new layer inserted. now 47 layers\n",
      "32-th iteration recorded\n",
      "33-th iteration, loss: 0.01569758358001088, 999 gd steps\n",
      "33-th iteration, new layer inserted. now 47 layers\n",
      "33-th iteration recorded\n",
      "34-th iteration, loss: 0.015681219694252967, 999 gd steps\n",
      "34-th iteration, new layer inserted. now 47 layers\n",
      "34-th iteration recorded\n",
      "35-th iteration, loss: 0.01566506527601142, 999 gd steps\n",
      "35-th iteration, new layer inserted. now 47 layers\n",
      "35-th iteration recorded\n",
      "36-th iteration, loss: 0.01565013072545585, 999 gd steps\n",
      "36-th iteration, new layer inserted. now 47 layers\n",
      "36-th iteration recorded\n",
      "37-th iteration, loss: 0.015637040141937047, 999 gd steps\n",
      "37-th iteration, new layer inserted. now 49 layers\n",
      "37-th iteration recorded\n",
      "38-th iteration, loss: 0.015628706898846035, 999 gd steps\n",
      "38-th iteration, new layer inserted. now 47 layers\n",
      "38-th iteration recorded\n",
      "39-th iteration, loss: 0.015616176465491165, 999 gd steps\n",
      "39-th iteration, new layer inserted. now 49 layers\n",
      "39-th iteration recorded\n",
      "40-th iteration, loss: 0.015602098303202215, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 51 layers\n",
      "40-th iteration recorded\n",
      "41-th iteration, loss: 0.015585497267568096, 999 gd steps\n",
      "41-th iteration, new layer inserted. now 51 layers\n",
      "41-th iteration recorded\n",
      "42-th iteration, loss: 0.015569913760996508, 999 gd steps\n",
      "42-th iteration, new layer inserted. now 51 layers\n",
      "42-th iteration recorded\n",
      "43-th iteration, loss: 0.015554591764356224, 999 gd steps\n",
      "43-th iteration, new layer inserted. now 51 layers\n",
      "43-th iteration recorded\n",
      "44-th iteration, loss: 0.015540222992167282, 999 gd steps\n",
      "44-th iteration, new layer inserted. now 51 layers\n",
      "44-th iteration recorded\n",
      "45-th iteration, loss: 0.015526167447856615, 999 gd steps\n",
      "45-th iteration, new layer inserted. now 51 layers\n",
      "45-th iteration recorded\n",
      "46-th iteration, loss: 0.015512324503021698, 999 gd steps\n",
      "46-th iteration, new layer inserted. now 51 layers\n",
      "46-th iteration recorded\n",
      "47-th iteration, loss: 0.01549827280390137, 999 gd steps\n",
      "47-th iteration, new layer inserted. now 51 layers\n",
      "47-th iteration recorded\n",
      "48-th iteration, loss: 0.015484415274121163, 999 gd steps\n",
      "48-th iteration, new layer inserted. now 51 layers\n",
      "48-th iteration recorded\n",
      "49-th iteration, loss: 0.015470732250708885, 999 gd steps\n",
      "49-th iteration, new layer inserted. now 51 layers\n",
      "49-th iteration recorded\n",
      "50-th iteration, loss: 0.015455781333239972, 999 gd steps\n",
      "50-th iteration, new layer inserted. now 51 layers\n",
      "50-th iteration recorded\n",
      "51-th iteration, loss: 0.015443097185109578, 999 gd steps\n",
      "51-th iteration, new layer inserted. now 53 layers\n",
      "51-th iteration recorded\n",
      "52-th iteration, loss: 0.015435033016496041, 999 gd steps\n",
      "52-th iteration, new layer inserted. now 55 layers\n",
      "52-th iteration recorded\n",
      "53-th iteration, loss: 0.015425805694859247, 999 gd steps\n",
      "53-th iteration, new layer inserted. now 57 layers\n",
      "53-th iteration recorded\n",
      "54-th iteration, loss: 0.015416876821133794, 999 gd steps\n",
      "54-th iteration, new layer inserted. now 55 layers\n",
      "54-th iteration recorded\n",
      "55-th iteration, loss: 0.015403542669226392, 999 gd steps\n",
      "55-th iteration, new layer inserted. now 57 layers\n",
      "55-th iteration recorded\n",
      "56-th iteration, loss: 0.015389998585594977, 999 gd steps\n",
      "56-th iteration, new layer inserted. now 55 layers\n",
      "56-th iteration recorded\n",
      "57-th iteration, loss: 0.015377032760338822, 999 gd steps\n",
      "57-th iteration, new layer inserted. now 57 layers\n",
      "57-th iteration recorded\n",
      "58-th iteration, loss: 0.015368776379925201, 999 gd steps\n",
      "58-th iteration, new layer inserted. now 59 layers\n",
      "58-th iteration recorded\n",
      "59-th iteration, loss: 0.015360225058850973, 999 gd steps\n",
      "59-th iteration, new layer inserted. now 57 layers\n",
      "59-th iteration recorded\n",
      "60-th iteration, loss: 0.01535207170487171, 999 gd steps\n",
      "60-th iteration, new layer inserted. now 57 layers\n",
      "60-th iteration recorded\n",
      "61-th iteration, loss: 0.015343107631358687, 999 gd steps\n",
      "61-th iteration, new layer inserted. now 55 layers\n",
      "61-th iteration recorded\n",
      "62-th iteration, loss: 0.01533059121385894, 999 gd steps\n",
      "62-th iteration, new layer inserted. now 57 layers\n",
      "62-th iteration recorded\n",
      "63-th iteration, loss: 0.015322534429749395, 999 gd steps\n",
      "63-th iteration, new layer inserted. now 55 layers\n",
      "63-th iteration recorded\n",
      "64-th iteration, loss: 0.015310145867435138, 999 gd steps\n",
      "64-th iteration, new layer inserted. now 55 layers\n",
      "64-th iteration recorded\n",
      "65-th iteration, loss: 0.015297970699205236, 999 gd steps\n",
      "65-th iteration, new layer inserted. now 57 layers\n",
      "65-th iteration recorded\n",
      "66-th iteration, loss: 0.015284768615455746, 999 gd steps\n",
      "66-th iteration, new layer inserted. now 57 layers\n",
      "66-th iteration recorded\n",
      "67-th iteration, loss: 0.01527244426380764, 999 gd steps\n",
      "67-th iteration, new layer inserted. now 55 layers\n",
      "67-th iteration recorded\n",
      "68-th iteration, loss: 0.015259306663320153, 999 gd steps\n",
      "68-th iteration, new layer inserted. now 55 layers\n",
      "68-th iteration recorded\n",
      "69-th iteration, loss: 0.015247415254474515, 999 gd steps\n",
      "69-th iteration, new layer inserted. now 57 layers\n",
      "69-th iteration recorded\n",
      "70-th iteration, loss: 0.015239714414787034, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "70-th iteration, new layer inserted. now 59 layers\n",
      "70-th iteration recorded\n",
      "71-th iteration, loss: 0.015230671148561787, 999 gd steps\n",
      "71-th iteration, new layer inserted. now 61 layers\n",
      "71-th iteration recorded\n",
      "72-th iteration, loss: 0.015221121478315879, 999 gd steps\n",
      "72-th iteration, new layer inserted. now 59 layers\n",
      "72-th iteration recorded\n",
      "73-th iteration, loss: 0.015212542472191037, 999 gd steps\n",
      "73-th iteration, new layer inserted. now 59 layers\n",
      "73-th iteration recorded\n",
      "74-th iteration, loss: 0.015204080661253082, 999 gd steps\n",
      "74-th iteration, new layer inserted. now 59 layers\n",
      "74-th iteration recorded\n",
      "75-th iteration, loss: 0.015194954313131372, 999 gd steps\n",
      "75-th iteration, new layer inserted. now 57 layers\n",
      "75-th iteration recorded\n",
      "76-th iteration, loss: 0.015186910670600197, 999 gd steps\n",
      "76-th iteration, new layer inserted. now 55 layers\n",
      "76-th iteration recorded\n",
      "77-th iteration, loss: 0.015173812646955106, 999 gd steps\n",
      "77-th iteration, new layer inserted. now 57 layers\n",
      "77-th iteration recorded\n",
      "78-th iteration, loss: 0.015159531908208686, 999 gd steps\n",
      "78-th iteration, new layer inserted. now 57 layers\n",
      "78-th iteration recorded\n",
      "79-th iteration, loss: 0.015151738681126289, 999 gd steps\n",
      "79-th iteration, new layer inserted. now 59 layers\n",
      "79-th iteration recorded\n",
      "80-th iteration, loss: 0.015143370744281511, 999 gd steps\n",
      "80-th iteration, new layer inserted. now 55 layers\n",
      "80-th iteration recorded\n",
      "81-th iteration, loss: 0.01512972546073821, 999 gd steps\n",
      "81-th iteration, new layer inserted. now 55 layers\n",
      "81-th iteration recorded\n",
      "82-th iteration, loss: 0.015117578264647144, 999 gd steps\n",
      "82-th iteration, new layer inserted. now 57 layers\n",
      "82-th iteration recorded\n",
      "83-th iteration, loss: 0.015105246843579348, 999 gd steps\n",
      "83-th iteration, new layer inserted. now 57 layers\n",
      "83-th iteration recorded\n",
      "84-th iteration, loss: 0.015097667917578902, 999 gd steps\n",
      "84-th iteration, new layer inserted. now 55 layers\n",
      "84-th iteration recorded\n",
      "85-th iteration, loss: 0.015085295460357591, 999 gd steps\n",
      "85-th iteration, new layer inserted. now 55 layers\n",
      "85-th iteration recorded\n",
      "86-th iteration, loss: 0.01507314648412124, 999 gd steps\n",
      "86-th iteration, new layer inserted. now 55 layers\n",
      "86-th iteration recorded\n",
      "87-th iteration, loss: 0.01506114234317282, 999 gd steps\n",
      "87-th iteration, new layer inserted. now 55 layers\n",
      "87-th iteration recorded\n",
      "88-th iteration, loss: 0.015049330873360163, 999 gd steps\n",
      "88-th iteration, new layer inserted. now 55 layers\n",
      "88-th iteration recorded\n",
      "89-th iteration, loss: 0.0150379849837671, 999 gd steps\n",
      "89-th iteration, new layer inserted. now 57 layers\n",
      "89-th iteration recorded\n",
      "90-th iteration, loss: 0.015023503453345605, 999 gd steps\n",
      "90-th iteration, new layer inserted. now 55 layers\n",
      "90-th iteration recorded\n",
      "91-th iteration, loss: 0.015012301171921021, 999 gd steps\n",
      "91-th iteration, new layer inserted. now 55 layers\n",
      "91-th iteration recorded\n",
      "92-th iteration, loss: 0.015000668942837877, 999 gd steps\n",
      "92-th iteration, new layer inserted. now 55 layers\n",
      "92-th iteration recorded\n",
      "93-th iteration, loss: 0.014987930927433987, 999 gd steps\n",
      "93-th iteration, new layer inserted. now 57 layers\n",
      "93-th iteration recorded\n",
      "94-th iteration, loss: 0.014980154001440225, 999 gd steps\n",
      "94-th iteration, new layer inserted. now 55 layers\n",
      "94-th iteration recorded\n",
      "95-th iteration, loss: 0.014972760998468223, 999 gd steps\n",
      "95-th iteration, new layer inserted. now 53 layers\n",
      "95-th iteration recorded\n",
      "96-th iteration, loss: 0.014957700156950705, 999 gd steps\n",
      "96-th iteration, new layer inserted. now 55 layers\n",
      "96-th iteration recorded\n",
      "97-th iteration, loss: 0.014901421567448832, 999 gd steps\n",
      "97-th iteration, new layer inserted. now 53 layers\n",
      "97-th iteration recorded\n",
      "98-th iteration, loss: 0.014891863707596582, 999 gd steps\n",
      "98-th iteration, new layer inserted. now 53 layers\n",
      "98-th iteration recorded\n",
      "99-th iteration, loss: 0.014882997787358956, 999 gd steps\n",
      "99-th iteration, new layer inserted. now 55 layers\n",
      "99-th iteration recorded\n",
      "100-th iteration, loss: 0.014877302618730605, 999 gd steps\n",
      "100-th iteration, new layer inserted. now 53 layers\n",
      "100-th iteration recorded\n",
      "101-th iteration, loss: 0.014868565783518315, 999 gd steps\n",
      "101-th iteration, new layer inserted. now 53 layers\n",
      "101-th iteration recorded\n",
      "102-th iteration, loss: 0.014859887480204292, 999 gd steps\n",
      "102-th iteration, new layer inserted. now 55 layers\n",
      "102-th iteration recorded\n",
      "103-th iteration, loss: 0.014854262392112646, 999 gd steps\n",
      "103-th iteration, new layer inserted. now 57 layers\n",
      "103-th iteration recorded\n",
      "104-th iteration, loss: 0.014823193124347886, 999 gd steps\n",
      "104-th iteration, new layer inserted. now 57 layers\n",
      "104-th iteration recorded\n",
      "105-th iteration, loss: 0.014816624550972782, 999 gd steps\n",
      "105-th iteration, new layer inserted. now 59 layers\n",
      "105-th iteration recorded\n",
      "106-th iteration, loss: 0.014809623864867796, 999 gd steps\n",
      "106-th iteration, new layer inserted. now 55 layers\n",
      "106-th iteration recorded\n",
      "107-th iteration, loss: 0.014801421944709013, 999 gd steps\n",
      "107-th iteration, new layer inserted. now 53 layers\n",
      "107-th iteration recorded\n",
      "108-th iteration, loss: 0.01479253577254378, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "108-th iteration, new layer inserted. now 55 layers\n",
      "108-th iteration recorded\n",
      "109-th iteration, loss: 0.014781455604957628, 999 gd steps\n",
      "109-th iteration, new layer inserted. now 55 layers\n",
      "109-th iteration recorded\n",
      "110-th iteration, loss: 0.014771440147804838, 999 gd steps\n",
      "110-th iteration, new layer inserted. now 55 layers\n",
      "110-th iteration recorded\n",
      "111-th iteration, loss: 0.014761487077487551, 999 gd steps\n",
      "111-th iteration, new layer inserted. now 55 layers\n",
      "111-th iteration recorded\n",
      "112-th iteration, loss: 0.014750645320512594, 999 gd steps\n",
      "112-th iteration, new layer inserted. now 55 layers\n",
      "112-th iteration recorded\n",
      "113-th iteration, loss: 0.014738887624173025, 999 gd steps\n",
      "113-th iteration, new layer inserted. now 55 layers\n",
      "113-th iteration recorded\n",
      "114-th iteration, loss: 0.014728882515340337, 999 gd steps\n",
      "114-th iteration, new layer inserted. now 55 layers\n",
      "114-th iteration recorded\n",
      "115-th iteration, loss: 0.014718737377802405, 999 gd steps\n",
      "115-th iteration, new layer inserted. now 55 layers\n",
      "115-th iteration recorded\n",
      "116-th iteration, loss: 0.014708473385674265, 999 gd steps\n",
      "116-th iteration, new layer inserted. now 57 layers\n",
      "116-th iteration recorded\n",
      "117-th iteration, loss: 0.014700918906900479, 999 gd steps\n",
      "117-th iteration, new layer inserted. now 59 layers\n",
      "117-th iteration recorded\n",
      "118-th iteration, loss: 0.014692609783579965, 999 gd steps\n",
      "118-th iteration, new layer inserted. now 59 layers\n",
      "118-th iteration recorded\n",
      "119-th iteration, loss: 0.014685022143534208, 999 gd steps\n",
      "119-th iteration, new layer inserted. now 59 layers\n",
      "119-th iteration recorded\n",
      "120-th iteration, loss: 0.014677355351485257, 999 gd steps\n",
      "120-th iteration, new layer inserted. now 59 layers\n",
      "120-th iteration recorded\n",
      "121-th iteration, loss: 0.014668851084659852, 999 gd steps\n",
      "121-th iteration, new layer inserted. now 59 layers\n",
      "121-th iteration recorded\n",
      "122-th iteration, loss: 0.014660699746140484, 999 gd steps\n",
      "122-th iteration, new layer inserted. now 55 layers\n",
      "122-th iteration recorded\n",
      "123-th iteration, loss: 0.014650053395554901, 999 gd steps\n",
      "123-th iteration, new layer inserted. now 57 layers\n",
      "123-th iteration recorded\n",
      "124-th iteration, loss: 0.014638046003382503, 999 gd steps\n",
      "124-th iteration, new layer inserted. now 57 layers\n",
      "124-th iteration recorded\n",
      "125-th iteration, loss: 0.014625961720235316, 999 gd steps\n",
      "125-th iteration, new layer inserted. now 57 layers\n",
      "125-th iteration recorded\n",
      "126-th iteration, loss: 0.014613795596478865, 999 gd steps\n",
      "126-th iteration, new layer inserted. now 55 layers\n",
      "126-th iteration recorded\n",
      "127-th iteration, loss: 0.014602636664911597, 999 gd steps\n",
      "127-th iteration, new layer inserted. now 57 layers\n",
      "127-th iteration recorded\n",
      "128-th iteration, loss: 0.014595344915216265, 999 gd steps\n",
      "128-th iteration, new layer inserted. now 55 layers\n",
      "128-th iteration recorded\n",
      "129-th iteration, loss: 0.01458355553635243, 999 gd steps\n",
      "129-th iteration, new layer inserted. now 55 layers\n",
      "129-th iteration recorded\n",
      "130-th iteration, loss: 0.014572112380560958, 999 gd steps\n",
      "130-th iteration, new layer inserted. now 57 layers\n",
      "130-th iteration recorded\n",
      "131-th iteration, loss: 0.014564518431393439, 999 gd steps\n",
      "131-th iteration, new layer inserted. now 59 layers\n",
      "131-th iteration recorded\n",
      "132-th iteration, loss: 0.014555977878899713, 999 gd steps\n",
      "132-th iteration, new layer inserted. now 57 layers\n",
      "132-th iteration recorded\n",
      "133-th iteration, loss: 0.01454218116020492, 999 gd steps\n",
      "133-th iteration, new layer inserted. now 59 layers\n",
      "133-th iteration recorded\n",
      "134-th iteration, loss: 0.014527982952520608, 999 gd steps\n",
      "134-th iteration, new layer inserted. now 55 layers\n",
      "134-th iteration recorded\n",
      "135-th iteration, loss: 0.0145151985758498, 999 gd steps\n",
      "135-th iteration, new layer inserted. now 55 layers\n",
      "135-th iteration recorded\n",
      "136-th iteration, loss: 0.014502686941534716, 999 gd steps\n",
      "136-th iteration, new layer inserted. now 57 layers\n",
      "136-th iteration recorded\n",
      "137-th iteration, loss: 0.014494438824108686, 999 gd steps\n",
      "137-th iteration, new layer inserted. now 55 layers\n",
      "137-th iteration recorded\n",
      "138-th iteration, loss: 0.014481071830450163, 999 gd steps\n",
      "138-th iteration, new layer inserted. now 55 layers\n",
      "138-th iteration recorded\n",
      "139-th iteration, loss: 0.014468024057459386, 999 gd steps\n",
      "139-th iteration, new layer inserted. now 55 layers\n",
      "139-th iteration recorded\n",
      "140-th iteration, loss: 0.014454174273235058, 999 gd steps\n",
      "140-th iteration, new layer inserted. now 55 layers\n",
      "140-th iteration recorded\n",
      "141-th iteration, loss: 0.014440626583805788, 999 gd steps\n",
      "141-th iteration, new layer inserted. now 55 layers\n",
      "141-th iteration recorded\n",
      "142-th iteration, loss: 0.014426752249392799, 999 gd steps\n",
      "142-th iteration, new layer inserted. now 57 layers\n",
      "142-th iteration recorded\n",
      "143-th iteration, loss: 0.014417443514413928, 999 gd steps\n",
      "143-th iteration, new layer inserted. now 55 layers\n",
      "143-th iteration recorded\n",
      "144-th iteration, loss: 0.014402418768934491, 999 gd steps\n",
      "144-th iteration, new layer inserted. now 55 layers\n",
      "144-th iteration recorded\n",
      "145-th iteration, loss: 0.013939783145944898, 999 gd steps\n",
      "145-th iteration, new layer inserted. now 51 layers\n",
      "145-th iteration recorded\n",
      "146-th iteration, loss: 0.013910369800341939, 999 gd steps\n",
      "146-th iteration, new layer inserted. now 51 layers\n",
      "146-th iteration recorded\n",
      "147-th iteration, loss: 0.013875637350685395, 999 gd steps\n",
      "147-th iteration, new layer inserted. now 53 layers\n",
      "147-th iteration recorded\n",
      "148-th iteration, loss: 0.013840296241575465, 999 gd steps\n",
      "148-th iteration, new layer inserted. now 53 layers\n",
      "148-th iteration recorded\n",
      "149-th iteration, loss: 0.013795442987144435, 999 gd steps\n",
      "149-th iteration, new layer inserted. now 51 layers\n",
      "149-th iteration recorded\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "0-th iteration recorded\n",
      "1-th iteration, loss: 0.6598180968910524, 36 gd steps\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "1-th iteration recorded\n",
      "2-th iteration, loss: 0.5253695015681179, 98 gd steps\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "2-th iteration recorded\n",
      "3-th iteration, loss: 0.41619380061309424, 81 gd steps\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "3-th iteration recorded\n",
      "4-th iteration, loss: 0.3399751099045046, 132 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "4-th iteration recorded\n",
      "5-th iteration, loss: 0.2739438005778388, 150 gd steps\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "5-th iteration recorded\n",
      "6-th iteration, loss: 0.26711046807120226, 266 gd steps\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "6-th iteration recorded\n",
      "7-th iteration, loss: 0.21679962781387999, 999 gd steps\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "7-th iteration recorded\n",
      "8-th iteration, loss: 0.18860304224117516, 999 gd steps\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "8-th iteration recorded\n",
      "9-th iteration, loss: 0.1565680791743385, 678 gd steps\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "9-th iteration recorded\n",
      "10-th iteration, loss: 0.13533057642915952, 999 gd steps\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "10-th iteration recorded\n",
      "11-th iteration, loss: 0.10355666208322623, 873 gd steps\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "11-th iteration recorded\n",
      "12-th iteration, loss: 0.09585956583232107, 999 gd steps\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "12-th iteration recorded\n",
      "13-th iteration, loss: 0.08506614898618865, 999 gd steps\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "13-th iteration recorded\n",
      "14-th iteration, loss: 0.07814397864002295, 999 gd steps\n",
      "14-th iteration, new layer inserted. now 29 layers\n",
      "14-th iteration recorded\n",
      "15-th iteration, loss: 0.07378341013583758, 999 gd steps\n",
      "15-th iteration, new layer inserted. now 31 layers\n",
      "15-th iteration recorded\n",
      "16-th iteration, loss: 0.06970035347691428, 999 gd steps\n",
      "16-th iteration, new layer inserted. now 33 layers\n",
      "16-th iteration recorded\n",
      "17-th iteration, loss: 0.06948672986485052, 999 gd steps\n",
      "17-th iteration, new layer inserted. now 35 layers\n",
      "17-th iteration recorded\n",
      "18-th iteration, loss: 0.06934944135211624, 999 gd steps\n",
      "18-th iteration, new layer inserted. now 37 layers\n",
      "18-th iteration recorded\n",
      "19-th iteration, loss: 0.0691019276110668, 999 gd steps\n",
      "19-th iteration, new layer inserted. now 37 layers\n",
      "19-th iteration recorded\n",
      "20-th iteration, loss: 0.06895160212594455, 999 gd steps\n",
      "20-th iteration, new layer inserted. now 37 layers\n",
      "20-th iteration recorded\n",
      "21-th iteration, loss: 0.06880404124075776, 999 gd steps\n",
      "21-th iteration, new layer inserted. now 38 layers\n",
      "21-th iteration recorded\n",
      "22-th iteration, loss: 0.06862078554360011, 999 gd steps\n",
      "22-th iteration, new layer inserted. now 38 layers\n",
      "22-th iteration recorded\n",
      "23-th iteration, loss: 0.0684922950636695, 999 gd steps\n",
      "23-th iteration, new layer inserted. now 36 layers\n",
      "23-th iteration recorded\n",
      "24-th iteration, loss: 0.0683138872161186, 999 gd steps\n",
      "24-th iteration, new layer inserted. now 38 layers\n",
      "24-th iteration recorded\n",
      "25-th iteration, loss: 0.06812086258696327, 999 gd steps\n",
      "25-th iteration, new layer inserted. now 40 layers\n",
      "25-th iteration recorded\n",
      "26-th iteration, loss: 0.06800365282530411, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "26-th iteration, new layer inserted. now 42 layers\n",
      "26-th iteration recorded\n",
      "27-th iteration, loss: 0.0675934502378834, 999 gd steps\n",
      "27-th iteration, new layer inserted. now 44 layers\n",
      "27-th iteration recorded\n",
      "28-th iteration, loss: 0.06744007970589815, 999 gd steps\n",
      "28-th iteration, new layer inserted. now 42 layers\n",
      "28-th iteration recorded\n",
      "29-th iteration, loss: 0.06726669015051474, 999 gd steps\n",
      "29-th iteration, new layer inserted. now 42 layers\n",
      "29-th iteration recorded\n",
      "30-th iteration, loss: 0.06710630598388313, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 44 layers\n",
      "30-th iteration recorded\n",
      "31-th iteration, loss: 0.06693847614563553, 999 gd steps\n",
      "31-th iteration, new layer inserted. now 44 layers\n",
      "31-th iteration recorded\n",
      "32-th iteration, loss: 0.06677731347713323, 999 gd steps\n",
      "32-th iteration, new layer inserted. now 46 layers\n",
      "32-th iteration recorded\n",
      "33-th iteration, loss: 0.06652744753233884, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "33-th iteration, new layer inserted. now 48 layers\n",
      "33-th iteration recorded\n",
      "34-th iteration, loss: 0.06586449304720916, 999 gd steps\n",
      "34-th iteration, new layer inserted. now 46 layers\n",
      "34-th iteration recorded\n",
      "35-th iteration, loss: 0.06531827984534432, 999 gd steps\n",
      "35-th iteration, new layer inserted. now 48 layers\n",
      "35-th iteration recorded\n",
      "36-th iteration, loss: 0.06449388240158441, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "36-th iteration, new layer inserted. now 48 layers\n",
      "36-th iteration recorded\n",
      "37-th iteration, loss: 0.06395309348789702, 999 gd steps\n",
      "37-th iteration, new layer inserted. now 48 layers\n",
      "37-th iteration recorded\n",
      "38-th iteration, loss: 0.05968506154857598, 999 gd steps\n",
      "38-th iteration, new layer inserted. now 48 layers\n",
      "38-th iteration recorded\n",
      "39-th iteration, loss: 0.05431338415744243, 999 gd steps\n",
      "39-th iteration, new layer inserted. now 46 layers\n",
      "39-th iteration recorded\n",
      "40-th iteration, loss: 0.04563574762727828, 999 gd steps\n",
      "40-th iteration, new layer inserted. now 46 layers\n",
      "40-th iteration recorded\n",
      "41-th iteration, loss: 0.04424218192872007, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 46 layers\n",
      "41-th iteration recorded\n",
      "42-th iteration, loss: 0.04393655256390164, 999 gd steps\n",
      "42-th iteration, new layer inserted. now 48 layers\n",
      "42-th iteration recorded\n",
      "43-th iteration, loss: 0.042941804769683435, 999 gd steps\n",
      "43-th iteration, new layer inserted. now 48 layers\n",
      "43-th iteration recorded\n",
      "44-th iteration, loss: 0.04246202972028193, 999 gd steps\n",
      "44-th iteration, new layer inserted. now 48 layers\n",
      "44-th iteration recorded\n",
      "45-th iteration, loss: 0.0417677781287439, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 50 layers\n",
      "45-th iteration recorded\n",
      "46-th iteration, loss: 0.04159458922200511, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "46-th iteration, new layer inserted. now 52 layers\n",
      "46-th iteration recorded\n",
      "47-th iteration, loss: 0.04144285999972292, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 53 layers\n",
      "47-th iteration recorded\n",
      "48-th iteration, loss: 0.041189621565630594, 999 gd steps\n",
      "48-th iteration, new layer inserted. now 55 layers\n",
      "48-th iteration recorded\n",
      "49-th iteration, loss: 0.040884824109112004, 999 gd steps\n",
      "49-th iteration, new layer inserted. now 55 layers\n",
      "49-th iteration recorded\n",
      "50-th iteration, loss: 0.04058823839607815, 999 gd steps\n",
      "50-th iteration, new layer inserted. now 55 layers\n",
      "50-th iteration recorded\n",
      "51-th iteration, loss: 0.04018896542588882, 999 gd steps\n",
      "51-th iteration, new layer inserted. now 55 layers\n",
      "51-th iteration recorded\n",
      "52-th iteration, loss: 0.039579562952031196, 999 gd steps\n",
      "52-th iteration, new layer inserted. now 57 layers\n",
      "52-th iteration recorded\n",
      "53-th iteration, loss: 0.028657732417515302, 674 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "53-th iteration, new layer inserted. now 53 layers\n",
      "53-th iteration recorded\n",
      "54-th iteration, loss: 0.026867536545974083, 999 gd steps\n",
      "54-th iteration, new layer inserted. now 53 layers\n",
      "54-th iteration recorded\n",
      "55-th iteration, loss: 0.02664200523741297, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "55-th iteration, new layer inserted. now 53 layers\n",
      "55-th iteration recorded\n",
      "56-th iteration, loss: 0.026445876350503876, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "56-th iteration, new layer inserted. now 53 layers\n",
      "56-th iteration recorded\n",
      "57-th iteration, loss: 0.026255519270400055, 999 gd steps\n",
      "57-th iteration, new layer inserted. now 55 layers\n",
      "57-th iteration recorded\n",
      "58-th iteration, loss: 0.026011305177069538, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "58-th iteration, new layer inserted. now 57 layers\n",
      "58-th iteration recorded\n",
      "59-th iteration, loss: 0.02567712195043912, 999 gd steps\n",
      "59-th iteration, new layer inserted. now 59 layers\n",
      "59-th iteration recorded\n",
      "60-th iteration, loss: 0.025362199553843084, 999 gd steps\n",
      "60-th iteration, new layer inserted. now 59 layers\n",
      "60-th iteration recorded\n",
      "61-th iteration, loss: 0.02464464966147268, 999 gd steps\n",
      "61-th iteration, new layer inserted. now 57 layers\n",
      "61-th iteration recorded\n",
      "62-th iteration, loss: 0.0221677914485414, 997 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "62-th iteration, new layer inserted. now 57 layers\n",
      "62-th iteration recorded\n",
      "63-th iteration, loss: 0.021400502797807766, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "63-th iteration, new layer inserted. now 59 layers\n",
      "63-th iteration recorded\n",
      "64-th iteration, loss: 0.021281811206454768, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "64-th iteration, new layer inserted. now 61 layers\n",
      "64-th iteration recorded\n",
      "65-th iteration, loss: 0.02111176033251, 999 gd steps\n",
      "65-th iteration, new layer inserted. now 63 layers\n",
      "65-th iteration recorded\n",
      "66-th iteration, loss: 0.020835062125157695, 999 gd steps\n",
      "66-th iteration, new layer inserted. now 59 layers\n",
      "66-th iteration recorded\n",
      "67-th iteration, loss: 0.01970163990688377, 749 gd steps\n",
      "67-th iteration, new layer inserted. now 53 layers\n",
      "67-th iteration recorded\n",
      "68-th iteration, loss: 0.01875359420859144, 999 gd steps\n",
      "68-th iteration, new layer inserted. now 53 layers\n",
      "68-th iteration recorded\n",
      "69-th iteration, loss: 0.01864052410662889, 999 gd steps\n",
      "69-th iteration, new layer inserted. now 53 layers\n",
      "69-th iteration recorded\n",
      "70-th iteration, loss: 0.01854282578779229, 999 gd steps\n",
      "70-th iteration, new layer inserted. now 54 layers\n",
      "70-th iteration recorded\n",
      "71-th iteration, loss: 0.018259190104285384, 999 gd steps\n",
      "71-th iteration, new layer inserted. now 54 layers\n",
      "71-th iteration recorded\n",
      "72-th iteration, loss: 0.017991159946248374, 999 gd steps\n",
      "72-th iteration, new layer inserted. now 52 layers\n",
      "72-th iteration recorded\n",
      "73-th iteration, loss: 0.017707937163474956, 999 gd steps\n",
      "73-th iteration, new layer inserted. now 54 layers\n",
      "73-th iteration recorded\n",
      "74-th iteration, loss: 0.017598977715404165, 999 gd steps\n",
      "74-th iteration, new layer inserted. now 54 layers\n",
      "74-th iteration recorded\n",
      "75-th iteration, loss: 0.017482828152932613, 999 gd steps\n",
      "75-th iteration, new layer inserted. now 52 layers\n",
      "75-th iteration recorded\n",
      "76-th iteration, loss: 0.017407279569278924, 999 gd steps\n",
      "76-th iteration, new layer inserted. now 52 layers\n",
      "76-th iteration recorded\n",
      "77-th iteration, loss: 0.017325365503098565, 999 gd steps\n",
      "77-th iteration, new layer inserted. now 52 layers\n",
      "77-th iteration recorded\n",
      "78-th iteration, loss: 0.017243129181129014, 999 gd steps\n",
      "78-th iteration, new layer inserted. now 52 layers\n",
      "78-th iteration recorded\n",
      "79-th iteration, loss: 0.01717401938910873, 999 gd steps\n",
      "79-th iteration, new layer inserted. now 52 layers\n",
      "79-th iteration recorded\n",
      "80-th iteration, loss: 0.017106948337926895, 999 gd steps\n",
      "80-th iteration, new layer inserted. now 54 layers\n",
      "80-th iteration recorded\n",
      "81-th iteration, loss: 0.016998919700547996, 999 gd steps\n",
      "81-th iteration, new layer inserted. now 54 layers\n",
      "81-th iteration recorded\n",
      "82-th iteration, loss: 0.016818719253202555, 999 gd steps\n",
      "82-th iteration, new layer inserted. now 54 layers\n",
      "82-th iteration recorded\n",
      "83-th iteration, loss: 0.016732121664139232, 999 gd steps\n",
      "83-th iteration, new layer inserted. now 54 layers\n",
      "83-th iteration recorded\n",
      "84-th iteration, loss: 0.016666751399739783, 999 gd steps\n",
      "84-th iteration, new layer inserted. now 52 layers\n",
      "84-th iteration recorded\n",
      "85-th iteration, loss: 0.016611169627713283, 999 gd steps\n",
      "85-th iteration, new layer inserted. now 52 layers\n",
      "85-th iteration recorded\n",
      "86-th iteration, loss: 0.016554952741841315, 999 gd steps\n",
      "86-th iteration, new layer inserted. now 52 layers\n",
      "86-th iteration recorded\n",
      "87-th iteration, loss: 0.01650374329203839, 999 gd steps\n",
      "87-th iteration, new layer inserted. now 54 layers\n",
      "87-th iteration recorded\n",
      "88-th iteration, loss: 0.016307797124679675, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "88-th iteration, new layer inserted. now 52 layers\n",
      "88-th iteration recorded\n",
      "89-th iteration, loss: 0.01626181799818565, 999 gd steps\n",
      "89-th iteration, new layer inserted. now 54 layers\n",
      "89-th iteration recorded\n",
      "90-th iteration, loss: 0.016170967829062598, 999 gd steps\n",
      "90-th iteration, new layer inserted. now 54 layers\n",
      "90-th iteration recorded\n",
      "91-th iteration, loss: 0.015966278769704086, 999 gd steps\n",
      "91-th iteration, new layer inserted. now 54 layers\n",
      "91-th iteration recorded\n",
      "92-th iteration, loss: 0.01586832209522525, 999 gd steps\n",
      "92-th iteration, new layer inserted. now 54 layers\n",
      "92-th iteration recorded\n",
      "93-th iteration, loss: 0.015794749926202892, 999 gd steps\n",
      "93-th iteration, new layer inserted. now 54 layers\n",
      "93-th iteration recorded\n",
      "94-th iteration, loss: 0.01573265314546957, 999 gd steps\n",
      "94-th iteration, new layer inserted. now 56 layers\n",
      "94-th iteration recorded\n",
      "95-th iteration, loss: 0.015672040409042926, 999 gd steps\n",
      "95-th iteration, new layer inserted. now 56 layers\n",
      "95-th iteration recorded\n",
      "96-th iteration, loss: 0.015619719863860196, 999 gd steps\n",
      "96-th iteration, new layer inserted. now 54 layers\n",
      "96-th iteration recorded\n",
      "97-th iteration, loss: 0.015570667831887574, 999 gd steps\n",
      "97-th iteration, new layer inserted. now 56 layers\n",
      "97-th iteration recorded\n",
      "98-th iteration, loss: 0.015526718887700738, 999 gd steps\n",
      "98-th iteration, new layer inserted. now 56 layers\n",
      "98-th iteration recorded\n",
      "99-th iteration, loss: 0.015479512611006904, 999 gd steps\n",
      "99-th iteration, new layer inserted. now 58 layers\n",
      "99-th iteration recorded\n",
      "100-th iteration, loss: 0.015420540923951027, 999 gd steps\n",
      "100-th iteration, new layer inserted. now 56 layers\n",
      "100-th iteration recorded\n",
      "101-th iteration, loss: 0.015375646898680498, 999 gd steps\n",
      "101-th iteration, new layer inserted. now 58 layers\n",
      "101-th iteration recorded\n",
      "102-th iteration, loss: 0.015338567536564918, 999 gd steps\n",
      "102-th iteration, new layer inserted. now 58 layers\n",
      "102-th iteration recorded\n",
      "103-th iteration, loss: 0.015295603620210723, 999 gd steps\n",
      "103-th iteration, new layer inserted. now 58 layers\n",
      "103-th iteration recorded\n",
      "104-th iteration, loss: 0.015094109059038786, 999 gd steps\n",
      "104-th iteration, new layer inserted. now 54 layers\n",
      "104-th iteration recorded\n",
      "105-th iteration, loss: 0.015031420926540893, 999 gd steps\n",
      "105-th iteration, new layer inserted. now 54 layers\n",
      "105-th iteration recorded\n",
      "106-th iteration, loss: 0.01497383154171953, 999 gd steps\n",
      "106-th iteration, new layer inserted. now 54 layers\n",
      "106-th iteration recorded\n",
      "107-th iteration, loss: 0.014917771252978424, 999 gd steps\n",
      "107-th iteration, new layer inserted. now 56 layers\n",
      "107-th iteration recorded\n",
      "108-th iteration, loss: 0.014869888767427416, 999 gd steps\n",
      "108-th iteration, new layer inserted. now 54 layers\n",
      "108-th iteration recorded\n",
      "109-th iteration, loss: 0.014818089786354967, 999 gd steps\n",
      "109-th iteration, new layer inserted. now 54 layers\n",
      "109-th iteration recorded\n",
      "110-th iteration, loss: 0.014773029954251974, 999 gd steps\n",
      "110-th iteration, new layer inserted. now 54 layers\n",
      "110-th iteration recorded\n",
      "111-th iteration, loss: 0.014727169163548088, 999 gd steps\n",
      "111-th iteration, new layer inserted. now 54 layers\n",
      "111-th iteration recorded\n",
      "112-th iteration, loss: 0.014682955044712419, 999 gd steps\n",
      "112-th iteration, new layer inserted. now 54 layers\n",
      "112-th iteration recorded\n",
      "113-th iteration, loss: 0.014639455061536508, 999 gd steps\n",
      "113-th iteration, new layer inserted. now 54 layers\n",
      "113-th iteration recorded\n",
      "114-th iteration, loss: 0.014596677654765786, 999 gd steps\n",
      "114-th iteration, new layer inserted. now 54 layers\n",
      "114-th iteration recorded\n",
      "115-th iteration, loss: 0.014553859270616922, 999 gd steps\n",
      "115-th iteration, new layer inserted. now 56 layers\n",
      "115-th iteration recorded\n",
      "116-th iteration, loss: 0.01450534315581652, 999 gd steps\n",
      "116-th iteration, new layer inserted. now 58 layers\n",
      "116-th iteration recorded\n",
      "117-th iteration, loss: 0.014475771176691016, 999 gd steps\n",
      "117-th iteration, new layer inserted. now 56 layers\n",
      "117-th iteration recorded\n",
      "118-th iteration, loss: 0.01444678670404403, 999 gd steps\n",
      "118-th iteration, new layer inserted. now 58 layers\n",
      "118-th iteration recorded\n",
      "119-th iteration, loss: 0.014230207102751415, 999 gd steps\n",
      "119-th iteration, new layer inserted. now 52 layers\n",
      "119-th iteration recorded\n",
      "120-th iteration, loss: 0.014181305352218444, 999 gd steps\n",
      "120-th iteration, new layer inserted. now 52 layers\n",
      "120-th iteration recorded\n",
      "121-th iteration, loss: 0.014125949335838595, 999 gd steps\n",
      "121-th iteration, new layer inserted. now 54 layers\n",
      "121-th iteration recorded\n",
      "122-th iteration, loss: 0.01409907755302638, 999 gd steps\n",
      "122-th iteration, new layer inserted. now 54 layers\n",
      "122-th iteration recorded\n",
      "123-th iteration, loss: 0.014071690509153113, 999 gd steps\n",
      "123-th iteration, new layer inserted. now 54 layers\n",
      "123-th iteration recorded\n",
      "124-th iteration, loss: 0.014041233458186376, 999 gd steps\n",
      "124-th iteration, new layer inserted. now 54 layers\n",
      "124-th iteration recorded\n",
      "125-th iteration, loss: 0.01400616488792277, 999 gd steps\n",
      "125-th iteration, new layer inserted. now 56 layers\n",
      "125-th iteration recorded\n",
      "126-th iteration, loss: 0.013949248601028206, 999 gd steps\n",
      "126-th iteration, new layer inserted. now 56 layers\n",
      "126-th iteration recorded\n",
      "127-th iteration, loss: 0.013803505993916136, 999 gd steps\n",
      "127-th iteration, new layer inserted. now 58 layers\n",
      "127-th iteration recorded\n",
      "128-th iteration, loss: 0.013596731775120205, 999 gd steps\n",
      "128-th iteration, new layer inserted. now 56 layers\n",
      "128-th iteration recorded\n",
      "129-th iteration, loss: 0.01241765642475687, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "129-th iteration, new layer inserted. now 54 layers\n",
      "129-th iteration recorded\n",
      "130-th iteration, loss: 0.012293158108256229, 999 gd steps\n",
      "130-th iteration, new layer inserted. now 56 layers\n",
      "130-th iteration recorded\n",
      "131-th iteration, loss: 0.01216590247302747, 999 gd steps\n",
      "131-th iteration, new layer inserted. now 54 layers\n",
      "131-th iteration recorded\n",
      "132-th iteration, loss: 0.012088176073451156, 999 gd steps\n",
      "132-th iteration, new layer inserted. now 54 layers\n",
      "132-th iteration recorded\n",
      "133-th iteration, loss: 0.012021048159173726, 999 gd steps\n",
      "133-th iteration, new layer inserted. now 56 layers\n",
      "133-th iteration recorded\n",
      "134-th iteration, loss: 0.011969526456292519, 999 gd steps\n",
      "134-th iteration, new layer inserted. now 58 layers\n",
      "134-th iteration recorded\n",
      "135-th iteration, loss: 0.011944474377722824, 999 gd steps\n",
      "135-th iteration, new layer inserted. now 60 layers\n",
      "135-th iteration recorded\n",
      "136-th iteration, loss: 0.011915065307860337, 999 gd steps\n",
      "136-th iteration, new layer inserted. now 60 layers\n",
      "136-th iteration recorded\n",
      "137-th iteration, loss: 0.011883363681807167, 999 gd steps\n",
      "137-th iteration, new layer inserted. now 60 layers\n",
      "137-th iteration recorded\n",
      "138-th iteration, loss: 0.010959527798959466, 999 gd steps\n",
      "138-th iteration, new layer inserted. now 56 layers\n",
      "138-th iteration recorded\n",
      "139-th iteration, loss: 0.010915198256294273, 999 gd steps\n",
      "139-th iteration, new layer inserted. now 56 layers\n",
      "139-th iteration recorded\n",
      "140-th iteration, loss: 0.010879520933411348, 999 gd steps\n",
      "140-th iteration, new layer inserted. now 56 layers\n",
      "140-th iteration recorded\n",
      "141-th iteration, loss: 0.01085481497087712, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "141-th iteration, new layer inserted. now 56 layers\n",
      "141-th iteration recorded\n",
      "142-th iteration, loss: 0.010771243568941257, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "142-th iteration, new layer inserted. now 56 layers\n",
      "142-th iteration recorded\n",
      "143-th iteration, loss: 0.010721408794552105, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "143-th iteration, new layer inserted. now 56 layers\n",
      "143-th iteration recorded\n",
      "144-th iteration, loss: 0.010701327301282792, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "144-th iteration, new layer inserted. now 56 layers\n",
      "144-th iteration recorded\n",
      "145-th iteration, loss: 0.010648072621104308, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.781210641583959e-06\n",
      "145-th iteration, new layer inserted. now 52 layers\n",
      "145-th iteration recorded\n",
      "146-th iteration, loss: 0.010603511005317956, 999 gd steps\n",
      "146-th iteration, new layer inserted. now 54 layers\n",
      "146-th iteration recorded\n",
      "147-th iteration, loss: 0.010587048851515901, 999 gd steps\n",
      "147-th iteration, new layer inserted. now 54 layers\n",
      "147-th iteration recorded\n",
      "148-th iteration, loss: 0.010571924370618723, 999 gd steps\n",
      "148-th iteration, new layer inserted. now 52 layers\n",
      "148-th iteration recorded\n",
      "149-th iteration, loss: 0.010552527829976, 999 gd steps\n",
      "149-th iteration, new layer inserted. now 54 layers\n",
      "149-th iteration recorded\n",
      "150-th iteration, loss: 0.01054327923879385, 999 gd steps\n",
      "150-th iteration, new layer inserted. now 52 layers\n",
      "150-th iteration recorded\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "0-th iteration recorded\n",
      "1-th iteration, loss: 0.6598180968910514, 37 gd steps\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "1-th iteration recorded\n",
      "2-th iteration, loss: 0.5253695015681221, 96 gd steps\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "2-th iteration recorded\n",
      "3-th iteration, loss: 0.4161938006130939, 76 gd steps\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "3-th iteration recorded\n",
      "4-th iteration, loss: 0.3399751099045046, 132 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "4-th iteration recorded\n",
      "5-th iteration, loss: 0.2739438005778496, 150 gd steps\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "5-th iteration recorded\n",
      "6-th iteration, loss: 0.2671104674319155, 305 gd steps\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "6-th iteration recorded\n",
      "7-th iteration, loss: 0.21679962817696408, 999 gd steps\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "7-th iteration recorded\n",
      "8-th iteration, loss: 0.18860304224117558, 999 gd steps\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "8-th iteration recorded\n",
      "9-th iteration, loss: 0.156568079174441, 669 gd steps\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "9-th iteration recorded\n",
      "10-th iteration, loss: 0.13533057642916738, 999 gd steps\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "10-th iteration recorded\n",
      "11-th iteration, loss: 0.10355373832487152, 862 gd steps\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "11-th iteration recorded\n",
      "12-th iteration, loss: 0.08816705266236374, 999 gd steps\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "12-th iteration recorded\n",
      "13-th iteration, loss: 0.06586670588059547, 801 gd steps\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "13-th iteration recorded\n",
      "14-th iteration, loss: 0.06388006220560283, 177 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "14-th iteration, new layer inserted. now 29 layers\n",
      "14-th iteration recorded\n",
      "15-th iteration, loss: 0.06311719640690504, 518 gd steps\n",
      "15-th iteration, new layer inserted. now 31 layers\n",
      "15-th iteration recorded\n",
      "16-th iteration, loss: 0.06158191543377419, 488 gd steps\n",
      "16-th iteration, new layer inserted. now 33 layers\n",
      "16-th iteration recorded\n",
      "17-th iteration, loss: 0.06091134091396251, 773 gd steps\n",
      "17-th iteration, new layer inserted. now 35 layers\n",
      "17-th iteration recorded\n",
      "18-th iteration, loss: 0.059065670239943316, 986 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "18-th iteration, new layer inserted. now 35 layers\n",
      "18-th iteration recorded\n",
      "19-th iteration, loss: 0.05847619400553339, 75 gd steps\n",
      "19-th iteration, new layer inserted. now 35 layers\n",
      "19-th iteration recorded\n",
      "20-th iteration, loss: 0.04876620837542461, 999 gd steps\n",
      "20-th iteration, new layer inserted. now 36 layers\n",
      "20-th iteration recorded\n",
      "21-th iteration, loss: 0.04792187742862072, 999 gd steps\n",
      "21-th iteration, new layer inserted. now 38 layers\n",
      "21-th iteration recorded\n",
      "22-th iteration, loss: 0.04735382862980039, 999 gd steps\n",
      "22-th iteration, new layer inserted. now 40 layers\n",
      "22-th iteration recorded\n",
      "23-th iteration, loss: 0.04678462772941457, 999 gd steps\n",
      "23-th iteration, new layer inserted. now 42 layers\n",
      "23-th iteration recorded\n",
      "24-th iteration, loss: 0.04596178054323792, 983 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 44 layers\n",
      "24-th iteration recorded\n",
      "25-th iteration, loss: 0.04577956632632858, 544 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "25-th iteration, new layer inserted. now 46 layers\n",
      "25-th iteration recorded\n",
      "26-th iteration, loss: 0.045602957622557304, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "26-th iteration, new layer inserted. now 48 layers\n",
      "26-th iteration recorded\n",
      "27-th iteration, loss: 0.04527541686149356, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "27-th iteration, new layer inserted. now 50 layers\n",
      "27-th iteration recorded\n",
      "28-th iteration, loss: 0.044720271876710196, 999 gd steps\n",
      "28-th iteration, new layer inserted. now 52 layers\n",
      "28-th iteration recorded\n",
      "29-th iteration, loss: 0.038154911041996004, 109 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "29-th iteration, new layer inserted. now 50 layers\n",
      "29-th iteration recorded\n",
      "30-th iteration, loss: 0.03630583057784648, 999 gd steps\n",
      "30-th iteration, new layer inserted. now 52 layers\n",
      "30-th iteration recorded\n",
      "31-th iteration, loss: 0.03597974174143364, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "31-th iteration, new layer inserted. now 50 layers\n",
      "31-th iteration recorded\n",
      "32-th iteration, loss: 0.03569107054237242, 999 gd steps\n",
      "32-th iteration, new layer inserted. now 52 layers\n",
      "32-th iteration recorded\n",
      "33-th iteration, loss: 0.03553672918365417, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "33-th iteration, new layer inserted. now 52 layers\n",
      "33-th iteration recorded\n",
      "34-th iteration, loss: 0.03531395703357316, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 54 layers\n",
      "34-th iteration recorded\n",
      "35-th iteration, loss: 0.035171667474590794, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "35-th iteration, new layer inserted. now 54 layers\n",
      "35-th iteration recorded\n",
      "36-th iteration, loss: 0.03499701345456135, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "36-th iteration, new layer inserted. now 56 layers\n",
      "36-th iteration recorded\n",
      "37-th iteration, loss: 0.034808848838864356, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "37-th iteration, new layer inserted. now 58 layers\n",
      "37-th iteration recorded\n",
      "38-th iteration, loss: 0.03439543761399055, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 60 layers\n",
      "38-th iteration recorded\n",
      "39-th iteration, loss: 0.034154923958435605, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "39-th iteration, new layer inserted. now 60 layers\n",
      "39-th iteration recorded\n",
      "40-th iteration, loss: 0.03401792225138822, 999 gd steps\n",
      "40-th iteration, new layer inserted. now 60 layers\n",
      "40-th iteration recorded\n",
      "41-th iteration, loss: 0.03371701018020275, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 62 layers\n",
      "41-th iteration recorded\n",
      "42-th iteration, loss: 0.03307268057968547, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 62 layers\n",
      "42-th iteration recorded\n",
      "43-th iteration, loss: 0.03292932225270466, 999 gd steps\n",
      "43-th iteration, new layer inserted. now 62 layers\n",
      "43-th iteration recorded\n",
      "44-th iteration, loss: 0.03283919618513353, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "44-th iteration, new layer inserted. now 64 layers\n",
      "44-th iteration recorded\n",
      "45-th iteration, loss: 0.03273070116642521, 999 gd steps\n",
      "45-th iteration, new layer inserted. now 64 layers\n",
      "45-th iteration recorded\n",
      "46-th iteration, loss: 0.032650587451319615, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "46-th iteration, new layer inserted. now 66 layers\n",
      "46-th iteration recorded\n",
      "47-th iteration, loss: 0.03251264239681533, 999 gd steps\n",
      "47-th iteration, new layer inserted. now 62 layers\n",
      "47-th iteration recorded\n",
      "48-th iteration, loss: 0.032432344875119146, 999 gd steps\n",
      "48-th iteration, new layer inserted. now 60 layers\n",
      "48-th iteration recorded\n",
      "49-th iteration, loss: 0.032347765085586726, 999 gd steps\n",
      "49-th iteration, new layer inserted. now 60 layers\n",
      "49-th iteration recorded\n",
      "50-th iteration, loss: 0.030601909812583008, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "50-th iteration, new layer inserted. now 56 layers\n",
      "50-th iteration recorded\n",
      "51-th iteration, loss: 0.019536930582991316, 761 gd steps\n",
      "51-th iteration, new layer inserted. now 50 layers\n",
      "51-th iteration recorded\n",
      "52-th iteration, loss: 0.01729774639263719, 757 gd steps\n",
      "52-th iteration, new layer inserted. now 43 layers\n",
      "52-th iteration recorded\n",
      "53-th iteration, loss: 0.016185584186146372, 955 gd steps\n",
      "53-th iteration, new layer inserted. now 45 layers\n",
      "53-th iteration recorded\n",
      "54-th iteration, loss: 0.015959093164029714, 999 gd steps\n",
      "54-th iteration, new layer inserted. now 47 layers\n",
      "54-th iteration recorded\n",
      "55-th iteration, loss: 0.015773846555467232, 999 gd steps\n",
      "55-th iteration, new layer inserted. now 49 layers\n",
      "55-th iteration recorded\n",
      "56-th iteration, loss: 0.0157260249738209, 999 gd steps\n",
      "56-th iteration, new layer inserted. now 49 layers\n",
      "56-th iteration recorded\n",
      "57-th iteration, loss: 0.015612379263109297, 999 gd steps\n",
      "57-th iteration, new layer inserted. now 49 layers\n",
      "57-th iteration recorded\n",
      "58-th iteration, loss: 0.015454320971285193, 999 gd steps\n",
      "58-th iteration, new layer inserted. now 47 layers\n",
      "58-th iteration recorded\n",
      "59-th iteration, loss: 0.01525688732728466, 999 gd steps\n",
      "59-th iteration, new layer inserted. now 47 layers\n",
      "59-th iteration recorded\n",
      "60-th iteration, loss: 0.015167140975235116, 999 gd steps\n",
      "60-th iteration, new layer inserted. now 49 layers\n",
      "60-th iteration recorded\n",
      "61-th iteration, loss: 0.015021607784669447, 999 gd steps\n",
      "61-th iteration, new layer inserted. now 49 layers\n",
      "61-th iteration recorded\n",
      "62-th iteration, loss: 0.014893313340646851, 999 gd steps\n",
      "62-th iteration, new layer inserted. now 47 layers\n",
      "62-th iteration recorded\n",
      "63-th iteration, loss: 0.014845362586505356, 999 gd steps\n",
      "63-th iteration, new layer inserted. now 47 layers\n",
      "63-th iteration recorded\n",
      "64-th iteration, loss: 0.014810568260777549, 999 gd steps\n",
      "64-th iteration, new layer inserted. now 49 layers\n",
      "64-th iteration recorded\n",
      "65-th iteration, loss: 0.014718561654017784, 999 gd steps\n",
      "65-th iteration, new layer inserted. now 49 layers\n",
      "65-th iteration recorded\n",
      "66-th iteration, loss: 0.014700812499407679, 999 gd steps\n",
      "66-th iteration, new layer inserted. now 47 layers\n",
      "66-th iteration recorded\n",
      "67-th iteration, loss: 0.01468746960160952, 999 gd steps\n",
      "67-th iteration, new layer inserted. now 47 layers\n",
      "67-th iteration recorded\n",
      "68-th iteration, loss: 0.014672934993285626, 999 gd steps\n",
      "68-th iteration, new layer inserted. now 49 layers\n",
      "68-th iteration recorded\n",
      "69-th iteration, loss: 0.014661592866220286, 999 gd steps\n",
      "69-th iteration, new layer inserted. now 47 layers\n",
      "69-th iteration recorded\n",
      "70-th iteration, loss: 0.014650538990261823, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.940687770592797e-06\n",
      "70-th iteration, new layer inserted. now 47 layers\n",
      "70-th iteration recorded\n",
      "71-th iteration, loss: 0.014643781973929569, 999 gd steps\n",
      "71-th iteration, new layer inserted. now 47 layers\n",
      "71-th iteration recorded\n",
      "72-th iteration, loss: 0.014637987818026471, 999 gd steps\n",
      "72-th iteration, new layer inserted. now 49 layers\n",
      "72-th iteration recorded\n",
      "73-th iteration, loss: 0.014631335052728782, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.241770229286973e-06\n",
      "73-th iteration, new layer inserted. now 47 layers\n",
      "73-th iteration recorded\n",
      "74-th iteration, loss: 0.014626684809898757, 999 gd steps\n",
      "74-th iteration, new layer inserted. now 47 layers\n",
      "74-th iteration recorded\n",
      "75-th iteration, loss: 0.014622483063887417, 999 gd steps\n",
      "75-th iteration, new layer inserted. now 47 layers\n",
      "75-th iteration recorded\n",
      "76-th iteration, loss: 0.014618627292135505, 999 gd steps\n",
      "76-th iteration, new layer inserted. now 47 layers\n",
      "76-th iteration recorded\n",
      "77-th iteration, loss: 0.014615075682365489, 999 gd steps\n",
      "77-th iteration, new layer inserted. now 49 layers\n",
      "77-th iteration recorded\n",
      "78-th iteration, loss: 0.0146110027111186, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.918039464331584e-06\n",
      "78-th iteration, new layer inserted. now 47 layers\n",
      "78-th iteration recorded\n",
      "79-th iteration, loss: 0.014608014191221817, 999 gd steps\n",
      "79-th iteration, new layer inserted. now 47 layers\n",
      "79-th iteration recorded\n",
      "80-th iteration, loss: 0.014605153026354806, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.381791246910019e-06\n",
      "80-th iteration, new layer inserted. now 47 layers\n",
      "80-th iteration recorded\n",
      "81-th iteration, loss: 0.014602281416287869, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.033988992004793e-06\n",
      "81-th iteration, new layer inserted. now 49 layers\n",
      "81-th iteration recorded\n",
      "82-th iteration, loss: 0.014599449077261711, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.1593467678576255e-06\n",
      "82-th iteration, new layer inserted. now 47 layers\n",
      "82-th iteration recorded\n",
      "83-th iteration, loss: 0.014596925381732804, 999 gd steps\n",
      "83-th iteration, new layer inserted. now 47 layers\n",
      "83-th iteration recorded\n",
      "84-th iteration, loss: 0.014593167753759401, 999 gd steps\n",
      "84-th iteration, new layer inserted. now 47 layers\n",
      "84-th iteration recorded\n",
      "85-th iteration, loss: 0.01459151079081594, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.643058373742486e-06\n",
      "85-th iteration, new layer inserted. now 49 layers\n",
      "85-th iteration recorded\n",
      "86-th iteration, loss: 0.014583701252746783, 999 gd steps\n",
      "86-th iteration, new layer inserted. now 47 layers\n",
      "86-th iteration recorded\n",
      "87-th iteration, loss: 0.014582473170352243, 999 gd steps\n",
      "87-th iteration, new layer inserted. now 49 layers\n",
      "87-th iteration recorded\n",
      "88-th iteration, loss: 0.014580217116314487, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.531484343555747e-06\n",
      "88-th iteration, new layer inserted. now 49 layers\n",
      "88-th iteration recorded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m film \u001b[39m=\u001b[39m FilmSimple(\u001b[39m'\u001b[39m\u001b[39mSiO2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTiO2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSiO2\u001b[39m\u001b[39m'\u001b[39m, d_init)\n\u001b[0;32m      7\u001b[0m design \u001b[39m=\u001b[39m make_reflection_design(film)\n\u001b[1;32m----> 9\u001b[0m design\u001b[39m.\u001b[39;49mTFNN_train(\u001b[39mmax\u001b[39;49m(\u001b[39m50\u001b[39;49m, run_num), record\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     11\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./raw_result/single_inc/reflection-0_inc-400to1000wls-init_single_layer_SiO2/\u001b[39m\u001b[39m{\u001b[39;00mrun_num\u001b[39m}\u001b[39;00m\u001b[39m_design.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\design.py:63\u001b[0m, in \u001b[0;36mDesign.TFNN_train\u001b[1;34m(self, needle_epoch, record, error, max_step)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# preparing\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[39m# hyperparameter: exit condition of gd\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# error = 1e-5\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# max_step = 1000\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(needle_epoch):\n\u001b[0;32m     62\u001b[0m     \u001b[39m# LM gradient descent\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     step_count \u001b[39m=\u001b[39m gd\u001b[39m.\u001b[39;49mLM_optimize_d_simple(\n\u001b[0;32m     64\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilm,\n\u001b[0;32m     65\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_specs,\n\u001b[0;32m     66\u001b[0m         error,\n\u001b[0;32m     67\u001b[0m         max_step\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-th iteration, loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mstep_count\u001b[39m}\u001b[39;00m\u001b[39m gd steps\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[39m# Needle insertion\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py:80\u001b[0m, in \u001b[0;36mLM_optimize_d_simple\u001b[1;34m(film, target_spec_ls, h_tol, max_step, show, record)\u001b[0m\n\u001b[0;32m     76\u001b[0m f_new \u001b[39m=\u001b[39m tmp\n\u001b[0;32m     77\u001b[0m \u001b[39m# After the \"swap\", f_new now points to the location of where f was. \u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m# It does not matter that f_new now has dirty data becaus f_new will\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m# be updated before next time F_new is calculated based on it.\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m stack_J(J, n_arrs_ls, d, target_spec_ls)\n\u001b[0;32m     81\u001b[0m stack_f(f, n_arrs_ls, d, target_spec_ls, target_spec)\n\u001b[0;32m     82\u001b[0m g \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(J\u001b[39m.\u001b[39mT, f)\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py:164\u001b[0m, in \u001b[0;36mstack_J\u001b[1;34m(J_old, n_arrs_ls, d, target_spec_ls, get_J)\u001b[0m\n\u001b[0;32m    162\u001b[0m     this_wls_num \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mWLS\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m# R and T\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# only reflectance\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     get_J(\n\u001b[0;32m    165\u001b[0m         J_old[i: i \u001b[39m+\u001b[39;49m this_wls_num, :],\n\u001b[0;32m    166\u001b[0m         s\u001b[39m.\u001b[39;49mWLS, \n\u001b[0;32m    167\u001b[0m         d,\n\u001b[0;32m    168\u001b[0m         n_arrs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m         n_arrs[\u001b[39m1\u001b[39;49m], \n\u001b[0;32m    170\u001b[0m         n_arrs[\u001b[39m2\u001b[39;49m], \n\u001b[0;32m    171\u001b[0m         s\u001b[39m.\u001b[39;49mINC_ANG\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m this_wls_num\n\u001b[0;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\gets\\get_jacobi.py:79\u001b[0m, in \u001b[0;36mget_jacobi_simple\u001b[1;34m(jacobi, wls, d, n_layers, n_sub, n_inc, inc_ang)\u001b[0m\n\u001b[0;32m     65\u001b[0m grid_size \u001b[39m=\u001b[39m (wls_size \u001b[39m+\u001b[39m block_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m block_size \u001b[39m# blocks per grid\u001b[39;00m\n\u001b[0;32m     67\u001b[0m forward_and_backward_propagation[grid_size, block_size](\n\u001b[0;32m     68\u001b[0m     jacobi_device,\n\u001b[0;32m     69\u001b[0m     wls_device,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     layer_number\n\u001b[0;32m     78\u001b[0m )\n\u001b[1;32m---> 79\u001b[0m cuda\u001b[39m.\u001b[39;49msynchronize()\n\u001b[0;32m     80\u001b[0m \u001b[39m# copy to pre-allocated space\u001b[39;00m\n\u001b[0;32m     81\u001b[0m jacobi_device\u001b[39m.\u001b[39mcopy_to_host(jacobi)\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\api.py:244\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m()\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynchronize\u001b[39m():\n\u001b[0;32m    243\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSynchronize the current context.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mreturn\u001b[39;00m current_context()\u001b[39m.\u001b[39;49msynchronize()\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:1499\u001b[0m, in \u001b[0;36mContext.synchronize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynchronize\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1499\u001b[0m     driver\u001b[39m.\u001b[39;49mcuCtxSynchronize()\n",
      "File \u001b[1;32mc:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:319\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_cuda_api_call\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m    318\u001b[0m     _logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mcall driver api: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, libfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m--> 319\u001b[0m     retcode \u001b[39m=\u001b[39m libfn(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_ctypes_error(fname, retcode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# deisgn starting with SiO2\n",
    "for run_num, init_ot in enumerate(np.linspace(1, 5000, 200)):\n",
    "    if run_num < 149:\n",
    "        continue\n",
    "    d_init = np.array([init_ot], dtype='float')\n",
    "    film = FilmSimple('SiO2', 'TiO2', 'SiO2', d_init)\n",
    "    design = make_reflection_design(film)\n",
    "\n",
    "    design.TFNN_train(max(50, run_num), record=True)\n",
    "\n",
    "    fname = f'./raw_result/single_inc/reflection-0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_design.pkl'\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(design, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration, loss: 0.7731634180144117, 18 gd steps\n",
      "insert gradient: -0.4740036432095595\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  76.26983158    0.         3088.92817899]\n",
      "1-th iteration, loss: 0.5748519421396786, 13 gd steps\n",
      "insert gradient: -0.2900328110652073\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  52.39836793  108.43031859  149.30800273    0.         2911.50605326]\n",
      "2-th iteration, loss: 0.4548781692790147, 76 gd steps\n",
      "insert gradient: -0.16443490541544972\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  68.38815136  100.59736686   59.53668294    0.           52.66706568\n",
      "   87.72916985 2903.00764762]\n",
      "3-th iteration, loss: 0.44044755920021866, 21 gd steps\n",
      "insert gradient: -0.24173232505139428\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  71.19599263  110.73834736   50.87254863   27.45874018   42.20006414\n",
      "   93.64968008 2898.08922787]\n",
      "4-th iteration, loss: 0.4201152374052001, 30 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  76.72218789  117.94613605   53.88794946   70.86151781   28.67964018\n",
      "   87.38883128  165.51955802    0.         2731.0727073 ]\n",
      "5-th iteration, loss: 0.3848169862989312, 19 gd steps\n",
      "insert gradient: -0.10249757972620939\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  63.07112168  128.86771285   63.94660059   81.44718459   19.52065912\n",
      "   83.42583023   99.39918742    0.           41.85228944   64.65363003\n",
      " 2718.82375785]\n",
      "6-th iteration, loss: 0.37905967948822106, 11 gd steps\n",
      "insert gradient: -0.05480897305500046\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[6.86327518e+01 1.30131763e+02 5.39861312e+01 7.92600829e+01\n",
      " 2.29251469e+01 8.20604024e+01 9.81295557e+01 1.98023876e+00\n",
      " 7.38023837e+00 0.00000000e+00 3.32110727e+01 6.95928542e+01\n",
      " 2.71855408e+03]\n",
      "7-th iteration, loss: 0.3591462742892094, 56 gd steps\n",
      "insert gradient: -0.0877216386033357\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  67.54916567  138.67302285   59.47010967   88.89286441   24.60673393\n",
      "   66.95567666   79.7008411    75.32179451   14.7798818   104.71952105\n",
      " 2711.93504161]\n",
      "8-th iteration, loss: 0.3506338540490089, 30 gd steps\n",
      "insert gradient: -0.03476211838507492\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  64.96916598  142.16160334   59.14271325   92.26824888   28.08567872\n",
      "   60.7734187    76.40814312  103.84047089   17.15372858   75.23846238\n",
      "  739.37952222    0.         1971.67872592]\n",
      "9-th iteration, loss: 0.3480416284760092, 14 gd steps\n",
      "insert gradient: -0.08854740633350121\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  67.79204955  142.28411448   56.13733638   96.11265379   28.76629917\n",
      "   57.81814625   75.4470999   106.40650988   22.44196152   59.86255254\n",
      "  286.52649667    0.          450.25592334   12.59721914 1967.42648863]\n",
      "10-th iteration, loss: 0.33161284366114163, 15 gd steps\n",
      "insert gradient: -0.17268713976076835\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[  73.76001462  135.16340984   56.65026212   95.91721143   29.08524242\n",
      "   54.58801929   77.56998644   96.71613898   26.60044284   54.51677557\n",
      "  275.81841266   27.67041429  443.46637766   26.5208422  1961.96907677]\n",
      "11-th iteration, loss: 0.24966331035457812, 246 gd steps\n",
      "insert gradient: -0.16400984322948498\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[  69.99733522  125.70875564  100.82117822  171.56611808   64.14307264\n",
      "   97.83444182   49.98937353   69.94222227  122.8872387     0.\n",
      "  122.8872387    86.46309466  452.33000356   68.31569791 1894.7356319 ]\n",
      "12-th iteration, loss: 0.18766409128004669, 109 gd steps\n",
      "insert gradient: -0.06615696097292204\n",
      "12-th iteration, new layer inserted. now 17 layers\n",
      "[  56.5489472    97.33275052  132.97641769  105.1708153    96.33687118\n",
      "   69.40195435   39.55635224   88.19625316   77.3847824   125.3514053\n",
      "   93.5062604   107.45676552  422.93344386  107.37730927  470.16555934\n",
      "    0.         1410.49667802]\n",
      "13-th iteration, loss: 0.14542962152087108, 85 gd steps\n",
      "insert gradient: -0.03473160274591098\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[  56.53278606   99.78438717  131.48937022   98.3433664    97.2027658\n",
      "   76.57821785   40.17915456   82.72061424   79.39307856  122.09417137\n",
      "   92.68301207  107.35633939  152.70405734    0.          274.86730321\n",
      "  104.80076705  421.04015007  132.84000048 1385.16410048]\n",
      "14-th iteration, loss: 0.12433912875853859, 25 gd steps\n",
      "insert gradient: -0.03575200658016186\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  57.43119686  101.62249159   76.22211392    0.           54.44436708\n",
      "   82.87379856  101.50184736   74.77475351   40.76130014   85.75746686\n",
      "   72.88178298  129.57489596   85.79144455  116.05612986  140.36586961\n",
      "   72.51006022  240.08195572  104.31111659  417.62638422  136.51495528\n",
      " 1384.80894692]\n",
      "15-th iteration, loss: 0.104653464959123, 69 gd steps\n",
      "insert gradient: -0.017122132629832365\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  62.86890185  102.31120865   40.41997431   34.7058355    52.34126504\n",
      "   96.6834285    97.4810358    76.75234047   41.29723529   90.13303888\n",
      "   72.51809804  120.40496887   90.30390849  114.4979127   130.03133772\n",
      "  123.66759627  122.30238731    0.          101.91865609  107.22191861\n",
      "  417.70667677  129.22421237 1379.805188  ]\n",
      "16-th iteration, loss: 0.08170381923504721, 115 gd steps\n",
      "insert gradient: -0.010908384800270192\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  59.23529137   99.80532385   44.70091964   35.47716277   46.89190146\n",
      "   93.20453695   97.80290629   82.70828614   44.94602189   91.27808982\n",
      "   72.199894    110.13317875   95.78910626  110.07181779  133.83529296\n",
      "  109.13981842   75.6899374   156.09231494   74.63549005   95.70096219\n",
      "  166.58530636    0.          249.87795954  121.01677158 1397.54604196]\n",
      "17-th iteration, loss: 0.0735429747064691, 87 gd steps\n",
      "insert gradient: -0.010751760487312667\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  55.52686061   96.34317156   44.88940408   33.88025868   50.08564082\n",
      "   96.82644285   94.29847223   88.18335011   46.48532795   92.06397293\n",
      "   69.68204224  109.71768759   96.50308171  109.82840844  107.67412929\n",
      "    0.           30.76403694  108.86831621   65.07031003  149.33010025\n",
      "   86.15796597  108.15049656  151.55338372   55.65641235  230.31672484\n",
      "  107.93967601 1421.43978055]\n",
      "18-th iteration, loss: 0.07040223962554495, 19 gd steps\n",
      "insert gradient: -0.006991610173341638\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[  53.64617426   97.0009796    44.43077838   34.30896173   49.82831024\n",
      "   98.95412039   93.52131918   89.39559607   48.16959755   92.87044163\n",
      "   65.9031801   112.48511473   95.6442813   110.54513242   99.53314121\n",
      "   17.87883027   26.10702181  110.11945828   66.73812097  148.70446619\n",
      "   85.9379291   109.65736554  149.44061163   62.7786062   229.46183031\n",
      "  104.05399885 1425.30860473]\n",
      "19-th iteration, loss: 0.06856422392976977, 20 gd steps\n",
      "insert gradient: -0.004732073206836759\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[  47.68539268  101.30284722   45.92850297   34.53265743   48.02786466\n",
      "  100.09888682   93.43347047   91.47213683   47.02663451   94.55449536\n",
      "   67.5098376   108.16062575   94.1675373   113.24692383   86.22968151\n",
      "   41.23114815   17.38237083  123.90917571   67.13962073  143.85771217\n",
      "   87.72806261  116.06548681  145.91245696   69.79009314  172.38513748\n",
      "    0.           57.46171249   96.00619562 1429.15183752]\n",
      "20-th iteration, loss: 0.06582897789665329, 16 gd steps\n",
      "insert gradient: -0.011802291730769772\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[4.97628767e+01 9.03109029e+01 0.00000000e+00 1.77635684e-14\n",
      " 4.24181224e+01 4.14300360e+01 5.18850782e+01 9.78043968e+01\n",
      " 9.22362829e+01 9.01132267e+01 5.01113396e+01 9.46391806e+01\n",
      " 6.62843395e+01 1.07874026e+02 9.39002774e+01 1.13342949e+02\n",
      " 8.31826346e+01 5.63382194e+01 1.10275618e+01 1.25095593e+02\n",
      " 6.93193797e+01 1.42564676e+02 8.79899482e+01 1.16850085e+02\n",
      " 1.44275107e+02 7.16052667e+01 1.70504314e+02 1.08524501e+01\n",
      " 5.11569116e+01 9.32406968e+01 1.43308730e+03]\n",
      "21-th iteration, loss: 0.061457748662156454, 129 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[5.24593283e+01 9.04737617e+01 2.18043048e-01 2.41269275e-03\n",
      " 3.81620349e+01 5.16294861e+01 4.93601328e+01 9.52079216e+01\n",
      " 9.33122064e+01 9.01389054e+01 5.04029301e+01 9.71866344e+01\n",
      " 6.53755853e+01 1.07907012e+02 9.22433278e+01 1.18777377e+02\n",
      " 7.24484798e+01 2.18927219e+02 6.82082451e+01 1.39487175e+02\n",
      " 9.26437464e+01 1.22042832e+02 1.42159667e+02 7.69121331e+01\n",
      " 1.74820277e+02 2.19697722e+01 3.78955213e+01 9.07150354e+01\n",
      " 1.80059608e+02 0.00000000e+00 1.26041725e+03]\n",
      "22-th iteration, loss: 0.05763051505727382, 587 gd steps\n",
      "insert gradient: -0.0039832862203995865\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[ 51.52419226  88.41060185  35.88890448  57.67154498  49.66960111\n",
      "  97.15447598  90.55287217  92.47684924  52.00150537  98.88203574\n",
      "  64.1499832  106.80528592  92.14926868 121.03608235  71.50237488\n",
      " 219.59806372  69.17062987 134.30115394  93.81750014 126.25067652\n",
      " 133.18468622 106.94868093 166.36923946  38.88768898  27.17966244\n",
      "  89.76594759 166.74040306  33.65634731 474.39454636   0.\n",
      " 790.65757727]\n",
      "23-th iteration, loss: 0.05472256201314915, 24 gd steps\n",
      "insert gradient: -0.002954276190014708\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[ 52.06150727  87.44545062  36.76145638  57.08642313  48.24256018\n",
      "  95.68984595  89.82218346  97.29279335  52.43885283  97.48240728\n",
      "  62.80904722 107.27379207  89.84360765 119.9728021   73.85949499\n",
      " 220.05942     69.63388044 131.05630653  92.60578503 134.6319759\n",
      " 132.70236467 104.75238688 164.51783075  43.40401815  25.44557688\n",
      "  85.87741727 166.55755346  40.62082933 270.0848235    0.\n",
      " 202.56361763  30.88029656 774.62829608]\n",
      "24-th iteration, loss: 0.05222607317986064, 20 gd steps\n",
      "insert gradient: -0.0017770362422013253\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[ 52.30589725  88.63851329  39.57333828  57.72605674  42.57241904\n",
      "  95.95187859  91.53522829  94.61106212  52.85400426  97.65642679\n",
      "  64.58711366 106.66394845  87.80830065 124.45830112  70.64473214\n",
      " 224.01299348  69.58756916 131.25016072  91.37158932 135.08256039\n",
      " 132.1861679  107.56099135 165.12328107  38.6304194   27.22883314\n",
      "  83.49754531 121.17797749   0.          48.471191    42.97570198\n",
      " 275.29750139  19.0153955  183.57623352  38.30155173 769.99121264]\n",
      "25-th iteration, loss: 0.050405111155613885, 67 gd steps\n",
      "insert gradient: -0.0018787955912807606\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[ 51.58569039  88.13210614  38.57897186  53.72963398  49.02788571\n",
      "  94.6091762   88.97044352  95.55115902  52.7210506   97.31912778\n",
      "  65.25847473 105.51434917  87.51476954 124.6998654   70.22334655\n",
      " 224.68688502  66.1768189  134.80176389  92.82040315 134.42329891\n",
      " 130.01496003 113.10397937 169.77681646  32.8545414   29.43683034\n",
      "  77.52021651 110.45570534  15.69148889  47.43840735  42.10179767\n",
      " 271.51082998  34.49622623 179.31068642  35.15245571 768.40168242]\n",
      "26-th iteration, loss: 0.049902322024155676, 84 gd steps\n",
      "insert gradient: -0.0022238377157088878\n",
      "26-th iteration, new layer inserted. now 37 layers\n",
      "[ 52.02741178  88.33272852  37.69156453  55.072055    48.60831852\n",
      "  94.82029839  89.05293386  94.40564839  53.20118458  98.04142568\n",
      "  64.88859819 104.47148596  86.80422573 127.008489    69.75946886\n",
      " 224.1848138   65.72511195 134.27889106  93.92526406 134.17155309\n",
      " 128.45127043 115.73498411 171.78680263  24.4888746   38.94538688\n",
      "  58.1810315  110.33333993  22.62595178  48.14902368  43.11191889\n",
      " 134.84064094   0.         134.84064094  40.30944223 177.98321741\n",
      "  32.2009947  767.42780141]\n",
      "27-th iteration, loss: 0.045947793403072375, 166 gd steps\n",
      "insert gradient: -0.002508766146301407\n",
      "27-th iteration, new layer inserted. now 39 layers\n",
      "[ 54.03105568  90.37211919  38.83460937  49.71134447  50.46129471\n",
      "  95.86473935  84.40798327 104.23709548  55.21098352  96.15232362\n",
      "  58.13073441 108.28749144  83.04030362 137.59821155  71.67135007\n",
      " 109.24011812   0.         109.24011812  66.44564948 142.65559244\n",
      "  93.54819674 127.78439556 127.019989   135.02947127 164.19221605\n",
      "  23.1729693   42.0841075   54.38414948 111.2322726   41.62545274\n",
      "  33.4689683   56.00955355 123.94506749  16.27275672 130.41794039\n",
      "  50.76644736 183.38283835  30.62119411 751.4756219 ]\n",
      "28-th iteration, loss: 0.04537085133803891, 36 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "28-th iteration, new layer inserted. now 41 layers\n",
      "[ 54.40115727  92.07018075  40.43113033  46.26028837  50.4627212\n",
      "  96.11984143  83.56609364 108.04755084  54.20862095  95.43273095\n",
      "  57.185587   109.46230983  83.76860912 134.02865742  76.07984483\n",
      "  96.18222503   9.76787645  87.6297027   71.9021632  142.01562786\n",
      "  93.64765526 124.5580978  127.32944774 137.194885   162.7784585\n",
      "  20.18575336  45.99699263  51.09185411 110.63048271  43.9488824\n",
      "  32.52959862  57.34233936 124.15859766  15.55592913 130.71258728\n",
      "  52.01544038 182.42033834  30.30192091 301.61734635   0.\n",
      " 452.42601952]\n",
      "29-th iteration, loss: 0.0435919308144893, 39 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "29-th iteration, new layer inserted. now 43 layers\n",
      "[ 53.8723817   93.1648479   41.76186774  43.64366713  50.41589775\n",
      "  97.30241577  83.98713443 107.32708338  53.72121348  94.8513841\n",
      "  57.24361482 110.16250895  83.22964025 135.57771573  75.51857251\n",
      " 102.21017911  11.28812721  78.9717435   71.74004338 142.21558571\n",
      "  93.60724487 125.39994084 127.45720802 138.6102837  162.15086946\n",
      "  18.95972521  48.39375616  47.62330658 110.65247068  47.46145932\n",
      "  32.53905135  56.52816264 122.47182101  15.44541696 133.48414692\n",
      "  51.8335582  181.79463372  26.44168843 284.5716446   19.06620938\n",
      " 182.59439029   0.         273.89158544]\n",
      "30-th iteration, loss: 0.04214848208664983, 81 gd steps\n",
      "insert gradient: -0.0023622819267494185\n",
      "30-th iteration, new layer inserted. now 45 layers\n",
      "[ 53.23484146  94.06026186  42.26178184  42.86480218  50.26757037\n",
      "  97.76847764  83.48298007 108.68817597  53.76762016  94.76076856\n",
      "  56.55592159 111.19964887  83.34219799 134.91900158  75.67004083\n",
      " 102.1580985   14.2998061   68.93444631  76.07555707 137.64413066\n",
      "  93.38648116 124.96062642 128.67960067 138.01651778  95.86501284\n",
      "   0.          63.91000856  20.24679494  46.45437025  49.59654288\n",
      " 110.39468503  53.7499      29.78572578  62.09128902 126.28544876\n",
      "  18.51433571 127.39310544  57.9205873  177.50071599  26.62365999\n",
      " 270.65587277  35.38853775 177.9953418   10.52683039 268.9903113 ]\n",
      "31-th iteration, loss: 0.042010116024846646, 16 gd steps\n",
      "insert gradient: -0.0017335917509112362\n",
      "31-th iteration, new layer inserted. now 47 layers\n",
      "[ 54.56394602  94.23390821  41.97322622  42.56810064  50.19331179\n",
      "  97.81693895  83.44132062 108.76279852  54.02586596  94.6360003\n",
      "  56.18096858 111.06375112  83.37364905 134.8453511   75.79211241\n",
      " 102.2651805   14.34606659  68.87113749  76.08247266 137.43795826\n",
      "  93.34053863 125.10506064 128.66859902 137.70072431  95.68820311\n",
      "   0.80830863  63.7281528   20.13992434  46.29688011  49.51911066\n",
      " 110.60353259  54.03103519  29.93385805  62.19448477  75.8631228\n",
      "   0.          50.5754152   18.839874   127.27881056  57.93094638\n",
      " 177.25017172  26.39387502 270.70857018  35.63223508 177.91771233\n",
      "  10.46557783 269.02975369]\n",
      "32-th iteration, loss: 0.040620484990339885, 23 gd steps\n",
      "insert gradient: -0.02050702488394372\n",
      "32-th iteration, new layer inserted. now 49 layers\n",
      "[ 53.95380019  95.07513844  44.61632303  39.28282145  49.57213446\n",
      "  97.57397974  81.78645961 112.00568281  54.50096355  94.44645229\n",
      "  54.85041696 107.80668538  85.6109112  131.74763618  76.26874839\n",
      " 106.22225048  13.81828743  67.42408724  75.7119065  132.33131146\n",
      "  94.15224401  94.96264213   0.          31.65421404 129.04242428\n",
      " 135.37823982  91.74152359   7.85580422  57.90513745  25.85092952\n",
      "  47.22096323  48.91440246 112.91977534  56.95491538  29.17138757\n",
      "  60.54396474  69.81190304  14.36290901  45.0583323   26.54497072\n",
      " 124.17978004  58.85794145 175.71011218  21.76027322 272.77529731\n",
      "  38.88273971 176.79438473   9.15644591 270.23635862]\n",
      "33-th iteration, loss: 0.040556140799197025, 11 gd steps\n",
      "insert gradient: -0.0005185180190938089\n",
      "33-th iteration, new layer inserted. now 51 layers\n",
      "[5.40290150e+01 9.50965953e+01 4.46124606e+01 3.92625524e+01\n",
      " 4.95737228e+01 9.75753936e+01 8.17555559e+01 1.12009803e+02\n",
      " 5.44999583e+01 9.44308005e+01 5.48106872e+01 1.07781056e+02\n",
      " 8.55957893e+01 1.31728140e+02 7.62585967e+01 1.06228692e+02\n",
      " 1.37999960e+01 6.74160623e+01 7.57173722e+01 1.32304428e+02\n",
      " 9.41789123e+01 9.50003796e+01 8.88484161e-02 3.16918689e+01\n",
      " 9.68216232e+01 0.00000000e+00 3.22738744e+01 1.35367299e+02\n",
      " 9.17944433e+01 7.89355553e+00 5.79216575e+01 2.58602875e+01\n",
      " 4.72134921e+01 4.88934005e+01 1.12888179e+02 5.69381587e+01\n",
      " 2.91445938e+01 6.05332717e+01 6.97883928e+01 1.43743447e+01\n",
      " 4.50317554e+01 2.65479807e+01 1.24165279e+02 5.88550204e+01\n",
      " 1.75701725e+02 2.17519964e+01 2.72775653e+02 3.88835925e+01\n",
      " 1.76795026e+02 9.15346956e+00 2.70234235e+02]\n",
      "34-th iteration, loss: 0.04039113758462779, 386 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 51 layers\n",
      "[ 55.07490021  95.33832351  44.47744057  38.94077554  49.61483247\n",
      "  97.64776318  81.57017093 112.17579117  54.61933369  94.3608156\n",
      "  54.68700803 107.55007783  85.6794868  131.54103179  76.30801269\n",
      " 106.51623053  13.82978987  67.39308497  75.81402061 132.04397515\n",
      "  94.16861324 126.48005079  96.57362683   0.88247254  32.01086889\n",
      "  67.73361287   0.          67.73361287  91.71031618   8.15683653\n",
      "  57.62085704  26.15953472  47.29007014  48.86455254 112.94948723\n",
      "  56.96961241  29.15266493  60.52461136  69.56273536  14.85388725\n",
      "  44.75157309  26.73329306 124.02433354  58.85988666 175.56989461\n",
      "  21.60931863 272.84801598  38.89798323 176.76602396   9.09866567\n",
      " 270.18098137]\n",
      "35-th iteration, loss: 0.037753059322748445, 418 gd steps\n",
      "insert gradient: -0.00039365923220401874\n",
      "35-th iteration, new layer inserted. now 51 layers\n",
      "[ 55.73178927  96.46793075  46.59799339  35.2900115   51.30309491\n",
      "  99.00600127  74.67289482 121.77785614  56.77588526  92.8687184\n",
      "  54.44613371 101.74954837  89.94070252 126.24651451  72.70404985\n",
      " 121.88635161  11.82490477  45.35629083   0.          15.11876361\n",
      "  80.49087913 122.00856751  98.81353816 115.03796161  93.7347243\n",
      "  12.9951091   22.5106017  149.78919533  87.83588285  21.95684436\n",
      "  41.96868582  41.8113032   48.76062461  45.44235604 113.90843191\n",
      "  60.47748201  30.035742    55.85702615  63.50478865  31.7651737\n",
      "  35.31485126  36.23072274 120.81399739  62.38037734 169.29080426\n",
      "  19.75075197 278.45843876  39.22312196 176.88586962   8.00914178\n",
      " 270.58200961]\n",
      "36-th iteration, loss: 0.03575210936031596, 30 gd steps\n",
      "insert gradient: -0.001697932844878486\n",
      "36-th iteration, new layer inserted. now 53 layers\n",
      "[5.56518316e+01 9.79513790e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.80016730e+01 3.60552417e+01 5.03687382e+01 1.01362219e+02\n",
      " 6.42608121e+01 1.32496468e+02 5.99232106e+01 9.23113021e+01\n",
      " 5.49827193e+01 1.01002712e+02 8.70409557e+01 1.34273951e+02\n",
      " 6.58539937e+01 1.31736933e+02 1.66750569e+01 3.41876027e+01\n",
      " 3.39532350e-02 3.24400967e+00 8.95265070e+01 1.20502126e+02\n",
      " 1.01684703e+02 1.07622071e+02 8.82059093e+01 1.87946015e+01\n",
      " 1.88868462e+01 1.55106238e+02 8.42899494e+01 4.46128397e+01\n",
      " 2.29806203e+01 6.86437038e+01 4.93633257e+01 3.63972472e+01\n",
      " 1.15545031e+02 5.96573302e+01 3.55389463e+01 4.57510035e+01\n",
      " 6.09671049e+01 4.50372788e+01 2.83105280e+01 4.37100966e+01\n",
      " 1.17592836e+02 6.93600341e+01 1.63736167e+02 2.05351400e+01\n",
      " 2.81436667e+02 3.72541509e+01 1.75155014e+02 7.49658424e+00\n",
      " 2.72305412e+02]\n",
      "37-th iteration, loss: 0.03366844005713994, 65 gd steps\n",
      "insert gradient: -0.002888335467572421\n",
      "37-th iteration, new layer inserted. now 49 layers\n",
      "[ 55.35880751  95.91443872  46.3538442   41.8121949   49.619285\n",
      "  99.30617814  66.15128523 132.92779053  60.64113008  93.70394146\n",
      "  55.2195814  100.6449701   79.77061121 143.91732399  67.88401263\n",
      " 123.63353845  20.73388735  14.50296698 104.47349596 118.52773038\n",
      " 101.78494583 111.7082053   85.08024104  27.70654704   6.93517316\n",
      " 169.45872553  80.00539525  58.53886829  14.37624106  83.25967619\n",
      "  54.05706317  25.9536478  118.41622858  54.55878797  45.93728875\n",
      "  37.18338789  52.68119781  55.79702481  24.58218589  45.54381064\n",
      " 119.5649574   77.2402069  163.6016448   19.78695768 282.19672123\n",
      "  35.16438077 174.37949508   7.2888636  274.00763445]\n",
      "38-th iteration, loss: 0.03191897936230219, 68 gd steps\n",
      "insert gradient: -0.0007898547605327756\n",
      "38-th iteration, new layer inserted. now 45 layers\n",
      "[ 55.32974258  95.10284471  46.70617575  45.99953903  48.18291983\n",
      "  98.36718832  65.43605423 131.65860443  62.71392896  95.03577692\n",
      "  54.22181875 100.47938094  78.43023222 145.0944782   67.65456943\n",
      " 130.45165193 130.57903467 119.05057768 101.51539762 112.99595747\n",
      "  85.80144342 208.71072264  78.85326239  64.96937809   9.52729211\n",
      "  90.96468182  59.02645035  15.24239537 121.20425415  54.47747157\n",
      "  49.97463622  35.58736196  47.49581914  62.41456016  22.3811872\n",
      "  46.57838823 121.31794606  81.36073633 164.8611181   19.15213666\n",
      " 282.79087879  35.83993684 173.73907011   7.6140133  275.41695931]\n",
      "39-th iteration, loss: 0.03111501145339737, 129 gd steps\n",
      "insert gradient: -0.0003726315199424656\n",
      "39-th iteration, new layer inserted. now 47 layers\n",
      "[ 55.00209154  96.18219484  47.55278578  45.48710705  47.20540474\n",
      "  97.84274996  61.73260897 135.08757624  63.26348865  94.8407977\n",
      "  55.21625771 100.5241354   73.20074248 148.24258357  69.16349783\n",
      " 133.99057667 129.90596016 114.9401312   98.8867319  120.73907725\n",
      "  84.37500379 212.46593878  67.46944245  77.59641391   1.88116667\n",
      " 104.82873902  65.37584546   7.06600899 122.66739237  49.50309534\n",
      "  53.2485716   40.82842078  36.55903754  77.93500789  16.41303322\n",
      "  54.08097093 117.52198175  90.42004856 132.55240622   0.\n",
      "  33.13810155  19.31167495 278.089452    36.86506462 172.11492876\n",
      "   7.04855565 274.26087344]\n",
      "40-th iteration, loss: 0.030967664928624766, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 49 layers\n",
      "[ 56.34730452  94.26201823  47.38969512  46.81779609  46.88940118\n",
      "  97.15844614  62.71017411 134.62765003  62.89392494  95.12851518\n",
      "  55.22971461 100.24325424  72.93364366 148.92629585  68.57829705\n",
      " 134.67163193 130.1018477  113.94121187  99.01763487 120.39447637\n",
      "  84.61936375 212.45798569  66.18466167  78.90641653   1.11412342\n",
      " 106.20633329  66.29735507   6.26783795  61.35468439   0.\n",
      "  61.35468439  48.85809164  52.70260117  41.80993087  35.42128397\n",
      "  78.94411671  16.12123853  54.67903112 118.07009268  90.41055931\n",
      " 130.43885109   8.62639335  31.69019694  23.16118561 273.42729193\n",
      "  35.78646027 172.40538714   7.04319778 274.30687084]\n",
      "41-th iteration, loss: 0.03045236275882057, 506 gd steps\n",
      "insert gradient: -0.0002960082153887389\n",
      "41-th iteration, new layer inserted. now 49 layers\n",
      "[ 55.47491397  95.08760468  47.06845822  46.74588228  46.6950574\n",
      "  96.77628413  61.18675649 133.97922975  64.50091097  95.71774116\n",
      "  55.43153272 100.15853776  71.76543008 149.72327311  68.54266617\n",
      " 136.44681939 129.91749461 111.55505742  99.092159   122.46368302\n",
      "  84.67302837 210.21450407  65.99669114 191.64568643  68.41639488\n",
      "  12.03326098  49.46658951  14.89939935  52.09006985  49.359502\n",
      "  55.0927142   41.67647068  33.41088996  81.47638243  14.71892716\n",
      "  56.1108005   90.69170893   0.          30.23056964  90.8992257\n",
      " 125.99232089  14.73596508  30.72104513  29.8884312  271.14365751\n",
      "  34.37942765 173.69660043   7.72669785 273.40652788]\n",
      "42-th iteration, loss: 0.02978114554603217, 67 gd steps\n",
      "insert gradient: -0.0014115226965467948\n",
      "42-th iteration, new layer inserted. now 49 layers\n",
      "[ 55.41081176  94.38198114  46.46918979  47.93574319  46.45534234\n",
      "  96.19132494  60.18391198 131.52193453  66.80714176  96.39777436\n",
      "  55.79789208  99.97910827  70.96606944 149.88699563  68.51902048\n",
      " 137.24174095 129.84125101 109.73294461  99.43530396 122.7374491\n",
      "  84.90052486 209.06830186  65.80625795 186.50501794  73.57168583\n",
      "  25.70456388  31.02940661  34.8097548   45.09203533  46.85529743\n",
      "  58.9980988   45.31862185  28.45032376  85.76512795  15.42753838\n",
      "  49.78420782  86.06897463   9.0311471   30.07239653  93.7038307\n",
      " 121.84175002  18.63204347  29.36232682  35.51544449 271.57366119\n",
      "  33.4751889  174.11672087   9.30812118 271.53033232]\n",
      "43-th iteration, loss: 0.029740569340985172, 16 gd steps\n",
      "insert gradient: -0.000326688142497816\n",
      "43-th iteration, new layer inserted. now 51 layers\n",
      "[ 55.02944058  94.29523379  46.74115474  48.31128991  46.69794499\n",
      "  96.17384249  60.12227575 131.45162464  66.82393773  96.38608441\n",
      "  55.7317631   99.91855357  70.97371365 149.97047189  68.49201517\n",
      " 137.30930244 129.89991111 109.69833256  99.39387781 122.71143477\n",
      "  84.93979749 156.807737     0.          52.26924567  65.7803947\n",
      " 186.46481324  73.57453781  25.89433372  30.86930947  35.00938711\n",
      "  44.98882442  46.79918863  59.03688738  45.39839824  28.391436\n",
      "  85.8011503   15.42689962  49.6587      85.98528178   9.1058084\n",
      "  30.08907655  93.73443624 121.78115435  18.66085637  29.33554255\n",
      "  35.56399181 271.60365771  33.50694253 174.14656894   9.33465485\n",
      " 271.53489998]\n",
      "44-th iteration, loss: 0.0287163304867728, 36 gd steps\n",
      "insert gradient: -0.003709706071746115\n",
      "44-th iteration, new layer inserted. now 51 layers\n",
      "[ 56.06278174  94.94434263  46.25122109  48.93696149  47.54488593\n",
      "  96.13779939  57.47399526 123.80730982  72.14504078  97.11791848\n",
      "  56.9041493   98.18731647  70.37328617 149.67281053  68.5474376\n",
      " 139.69065994 130.2284706  107.79468217  99.57141355 121.98324235\n",
      "  82.52031346 146.71431667   7.14146361  50.31777956  62.53226033\n",
      " 184.88911203  76.05572517  51.68976994  12.25724865  65.93103094\n",
      "  35.36593191  39.9816474   63.54274162  55.40372837  21.17734816\n",
      "  91.85891527  21.23012802  32.90578428  74.46020892  19.92586115\n",
      "  30.7754947   99.68427624 112.42102127  22.94065833  28.86917723\n",
      "  42.92360786 274.87045363  33.76116578 172.51422236  10.59518858\n",
      " 271.21625793]\n",
      "45-th iteration, loss: 0.028579316335939842, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 53 layers\n",
      "[ 53.92193176  94.21888645  47.03583382  49.9205929   47.13847679\n",
      "  95.80472646  57.86449592 123.31582002  72.27515765  97.32689077\n",
      "  56.85223734  98.48152065  70.68307672 150.06624152  68.00871397\n",
      " 139.70870533 130.08954658 107.66185055  99.72726404 121.88692032\n",
      "  82.4440853  146.33286899   7.2778765   50.26373754  62.55794048\n",
      " 184.90376464  75.91036026  52.03201228  12.19664454  66.29825797\n",
      "  35.51659591  39.85142376  63.5110835   55.60692026  21.21348677\n",
      "  91.9378459   21.36612371  32.48419508  74.41301936  20.31972312\n",
      "  30.56600198  99.70106419 112.25982435  23.09425545  28.86340944\n",
      "  43.16872652 275.10014211  33.87548933  43.13849888   0.\n",
      " 129.41549663  10.69578318 271.24920133]\n",
      "46-th iteration, loss: 0.02713993327556677, 67 gd steps\n",
      "insert gradient: -0.00038375354934826466\n",
      "46-th iteration, new layer inserted. now 53 layers\n",
      "[ 55.6343712   95.42757601  46.34128869  54.61803438  45.06739849\n",
      "  94.92105353  56.12711539 107.9512228   78.71898942  99.63615292\n",
      "  58.69491404 100.23877636  69.20431947 150.1533256   66.36100741\n",
      " 143.35132073 130.11345354 105.66251943 100.41093504 119.93042445\n",
      "  82.41145488 132.49552801  11.49503896  49.00672566  65.99238706\n",
      " 184.09298436  73.12242795  66.43035809   2.82550586  81.38662424\n",
      "  40.0489613   27.35636043  63.40652859  71.0818977   15.33321331\n",
      "  90.3969866   28.92394915  19.83912707  70.8269337   29.08716311\n",
      "  27.86547795 100.00742061 105.23460448  28.63630719  26.88684816\n",
      "  50.28214672 276.83110433  45.11917032  40.84818736  12.39512397\n",
      " 122.04386443  11.44820548 266.90664267]\n",
      "47-th iteration, loss: 0.026602769948949775, 44 gd steps\n",
      "insert gradient: -0.002064963152394626\n",
      "47-th iteration, new layer inserted. now 53 layers\n",
      "[5.63344766e+01 9.32770940e+01 5.02014596e+01 5.67303513e+01\n",
      " 4.18366021e+01 9.10649938e+01 5.70888963e+01 1.06772364e+02\n",
      " 0.00000000e+00 7.10542736e-15 7.89213719e+01 1.02424097e+02\n",
      " 5.72953910e+01 1.00518608e+02 7.01507405e+01 1.48801833e+02\n",
      " 6.64672878e+01 1.41392418e+02 1.31479164e+02 1.04714819e+02\n",
      " 1.00359250e+02 1.19909717e+02 8.19549692e+01 1.28220824e+02\n",
      " 1.42550126e+01 4.74717984e+01 6.51484327e+01 1.82937484e+02\n",
      " 7.50602952e+01 1.56604669e+02 4.30832236e+01 2.16443095e+01\n",
      " 6.37791947e+01 7.77010587e+01 1.34279011e+01 8.86189107e+01\n",
      " 3.22865738e+01 1.56426736e+01 7.01845801e+01 3.25073618e+01\n",
      " 2.68791294e+01 9.98718183e+01 1.03678243e+02 2.99821316e+01\n",
      " 2.62069327e+01 5.33122336e+01 2.76851126e+02 4.95973882e+01\n",
      " 4.05058259e+01 1.83484459e+01 1.18085118e+02 1.08765610e+01\n",
      " 2.64175707e+02]\n",
      "48-th iteration, loss: 0.026601657318150847, 6 gd steps\n",
      "insert gradient: -0.0002438109339104683\n",
      "48-th iteration, new layer inserted. now 55 layers\n",
      "[5.63338935e+01 9.32771964e+01 5.02007278e+01 5.67301326e+01\n",
      " 4.18366044e+01 9.10655626e+01 5.70904302e+01 1.06774312e+02\n",
      " 6.13514787e-03 1.94782840e-03 7.89275071e+01 1.02426200e+02\n",
      " 5.72974185e+01 1.00519341e+02 7.01516284e+01 1.48802130e+02\n",
      " 6.64688161e+01 1.41392863e+02 1.31480056e+02 1.04715134e+02\n",
      " 1.00359609e+02 1.19910074e+02 8.19553240e+01 1.28220749e+02\n",
      " 1.42553947e+01 4.74717644e+01 6.51486081e+01 1.82937341e+02\n",
      " 7.50600729e+01 1.56604187e+02 4.30830570e+01 2.16440268e+01\n",
      " 6.37789917e+01 7.77013330e+01 1.34276847e+01 8.86187366e+01\n",
      " 3.22869948e+01 1.56423706e+01 7.01845056e+01 3.25075438e+01\n",
      " 2.68786908e+01 9.98716920e+01 1.03678143e+02 2.99822757e+01\n",
      " 2.62069565e+01 5.33124133e+01 2.76851203e+02 4.95977301e+01\n",
      " 4.05057240e+01 1.83486611e+01 1.18085046e+02 1.08766367e+01\n",
      " 1.98131737e+02 0.00000000e+00 6.60439124e+01]\n",
      "49-th iteration, loss: 0.026462871353645327, 23 gd steps\n",
      "insert gradient: -0.0028292125352585296\n",
      "49-th iteration, new layer inserted. now 53 layers\n",
      "[ 55.65084901  94.64381787  49.2651982   57.09436646  42.13957539\n",
      "  91.988236    56.71061403 105.86110939  79.29893539 101.91701762\n",
      "  57.64499962 100.87770483  70.39200494 147.49967387  67.21029936\n",
      " 140.53755047 131.6059657  104.77936567 100.1232537  120.29390934\n",
      "  81.87943798 127.32151902  14.54444901  46.95560824  65.73599483\n",
      " 182.75422293  74.92116904 155.58327423  43.78376837  20.90294886\n",
      "  63.71423087  79.56548028  12.95003827  87.79459949  33.70878527\n",
      "  14.20502396  69.82351053  33.58235174  26.0824707   99.56631701\n",
      " 103.27449774  30.59713492  26.05092413  53.98792947 276.79437356\n",
      "  51.06633601  39.69312915  19.09092861 117.24550093  10.85116499\n",
      " 197.35202798   2.95260411  64.92617685]\n",
      "50-th iteration, loss: 0.026454297232045197, 17 gd steps\n",
      "insert gradient: -0.0006547197940192814\n",
      "50-th iteration, new layer inserted. now 55 layers\n",
      "[5.56301152e+01 9.46153960e+01 4.91808695e+01 5.70373526e+01\n",
      " 4.20503796e+01 9.19577067e+01 5.66898829e+01 1.05855858e+02\n",
      " 0.00000000e+00 7.10542736e-15 7.93286508e+01 1.01928713e+02\n",
      " 5.76597484e+01 1.00887655e+02 7.04178881e+01 1.47500527e+02\n",
      " 6.72296342e+01 1.40536305e+02 1.31630388e+02 1.04789455e+02\n",
      " 1.00135375e+02 1.20303174e+02 8.18757007e+01 1.27308993e+02\n",
      " 1.45526608e+01 4.69444646e+01 6.57407625e+01 1.82750817e+02\n",
      " 7.49120001e+01 1.55561174e+02 4.37844270e+01 2.08799689e+01\n",
      " 6.37024098e+01 7.95885962e+01 1.29246081e+01 8.77733682e+01\n",
      " 3.37114067e+01 1.41753245e+01 6.98072640e+01 3.35931454e+01\n",
      " 2.60724422e+01 9.95613137e+01 1.03265720e+02 3.06041214e+01\n",
      " 2.60389130e+01 5.39917557e+01 2.76780338e+02 5.10862611e+01\n",
      " 3.96813990e+01 1.91038813e+01 1.17233071e+02 1.08481465e+01\n",
      " 1.97341159e+02 2.97780777e+00 6.48935787e+01]\n",
      "51-th iteration, loss: 0.026453601946908635, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "51-th iteration, new layer inserted. now 57 layers\n",
      "[5.56333843e+01 9.46172055e+01 4.91832461e+01 5.70403936e+01\n",
      " 4.20532590e+01 9.19594854e+01 5.66930853e+01 1.05855633e+02\n",
      " 2.13413208e-03 4.58730951e-04 7.93307024e+01 1.01929765e+02\n",
      " 5.76615459e+01 1.00887982e+02 7.04178396e+01 1.47498387e+02\n",
      " 6.72290576e+01 1.40533283e+02 1.31629195e+02 1.04788608e+02\n",
      " 1.00132763e+02 1.20302869e+02 8.18723448e+01 1.27305592e+02\n",
      " 1.45512794e+01 4.69413453e+01 6.57396666e+01 1.82748458e+02\n",
      " 7.49082113e+01 1.55556071e+02 4.37830380e+01 2.08751305e+01\n",
      " 6.36995644e+01 7.95924487e+01 1.29187597e+01 8.77690230e+01\n",
      " 3.37110151e+01 1.41695065e+01 6.98037021e+01 3.35947293e+01\n",
      " 2.60701524e+01 9.95602239e+01 1.03264010e+02 3.06051110e+01\n",
      " 2.60365149e+01 5.39923810e+01 2.76777544e+02 5.10899432e+01\n",
      " 3.96802485e+01 1.91071096e+01 1.17231373e+02 1.08478178e+01\n",
      " 1.97339686e+02 2.98295087e+00 3.24440462e+01 0.00000000e+00\n",
      " 3.24440462e+01]\n",
      "52-th iteration, loss: 0.025794902761551584, 18 gd steps\n",
      "insert gradient: -0.0001691573589269359\n",
      "52-th iteration, new layer inserted. now 55 layers\n",
      "[ 55.69047556  94.36294145  48.58818915  58.48185105  42.40128377\n",
      "  91.42330798  55.86111118 104.22936792  79.57914215 102.35310801\n",
      "  58.96185739 101.17785009  70.95208532 146.69937607  67.19853557\n",
      " 139.75494986 131.82847137 104.4536994  100.35143944 120.51003858\n",
      "  81.45518536 125.53288494  15.47707491  45.44236305  66.52955986\n",
      " 182.99314111  74.89844411 153.29260858  45.85317411  18.85815571\n",
      "  63.25951777  84.09782498  11.85606439  85.65520833  35.95331187\n",
      "  11.61802767  68.91946684  35.68459675  25.67162939  99.41949679\n",
      " 102.42128166  31.88342365  25.34586533  55.19486541 276.91908549\n",
      "  52.87093287  37.23706931  22.20214128 115.20738927  11.13752516\n",
      " 192.97428734   9.18505076  26.04490847  13.23539202  26.26362422]\n",
      "53-th iteration, loss: 0.025299549715123666, 55 gd steps\n",
      "insert gradient: -0.00020992704245532422\n",
      "53-th iteration, new layer inserted. now 57 layers\n",
      "[ 55.75930568  94.34448839  48.69764307  60.02721733  41.76389308\n",
      "  90.38200869  56.79263081 102.81916765  79.52320587 104.16337422\n",
      "  58.07779376 101.64126116  71.74257139 146.20899833  67.09437945\n",
      " 139.21821469 132.10305958 103.70929041 101.20465111 119.72423781\n",
      "  81.38715221 122.40023474  16.90943917  43.47273958  66.27071287\n",
      " 184.91346792  75.54286306 149.18954842  50.0454137   16.32342336\n",
      "  60.35266879  90.28056909  11.72969164  80.1834166   40.02385424\n",
      "   8.57968221  66.84004039  39.60422129  25.2889555   98.20869959\n",
      " 102.985279    32.80436852  24.03432676  57.31756963 207.63106644\n",
      "   0.          69.21035548  55.96012637  33.57769994  29.67592454\n",
      " 113.28265906  11.96365265 188.06028652  17.15014303  14.74313209\n",
      "  22.51736346  19.70162445]\n",
      "54-th iteration, loss: 0.025296248729293563, 18 gd steps\n",
      "insert gradient: -0.0014021770270544992\n",
      "54-th iteration, new layer inserted. now 59 layers\n",
      "[5.57620092e+01 9.43483651e+01 4.87090333e+01 6.00400687e+01\n",
      " 4.17764362e+01 9.03726368e+01 5.67351464e+01 1.02808862e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.95271850e+01 1.04161839e+02\n",
      " 5.80908190e+01 1.01637445e+02 7.17362824e+01 1.46203511e+02\n",
      " 6.70821088e+01 1.39204610e+02 1.32091364e+02 1.03704163e+02\n",
      " 1.01205816e+02 1.19721708e+02 8.13812707e+01 1.22391866e+02\n",
      " 1.69105892e+01 4.34702170e+01 6.62671170e+01 1.84921299e+02\n",
      " 7.55459529e+01 1.49181558e+02 5.00602400e+01 1.63252154e+01\n",
      " 6.03476651e+01 9.02971048e+01 1.17435952e+01 8.01660756e+01\n",
      " 4.00345212e+01 8.58075480e+00 6.68361967e+01 3.96126251e+01\n",
      " 2.52947473e+01 9.82070180e+01 1.02991922e+02 3.28031315e+01\n",
      " 2.40295422e+01 5.73233212e+01 2.07629312e+02 6.16192024e-02\n",
      " 6.92086511e+01 5.59650603e+01 3.35633604e+01 2.96933782e+01\n",
      " 1.13282354e+02 1.19693660e+01 1.88052755e+02 1.71626048e+01\n",
      " 1.47232739e+01 2.25310963e+01 1.96950074e+01]\n",
      "55-th iteration, loss: 0.025295660405513398, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "55-th iteration, new layer inserted. now 61 layers\n",
      "[5.57621101e+01 9.43484805e+01 4.87093532e+01 6.00404032e+01\n",
      " 4.17771426e+01 9.03730488e+01 5.67360278e+01 1.02810288e+02\n",
      " 4.15520574e-03 1.42612057e-03 7.95313403e+01 1.04163539e+02\n",
      " 5.80929079e+01 1.01638059e+02 7.17369319e+01 1.46204048e+02\n",
      " 6.70830864e+01 1.39204969e+02 8.80612528e+01 0.00000000e+00\n",
      " 4.40306264e+01 1.03704326e+02 1.01206146e+02 1.19721876e+02\n",
      " 8.13814780e+01 1.22391871e+02 1.69109154e+01 4.34703103e+01\n",
      " 6.62672085e+01 1.84921453e+02 7.55460972e+01 1.49181488e+02\n",
      " 5.00604888e+01 1.63252783e+01 6.03476357e+01 9.02973243e+01\n",
      " 1.17437525e+01 8.01658393e+01 4.00346526e+01 8.58076577e+00\n",
      " 6.68361431e+01 3.96127431e+01 2.52948202e+01 9.82069878e+01\n",
      " 1.02991978e+02 3.28030998e+01 2.40294557e+01 5.73233819e+01\n",
      " 2.07629260e+02 6.24083500e-02 6.92086010e+01 5.59651210e+01\n",
      " 3.35631724e+01 2.96936079e+01 1.13282347e+02 1.19694340e+01\n",
      " 1.88052653e+02 1.71627703e+01 1.47230073e+01 2.25312754e+01\n",
      " 1.96949211e+01]\n",
      "56-th iteration, loss: 0.02462456139667464, 59 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "56-th iteration, new layer inserted. now 61 layers\n",
      "[ 55.77930999  94.87585743  49.41814457  61.46835041  41.36228295\n",
      "  89.93213771  55.14995334 101.82491547  80.55439997 104.08785572\n",
      "  58.07643711 101.49857967  72.05270165 145.70345394  66.84437842\n",
      " 136.35414672  83.49888488  14.56428262  35.72257295 104.71184148\n",
      " 102.92000464 122.50564814  80.63122486 120.26177158  19.05838473\n",
      "  39.60475876  65.73973215 186.91075613  77.67013907 145.7127706\n",
      "  52.81960612  14.9306107   57.73686154  93.23430813  12.02253764\n",
      "  75.28025583  43.16560666   6.10144038  65.99636984  44.27450141\n",
      "  24.86228374  64.31037663   0.          32.15518832 103.39637997\n",
      "  32.57635431  23.29550111  57.66854384 203.8923991    6.86649057\n",
      "  67.13550394  57.8851616   30.7721301   33.35514497 114.05888444\n",
      "  13.4942612  186.03201224  19.85896023  10.1454171   25.70711584\n",
      "  17.60833173]\n",
      "57-th iteration, loss: 0.023688756195410245, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "57-th iteration, new layer inserted. now 63 layers\n",
      "[ 55.88973614  94.33792901  49.090864    67.19691578  41.23494631\n",
      "  87.96073537  54.21090353  99.43668835  81.42805273 104.57060599\n",
      "  58.62996356 103.39332285  74.79730128 140.04088649  68.77507061\n",
      " 126.2418948   82.40624504  35.52068637  20.72149504 109.1698313\n",
      " 105.58191069 128.93594124  78.53246374 114.01342193  24.77067133\n",
      "  28.801424    65.44786901 127.76300188   0.          63.88150094\n",
      "  83.71205853 135.92058725  59.42298207  12.94067749  49.98083673\n",
      "  96.17693093  14.67066432  62.69341587  47.38981126   1.7735305\n",
      "  64.44306142  55.08641778  25.45916711  58.18345363   6.62646686\n",
      "  26.28581364 100.98176544  31.15036354  23.73003958  60.43922792\n",
      " 200.2387318   12.65527526  66.19839938  59.60847947  27.88686461\n",
      "  39.86077341 115.09929904  16.06135612 179.59796327  23.07296465\n",
      "   5.76190389  28.74743952  14.88711259]\n",
      "58-th iteration, loss: 0.023656435389635744, 17 gd steps\n",
      "insert gradient: -0.00022485240118637565\n",
      "58-th iteration, new layer inserted. now 65 layers\n",
      "[5.58271726e+01 9.43077355e+01 4.90555356e+01 6.71797944e+01\n",
      " 4.12088998e+01 8.79499240e+01 5.42053885e+01 9.94213808e+01\n",
      " 8.13596511e+01 1.04548202e+02 5.86208295e+01 1.03393390e+02\n",
      " 7.48333443e+01 1.40060529e+02 6.87959874e+01 1.26268958e+02\n",
      " 8.24545582e+01 3.55851186e+01 2.07758572e+01 1.09191954e+02\n",
      " 1.05629986e+02 1.28951155e+02 7.85733725e+01 1.14029954e+02\n",
      " 2.48214895e+01 2.88185574e+01 6.54767477e+01 1.27778628e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.91637140e-02 6.38971389e+01\n",
      " 8.37299392e+01 1.35917762e+02 5.94324399e+01 1.29486085e+01\n",
      " 4.99902189e+01 9.61906711e+01 1.47117367e+01 6.27038482e+01\n",
      " 4.74321239e+01 1.79598532e+00 6.44808575e+01 5.51125511e+01\n",
      " 2.54573080e+01 5.81731709e+01 6.65378101e+00 2.62802148e+01\n",
      " 1.00983357e+02 3.11484471e+01 2.37400727e+01 6.04446906e+01\n",
      " 2.00234850e+02 1.26528672e+01 6.61907423e+01 5.96032253e+01\n",
      " 2.78751714e+01 3.98655965e+01 1.15095148e+02 1.60707564e+01\n",
      " 1.79581671e+02 2.30762946e+01 5.75800625e+00 2.87479213e+01\n",
      " 1.48935777e+01]\n",
      "59-th iteration, loss: 0.02353162022543898, 19 gd steps\n",
      "insert gradient: -0.0002106348667856535\n",
      "59-th iteration, new layer inserted. now 65 layers\n",
      "[5.71178592e+01 9.45234660e+01 4.89857414e+01 6.75128527e+01\n",
      " 4.19246938e+01 8.81123981e+01 5.39014995e+01 1.00264040e+02\n",
      " 8.11357484e+01 1.05073585e+02 5.83339837e+01 1.02485613e+02\n",
      " 7.66098512e+01 1.39658360e+02 6.80188145e+01 1.26172840e+02\n",
      " 8.10423688e+01 4.03104055e+01 1.83724868e+01 1.08872301e+02\n",
      " 1.07782635e+02 1.28572012e+02 7.82445738e+01 1.13767834e+02\n",
      " 2.52380998e+01 2.65009457e+01 6.56415331e+01 1.26254265e+02\n",
      " 2.29529170e+00 6.24745250e+01 8.47256821e+01 1.34569866e+02\n",
      " 5.96557816e+01 1.23542723e+01 4.87841042e+01 9.69025726e+01\n",
      " 0.00000000e+00 1.42108547e-14 1.59978891e+01 5.94677527e+01\n",
      " 4.82998018e+01 1.72814175e-01 6.48067025e+01 5.85079119e+01\n",
      " 2.51669218e+01 5.56823512e+01 8.85079164e+00 2.41200088e+01\n",
      " 1.01160051e+02 2.93569060e+01 2.42141780e+01 6.18242794e+01\n",
      " 2.00600401e+02 1.27851382e+01 6.52624014e+01 5.92688525e+01\n",
      " 2.77192024e+01 4.20320286e+01 1.14985231e+02 1.74913764e+01\n",
      " 1.77058398e+02 2.34482003e+01 5.22627230e+00 2.89030591e+01\n",
      " 1.47546887e+01]\n",
      "60-th iteration, loss: 0.02318207372790979, 47 gd steps\n",
      "insert gradient: -0.008610158198926762\n",
      "60-th iteration, new layer inserted. now 63 layers\n",
      "[ 54.78381841  93.83629755  49.17549799  68.61925661  42.0525502\n",
      "  88.04523615  54.23644833  99.98475064  80.99820414 105.11449786\n",
      "  58.90701945 103.126687    77.67235319 138.08923706  67.61506089\n",
      " 125.55937463  79.69711675  45.05374885  15.28438649 108.94260218\n",
      " 110.35000888 128.20805183  77.88311094 113.85879593  24.98452921\n",
      "  24.11949371  66.30002828 124.30796398   4.32220143  61.16427523\n",
      "  85.94046408 133.81621107  60.37379726  10.9063503   47.27554447\n",
      "  97.81976342   0.87360142   0.81711618  16.99755139  54.48132788\n",
      " 113.40753642  62.17933326  24.60234801  52.60050545  11.35131658\n",
      "  21.46577395 101.75417383  27.20245619  24.9421226   63.68599402\n",
      " 201.1790676   12.51541402  63.65693072  58.62215178  27.72881255\n",
      "  44.62007377 115.32035601  18.853846   173.26836617  23.61508257\n",
      "   4.78045922  28.80778886  14.11737682]\n",
      "61-th iteration, loss: 0.022623787657348824, 74 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "61-th iteration, new layer inserted. now 65 layers\n",
      "[ 54.52203739  93.26509891  48.51267171  72.16693798  43.01047851\n",
      "  87.57188363  54.10471447 100.11827417  81.44580027 104.83798969\n",
      "  59.53684762 103.51973531  80.99052173 134.38897734  67.11383018\n",
      " 124.26122783  77.40561376  52.22507193  10.26018785 109.57398318\n",
      " 116.15399682 125.90515844  77.44346182 116.32084971  22.11014\n",
      "  21.03765765  68.33925254 117.97376532   9.29491066  57.22959271\n",
      "  88.66447345 133.02076401  61.58891772   6.84341537  45.33749949\n",
      "  99.45524433   2.0772254    0.59246812  20.23683491  44.97607014\n",
      " 112.81363626  70.46527542  23.40654917  45.25544336  14.54511537\n",
      "  14.48470543 105.52351465  22.14643315  27.47523557  66.38009755\n",
      " 135.57134794   0.          67.78567397  13.30217349  58.08363885\n",
      "  59.06095267  28.85155406  49.26955256 117.18413385  19.3062005\n",
      " 164.54002016  24.26821659   3.07145777  28.80655247  14.05447366]\n",
      "62-th iteration, loss: 0.02193915823431691, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "62-th iteration, new layer inserted. now 63 layers\n",
      "[5.62443580e+01 9.40505547e+01 4.89427842e+01 7.31715372e+01\n",
      " 4.36446381e+01 8.67045613e+01 5.31075462e+01 9.90230762e+01\n",
      " 8.25533811e+01 3.50592017e+01 0.00000000e+00 7.01184034e+01\n",
      " 5.99319295e+01 1.02370615e+02 8.42252461e+01 1.29394554e+02\n",
      " 6.80629957e+01 1.22923494e+02 7.65295950e+01 5.34962588e+01\n",
      " 7.65499801e+00 1.10351855e+02 1.20214546e+02 1.24617313e+02\n",
      " 7.74790613e+01 1.18781512e+02 1.74845581e+01 2.08928842e+01\n",
      " 7.12903249e+01 1.11489050e+02 1.42408117e+01 5.00554425e+01\n",
      " 9.07978272e+01 1.34710748e+02 6.32428268e+01 4.21304776e-02\n",
      " 4.66097740e+01 1.01059974e+02 2.70293774e+01 3.41162888e+01\n",
      " 1.13638757e+02 7.31917354e+01 2.07367496e+01 3.73038245e+01\n",
      " 1.56124080e+01 1.08062891e+01 1.04114511e+02 2.02432106e+01\n",
      " 3.72710661e+01 6.71024315e+01 1.28520237e+02 1.41063592e+01\n",
      " 6.17283844e+01 1.64201328e+01 5.72972881e+01 6.27852719e+01\n",
      " 2.98866960e+01 5.44642532e+01 1.21965005e+02 1.66677777e+01\n",
      " 1.54896598e+02 5.39942402e+01 1.61742930e+01]\n",
      "63-th iteration, loss: 0.02187246806485595, 7 gd steps\n",
      "insert gradient: -0.00040008673320467695\n",
      "63-th iteration, new layer inserted. now 65 layers\n",
      "[5.62433628e+01 9.40502414e+01 4.89432574e+01 7.31725342e+01\n",
      " 4.36477780e+01 8.67079587e+01 5.31184782e+01 9.90341339e+01\n",
      " 8.25849739e+01 3.50716695e+01 2.99804482e-02 7.01308712e+01\n",
      " 5.99443936e+01 1.02375034e+02 8.42321723e+01 1.29400628e+02\n",
      " 6.80727379e+01 1.22927380e+02 7.65344734e+01 5.35008414e+01\n",
      " 7.66494668e+00 1.10355521e+02 1.20218379e+02 1.24619280e+02\n",
      " 7.74811590e+01 1.18782787e+02 1.74879654e+01 2.08946653e+01\n",
      " 7.12926837e+01 1.11489774e+02 1.42421030e+01 5.00556569e+01\n",
      " 9.07988547e+01 1.34711006e+02 6.32437548e+01 4.28378272e-02\n",
      " 4.66107023e+01 1.01060293e+02 2.70298486e+01 3.41163798e+01\n",
      " 1.13639150e+02 7.31919352e+01 2.07369060e+01 3.73035743e+01\n",
      " 1.56122641e+01 1.08065292e+01 1.04114192e+02 2.02433796e+01\n",
      " 3.72714825e+01 6.71026024e+01 1.28520680e+02 1.41070448e+01\n",
      " 6.17289315e+01 1.64208797e+01 5.72979898e+01 6.27858596e+01\n",
      " 0.00000000e+00 7.10542736e-15 2.98878954e+01 5.44649542e+01\n",
      " 1.21965895e+02 1.66683794e+01 1.54897017e+02 5.39946155e+01\n",
      " 1.61747054e+01]\n",
      "64-th iteration, loss: 0.021852407708785958, 176 gd steps\n",
      "insert gradient: -0.00038328110513157995\n",
      "64-th iteration, new layer inserted. now 63 layers\n",
      "[5.62108346e+01 9.40332671e+01 4.89263594e+01 7.31673602e+01\n",
      " 4.36336298e+01 8.67020105e+01 5.31190285e+01 9.90371476e+01\n",
      " 8.26025139e+01 1.05204955e+02 5.99309306e+01 1.02374782e+02\n",
      " 8.42480485e+01 1.29403926e+02 6.80778243e+01 1.22935434e+02\n",
      " 7.65515077e+01 5.35137760e+01 7.69368453e+00 1.10367084e+02\n",
      " 1.20229976e+02 1.24618366e+02 7.74782142e+01 1.18785929e+02\n",
      " 1.74872487e+01 2.08967321e+01 7.12957660e+01 1.11487600e+02\n",
      " 1.42389614e+01 5.00450492e+01 9.07881136e+01 1.34704678e+02\n",
      " 6.32390239e+01 3.23169423e-02 4.66059734e+01 1.01058874e+02\n",
      " 2.70262616e+01 3.41113672e+01 1.13636795e+02 7.31921836e+01\n",
      " 2.07367891e+01 3.72976305e+01 1.56045906e+01 1.08080081e+01\n",
      " 1.04104992e+02 2.02445032e+01 3.72767060e+01 6.71043051e+01\n",
      " 1.28527932e+02 1.41211746e+01 6.17377430e+01 1.64346077e+01\n",
      " 5.73103455e+01 6.27972895e+01 2.49008293e-02 1.14205295e-02\n",
      " 2.99128005e+01 5.44811157e+01 1.21987839e+02 1.66826990e+01\n",
      " 1.54906182e+02 5.40028378e+01 1.61841396e+01]\n",
      "65-th iteration, loss: 0.02136643700637437, 18 gd steps\n",
      "insert gradient: -0.0020795260668395447\n",
      "65-th iteration, new layer inserted. now 61 layers\n",
      "[ 56.6883918   93.88077726  49.68911385  73.72699225  41.854005\n",
      "  86.41967988  55.22020959  99.78226149  82.54499723 105.28183141\n",
      "  58.07539    102.52584904  85.42301144 130.06725229  66.14508243\n",
      " 122.24981101  79.74076245  53.65972325   6.8816586  111.70831454\n",
      " 120.25299363 123.2176383   75.14825157 120.83699831  15.42476354\n",
      "  20.95816223  73.89806038 112.1850028   14.72604875  46.28361687\n",
      "  92.93928226 130.7262297  110.99750416 100.67143153  27.75467377\n",
      "  31.75152879 115.38942589  73.34437649  20.5912626   33.77087436\n",
      "  15.31837756  10.43198404 101.63907374  23.82280144  40.82161528\n",
      "  66.86683925 128.27168387  18.83577418  59.17439814  18.05120181\n",
      "  56.70766325  63.33182407   0.15468394   0.45248659  30.08810247\n",
      "  57.20828946 124.70229088  17.05205646 152.10571029  53.98246577\n",
      "  18.70409413]\n",
      "66-th iteration, loss: 0.021322378425679474, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "66-th iteration, new layer inserted. now 63 layers\n",
      "[5.61709188e+01 9.37901683e+01 4.97549872e+01 7.38717156e+01\n",
      " 4.22230207e+01 8.64826093e+01 5.50022311e+01 9.97115085e+01\n",
      " 8.25084075e+01 1.05286570e+02 5.83337559e+01 1.02579760e+02\n",
      " 8.55509435e+01 1.30132867e+02 6.61364900e+01 1.22243830e+02\n",
      " 8.00327224e+01 5.36889901e+01 6.61142281e+00 1.11805668e+02\n",
      " 1.20392832e+02 1.23149112e+02 7.49909907e+01 1.20993052e+02\n",
      " 1.52912117e+01 2.10458214e+01 7.40598705e+01 1.12278321e+02\n",
      " 1.47823039e+01 4.60581855e+01 9.30310984e+01 1.30496947e+02\n",
      " 1.10785890e+02 1.00645294e+02 2.78408108e+01 3.16532708e+01\n",
      " 7.70163099e+01 0.00000000e+00 3.85081550e+01 7.33425183e+01\n",
      " 2.05795676e+01 3.35894328e+01 1.54254877e+01 1.04311499e+01\n",
      " 1.01572847e+02 2.39912446e+01 4.08478561e+01 6.68365308e+01\n",
      " 1.28299931e+02 1.90598111e+01 5.90989472e+01 1.82000884e+01\n",
      " 5.67779501e+01 6.33820926e+01 8.05322918e-02 5.00520553e-01\n",
      " 3.00205711e+01 5.73046353e+01 1.24945517e+02 1.72545606e+01\n",
      " 1.52087748e+02 5.39916764e+01 1.87108408e+01]\n",
      "67-th iteration, loss: 0.02131602800608749, 13 gd steps\n",
      "insert gradient: -0.001451247437923566\n",
      "67-th iteration, new layer inserted. now 65 layers\n",
      "[5.61688454e+01 9.37924305e+01 4.97571627e+01 7.38716688e+01\n",
      " 4.22245052e+01 8.64812174e+01 5.49912339e+01 9.97030765e+01\n",
      " 0.00000000e+00 1.42108547e-14 8.24836876e+01 1.05279253e+02\n",
      " 5.83352767e+01 1.02578690e+02 8.55457693e+01 1.30128156e+02\n",
      " 6.61301635e+01 1.22241467e+02 8.00417253e+01 5.36878894e+01\n",
      " 6.59457873e+00 1.11807501e+02 1.20398472e+02 1.23148705e+02\n",
      " 7.49916430e+01 1.21000221e+02 1.52886689e+01 2.10502399e+01\n",
      " 7.40687382e+01 1.12285301e+02 1.47936118e+01 4.60562978e+01\n",
      " 9.30414773e+01 1.30492516e+02 1.10782192e+02 1.00645100e+02\n",
      " 2.78448979e+01 3.16506705e+01 7.70200893e+01 3.22361356e-02\n",
      " 3.85119336e+01 7.33392847e+01 2.05643219e+01 3.35742099e+01\n",
      " 1.54210665e+01 1.04271998e+01 1.01558842e+02 2.39915923e+01\n",
      " 4.08431479e+01 6.68341125e+01 1.28301251e+02 1.90684781e+01\n",
      " 5.90979917e+01 1.82065603e+01 5.67817629e+01 6.33847746e+01\n",
      " 7.87496083e-02 5.03161208e-01 3.00190463e+01 5.73072785e+01\n",
      " 1.24952397e+02 1.72608821e+01 1.52086968e+02 5.39916794e+01\n",
      " 1.87107505e+01]\n",
      "68-th iteration, loss: 0.02131511934767671, 6 gd steps\n",
      "insert gradient: -0.00032235784206678447\n",
      "68-th iteration, new layer inserted. now 67 layers\n",
      "[5.61688294e+01 9.37926087e+01 4.97574914e+01 7.38718990e+01\n",
      " 4.22252639e+01 8.64818209e+01 5.49927240e+01 9.97046675e+01\n",
      " 4.37556840e-03 1.59095981e-03 8.24880632e+01 1.05281075e+02\n",
      " 5.83374850e+01 1.02579342e+02 8.55465405e+01 1.30128884e+02\n",
      " 6.61312705e+01 1.22241900e+02 8.00426729e+01 5.36884334e+01\n",
      " 6.59512391e+00 1.11808039e+02 1.20399053e+02 1.23148792e+02\n",
      " 7.49916449e+01 1.21000552e+02 1.52885479e+01 2.10503888e+01\n",
      " 7.40691677e+01 1.12285526e+02 1.47936716e+01 4.60559206e+01\n",
      " 9.30416328e+01 1.30492179e+02 1.10781886e+02 1.00645088e+02\n",
      " 2.78449630e+01 3.16505452e+01 7.70204660e+01 0.00000000e+00\n",
      " 7.10542736e-15 3.36241820e-02 3.85123103e+01 7.33394218e+01\n",
      " 2.05646684e+01 3.35741389e+01 1.54214667e+01 1.04273470e+01\n",
      " 1.01559028e+02 2.39920459e+01 4.08433689e+01 6.68341170e+01\n",
      " 1.28301385e+02 1.90688798e+01 5.90979916e+01 1.82068569e+01\n",
      " 5.67819558e+01 6.33848977e+01 7.86677214e-02 5.03282541e-01\n",
      " 3.00189755e+01 5.73073860e+01 1.24952686e+02 1.72611573e+01\n",
      " 1.52086937e+02 5.39916845e+01 1.87107531e+01]\n",
      "69-th iteration, loss: 0.020587885624210735, 22 gd steps\n",
      "insert gradient: -0.004655961771466611\n",
      "69-th iteration, new layer inserted. now 63 layers\n",
      "[5.64270925e+01 9.57403129e+01 5.10760448e+01 7.34223880e+01\n",
      " 4.23903734e+01 8.68542741e+01 5.44687064e+01 1.01082341e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.21898090e+01 1.05547174e+02\n",
      " 5.86841171e+01 1.01733026e+02 8.69796227e+01 1.30913652e+02\n",
      " 6.47886134e+01 1.17943025e+02 8.49131739e+01 5.65605775e+01\n",
      " 1.96224661e+00 1.15645249e+02 1.21943978e+02 1.23129406e+02\n",
      " 7.16572007e+01 1.24490948e+02 1.03254813e+01 2.38503141e+01\n",
      " 7.64168671e+01 1.14438427e+02 1.54646147e+01 4.14230893e+01\n",
      " 9.82160949e+01 1.25807500e+02 1.10459629e+02 9.98575008e+01\n",
      " 3.24295668e+01 2.47813461e+01 7.27191922e+01 1.19674415e+01\n",
      " 3.45550561e+01 7.44714753e+01 2.23929835e+01 2.75316771e+01\n",
      " 1.69932304e+01 5.98629242e+00 1.00983894e+02 3.18644611e+01\n",
      " 4.21648353e+01 6.54524021e+01 1.29716209e+02 2.33557081e+01\n",
      " 5.40069831e+01 1.96283876e+01 5.68256291e+01 6.83804236e+01\n",
      " 2.91828840e+01 5.89758307e+01 1.26911803e+02 1.65846720e+01\n",
      " 1.49342491e+02 5.47704208e+01 1.95136352e+01]\n",
      "70-th iteration, loss: 0.02054850777952779, 9 gd steps\n",
      "insert gradient: -0.000987776063635495\n",
      "70-th iteration, new layer inserted. now 65 layers\n",
      "[5.64250338e+01 9.57381157e+01 5.10727197e+01 7.34215967e+01\n",
      " 4.23887810e+01 8.68543162e+01 5.44724420e+01 1.01085512e+02\n",
      " 9.44500331e-03 3.15581500e-03 8.21992595e+01 1.05551429e+02\n",
      " 5.86911331e+01 1.01736256e+02 8.69838242e+01 1.30916679e+02\n",
      " 6.47918825e+01 1.17944649e+02 8.49206237e+01 5.65630333e+01\n",
      " 1.96204189e+00 1.15647927e+02 1.21948591e+02 1.23131356e+02\n",
      " 7.16588254e+01 1.24492847e+02 1.03287215e+01 2.38537664e+01\n",
      " 7.64175602e+01 1.14439087e+02 1.54675138e+01 4.14236132e+01\n",
      " 9.82175062e+01 1.25807283e+02 1.10457957e+02 9.98532389e+01\n",
      " 3.24190899e+01 2.47678692e+01 7.26975584e+01 1.19528451e+01\n",
      " 3.45268346e+01 7.44549440e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.23449327e+01 2.74994775e+01 1.69593508e+01 5.96616369e+00\n",
      " 1.00944973e+02 3.18470690e+01 4.21449611e+01 6.54461971e+01\n",
      " 1.29711729e+02 2.33559344e+01 5.40059190e+01 1.96301587e+01\n",
      " 5.68255254e+01 6.83805608e+01 2.91817929e+01 5.89763296e+01\n",
      " 1.26914771e+02 1.65866960e+01 1.49344009e+02 5.47715428e+01\n",
      " 1.95138945e+01]\n",
      "71-th iteration, loss: 0.020273088213369397, 46 gd steps\n",
      "insert gradient: -0.0014860723943532876\n",
      "71-th iteration, new layer inserted. now 63 layers\n",
      "[5.65843226e+01 9.51690937e+01 5.06842957e+01 7.41852461e+01\n",
      " 4.26418958e+01 8.67835930e+01 5.49023480e+01 1.00981849e+02\n",
      " 5.75637436e-03 1.79331405e-03 8.17841238e+01 1.05821779e+02\n",
      " 5.96946899e+01 1.02186308e+02 8.70085856e+01 1.31252390e+02\n",
      " 6.37108083e+01 1.16985285e+02 8.78656876e+01 1.74364523e+02\n",
      " 1.22546299e+02 1.23150244e+02 7.05197881e+01 1.25348080e+02\n",
      " 9.45792891e+00 2.58212225e+01 7.51493016e+01 1.14354315e+02\n",
      " 1.56564187e+01 4.08738832e+01 9.94879369e+01 1.25478900e+02\n",
      " 1.10573115e+02 9.98528394e+01 3.41550159e+01 2.34644680e+01\n",
      " 7.13644066e+01 1.28763335e+01 3.26667652e+01 7.44901935e+01\n",
      " 1.49265049e-01 3.04738049e-02 2.24965734e+01 2.67658276e+01\n",
      " 1.78835391e+01 4.06857070e+00 1.01977711e+02 3.38352249e+01\n",
      " 4.15957221e+01 6.46126813e+01 1.29441544e+02 2.45668107e+01\n",
      " 5.35267488e+01 2.06836057e+01 5.69472889e+01 6.87052914e+01\n",
      " 2.83670286e+01 5.90632062e+01 1.27739995e+02 1.72032140e+01\n",
      " 1.48982882e+02 5.52429456e+01 1.93185915e+01]\n",
      "72-th iteration, loss: 0.020026259498134674, 33 gd steps\n",
      "insert gradient: -0.0020066407970801563\n",
      "72-th iteration, new layer inserted. now 63 layers\n",
      "[5.66051246e+01 9.55435950e+01 5.14454926e+01 7.45392932e+01\n",
      " 4.17304805e+01 8.64494553e+01 5.54865750e+01 1.01631874e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.15049213e+01 1.06323881e+02\n",
      " 5.90875397e+01 1.02105113e+02 8.73071728e+01 1.32430903e+02\n",
      " 6.31627545e+01 1.14522158e+02 8.98948221e+01 1.73571882e+02\n",
      " 1.22291306e+02 1.23637208e+02 6.97218279e+01 1.26599195e+02\n",
      " 8.75928910e+00 2.85112335e+01 7.30012241e+01 1.13916252e+02\n",
      " 1.59053498e+01 4.02479886e+01 1.00713523e+02 1.24628271e+02\n",
      " 1.10581539e+02 9.99116496e+01 3.64318519e+01 2.03620009e+01\n",
      " 6.94868421e+01 1.56813575e+01 3.01523651e+01 7.45363074e+01\n",
      " 1.20012776e-01 5.55291893e-02 2.24733345e+01 2.56143530e+01\n",
      " 1.96609952e+01 4.25917275e-01 1.03881384e+02 3.80441532e+01\n",
      " 4.14308453e+01 6.37157298e+01 1.29557844e+02 2.58239256e+01\n",
      " 5.21108293e+01 2.17030473e+01 5.69409211e+01 6.96794921e+01\n",
      " 2.80744854e+01 5.93116236e+01 1.28152320e+02 1.79190789e+01\n",
      " 1.47894419e+02 5.62997354e+01 1.87370680e+01]\n",
      "73-th iteration, loss: 0.019941163912972872, 49 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "73-th iteration, new layer inserted. now 59 layers\n",
      "[ 56.87439982  95.68387219  51.30878686  74.54517036  41.8516203\n",
      "  86.45342942  55.32691514 101.67543753  81.39364256 106.55343075\n",
      "  59.39292247 101.97475935  87.35879224 132.46966994  62.90908658\n",
      " 114.08627222  90.77533458 173.1433109   81.53800126   0.\n",
      "  40.76900063 123.85787081  69.25277012 127.09259523   8.42378022\n",
      "  29.55945871  72.38831156 113.93101314  15.82480939  39.93396791\n",
      " 101.24107246 124.55552962 110.6948001  100.10934771  37.26562844\n",
      "  18.89037696  68.89213189  16.57779064  29.20946688  74.72963763\n",
      "  22.18277383  25.33499425 125.13508128  39.75262631  41.12521708\n",
      "  63.16974595 129.60152658  26.15709137  51.68428827  22.25310907\n",
      "  57.06036209  70.11801829  27.86524262  59.25407274 128.30164416\n",
      "  18.16009983 147.67124461  56.69635952  18.26648381]\n",
      "74-th iteration, loss: 0.018478701643610644, 92 gd steps\n",
      "insert gradient: -0.002230119461762737\n",
      "74-th iteration, new layer inserted. now 57 layers\n",
      "[ 56.01845662  94.67864478  50.19486071  74.52483694  44.06670389\n",
      "  88.27632333  56.70611943 103.60183181  79.41810842 106.99716742\n",
      "  60.14250038 101.05773363  90.35337342 127.5320817   63.78509454\n",
      " 109.97953683  89.59221651 159.08882185  86.36819719  24.80565391\n",
      "  21.08777696 140.09480292  68.58205676 126.26794051   6.33069886\n",
      "  38.4744898   67.00713968 112.30146383  13.46925677  24.63685893\n",
      " 112.68533948 136.10814965 110.75445918 101.31685131 109.35907956\n",
      "  17.22916397  23.50119963  79.76777869  14.10671428  39.85734924\n",
      " 132.86153279  62.8302387   36.0989197   55.27345912 132.40623529\n",
      "  32.64096887  43.29937024  30.25629316  54.75316242  79.98966452\n",
      "  25.94217194  53.73568895 130.0955435   18.99418638 143.91506572\n",
      "  67.93901206  11.62525355]\n",
      "75-th iteration, loss: 0.018472707058670206, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "75-th iteration, new layer inserted. now 59 layers\n",
      "[ 56.02772769  94.68322317  50.19693561  74.52037625  44.0575384\n",
      "  88.27483509  56.70769817 103.60711501  79.43551896 107.004884\n",
      "  60.14770298 101.06336119  90.36741631 127.5364143   63.78612122\n",
      " 109.98039952  89.59130467 159.08201596  86.36434566  24.81342534\n",
      "  21.07775503 140.09139809  68.57124373 126.26668341   6.31968447\n",
      "  38.47547248  67.00558666 112.2983985   13.44930455  24.62515845\n",
      " 112.67974544  45.37066542   0.          90.74133084 110.75098325\n",
      " 101.31263262 109.34587818  17.22202759  23.49032515  79.76256791\n",
      "  14.0944308   39.84852589 132.8472746   62.82720396  36.09503447\n",
      "  55.27409086 132.40944447  32.64347502  43.30302893  30.26112783\n",
      "  54.7547768   79.9919309   25.94516905  53.73629851 130.09898915\n",
      "  18.99753905 143.91852831  67.94117868  11.62287121]\n",
      "76-th iteration, loss: 0.017720781430245316, 41 gd steps\n",
      "insert gradient: -7.100899204656231e-05\n",
      "76-th iteration, new layer inserted. now 61 layers\n",
      "[5.72368432e+01 9.47579531e+01 4.97366308e+01 7.49187358e+01\n",
      " 4.52911830e+01 8.90672854e+01 5.62169097e+01 1.03565933e+02\n",
      " 7.98311843e+01 1.06607311e+02 5.95193947e+01 1.01839780e+02\n",
      " 9.19802237e+01 1.26829943e+02 6.30227858e+01 1.09212911e+02\n",
      " 9.12245827e+01 1.47843882e+02 8.92861857e+01 3.72769007e+01\n",
      " 1.46305452e+01 1.41283178e+02 6.73539552e+01 1.29321827e+02\n",
      " 2.01779299e+00 4.40081547e+01 6.91113598e+01 1.11862163e+02\n",
      " 9.39241771e+00 1.70101299e+01 1.12267478e+02 4.77717668e+01\n",
      " 4.22051294e+00 9.37384436e+01 1.13102909e+02 1.02147007e+02\n",
      " 1.03828879e+02 1.37579364e+01 0.00000000e+00 1.77635684e-15\n",
      " 2.35137523e+01 8.10600671e+01 1.60887025e+01 4.19265613e+01\n",
      " 1.31440990e+02 6.77604235e+01 3.52042711e+01 5.38190733e+01\n",
      " 1.35228820e+02 3.27061834e+01 4.05646421e+01 3.34191417e+01\n",
      " 5.32612117e+01 8.10792435e+01 2.56955493e+01 5.26818419e+01\n",
      " 1.31236780e+02 1.81840666e+01 1.43816940e+02 7.04134957e+01\n",
      " 1.03322092e+01]\n",
      "77-th iteration, loss: 0.017200274322878947, 27 gd steps\n",
      "insert gradient: -0.002749321945507792\n",
      "77-th iteration, new layer inserted. now 59 layers\n",
      "[ 55.76066761  94.78719744  51.20960655  74.34029144  44.60127974\n",
      "  90.42236155  58.41898061 104.17973454  79.28287919 106.12250193\n",
      "  58.22118548 101.50556405  93.56444633 126.45519166  62.46046831\n",
      " 109.15884696  92.17456799 140.31236964  87.52661495  48.42889123\n",
      "   9.62374886 143.61509084  67.3695984  180.09060351  70.5282449\n",
      " 111.89116187   3.56360051  11.55373224 112.8145416   50.01765738\n",
      "   5.5844632   96.76371863 116.2373602  103.24205805  99.74852691\n",
      "  14.2260589    0.27930426   0.40066692  23.80705933  82.37624263\n",
      "  17.80978476  39.46112729 131.77042996  71.4819493   33.8421239\n",
      "  52.04873628 137.58870934  32.22940003  38.79992032  35.18306704\n",
      "  53.22639679  81.8841354   25.2239423   52.2309296  131.82148491\n",
      "  16.53757116 144.88997549  72.2740535   10.39018134]\n",
      "78-th iteration, loss: 0.0165297963788849, 38 gd steps\n",
      "insert gradient: -0.012573953363061361\n",
      "78-th iteration, new layer inserted. now 55 layers\n",
      "[ 54.66108316  94.84970123  52.66210543  75.34468265  44.30112771\n",
      "  91.14282884  59.72162015  79.52912997   0.          26.50970999\n",
      "  77.93169497 106.10616022  58.34184809 101.03061862  96.31650086\n",
      " 122.48741956  61.05872259 109.84303538  94.49862459 131.36474147\n",
      "  82.97147681 217.42908279  71.98778479 182.18755206  69.62272811\n",
      " 123.84033575 110.25798206  55.10266603   3.68482508 100.75872292\n",
      " 120.59638637 106.09890357  94.47181758  13.53631953  25.24034418\n",
      "  85.50155846  18.76345094  34.33491612 135.09672593  75.726403\n",
      "  31.30446098  50.3499939  137.67309382  31.54659636  38.85994121\n",
      "  39.99277128  51.53976566  81.63652871  24.1473775   52.62429572\n",
      " 130.56812899  18.52674727 145.31157417  73.58218066   8.83087331]\n",
      "79-th iteration, loss: 0.016471390600632293, 7 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "79-th iteration, new layer inserted. now 57 layers\n",
      "[5.46621967e+01 9.48501999e+01 5.26630042e+01 7.53461098e+01\n",
      " 4.43059648e+01 9.11473023e+01 5.97348285e+01 7.95420384e+01\n",
      " 3.34591495e-02 2.65226185e+01 7.79624832e+01 1.06117932e+02\n",
      " 5.83540233e+01 1.01035018e+02 9.63241231e+01 1.22491670e+02\n",
      " 6.10637559e+01 1.09844804e+02 9.45004804e+01 1.31366211e+02\n",
      " 8.29734700e+01 2.17429815e+02 7.19874478e+01 1.82187586e+02\n",
      " 6.96225963e+01 1.23840157e+02 1.10257068e+02 5.51022315e+01\n",
      " 3.68298632e+00 1.00758233e+02 1.20594876e+02 1.06097332e+02\n",
      " 9.44686585e+01 1.35332266e+01 2.52366346e+01 8.55006519e+01\n",
      " 1.87622130e+01 3.43343346e+01 1.35095844e+02 7.57261309e+01\n",
      " 3.13041929e+01 5.03499883e+01 6.88364126e+01 0.00000000e+00\n",
      " 6.88364126e+01 3.15463556e+01 3.88597503e+01 3.99928872e+01\n",
      " 5.15395518e+01 8.16364364e+01 2.41474127e+01 5.26243984e+01\n",
      " 1.30568039e+02 1.85269409e+01 1.45311557e+02 7.35821693e+01\n",
      " 8.83083490e+00]\n",
      "80-th iteration, loss: 0.01643808391898618, 73 gd steps\n",
      "insert gradient: -0.00025975502561603595\n",
      "80-th iteration, new layer inserted. now 55 layers\n",
      "[5.46863477e+01 9.48390687e+01 5.26097533e+01 7.53361940e+01\n",
      " 4.43143679e+01 9.11413050e+01 5.97051208e+01 1.06081644e+02\n",
      " 7.80071057e+01 1.06131197e+02 5.83744261e+01 1.01042751e+02\n",
      " 9.63523594e+01 1.22482032e+02 6.10557525e+01 1.09845617e+02\n",
      " 9.44989123e+01 1.31351618e+02 8.29483416e+01 2.17422283e+02\n",
      " 7.19412189e+01 1.82172819e+02 6.95871928e+01 1.23823567e+02\n",
      " 1.10214214e+02 5.50910016e+01 3.62618029e+00 1.00743314e+02\n",
      " 1.20551091e+02 1.06059826e+02 9.43954920e+01 1.34598218e+01\n",
      " 2.51505290e+01 8.54869554e+01 1.87512633e+01 3.43205027e+01\n",
      " 1.35085353e+02 7.57215264e+01 3.12985381e+01 5.03530925e+01\n",
      " 6.88313284e+01 5.69644338e-02 6.88313153e+01 3.15393026e+01\n",
      " 3.88547431e+01 3.99994821e+01 5.15286004e+01 8.16301906e+01\n",
      " 2.41442920e+01 5.26295396e+01 1.30566383e+02 1.85381138e+01\n",
      " 1.45312394e+02 7.35803902e+01 8.82392231e+00]\n",
      "81-th iteration, loss: 0.01599387481000284, 22 gd steps\n",
      "insert gradient: -0.004385291456508172\n",
      "81-th iteration, new layer inserted. now 57 layers\n",
      "[5.32630259e+01 9.45212676e+01 5.23179793e+01 7.64633903e+01\n",
      " 4.56718931e+01 9.13081316e+01 5.83115189e+01 1.07288014e+02\n",
      " 0.00000000e+00 7.10542736e-15 7.78699367e+01 1.05898577e+02\n",
      " 5.89893800e+01 1.00996041e+02 9.78505027e+01 1.18428843e+02\n",
      " 6.22195042e+01 1.11247284e+02 9.59281880e+01 1.26716892e+02\n",
      " 8.28013554e+01 2.17469965e+02 7.22279566e+01 1.84257673e+02\n",
      " 6.77168810e+01 1.22994408e+02 1.05799599e+02 5.83646488e+01\n",
      " 2.08460179e+00 1.02836177e+02 1.23055285e+02 1.08533690e+02\n",
      " 9.33988074e+01 1.12585931e+01 2.49235676e+01 8.83723367e+01\n",
      " 1.97884285e+01 2.94603339e+01 1.37967888e+02 7.55418509e+01\n",
      " 3.10173495e+01 5.14743549e+01 6.44988898e+01 1.15604070e+01\n",
      " 6.29187400e+01 2.91232469e+01 4.02314011e+01 4.18628915e+01\n",
      " 4.90010867e+01 8.09997400e+01 2.37064754e+01 5.29913289e+01\n",
      " 1.29230161e+02 1.93789177e+01 1.46276976e+02 7.32474301e+01\n",
      " 6.66199445e+00]\n",
      "82-th iteration, loss: 0.01579074357003423, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "82-th iteration, new layer inserted. now 57 layers\n",
      "[ 56.27321059  94.60139043  50.38040876  76.57732454  46.73013255\n",
      "  91.32122877  57.81062887 107.06044122  77.9776195  105.90969623\n",
      "  59.34963854 101.19230007  98.58426715 117.78049293  61.52593761\n",
      " 111.17991784  96.47381338 126.00977375  83.1755792  217.19386097\n",
      "  72.61545675 185.12898673  67.31010918 123.15927977 105.42712082\n",
      "  59.21455043   1.43928609 103.52165229 123.57381329 109.38674231\n",
      "  93.3664314   10.18481439  24.85054372  89.38173013  20.10523139\n",
      "  27.97643659  69.54939611   0.          69.54939611  75.09177812\n",
      "  31.04815933  52.2258846   64.30738458  14.09748418  61.29779978\n",
      "  28.71189303  40.51150161  41.93723522  48.24988824  80.788617\n",
      "  23.73029581  53.31560528 128.85855092  19.24156945 146.57354806\n",
      "  73.03388418   6.68633847]\n",
      "83-th iteration, loss: 0.015788073516847786, 7 gd steps\n",
      "insert gradient: -0.00035864338739574804\n",
      "83-th iteration, new layer inserted. now 59 layers\n",
      "[5.62748214e+01 9.46019537e+01 5.03799825e+01 7.65761095e+01\n",
      " 4.67266400e+01 9.13186092e+01 5.78038488e+01 1.07054563e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.79641566e+01 1.05904463e+02\n",
      " 5.93434647e+01 1.01189679e+02 9.85793881e+01 1.17777296e+02\n",
      " 6.15224814e+01 1.11178905e+02 9.64717479e+01 1.26008336e+02\n",
      " 8.31733689e+01 2.17192697e+02 7.26143322e+01 1.85129020e+02\n",
      " 6.73096244e+01 1.23159278e+02 1.05426371e+02 5.92142487e+01\n",
      " 1.43720495e+00 1.03521331e+02 1.23573135e+02 1.09386618e+02\n",
      " 9.33658518e+01 1.01837205e+01 2.48498577e+01 8.93818351e+01\n",
      " 2.01045588e+01 2.79755738e+01 6.95493590e+01 1.15798861e-03\n",
      " 6.95493590e+01 7.50914177e+01 3.10478530e+01 5.22260525e+01\n",
      " 6.43072435e+01 1.40982742e+01 6.12970811e+01 2.87118747e+01\n",
      " 4.05117790e+01 4.19371869e+01 4.82496039e+01 8.07886119e+01\n",
      " 2.37303163e+01 5.33156367e+01 1.28858502e+02 1.92415484e+01\n",
      " 1.46573713e+02 7.30337650e+01 6.68617454e+00]\n",
      "84-th iteration, loss: 0.015583177924339792, 23 gd steps\n",
      "insert gradient: -0.00042758146275769345\n",
      "84-th iteration, new layer inserted. now 57 layers\n",
      "[ 58.73184426  96.13037899  51.29640352  77.08072776  45.6653917\n",
      "  90.51964703  57.31038588 106.69323274  78.42199942 106.26566059\n",
      "  59.31020664 101.11037062  99.65303788 115.39586031  61.41230368\n",
      " 112.2080039   97.09588248 123.95687738  83.35651915 161.54771987\n",
      "   0.          53.84923996  74.5780398  187.55558068  65.8234633\n",
      " 125.41842486 103.90702968 167.26810514 123.63925673 112.65890343\n",
      "  93.46120659   7.11123164  24.76570839  92.19553454  21.22436581\n",
      "  24.39566154  69.63449497   3.90037689  68.18001731  71.90496358\n",
      "  31.23672621  54.05812224  62.4256991   20.50582408  55.06916072\n",
      "  28.9701813   42.77754158  42.55177022  47.08593752  80.5365341\n",
      "  23.51857457  54.26071239 127.50279529  19.9922682  147.84288598\n",
      "  71.61671276   6.89047712]\n",
      "85-th iteration, loss: 0.015245314918268693, 34 gd steps\n",
      "insert gradient: -0.005162947337316932\n",
      "85-th iteration, new layer inserted. now 59 layers\n",
      "[5.63276631e+01 9.54075154e+01 5.14936446e+01 7.85782224e+01\n",
      " 4.68173387e+01 9.01962782e+01 5.65002077e+01 1.06315759e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.86076493e+01 1.06257080e+02\n",
      " 5.90989273e+01 1.01232617e+02 1.00622483e+02 1.14577767e+02\n",
      " 6.17518894e+01 1.12606510e+02 9.74202722e+01 1.22044312e+02\n",
      " 8.05964547e+01 1.55855576e+02 6.83319687e+00 4.81989904e+01\n",
      " 7.30466606e+01 1.91617298e+02 6.68535632e+01 1.27357901e+02\n",
      " 1.02828860e+02 1.64653868e+02 1.24315554e+02 1.13343586e+02\n",
      " 9.32096912e+01 3.26128546e+00 2.45945588e+01 9.43484446e+01\n",
      " 2.05124169e+01 2.39112532e+01 7.08265112e+01 7.49608713e+00\n",
      " 6.57604534e+01 6.94622271e+01 3.17221765e+01 5.46575255e+01\n",
      " 6.13701479e+01 2.33694367e+01 5.12026061e+01 2.97442026e+01\n",
      " 4.46258071e+01 4.29012653e+01 4.69192588e+01 8.05279026e+01\n",
      " 2.30051260e+01 5.48227829e+01 1.26302690e+02 2.13580859e+01\n",
      " 1.48764452e+02 7.05945632e+01 6.53594807e+00]\n",
      "86-th iteration, loss: 0.015221204587190623, 7 gd steps\n",
      "insert gradient: -6.317483377474402e-05\n",
      "86-th iteration, new layer inserted. now 61 layers\n",
      "[5.63309297e+01 0.00000000e+00 7.10542736e-15 9.54094056e+01\n",
      " 5.14961580e+01 7.85795708e+01 4.68216664e+01 9.02019112e+01\n",
      " 5.65180367e+01 1.06331441e+02 3.63950950e-02 1.56815759e-02\n",
      " 7.86440447e+01 1.06271794e+02 5.91149239e+01 1.01238157e+02\n",
      " 1.00629689e+02 1.14582683e+02 6.17588437e+01 1.12608783e+02\n",
      " 9.74223998e+01 1.22045859e+02 8.05982105e+01 1.55855058e+02\n",
      " 6.83510840e+00 4.81983457e+01 7.30469473e+01 1.91617774e+02\n",
      " 6.68539801e+01 1.27358052e+02 1.02827851e+02 1.64653218e+02\n",
      " 1.24315317e+02 1.13343758e+02 9.32102983e+01 3.26118864e+00\n",
      " 2.45952315e+01 9.43487274e+01 2.05118386e+01 2.39112885e+01\n",
      " 7.08264264e+01 7.49611941e+00 6.57596442e+01 6.94617804e+01\n",
      " 3.17219536e+01 5.46574260e+01 6.13699881e+01 2.33697247e+01\n",
      " 5.12020593e+01 2.97443434e+01 4.46260709e+01 4.29011053e+01\n",
      " 4.69189473e+01 8.05279136e+01 2.30052377e+01 5.48228039e+01\n",
      " 1.26302530e+02 2.13582922e+01 1.48764558e+02 7.05944366e+01\n",
      " 6.53587318e+00]\n",
      "87-th iteration, loss: 0.014837767607703469, 50 gd steps\n",
      "insert gradient: -0.005645592524204409\n",
      "87-th iteration, new layer inserted. now 57 layers\n",
      "[ 55.8031336   95.43080118  51.6942278   79.43340908  46.44251813\n",
      "  90.22838741  57.02879129  79.56341288   0.          26.52113763\n",
      "  78.56923112 106.07316633  59.28765048 101.13158383 100.2912768\n",
      " 114.22935464  62.39465496 113.23474787  97.52602776 120.87131857\n",
      "  79.32487443 147.67122007  12.75045588  39.80989476  72.31318057\n",
      " 197.3822214   67.98553594 131.84748666 100.28403523 160.16237277\n",
      " 125.53747598 116.10656045 119.42394884  97.6808658   19.29930585\n",
      "  24.32485215  72.916805    11.17737577  59.58346668  65.77912292\n",
      "  33.14297528  55.29317013  60.47938425  26.3375612   46.9110228\n",
      "  31.21854476  47.08694442  42.45526757  45.74765901  80.74845101\n",
      "  23.41193713  55.65697644 125.11283045  23.27198308 147.90832229\n",
      "  69.27204583   6.71846477]\n",
      "88-th iteration, loss: 0.014811669788732659, 8 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "88-th iteration, new layer inserted. now 59 layers\n",
      "[5.58044664e+01 9.54316052e+01 5.16960144e+01 7.94351851e+01\n",
      " 4.64473009e+01 9.02328266e+01 5.70416826e+01 7.95755520e+01\n",
      " 2.68904398e-02 2.65332759e+01 7.85986206e+01 1.06084400e+02\n",
      " 5.92988371e+01 1.01135605e+02 1.00298370e+02 1.14232990e+02\n",
      " 6.23988040e+01 1.13236193e+02 9.75279270e+01 1.20872753e+02\n",
      " 7.93270519e+01 1.47671311e+02 1.27525414e+01 3.98100879e+01\n",
      " 7.23136211e+01 1.97382650e+02 6.79858354e+01 1.31847740e+02\n",
      " 1.00283774e+02 1.60162008e+02 1.25537003e+02 1.16106279e+02\n",
      " 1.19423349e+02 9.76807825e+01 1.92987236e+01 2.43245974e+01\n",
      " 7.29165738e+01 1.11774024e+01 5.95827993e+01 6.57788330e+01\n",
      " 3.31428962e+01 5.52931051e+01 6.04793094e+01 2.63377058e+01\n",
      " 4.69107922e+01 3.12185759e+01 4.70870179e+01 4.24551810e+01\n",
      " 4.57475335e+01 8.07484776e+01 2.34120582e+01 5.56570174e+01\n",
      " 8.34084913e+01 0.00000000e+00 4.17042457e+01 2.32719908e+01\n",
      " 1.47908268e+02 6.92720109e+01 6.71850024e+00]\n",
      "89-th iteration, loss: 0.014807093028610134, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "89-th iteration, new layer inserted. now 59 layers\n",
      "[5.58099946e+01 9.54335098e+01 5.16982143e+01 7.94378466e+01\n",
      " 4.64521959e+01 9.02357006e+01 5.70483038e+01 1.06121453e+02\n",
      " 7.86143678e+01 1.06089860e+02 5.93030174e+01 1.01136802e+02\n",
      " 1.00301427e+02 1.14233591e+02 6.23987376e+01 1.13236617e+02\n",
      " 9.75285026e+01 1.20873133e+02 7.93281477e+01 1.47667396e+02\n",
      " 1.27523500e+01 3.98057482e+01 7.23101915e+01 1.97383010e+02\n",
      " 6.79841244e+01 1.31848177e+02 1.00280265e+02 1.60158565e+02\n",
      " 1.25532594e+02 1.16104095e+02 1.19419025e+02 9.76802007e+01\n",
      " 1.92953088e+01 2.43231056e+01 7.29150607e+01 1.11774004e+01\n",
      " 1.98594474e+01 0.00000000e+00 3.97188948e+01 6.57767418e+01\n",
      " 3.31419720e+01 5.52925729e+01 6.04789232e+01 2.63388274e+01\n",
      " 4.69097637e+01 3.12190881e+01 4.70876523e+01 4.24545454e+01\n",
      " 4.57465454e+01 8.07485126e+01 2.34125434e+01 5.56573158e+01\n",
      " 8.34081822e+01 1.54940979e-03 4.17039366e+01 2.32720870e+01\n",
      " 1.47907901e+02 6.92716898e+01 6.71857523e+00]\n",
      "90-th iteration, loss: 0.014448170428052671, 19 gd steps\n",
      "insert gradient: -0.00010766137645585557\n",
      "90-th iteration, new layer inserted. now 61 layers\n",
      "[5.30812557e+01 0.00000000e+00 7.10542736e-15 9.55951489e+01\n",
      " 5.48392831e+01 8.13607269e+01 4.46476996e+01 8.91064107e+01\n",
      " 5.73760664e+01 1.06141496e+02 7.91334017e+01 1.06718801e+02\n",
      " 5.81569151e+01 1.00283690e+02 1.01968619e+02 1.11632998e+02\n",
      " 6.13124539e+01 1.15179113e+02 9.77324760e+01 1.18658521e+02\n",
      " 7.95886149e+01 1.36508498e+02 1.89226771e+01 3.17413401e+01\n",
      " 7.06493834e+01 2.05319237e+02 6.88332025e+01 1.39546799e+02\n",
      " 9.74756644e+01 1.52173690e+02 1.26353033e+02 1.20164004e+02\n",
      " 1.17930001e+02 1.02301864e+02 1.83030923e+01 1.94421217e+01\n",
      " 7.65363182e+01 1.64728068e+01 1.26494144e+01 9.24644589e+00\n",
      " 3.27643437e+01 6.65990582e+01 3.74282117e+01 5.18282206e+01\n",
      " 5.94284048e+01 3.14397986e+01 4.17529617e+01 3.26979541e+01\n",
      " 5.10647011e+01 4.02338718e+01 4.35289582e+01 8.25938073e+01\n",
      " 2.45287953e+01 5.47169522e+01 8.26031529e+01 2.11372664e+00\n",
      " 4.09091822e+01 2.57669823e+01 1.44491419e+02 6.88090485e+01\n",
      " 6.40314889e+00]\n",
      "91-th iteration, loss: 0.014292877584817865, 27 gd steps\n",
      "insert gradient: -0.0005113820908554211\n",
      "91-th iteration, new layer inserted. now 61 layers\n",
      "[5.60029157e+01 9.56608935e+01 5.37007625e+01 8.11737298e+01\n",
      " 4.48841448e+01 8.90446044e+01 5.69386139e+01 1.06026539e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.91797857e+01 1.06712896e+02\n",
      " 5.84021857e+01 1.00393609e+02 1.02140999e+02 1.11567931e+02\n",
      " 6.12708789e+01 1.15175750e+02 9.77363390e+01 1.18769043e+02\n",
      " 7.99007073e+01 1.36306222e+02 1.89996763e+01 3.15772114e+01\n",
      " 7.04717109e+01 2.05308251e+02 6.85662582e+01 1.39549460e+02\n",
      " 9.73063099e+01 1.51862926e+02 1.25966160e+02 1.20026380e+02\n",
      " 1.17636709e+02 1.02320584e+02 1.82184961e+01 1.93159973e+01\n",
      " 7.65588686e+01 1.66224717e+01 1.24748449e+01 9.40852207e+00\n",
      " 3.26388275e+01 6.66585799e+01 3.75659445e+01 5.17235016e+01\n",
      " 5.93737606e+01 3.15829317e+01 4.16820598e+01 3.26539027e+01\n",
      " 5.11022146e+01 4.01509228e+01 4.33609039e+01 8.26266940e+01\n",
      " 2.46604247e+01 5.46849557e+01 8.25251553e+01 2.12759056e+00\n",
      " 4.08321582e+01 2.55882338e+01 1.44384630e+02 6.88122261e+01\n",
      " 6.48540230e+00]\n",
      "92-th iteration, loss: 0.014213487252755036, 24 gd steps\n",
      "insert gradient: -0.0003629366240843841\n",
      "92-th iteration, new layer inserted. now 59 layers\n",
      "[ 57.17054812  95.42945813  51.57766393  81.45932161  46.67554192\n",
      "  89.26910674  56.20848579 105.74236258  79.38885657 106.57636297\n",
      "  58.80823165 100.42233957 102.25488453 111.12956914  60.97047265\n",
      " 114.94129501  97.58114333 118.90583301  80.20360669 135.47558653\n",
      "  19.17591171  31.22377355  70.1878373  205.54260278  68.35658832\n",
      " 140.08095521  97.32387355 151.2946175  125.59536385 120.27518589\n",
      " 117.45556451 102.67249509  18.07809245  18.82067087  76.89030748\n",
      "  17.12107249  11.79835345  10.14438528  32.0576829   66.89078368\n",
      "  38.05135405  51.25949301  59.26123328  32.20491927  41.37653495\n",
      "  32.53827546  51.36999689  39.85335426  42.85603065  82.8102074\n",
      "  24.96158153  54.48364076  82.42621744   2.24683352  40.74333845\n",
      "  25.17683906 144.10577001  68.86413457   6.48312791]\n",
      "93-th iteration, loss: 0.014186622688389166, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "93-th iteration, new layer inserted. now 61 layers\n",
      "[ 56.81562764  95.43934519  51.76907291  81.52288272  46.57567247\n",
      "  89.19917591  56.23421886 105.70339938  79.47703338 106.54542011\n",
      "  58.78501419 100.37032057 102.24513537 110.93186755  61.05077096\n",
      " 114.87450949  97.51162246 118.96775161  80.25486864 134.94724472\n",
      "  19.33815585  31.03154493  70.16839359 205.77226904  68.33312791\n",
      " 140.42284169  97.22716943 150.86058124  83.56833496   0.\n",
      "  41.78416748 120.5830833  117.46848993 102.95852367  18.09449054\n",
      "  18.56086127  77.1519013   17.34496338  11.28892679  10.58690492\n",
      "  31.56587706  67.0548558   38.31434689  50.85457687  59.15248939\n",
      "  32.63987528  41.19669565  32.53021603  51.57797462  39.61017951\n",
      "  42.53679406  82.96896921  25.08116475  54.25145232  82.42146463\n",
      "   2.39245441  40.75732914  24.9527391  143.94594961  68.90342219\n",
      "   6.46382277]\n",
      "94-th iteration, loss: 0.013249164252275129, 76 gd steps\n",
      "insert gradient: -0.0008518798468700319\n",
      "94-th iteration, new layer inserted. now 61 layers\n",
      "[5.70516234e+01 9.56928116e+01 5.23459572e+01 8.47299776e+01\n",
      " 4.67971199e+01 8.77274021e+01 5.61475676e+01 1.05242363e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.98776155e+01 1.05771109e+02\n",
      " 5.93028756e+01 9.97066711e+01 1.04686397e+02 1.06919979e+02\n",
      " 6.02863307e+01 1.15548867e+02 9.90790901e+01 1.15026849e+02\n",
      " 8.29511611e+01 1.21665006e+02 2.15775766e+01 2.61150341e+01\n",
      " 7.29909380e+01 2.13755613e+02 6.55379231e+01 1.48799428e+02\n",
      " 9.36157428e+01 1.42102931e+02 7.58870867e+01 2.52325270e+01\n",
      " 2.23875406e+01 1.32820988e+02 1.20806479e+02 1.12679242e+02\n",
      " 1.44491995e+01 7.17277670e+00 8.60790559e+01 5.21947131e+01\n",
      " 2.05956555e+01 7.37827743e+01 5.01684651e+01 4.09130817e+01\n",
      " 5.55672135e+01 4.38211759e+01 3.50861084e+01 3.32213710e+01\n",
      " 5.85715826e+01 3.43437128e+01 3.71387281e+01 8.90117977e+01\n",
      " 2.92926316e+01 4.66716899e+01 8.22859831e+01 4.74273950e+00\n",
      " 4.14440764e+01 2.33282642e+01 1.39533466e+02 7.37651855e+01\n",
      " 4.74190909e+00]\n",
      "95-th iteration, loss: 0.013246794774508165, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "95-th iteration, new layer inserted. now 63 layers\n",
      "[5.70439040e+01 9.56865796e+01 5.23315560e+01 8.47203346e+01\n",
      " 4.67828707e+01 8.77215802e+01 5.61422830e+01 1.05243317e+02\n",
      " 7.73485209e-03 1.30736947e-03 7.98853546e+01 1.05774694e+02\n",
      " 5.93048627e+01 9.97088197e+01 1.04693213e+02 1.06922172e+02\n",
      " 6.02885119e+01 1.15549034e+02 9.90822350e+01 1.15028386e+02\n",
      " 8.29549502e+01 1.21666451e+02 2.15824571e+01 2.61158651e+01\n",
      " 7.29937905e+01 1.42504846e+02 0.00000000e+00 7.12524228e+01\n",
      " 6.55384659e+01 1.48801007e+02 9.36137474e+01 1.42102401e+02\n",
      " 7.58862603e+01 2.52330942e+01 2.23837023e+01 1.32820243e+02\n",
      " 1.20805907e+02 1.12679841e+02 1.44472060e+01 7.17040695e+00\n",
      " 8.60784074e+01 5.21951780e+01 2.05905093e+01 7.37820439e+01\n",
      " 5.01691299e+01 4.09122161e+01 5.55657207e+01 4.38228062e+01\n",
      " 3.50860351e+01 3.32203916e+01 5.85705735e+01 3.43418761e+01\n",
      " 3.71367877e+01 8.90122954e+01 2.92933716e+01 4.66701481e+01\n",
      " 8.22856691e+01 4.74413487e+00 4.14441884e+01 2.33266974e+01\n",
      " 1.39533081e+02 7.37657652e+01 4.74275121e+00]\n",
      "96-th iteration, loss: 0.013057546913300053, 26 gd steps\n",
      "insert gradient: -0.003266081217180332\n",
      "96-th iteration, new layer inserted. now 63 layers\n",
      "[5.65475715e+01 9.59473141e+01 5.24551138e+01 8.46213064e+01\n",
      " 4.75782639e+01 8.82153104e+01 5.56836749e+01 1.05255606e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.96698369e+01 1.06679783e+02\n",
      " 5.88494585e+01 9.98651272e+01 1.04431187e+02 1.07217330e+02\n",
      " 6.09718789e+01 1.14490266e+02 9.93817846e+01 1.14447709e+02\n",
      " 8.28709249e+01 1.21210608e+02 2.22986884e+01 2.36025963e+01\n",
      " 7.19639552e+01 1.38260568e+02 4.90174565e+00 6.70693566e+01\n",
      " 6.49263713e+01 1.51970717e+02 9.30835972e+01 1.41228725e+02\n",
      " 7.64088578e+01 2.72620197e+01 1.97320239e+01 1.32527476e+02\n",
      " 1.22147778e+02 1.14541255e+02 1.40735885e+01 5.51937006e+00\n",
      " 8.72460279e+01 5.48862695e+01 1.82486971e+01 7.43348987e+01\n",
      " 5.23424618e+01 4.01758106e+01 5.43912407e+01 4.64677963e+01\n",
      " 3.52544290e+01 3.23635846e+01 5.86389883e+01 3.34459989e+01\n",
      " 3.60149255e+01 8.99835783e+01 3.04175088e+01 4.46923268e+01\n",
      " 8.20810147e+01 6.51251594e+00 4.18278353e+01 2.18950901e+01\n",
      " 1.39182664e+02 7.44806749e+01 5.17603137e+00]\n",
      "97-th iteration, loss: 0.012977433637845317, 37 gd steps\n",
      "insert gradient: -6.921094713849843e-05\n",
      "97-th iteration, new layer inserted. now 65 layers\n",
      "[5.66548387e+01 9.57939297e+01 0.00000000e+00 7.10542736e-15\n",
      " 5.22441766e+01 8.45926604e+01 4.72516183e+01 8.80793933e+01\n",
      " 5.56541888e+01 1.05201459e+02 3.70101964e-02 6.00333919e-03\n",
      " 7.97069099e+01 1.06692821e+02 5.89347852e+01 9.99316302e+01\n",
      " 1.04751446e+02 1.07233871e+02 6.05448645e+01 1.14293205e+02\n",
      " 9.97002491e+01 1.14394371e+02 8.28601107e+01 1.21308116e+02\n",
      " 2.26119899e+01 2.27420046e+01 7.20525802e+01 1.37168358e+02\n",
      " 5.85916851e+00 6.59511327e+01 6.50453351e+01 1.52616373e+02\n",
      " 9.29459571e+01 1.40847349e+02 7.67041187e+01 2.76773284e+01\n",
      " 1.90996601e+01 1.32466282e+02 1.22412731e+02 1.14977586e+02\n",
      " 1.41004620e+01 5.14847994e+00 8.75425435e+01 5.55664023e+01\n",
      " 1.76095788e+01 7.45112367e+01 5.29330256e+01 3.99807604e+01\n",
      " 5.39601662e+01 4.72094575e+01 3.54409752e+01 3.20437643e+01\n",
      " 5.86412864e+01 3.33331366e+01 3.57046900e+01 9.01757182e+01\n",
      " 3.07649320e+01 4.42921785e+01 8.20224987e+01 6.93103676e+00\n",
      " 4.20002613e+01 2.16539205e+01 1.39029636e+02 7.46542915e+01\n",
      " 5.26636860e+00]\n",
      "98-th iteration, loss: 0.01204539511527688, 101 gd steps\n",
      "insert gradient: -0.001391556664208718\n",
      "98-th iteration, new layer inserted. now 61 layers\n",
      "[5.63007603e+01 9.57623108e+01 5.30064244e+01 8.74320920e+01\n",
      " 4.82820919e+01 8.78048548e+01 5.51168830e+01 1.04155871e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.91652392e+01 1.08112593e+02\n",
      " 5.94429362e+01 1.00450611e+02 1.05914250e+02 1.06697913e+02\n",
      " 5.98676309e+01 1.11023912e+02 1.03024005e+02 1.09715021e+02\n",
      " 7.95677851e+01 1.29145993e+02 2.69335830e+01 3.45605959e+00\n",
      " 7.96605413e+01 1.15445065e+02 2.46210548e+01 4.09225450e+01\n",
      " 6.73177188e+01 1.63619362e+02 9.20829221e+01 1.36258471e+02\n",
      " 7.97871684e+01 3.31599834e+01 1.33842813e+01 1.25602298e+02\n",
      " 1.25577532e+02 1.24890885e+02 1.01524976e+02 7.40648550e+01\n",
      " 7.24112559e+00 7.69530175e+01 6.47608627e+01 3.92961514e+01\n",
      " 4.27308649e+01 6.28955905e+01 4.00899606e+01 2.47303147e+01\n",
      " 5.99431313e+01 3.65807158e+01 2.69532360e+01 9.19343408e+01\n",
      " 3.88144541e+01 3.66772683e+01 7.66694371e+01 1.67266817e+01\n",
      " 4.50395149e+01 1.99225553e+01 1.35911460e+02 7.51602259e+01\n",
      " 8.56045864e+00]\n",
      "99-th iteration, loss: 0.012042724959190091, 7 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "99-th iteration, new layer inserted. now 63 layers\n",
      "[5.63005970e+01 9.57622434e+01 5.30064754e+01 8.74323651e+01\n",
      " 4.82831601e+01 8.78061484e+01 5.51211729e+01 1.04160233e+02\n",
      " 1.12760372e-02 4.36140195e-03 7.91765153e+01 1.08117012e+02\n",
      " 5.94472583e+01 1.00452240e+02 1.05917312e+02 1.06699110e+02\n",
      " 5.98688175e+01 1.11024272e+02 1.03024853e+02 1.09715472e+02\n",
      " 7.95684231e+01 1.29146504e+02 2.69343034e+01 3.45590568e+00\n",
      " 7.96612447e+01 1.15445061e+02 2.46218079e+01 4.09223056e+01\n",
      " 6.73177680e+01 1.63619585e+02 9.20831079e+01 1.36258580e+02\n",
      " 7.97872741e+01 3.31601572e+01 1.33842092e+01 1.25602229e+02\n",
      " 8.37182941e+01 0.00000000e+00 4.18591471e+01 1.24890853e+02\n",
      " 1.01524758e+02 7.40649720e+01 7.24079638e+00 7.69529889e+01\n",
      " 6.47609537e+01 3.92962021e+01 4.27306652e+01 6.28958444e+01\n",
      " 4.00901967e+01 2.47301493e+01 5.99431758e+01 3.65809481e+01\n",
      " 2.69531108e+01 9.19343495e+01 3.88146886e+01 3.66772288e+01\n",
      " 7.66693149e+01 1.67268219e+01 4.50396105e+01 1.99225326e+01\n",
      " 1.35911541e+02 7.51601461e+01 8.56049337e+00]\n",
      "0-th iteration, loss: 0.7727817661388309, 18 gd steps\n",
      "insert gradient: -0.4954825881898139\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  76.91985468    0.         3115.25411449]\n",
      "1-th iteration, loss: 0.574472203590547, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.00776701e+01 1.38196085e+00 0.00000000e+00 1.11938829e+02\n",
      " 3.08820007e+03]\n",
      "2-th iteration, loss: 0.5703892808571259, 34 gd steps\n",
      "insert gradient: -0.2833258522067357\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.72168935  110.37771176  150.63902554    0.         2937.46099795]\n",
      "3-th iteration, loss: 0.45482478630406126, 38 gd steps\n",
      "insert gradient: -0.1626870984648038\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  68.34547517  100.90250709   59.57164369    0.           52.69799249\n",
      "   86.97720032 2914.11156456]\n",
      "4-th iteration, loss: 0.432528967744261, 28 gd steps\n",
      "insert gradient: -0.13016115260703354\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[7.28187436e+01 1.08211743e+02 5.94433240e+01 0.00000000e+00\n",
      " 2.93098879e-14 3.44515988e+01 3.62840316e+01 9.35232075e+01\n",
      " 2.90532908e+03]\n",
      "5-th iteration, loss: 0.4200816084270641, 37 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  74.61924583  118.18795448   56.6747367    69.05194302   26.52261928\n",
      "   92.72273693  165.92426169    0.         2737.75031796]\n",
      "6-th iteration, loss: 0.38606201337711, 14 gd steps\n",
      "insert gradient: -0.06954181421969415\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[6.25389897e+01 1.33492448e+02 5.92728800e+01 7.54975373e+01\n",
      " 2.46018330e+01 7.49139202e+01 1.49148263e+02 0.00000000e+00\n",
      " 3.37507799e-14 4.95039859e+01 2.73072242e+03]\n",
      "7-th iteration, loss: 0.377173288895127, 21 gd steps\n",
      "insert gradient: -0.035366361767375176\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  66.54939774  127.30023711   57.07324627   78.00893813   23.16267104\n",
      "   80.81816556  141.00422212   79.59677057 2321.43193518    0.\n",
      "  403.72729307]\n",
      "8-th iteration, loss: 0.32127551634986373, 27 gd steps\n",
      "insert gradient: -0.13485599893665126\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  68.84567052  109.30519064   43.69138565   56.72059594   45.76665916\n",
      "   98.50133525  103.37724837  109.95681616 2308.02935591   92.88470665\n",
      "  248.73361068    0.          116.07568499]\n",
      "9-th iteration, loss: 0.27400967090880535, 23 gd steps\n",
      "insert gradient: -0.10077636167681402\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  68.80771861  113.27675833   44.64148818   56.62346045   47.55024157\n",
      "   94.09209382  103.30002601  108.88909732 2300.43118045  130.98520786\n",
      "   96.65955301    0.          120.82444127   84.14322295   87.99355777]\n",
      "10-th iteration, loss: 0.24581491127237048, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[  72.07485148  111.19000039   44.87040826   53.27741116   52.41518179\n",
      "   94.05519903  105.30847511  100.09338716  143.95335822    0.\n",
      " 2159.30037337  120.90713972   86.51869451   49.22001969   93.31206492\n",
      "   96.24782978   85.03445254]\n",
      "11-th iteration, loss: 0.2227918479815716, 24 gd steps\n",
      "insert gradient: -0.14844871605379753\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[6.54636287e+01 1.12350735e+02 5.06497475e+01 5.69347461e+01\n",
      " 4.95687631e+01 9.99398319e+01 0.00000000e+00 2.30926389e-14\n",
      " 1.02241293e+02 1.05250576e+02 1.29768846e+02 3.34776740e+01\n",
      " 2.15222312e+03 1.07663319e+02 8.84299880e+01 8.18719297e+01\n",
      " 9.05038557e+01 8.31715366e+01 9.05407721e+01]\n",
      "12-th iteration, loss: 0.19508446748901936, 108 gd steps\n",
      "insert gradient: -0.052856303672611474\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[  64.87893199  113.1526911    52.35950196   69.38722934   48.36622146\n",
      "   97.88681838  106.76447014  113.73044899  110.01793782   92.51660907\n",
      " 1524.40366523    0.          609.76146609  105.9036389    86.70107676\n",
      "   98.45338445   89.97966062   91.32037919   95.07499806]\n",
      "13-th iteration, loss: 0.1832005566908234, 87 gd steps\n",
      "insert gradient: -0.027779014453939422\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[  67.91646966  103.55343298   50.41527311   81.89427175   50.27481481\n",
      "   98.93908877  106.90022915  102.62743719  101.04221467  119.73891377\n",
      " 1515.40008672   15.30658588  349.12504127    0.          249.37502948\n",
      "  101.82643005   97.52467691   93.01132142  100.80186735   92.39157358\n",
      "   98.82514351]\n",
      "14-th iteration, loss: 0.16577101144813775, 108 gd steps\n",
      "insert gradient: -0.040048347756491486\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  64.62307798   95.7072076    48.27752456   79.40871682   58.55692167\n",
      "  110.53991664   88.78095285  131.59046798   81.85860078  154.7596412\n",
      " 1510.90342876   42.78012867  171.037697      0.          142.53141417\n",
      "   44.83905219  236.82103684   88.65681086  105.66408847   89.06691615\n",
      "  110.3087851    86.42824705  107.0982786 ]\n",
      "15-th iteration, loss: 0.1429901739661555, 67 gd steps\n",
      "insert gradient: -0.026992716130857\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  62.67621274  101.3031488    50.35072226   78.45009631   58.30407082\n",
      "  108.89469187   94.10014031  127.46724733   78.06399286  172.50092722\n",
      " 1501.97934689   44.05281866  153.25001837   50.51492292  123.07094081\n",
      "   95.22644112   90.59738266    0.          135.896074     84.96537468\n",
      "  110.62717683   80.88353235  107.41152294   80.16913227  108.65539433]\n",
      "16-th iteration, loss: 0.12583280436950303, 29 gd steps\n",
      "insert gradient: -0.01866918177620708\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  63.64584914  102.33007774   48.97282284   74.89581836   63.32289219\n",
      "  107.49636194   78.49946895  153.06988055   78.41875172  181.76677309\n",
      " 1499.31560466   40.37576945  144.34085515   91.51292759  114.6381848\n",
      "   99.49288242   69.68277637   42.43362606  129.62770184   77.81068582\n",
      "  111.82756897   68.55920209  102.95427139   77.17836736  110.08674474]\n",
      "17-th iteration, loss: 0.11904444621347239, 44 gd steps\n",
      "insert gradient: -0.07928747317685289\n",
      "17-th iteration, new layer inserted. now 25 layers\n",
      "[  67.34879685  105.97847042   50.95104558   71.74907403   58.76376476\n",
      "  108.10475429   70.97181416  165.56510916   80.04447096  196.55294927\n",
      " 1499.18052517   48.32134169  125.96260631  113.95197079  111.02085955\n",
      "   92.31857269   67.04019308   64.48066004  123.62581487   78.37390081\n",
      "  115.60225584   60.01886069  105.57078693   71.98401052  109.51852263]\n",
      "18-th iteration, loss: 0.11778443995378966, 34 gd steps\n",
      "insert gradient: -0.048811412632299864\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[  67.78855577  105.65229961   49.86294071   73.01702739   55.89015791\n",
      "  114.06198665   69.40790329  164.64431645   80.78865075  202.06683615\n",
      " 1495.62475365   60.04896616  119.20920011  117.08820072  112.77121908\n",
      "   65.34948424    0.           18.67128121   69.36824342   65.20052142\n",
      "  122.71754915   79.81507754  115.93620853   58.54772453  105.63608885\n",
      "   72.70545154  108.62287771]\n",
      "19-th iteration, loss: 0.117766537338024, 6 gd steps\n",
      "insert gradient: -0.01356089890888717\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[6.77934037e+01 1.05654606e+02 4.98660016e+01 7.30185587e+01\n",
      " 5.58947215e+01 1.14062272e+02 6.94106847e+01 1.64648515e+02\n",
      " 8.07944743e+01 2.02069512e+02 1.49562845e+03 6.00520040e+01\n",
      " 1.19212985e+02 1.17093064e+02 1.12782792e+02 6.53598067e+01\n",
      " 2.82668126e-02 1.86816037e+01 6.93962377e+01 6.52099703e+01\n",
      " 1.22727702e+02 7.98183606e+01 5.79704822e+01 0.00000000e+00\n",
      " 5.79704822e+01 5.85500306e+01 1.05638915e+02 7.27059855e+01\n",
      " 1.08624330e+02]\n",
      "20-th iteration, loss: 0.1139525825495546, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 29 layers\n",
      "[  67.86763273  107.2756197    51.40128571   70.58763107   56.59908637\n",
      "  111.41229761   69.23756645  164.27414779   83.03839145  201.54248875\n",
      " 1494.09607056   65.20365083   29.35826459    0.           88.07479378\n",
      "  116.25390542  111.24368629   91.99356024   63.97578283   75.29476695\n",
      "  121.94211983   86.00671421   52.30584192   26.28652639   38.59790205\n",
      "   64.8725107   102.79825154   77.46789315  109.67499645]\n",
      "21-th iteration, loss: 0.1122640599180583, 19 gd steps\n",
      "insert gradient: -0.04291816012329515\n",
      "21-th iteration, new layer inserted. now 29 layers\n",
      "[  68.33344795  107.33450093   50.37980838   70.49773618   58.5006584\n",
      "  107.96926677   71.66648332  158.62957318   87.00625595  196.47803805\n",
      " 1491.94073562   75.13868959   28.65549546   13.74780631   80.75937661\n",
      "  101.48666486  113.43606086   99.53622223   59.98763101   80.9809085\n",
      "  122.01697495   87.55654187   45.33821886   37.41869757   32.46901011\n",
      "   73.97317571  100.03700122   80.91854695  109.46446886]\n",
      "22-th iteration, loss: 0.11115858595451897, 36 gd steps\n",
      "insert gradient: -0.009584313422203781\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[  67.57382515  107.27387029   52.23536837   70.49393193   56.61308252\n",
      "  110.76481669   72.15862636  159.89078767   85.31670111  195.05313933\n",
      " 1492.61037013   80.36806748   26.65395414   19.57473673   76.19686622\n",
      "  105.05682546   83.09997125    0.           27.69999042  103.96308079\n",
      "   57.45871978   83.49841193  120.85239094   88.45160106   47.47758712\n",
      "   36.03896374   32.51489837   73.90418582  100.55107375   79.17736535\n",
      "  109.73719124]\n",
      "23-th iteration, loss: 0.10915806093765142, 20 gd steps\n",
      "insert gradient: -0.0009882038582207666\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[ 67.90753352 111.38937288  51.71631356  69.05184716  55.16187531\n",
      " 112.69015631  68.74263734 161.61446686  88.75024753 200.05225331\n",
      " 850.25421354   0.         637.69066016  91.93582423  25.34829911\n",
      "  21.98738204  68.92717608 102.79098863  76.07105188  19.96592853\n",
      "  20.69474684 107.39298906  59.4248946   84.21787317 122.29546201\n",
      "  83.35841147  47.57873389  40.38481936  29.38789106  70.7233743\n",
      " 101.21858566  80.69574206 110.76325808]\n",
      "24-th iteration, loss: 0.10566194874424795, 92 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[ 68.74505956 107.63398402  52.35457527  70.3548209   54.54677719\n",
      " 109.26716313  72.32802087 161.11406456  86.645784   197.72941388\n",
      " 121.29034468   0.         727.74206806  21.08785492 626.67292562\n",
      "  88.87986039  30.53378829  20.54343239  59.58751806 113.10677804\n",
      "  67.18576776  29.18638531  23.12389284  87.70535742  70.46477768\n",
      "  83.4469824  119.71262968  82.01875348  46.40151334  42.52065285\n",
      "  28.97830447  70.32419274 101.06044046  80.53551658 109.0924264 ]\n",
      "25-th iteration, loss: 0.08368317387943648, 24 gd steps\n",
      "insert gradient: -0.0035504982847066437\n",
      "25-th iteration, new layer inserted. now 37 layers\n",
      "[ 66.03441673 105.50339751  48.7299955   82.43226492  57.84794851\n",
      " 104.08562543  58.5997224  167.83344719  85.7258243  187.64578849\n",
      " 108.9451735   68.37607576 692.03702348  55.98550284 617.41599988\n",
      "  99.45739507  37.7841749   18.86349594  63.69877013  90.64064955\n",
      "  63.77593669  46.55112294  12.4595522  103.92300047  70.03817502\n",
      "  81.4639878  118.72838331  79.87315887  45.71450567  45.91649592\n",
      "  28.95724864  68.01499156 102.41852849  79.86868541  56.7121356\n",
      "   0.          56.7121356 ]\n",
      "26-th iteration, loss: 0.04992895499361607, 119 gd steps\n",
      "insert gradient: -0.0034485873837329204\n",
      "26-th iteration, new layer inserted. now 39 layers\n",
      "[6.78241358e+01 1.14129178e+02 5.49844350e+01 6.79697261e+01\n",
      " 5.55471493e+01 1.10641717e+02 6.78975188e+01 1.11981101e+02\n",
      " 1.34266324e+02 1.72866574e+02 9.92488246e+01 1.18724012e+02\n",
      " 6.45859039e+02 9.36733876e+01 6.24173187e+02 9.11986095e+01\n",
      " 4.79843769e+01 1.76606974e+01 5.29457163e+01 9.57778493e+01\n",
      " 0.00000000e+00 1.06581410e-14 6.31635720e+01 6.77321612e+01\n",
      " 6.20909266e+00 9.78988686e+01 6.46554243e+01 8.81080157e+01\n",
      " 1.24781297e+02 7.93611818e+01 4.64056020e+01 4.55785312e+01\n",
      " 2.18028299e+01 6.86141694e+01 1.00726395e+02 8.76387392e+01\n",
      " 5.17278992e+01 1.24996237e+01 4.75818465e+01]\n",
      "27-th iteration, loss: 0.04893550757753632, 24 gd steps\n",
      "insert gradient: -0.012258241027135613\n",
      "27-th iteration, new layer inserted. now 39 layers\n",
      "[ 69.3705574  113.68905044  54.06230324  67.01731416  56.50926348\n",
      " 111.3965734   66.36031887 113.95569116 139.41474976 162.71619776\n",
      "  99.75035861 120.36922284 642.23494912 100.40519672 624.27380402\n",
      "  90.252514    48.80170532  18.3310229   51.09114684  81.01902518\n",
      "   0.          16.20380504  62.55965116  70.55064044   4.95647812\n",
      "  98.77741742  63.68597059  90.3900539  124.47436516  79.82039334\n",
      "  45.08660167  44.33756808  22.32666098  68.47318881 102.2141516\n",
      "  87.82474738  52.00312818  11.91142835  47.10866268]\n",
      "28-th iteration, loss: 0.04886945871329458, 199 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "28-th iteration, new layer inserted. now 39 layers\n",
      "[ 69.33534179 113.69634822  54.11284357  67.03639513  56.52208923\n",
      " 111.36705859  66.28682338 113.89062109 139.30132899 162.55671451\n",
      "  99.74644936 120.36635341 107.02842013   0.         535.14210063\n",
      " 100.51634485 624.41031666  90.30097138  48.88997555  18.37468135\n",
      "  51.13045904  97.28816593  62.55418455  70.61150065   5.00174283\n",
      "  98.79706261  63.62077608  90.41239893 124.47319752  79.83386159\n",
      "  45.09637424  44.34525788  22.38998496  68.48164962 102.21267493\n",
      "  87.81169375  51.99658227  11.91989705  47.1130804 ]\n",
      "29-th iteration, loss: 0.0441756776452365, 44 gd steps\n",
      "insert gradient: -0.0243287950187734\n",
      "29-th iteration, new layer inserted. now 39 layers\n",
      "[ 66.52481753 113.82234973  55.900065    68.13294387  55.68930144\n",
      " 110.22113614  63.36898204 112.87146286 144.70863735 135.63537619\n",
      " 106.43765905 121.63774288 100.88373002  37.03013767 513.08532359\n",
      " 110.42261306 627.76208997  85.658654    48.96113184  25.55660661\n",
      "  44.04095441  99.81270189  64.39294832 185.89726668  56.72013037\n",
      "  63.66230546   0.          31.83115273 121.32246338  82.61098259\n",
      "  47.20641644  44.2716022   24.63771884  66.44748602  99.95834964\n",
      "  88.17574261  55.37478054   5.81914685  45.66929019]\n",
      "30-th iteration, loss: 0.044113953567260396, 7 gd steps\n",
      "insert gradient: -0.003419610632908207\n",
      "30-th iteration, new layer inserted. now 41 layers\n",
      "[6.65287175e+01 1.13824425e+02 5.59015931e+01 6.81332141e+01\n",
      " 5.56902870e+01 1.10222327e+02 6.33695759e+01 1.12869996e+02\n",
      " 1.44708411e+02 1.35634599e+02 1.06439737e+02 1.21636423e+02\n",
      " 1.00880004e+02 3.70316982e+01 5.13083352e+02 1.10421604e+02\n",
      " 3.76658570e+02 0.00000000e+00 2.51105713e+02 8.56603970e+01\n",
      " 4.89686879e+01 2.55622174e+01 4.40549861e+01 9.98268677e+01\n",
      " 6.44261758e+01 1.85911162e+02 5.67435790e+01 6.36786868e+01\n",
      " 3.76263758e-02 3.18475341e+01 1.21354406e+02 8.26227631e+01\n",
      " 4.72197823e+01 4.42778703e+01 2.46453421e+01 6.64499144e+01\n",
      " 9.99604297e+01 8.81769533e+01 5.53772372e+01 5.82127086e+00\n",
      " 4.56713546e+01]\n",
      "31-th iteration, loss: 0.0390385171229429, 999 gd steps\n",
      "insert gradient: -0.0013562179317085895\n",
      "31-th iteration, new layer inserted. now 39 layers\n",
      "[ 67.01365709 113.44499627  56.87607762  69.79908312  54.8355018\n",
      " 110.23664712  63.85169915 110.43379649 146.16892559 133.00116942\n",
      " 108.36176611 119.79899691  99.77285208  50.09663253 506.45394373\n",
      " 103.95162324 366.32402907  27.14893286 248.26659534  87.01066031\n",
      "  50.97536875  22.4342727   43.6903994  100.93427072  63.0682064\n",
      " 190.38659581  57.67747675  95.57097978 119.7833821   83.0667084\n",
      "  46.39265593  43.04223976  24.52518454  66.50236496 104.3096139\n",
      "  89.43659088  57.88025334   6.35895177  46.13250856]\n",
      "32-th iteration, loss: 0.03506297122348355, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "32-th iteration, new layer inserted. now 41 layers\n",
      "[ 61.03842247 108.79173987  56.77541976  83.38601887  53.29866662\n",
      " 110.3666066   64.81039562 102.49878459 150.65895622 139.67719122\n",
      " 107.15512005 110.53962505  82.08684269 112.25548365 487.73116506\n",
      " 115.64013726 346.36544569  66.90743524 241.55586894  84.95784416\n",
      "  62.25817992   9.91524247  42.07056236  96.24725245  59.97963876\n",
      " 201.30851199  56.42690996  20.20048989   0.          80.80195954\n",
      " 120.2332458   77.58711929  46.09211988  44.28123206  28.9605699\n",
      "  68.44519846 101.47195109  81.46896932  51.72240531   4.71100168\n",
      "  50.78849323]\n",
      "33-th iteration, loss: 0.03456172212958566, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "33-th iteration, new layer inserted. now 43 layers\n",
      "[6.12096464e+01 1.08847923e+02 5.67221084e+01 8.33227911e+01\n",
      " 5.32586602e+01 1.10366608e+02 6.48392338e+01 1.02561497e+02\n",
      " 1.50755363e+02 1.39744842e+02 1.07243879e+02 1.10585683e+02\n",
      " 8.21763396e+01 1.12340346e+02 4.87799908e+02 1.15640667e+02\n",
      " 3.46330346e+02 6.69593908e+01 2.41661093e+02 8.50020325e+01\n",
      " 6.22325627e+01 1.00156743e+01 4.20659679e+01 9.61395282e+01\n",
      " 5.97693617e+01 2.01315220e+02 5.66752405e+01 2.01833812e+01\n",
      " 9.70638253e-02 1.61571845e+01 0.00000000e+00 6.46287382e+01\n",
      " 1.19953856e+02 7.75313757e+01 4.60706851e+01 4.43443337e+01\n",
      " 2.91575208e+01 6.85638467e+01 1.01717577e+02 8.15535371e+01\n",
      " 5.18363808e+01 4.83486257e+00 5.08874802e+01]\n",
      "34-th iteration, loss: 0.03359244903227688, 161 gd steps\n",
      "insert gradient: -0.0021191237540660658\n",
      "34-th iteration, new layer inserted. now 41 layers\n",
      "[ 63.2253166  109.53267745  56.29332735  82.78306515  52.85111428\n",
      " 109.91610573  63.90724557 102.81603308  90.6558548    0.\n",
      "  60.43723653 139.93591973 107.56719111 110.68711815  82.62899512\n",
      " 113.43715693 488.22056436 115.52819317 346.01109865  67.58229043\n",
      " 242.17556499  85.58653931  62.61030206  10.13758846  42.63541876\n",
      "  96.38637852  59.41761741 201.27503386  57.22635296 100.9029241\n",
      " 119.00871449  77.70190794  45.97770697  44.18048213  29.40399489\n",
      "  69.008408   102.67124513  82.03507033  52.36467377   5.2974493\n",
      "  51.32219961]\n",
      "35-th iteration, loss: 0.028545537673599363, 30 gd steps\n",
      "insert gradient: -0.0045371616284278354\n",
      "35-th iteration, new layer inserted. now 41 layers\n",
      "[ 62.04333693 117.47291862  54.51167699  71.15139099  56.63302058\n",
      " 116.86702909  63.94781481 105.79454154  67.26754653  33.24099743\n",
      "  63.89579742 135.98093793 103.11324508 114.96650972  79.58198856\n",
      " 146.76344421 477.91512956 119.55221531 334.72375571  89.71446345\n",
      " 237.86963477  88.47311368  50.73688739   9.71416626  54.66393471\n",
      " 101.11926745  56.28603447 200.39772195  60.3408387   97.66315044\n",
      " 114.91200747  85.87369599  41.50569457  43.5593665   31.08504906\n",
      "  69.41119189 104.90288331  83.88187528  50.00672649   8.89350519\n",
      "  49.15574886]\n",
      "36-th iteration, loss: 0.028005217910373376, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "36-th iteration, new layer inserted. now 43 layers\n",
      "[ 61.87454009 116.80926804  53.98634509  71.18518036  56.76907219\n",
      " 117.05806671  64.50474542 105.97735213  66.66354367  33.5498409\n",
      "  63.23560401 135.79246067 103.37840051 114.76612323  80.0377808\n",
      " 146.94882453 476.9203242  119.71351277 334.59001808  90.17981802\n",
      " 238.40033018  88.60928435  50.68901052  10.06381511  54.28512286\n",
      " 100.41345398  55.42501843 200.19685631  60.4230735   39.12871549\n",
      "   0.          58.69307323 115.72014624  86.26094484  42.84663355\n",
      "  44.55164583  31.58092181  68.83280829 102.50114866  83.0966199\n",
      "  49.16949166   8.41790351  48.66638893]\n",
      "37-th iteration, loss: 0.027786072573440092, 6 gd steps\n",
      "insert gradient: -0.000788000927988541\n",
      "37-th iteration, new layer inserted. now 45 layers\n",
      "[6.18757763e+01 1.16808531e+02 5.39861545e+01 7.11860823e+01\n",
      " 5.67720633e+01 1.17059529e+02 6.45077178e+01 1.05979413e+02\n",
      " 6.66649236e+01 3.35529886e+01 6.32385402e+01 1.35795034e+02\n",
      " 1.03383473e+02 1.14768573e+02 8.00462340e+01 1.46953638e+02\n",
      " 4.76923467e+02 1.19718034e+02 2.00758228e+02 0.00000000e+00\n",
      " 1.33838819e+02 9.01855819e+01 2.38414337e+02 8.86180418e+01\n",
      " 5.07141319e+01 1.00807125e+01 5.43168123e+01 1.00440652e+02\n",
      " 5.54804002e+01 2.00220039e+02 6.04815796e+01 3.91582314e+01\n",
      " 8.11539856e-02 5.87225892e+01 1.15757414e+02 8.62728210e+01\n",
      " 4.28604728e+01 4.45586819e+01 3.15891893e+01 6.88349925e+01\n",
      " 1.02503187e+02 8.30971303e+01 4.91704796e+01 8.41886190e+00\n",
      " 4.86672917e+01]\n",
      "38-th iteration, loss: 0.02775633085391236, 7 gd steps\n",
      "insert gradient: -0.003337131026478812\n",
      "38-th iteration, new layer inserted. now 45 layers\n",
      "[6.18757830e+01 1.16807224e+02 5.39852587e+01 7.11862804e+01\n",
      " 5.67724277e+01 1.17059650e+02 6.45089288e+01 1.05980329e+02\n",
      " 6.66642845e+01 3.35547418e+01 6.32383282e+01 1.35795517e+02\n",
      " 1.03385278e+02 1.14768815e+02 8.00488913e+01 1.46955405e+02\n",
      " 4.76921677e+02 1.19717982e+02 2.00756512e+02 3.60989057e-03\n",
      " 1.33837102e+02 9.01859020e+01 2.38413760e+02 8.86178038e+01\n",
      " 5.07113403e+01 1.00771988e+01 5.43126464e+01 1.00435890e+02\n",
      " 5.54652520e+01 2.00211770e+02 6.04573255e+01 3.91493345e+01\n",
      " 3.72209474e-02 5.87136785e+01 1.15749545e+02 8.62705649e+01\n",
      " 4.28594530e+01 4.45593329e+01 3.15906333e+01 6.88352074e+01\n",
      " 1.02503215e+02 8.30970846e+01 4.91709621e+01 8.41946063e+00\n",
      " 4.86678645e+01]\n",
      "39-th iteration, loss: 0.027557429452203477, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "39-th iteration, new layer inserted. now 45 layers\n",
      "[ 61.90192179 116.56244045  53.8358673   71.24769244  56.84709178\n",
      " 117.09211027  64.7368133  106.15648346  66.54643305  33.86482127\n",
      "  63.1211496  135.80988359 103.62759343 114.74021176  80.53744743\n",
      " 147.2529181  476.51273584 119.66682701 200.33228193   0.61104826\n",
      " 133.41670057  90.33132178  95.46311756   0.         143.19467634\n",
      "  88.74258119  50.78215486  10.07370851  54.31935367 100.39985589\n",
      "  55.40857284 200.15336351  60.43099884  97.9638956  115.7902513\n",
      "  86.24068916  43.00596909  44.81321613  31.91879362  68.84365451\n",
      " 102.41278268  83.0423188   49.18307411   8.44762805  48.70212786]\n",
      "40-th iteration, loss: 0.019088579445471942, 57 gd steps\n",
      "insert gradient: -0.0008933845413082937\n",
      "40-th iteration, new layer inserted. now 47 layers\n",
      "[ 68.34849581 112.77777605  55.41935419  71.4580804   58.42869065\n",
      " 112.23044379  66.77908899 109.23121267  52.24009071  57.28138184\n",
      "  69.43083886 117.76149146 101.66950071 125.0834244   71.06605212\n",
      " 141.895373     0.          35.47384325 470.30363171 127.35359197\n",
      " 163.51771922  67.56845268 117.20385991 117.02628855  82.05308827\n",
      "  40.15862514 132.66198842  89.18623969  54.5333695   27.02370022\n",
      "  40.47684748  91.54561406  54.01150015 193.65351027  60.96057474\n",
      " 103.25965425 117.29914874  90.4212767   44.89463227  40.68261975\n",
      "  35.7295182   63.92126374 102.18410764  79.4174645   55.43140413\n",
      "   7.66981627  41.46978913]\n",
      "41-th iteration, loss: 0.01901858928386924, 21 gd steps\n",
      "insert gradient: -0.0007356660127597274\n",
      "41-th iteration, new layer inserted. now 47 layers\n",
      "[6.83101979e+01 1.12778630e+02 5.54561148e+01 7.14816169e+01\n",
      " 5.84624985e+01 1.12248810e+02 6.68158901e+01 1.09245949e+02\n",
      " 5.22633688e+01 5.73143443e+01 6.93980197e+01 1.17750505e+02\n",
      " 1.01728575e+02 1.25140538e+02 7.10732082e+01 1.41994152e+02\n",
      " 3.10192014e-01 3.55732037e+01 4.70314878e+02 1.27279507e+02\n",
      " 1.63280071e+02 6.76600076e+01 1.17217733e+02 1.17050168e+02\n",
      " 8.20330698e+01 4.02539402e+01 1.32653645e+02 8.91977139e+01\n",
      " 5.45314485e+01 2.71001731e+01 4.05022886e+01 9.15290537e+01\n",
      " 5.39803864e+01 1.93667361e+02 6.08928576e+01 1.03238665e+02\n",
      " 1.17302327e+02 9.04484216e+01 4.49021860e+01 4.06388377e+01\n",
      " 3.57149841e+01 6.39155967e+01 1.02109557e+02 7.93807769e+01\n",
      " 5.54362507e+01 7.70362811e+00 4.14462628e+01]\n",
      "42-th iteration, loss: 0.014426692907536363, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.48130404 109.83826015  62.50264955  73.41345198  57.39448907\n",
      " 105.08116094  68.91418744 107.62266441  50.87962658  64.00027051\n",
      "  72.9383339  100.67520552 102.79993738 132.2161869   62.7617103\n",
      " 141.90060304  11.18922556  30.51383803 473.93313348 124.52236995\n",
      " 148.98968314 110.36314322 104.47977564 119.02787691  78.83074145\n",
      "  86.89848214 122.49801113  94.14825814  46.35409468  35.40921989\n",
      "  40.83602699  84.75781936  48.93581715 195.70223986  60.16603756\n",
      "  53.84912042   0.          53.84912042 114.75879682  96.24498826\n",
      "  54.8697231   34.05673715  34.84600289  69.87221905  96.10907265\n",
      "  71.53025843  56.71171771  14.23573544  36.60496885]\n",
      "43-th iteration, loss: 0.012126993891978258, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 49 layers\n",
      "[ 63.78684474 112.93721875  60.14146946  77.48400683  56.3486854\n",
      " 106.76788917  65.94876626 105.1143377   51.72894863  69.38889313\n",
      "  72.34806279 104.64101411  97.62462414 135.29425401  68.75644868\n",
      " 138.9076721   22.79450807  29.66658533 459.87480098 122.49386886\n",
      " 150.04041028 108.66420684 103.40838446 119.29954764  79.55764487\n",
      " 101.74633161 115.47529004  99.79119721  48.64890026  27.01997502\n",
      "  42.88850635  89.30776087  47.48761789  98.44281452   0.\n",
      "  98.44281452  55.19264933 115.7637205  115.38805181  92.74633264\n",
      "  53.84288109  36.94557604  30.29553058  79.89558449  92.21026993\n",
      "  78.16588856  51.87005908  15.4375453   46.28959254]\n",
      "44-th iteration, loss: 0.010355805726440164, 72 gd steps\n",
      "insert gradient: -0.00019843886817408706\n",
      "44-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.00393166 118.72062837  54.3620174   84.94185603  52.87469345\n",
      " 110.27226624  65.22286805 100.74933023  53.70929072  78.40401761\n",
      "  68.67645129 100.12645443  89.64704709 142.35785871  82.10316162\n",
      " 117.74169839  27.16204741  44.09529453 447.32068209 128.33163207\n",
      " 145.49916858 117.74233747 103.81838422 109.98398618  85.99020782\n",
      " 114.52605859 105.46869974 101.06436801  51.74325517  15.03895711\n",
      "  47.81648768  95.71764553  49.80981413  76.63253562  13.36678694\n",
      "  84.59411887  55.77915444 118.87542186 114.13617345  89.96581958\n",
      "  52.64294468  47.76973617  20.58553581  91.07199319  90.43940967\n",
      "  90.1410578   37.31965679  23.85068344  51.2738379 ]\n",
      "45-th iteration, loss: 0.009970772199905603, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 51 layers\n",
      "[ 66.8688923  119.84282121  51.61535938  84.73173609  53.77995297\n",
      " 111.18959709  66.39712773 100.81503502  50.63515399  86.72623864\n",
      "  65.6616667  104.65736545  74.46531821 158.60055021  90.01240136\n",
      " 102.57275647  27.36190726  53.01004425 445.14338168 129.16532076\n",
      " 142.77381881 122.40429922 102.57894735 113.56435743  85.13092647\n",
      " 121.3688891  101.70922003  99.4384688   52.34387942  15.82186905\n",
      "  48.93788684  93.43059488  52.21347272  69.17225149  13.7801031\n",
      "  86.9658645   59.53904128  56.86913881   0.          56.86913881\n",
      " 112.33625388  91.20604038  52.46274567  52.96133016  18.96779192\n",
      "  88.85804748  92.40466552  89.18338539  33.75016275  29.53359927\n",
      "  49.5008038 ]\n",
      "46-th iteration, loss: 0.00988913681725323, 12 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "46-th iteration, new layer inserted. now 51 layers\n",
      "[ 66.87743196 119.84677324  51.64711776  84.75273565  53.80117437\n",
      " 111.18422227  66.39400343 100.81916298  50.62822844  86.71287798\n",
      "  65.64325488 104.65719782  74.44665198 158.61736247  90.04089709\n",
      " 102.59365943  27.42080799  53.04132432 445.14900619  64.5989279\n",
      "   0.          64.5989279  142.81106526 122.41728432 102.56826889\n",
      " 113.49259101  85.05028725 121.25624478 101.58525147  99.36321729\n",
      "  52.25918937  15.7895811   48.88680078  93.42836565  52.24256873\n",
      "  69.19556343  13.82570931  86.99095738  59.58643008 113.78280574\n",
      " 112.38514215  91.22057566  52.47620773  52.96978105  18.9923729\n",
      "  88.86701838  92.41718941  89.18991016  33.76476241  29.5434837\n",
      "  49.51260605]\n",
      "47-th iteration, loss: 0.009690179714692526, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 51 layers\n",
      "[ 66.54772212 119.79426335  52.7578902   85.28986329  53.80381428\n",
      " 110.48138065  66.20182203 101.88405944  51.5009501   85.81047436\n",
      "  64.10308745 104.93932794  72.5590285  160.01480314  91.59332323\n",
      " 103.4039859   28.81078322  53.7644188  443.77436273 130.26637412\n",
      " 142.67142835 121.00995838 103.75504375 112.91581584  85.94822976\n",
      " 120.71394281 101.80953556  99.2672294   51.52431844  15.5139135\n",
      "  48.34401873  94.15216049  52.83709685  69.59825283  13.73076857\n",
      "  87.37033174  59.19540128  56.51488823   0.          56.51488823\n",
      " 113.47312168  91.30963192  52.12472454  52.48451692  18.91892908\n",
      "  88.92103439  91.83415395  89.36868303  34.38446196  30.04872034\n",
      "  50.44434209]\n",
      "48-th iteration, loss: 0.009681108611767665, 84 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "48-th iteration, new layer inserted. now 51 layers\n",
      "[ 66.54794247 119.79610546  52.77318595  85.29976521  53.81242215\n",
      " 110.47434002  66.20030051 101.89974705  51.50774117  85.79492534\n",
      "  64.07634921 104.94747267  72.5562664  160.05309064  91.6383485\n",
      " 103.41820039  28.83401605  53.77740096 110.9368722    0.\n",
      " 332.81061661 130.25733498 142.65743846 120.99227443 103.76693852\n",
      " 112.91730502  85.94934686 120.72140128 101.82029802  99.28345192\n",
      "  51.55157085  15.53319701  48.36495818  94.17625786  52.86726079\n",
      "  69.62436511  13.77058594  87.39912691  59.21354748 113.01509542\n",
      " 113.46894051  91.30526527  52.11595038  52.47726492  18.92975405\n",
      "  88.92779932  91.84539186  89.37049329  34.382922    30.04551262\n",
      "  50.44146664]\n",
      "49-th iteration, loss: 0.009017061618584093, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "49-th iteration, new layer inserted. now 53 layers\n",
      "[ 66.55507904 116.50325025  56.05618067  84.56637525  55.2221456\n",
      " 106.89670369  65.97009984 104.83200608  52.56244901  83.70234719\n",
      "  64.40571633 109.32246028  60.68064025 165.6278084   95.46190639\n",
      " 106.90184572  33.52061934  53.20431535 112.23220199  22.34428866\n",
      " 156.80790534   0.         156.80790534 135.72874121 138.65932756\n",
      " 112.09637772 107.31738277 115.82287712  84.72735606 116.81335467\n",
      " 101.15543225 100.49341579  53.50559498  16.2969028   50.09992985\n",
      "  95.39745652  52.851731    70.654376    10.59384457  89.45177718\n",
      "  63.1076986  105.58831394 115.34515372  88.12165375  52.2285936\n",
      "  51.42077794  17.70763004  88.67890722  93.03706479  90.23091737\n",
      "  34.81158829  31.17988922  51.24527515]\n",
      "50-th iteration, loss: 0.008671610158294086, 18 gd steps\n",
      "insert gradient: -9.99485567949401e-05\n",
      "50-th iteration, new layer inserted. now 55 layers\n",
      "[6.64768852e+01 1.16410912e+02 5.61020236e+01 8.46496886e+01\n",
      " 5.51706139e+01 1.06787579e+02 6.58159783e+01 1.04970615e+02\n",
      " 5.28345259e+01 8.37262426e+01 6.40935264e+01 1.09239253e+02\n",
      " 6.09711525e+01 1.65848747e+02 9.56102258e+01 1.06879965e+02\n",
      " 3.35550965e+01 5.32874126e+01 1.11973054e+02 2.29033429e+01\n",
      " 1.56527558e+02 0.00000000e+00 1.42108547e-14 1.87119288e+00\n",
      " 1.56527250e+02 1.35480577e+02 1.38540390e+02 1.11831080e+02\n",
      " 1.06546803e+02 1.15901678e+02 8.45810589e+01 1.17246124e+02\n",
      " 1.01870260e+02 1.00821115e+02 5.31543769e+01 1.55887730e+01\n",
      " 4.93389320e+01 9.53553157e+01 5.32866810e+01 7.11762393e+01\n",
      " 1.15945641e+01 8.97869037e+01 6.28280587e+01 1.05294274e+02\n",
      " 1.15140554e+02 8.81697192e+01 5.22867857e+01 5.13041527e+01\n",
      " 1.76507972e+01 8.87169498e+01 9.28710189e+01 9.01588859e+01\n",
      " 3.45213977e+01 3.09781798e+01 5.11503911e+01]\n",
      "51-th iteration, loss: 0.008596069720420796, 19 gd steps\n",
      "insert gradient: -0.00015990174917426993\n",
      "51-th iteration, new layer inserted. now 55 layers\n",
      "[6.64680812e+01 1.16399935e+02 5.61067824e+01 8.46527153e+01\n",
      " 5.51447301e+01 1.06761580e+02 6.57881750e+01 1.04989897e+02\n",
      " 5.28733422e+01 8.37321781e+01 6.40516008e+01 1.09222743e+02\n",
      " 6.10151431e+01 1.65896533e+02 9.56665429e+01 1.06881680e+02\n",
      " 3.35661870e+01 5.32871428e+01 1.11881309e+02 2.29562667e+01\n",
      " 1.56472555e+02 2.18659557e-01 1.13496395e-03 2.08985168e+00\n",
      " 1.56478278e+02 1.35417805e+02 1.38488999e+02 1.11842616e+02\n",
      " 1.06464834e+02 1.15885593e+02 8.45226859e+01 1.17228950e+02\n",
      " 1.01874817e+02 1.00809657e+02 5.30971257e+01 1.55268875e+01\n",
      " 4.92629556e+01 9.53543582e+01 5.33237428e+01 7.12121542e+01\n",
      " 1.16406961e+01 8.97974738e+01 6.27393022e+01 1.05234447e+02\n",
      " 1.15102180e+02 8.81752008e+01 5.23012295e+01 5.12941883e+01\n",
      " 1.76766105e+01 8.87384848e+01 9.28953652e+01 9.01678580e+01\n",
      " 3.45145748e+01 3.09602672e+01 5.11457907e+01]\n",
      "52-th iteration, loss: 0.0046563094397820345, 115 gd steps\n",
      "insert gradient: -1.0020160480186154e-05\n",
      "52-th iteration, new layer inserted. now 55 layers\n",
      "[6.11080639e+01 1.13134927e+02 5.77328643e+01 8.63054168e+01\n",
      " 5.57767737e+01 1.07736827e+02 6.54458699e+01 1.04435896e+02\n",
      " 5.34931716e+01 8.64554895e+01 6.82891593e+01 1.02353210e+02\n",
      " 5.68541893e+01 1.63637249e+02 9.93059428e+01 1.02237937e+02\n",
      " 3.09584098e+01 5.83825181e+01 1.04235702e+02 5.50391339e+01\n",
      " 1.32139097e+02 7.50187969e+01 1.48388610e+02 1.40010106e+02\n",
      " 1.25396461e+02 1.17139062e+02 9.99501596e+01 1.18372118e+02\n",
      " 8.38140203e+01 1.16995677e+02 1.02238295e+02 1.01764372e+02\n",
      " 5.73563543e+01 1.16646282e+01 4.68404179e+01 9.69578420e+01\n",
      " 5.34077530e+01 6.92407340e+01 1.42828291e+01 9.08052324e+01\n",
      " 6.25143148e+01 1.03023623e+02 1.18732088e+02 8.91864403e+01\n",
      " 5.28792103e+01 5.05325932e+01 2.22659272e+01 8.74602750e+01\n",
      " 8.07985924e+01 9.08143294e+01 3.32540219e+01 0.00000000e+00\n",
      " 3.55271368e-15 2.79583206e+01 5.39444887e+01]\n",
      "53-th iteration, loss: 0.004187680591992536, 34 gd steps\n",
      "insert gradient: -4.3847339759917035e-05\n",
      "53-th iteration, new layer inserted. now 55 layers\n",
      "[ 61.65934908 112.14122704  56.99563153  87.50026788  55.74129878\n",
      " 107.29313236  64.25143654 103.66512595  53.90247925  87.68316367\n",
      "  68.59559615 101.32455244  58.07954196 161.58095586 101.37687089\n",
      "  97.2935195   30.4788077   62.16425546 101.05977366  66.68220646\n",
      " 125.87574741  83.97779621 113.87878907   0.          37.95959636\n",
      " 140.88331489 116.97630868 119.27184751  99.86491909 120.37506921\n",
      "  84.81020278 119.51121254  98.91717043 101.25630901  57.67207391\n",
      "  13.42796839  46.09001606  97.68355198  54.38846351  69.63837352\n",
      "  13.87294149  91.82065411  63.04472352 104.10760845 116.29949298\n",
      "  88.57757294  52.68010519  52.20504028  20.99219469  87.89855299\n",
      "  80.67328971  90.59349754  30.54254148  34.13348009  55.82720381]\n",
      "54-th iteration, loss: 0.003768481888712712, 46 gd steps\n",
      "insert gradient: -0.00010581001576778217\n",
      "54-th iteration, new layer inserted. now 57 layers\n",
      "[ 63.23652023 112.08321096  55.59847545  87.68052541  55.61295627\n",
      " 107.76924769  64.07454222 102.01985413  53.22837944  88.8303113\n",
      "  68.12133583  99.54686497  58.38402167 159.33922453 106.10335982\n",
      "  89.85058375  29.79088585  67.29932137  96.96558463  81.98044994\n",
      " 119.32696307  91.2528128  114.6810134    6.5807949   38.0826467\n",
      " 104.52685735   0.          34.84228578 107.363097   119.12567807\n",
      "  99.51755351 123.90326738  83.94529749 123.18122522  99.3177351\n",
      "  99.91134415  56.21917801  16.37458705  45.42763213  98.39779063\n",
      "  55.98539861  70.60526681  13.62004415  91.75495417  63.41445307\n",
      " 105.16602582 114.65174959  87.84390591  52.78179163  55.07629799\n",
      "  18.27054207  87.54810373  81.86312175  89.67798462  27.55262107\n",
      "  39.85726812  55.88655226]\n",
      "55-th iteration, loss: 0.003765608735588349, 10 gd steps\n",
      "insert gradient: -1.0734249564972514e-05\n",
      "55-th iteration, new layer inserted. now 59 layers\n",
      "[6.32368447e+01 1.12083603e+02 5.55994179e+01 8.76813631e+01\n",
      " 5.56141641e+01 1.07769616e+02 6.40745320e+01 1.02019828e+02\n",
      " 5.32284421e+01 8.88303172e+01 6.81204799e+01 9.95466442e+01\n",
      " 5.83842640e+01 1.59339022e+02 1.06106757e+02 8.98499799e+01\n",
      " 2.97943137e+01 6.73032217e+01 9.69684961e+01 0.00000000e+00\n",
      " 1.42108547e-14 8.19868737e+01 1.19333400e+02 9.12596033e+01\n",
      " 1.14691986e+02 6.58953325e+00 3.80945367e+01 1.04534867e+02\n",
      " 1.99921444e-02 3.48502959e+01 1.07374459e+02 1.19132511e+02\n",
      " 9.95282970e+01 1.23913130e+02 8.39557574e+01 1.23191675e+02\n",
      " 9.93314635e+01 9.99202354e+01 5.62343743e+01 1.63830979e+01\n",
      " 4.54411985e+01 9.84060411e+01 5.59972407e+01 7.06107340e+01\n",
      " 1.36309263e+01 9.17586183e+01 6.34193063e+01 1.05169485e+02\n",
      " 1.14656661e+02 8.78456237e+01 5.27836605e+01 5.50781027e+01\n",
      " 1.82724244e+01 8.75489492e+01 8.18650331e+01 8.96780459e+01\n",
      " 2.75524716e+01 3.98592819e+01 5.58862808e+01]\n",
      "56-th iteration, loss: 0.003711969122305652, 46 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.768265920226779e-06\n",
      "56-th iteration, new layer inserted. now 59 layers\n",
      "[6.31850854e+01 1.12089567e+02 5.57410798e+01 8.78983553e+01\n",
      " 5.58701628e+01 1.07834326e+02 6.40476870e+01 1.02039163e+02\n",
      " 5.32975061e+01 8.88997439e+01 6.78144784e+01 9.94446869e+01\n",
      " 5.82125131e+01 1.58877528e+02 0.00000000e+00 1.42108547e-14\n",
      " 1.07148072e+02 8.89351586e+01 2.99442184e+01 6.82736410e+01\n",
      " 9.62690333e+01 8.56497443e+01 1.18523751e+02 9.19577504e+01\n",
      " 1.14844165e+02 7.38646565e+00 3.77804691e+01 1.03846863e+02\n",
      " 1.41130840e-02 3.41621953e+01 1.07101751e+02 1.19006496e+02\n",
      " 9.90199606e+01 1.24946554e+02 8.35558026e+01 1.23839640e+02\n",
      " 9.96750046e+01 9.99899262e+01 5.62423737e+01 1.71756493e+01\n",
      " 4.53004047e+01 9.83488538e+01 5.58374280e+01 7.08129899e+01\n",
      " 1.38658829e+01 9.19407295e+01 6.36079749e+01 1.05322000e+02\n",
      " 1.14448378e+02 8.76920599e+01 5.24854209e+01 5.52943403e+01\n",
      " 1.79790788e+01 8.75742297e+01 8.20884908e+01 8.95001493e+01\n",
      " 2.72580474e+01 4.08303928e+01 5.56268873e+01]\n",
      "57-th iteration, loss: 0.0036653594406293685, 80 gd steps\n",
      "insert gradient: -1.4598338782346858e-05\n",
      "57-th iteration, new layer inserted. now 55 layers\n",
      "[ 63.01160399 112.02094608  55.86460457  88.08491358  55.92160627\n",
      " 107.87883523  64.15125718 102.15200303  53.42466056  89.06559396\n",
      "  67.594784    99.36748341  57.91159    158.19517592 108.56357073\n",
      "  87.8031868   29.89530948  69.34809295  95.59154837  87.51716091\n",
      " 117.71699848  92.75224525 115.2005946    8.49870781  37.47936744\n",
      " 135.95687671 106.83971189 118.50846055  98.35400383 126.33825125\n",
      "  83.16847126 124.66671651 100.21718    100.25622344  56.32377772\n",
      "  17.82120983  45.05549026  98.38170837  55.74996164  70.97908016\n",
      "  13.88876438  92.09583298  63.74414993 105.50134696 114.30604485\n",
      "  87.57337212  52.22800664  55.5271794   17.752067    87.64958599\n",
      "  82.2422792   89.31921863  26.97390548  41.77238278  55.26505539]\n",
      "58-th iteration, loss: 0.0035552789304331015, 63 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.504616079086112e-06\n",
      "58-th iteration, new layer inserted. now 57 layers\n",
      "[ 62.7525088  111.97000413  55.83614405  88.34422433  55.8987616\n",
      " 108.02089633  64.06116098 102.02554493  53.34046999  89.8185312\n",
      "  67.41074443  99.2486193   57.66649967 157.00514261  55.56498075\n",
      "   0.          55.56498075  83.8564289   29.70486449  72.52270101\n",
      "  93.4493888   93.46879168 115.58108405  94.97081968 116.03256674\n",
      "  10.40692531  36.14284894 132.82454658 107.28818957 114.71552193\n",
      "  96.59444372 130.97146495  81.2037856  127.26813425 101.03427991\n",
      " 102.12766014  57.69146117  18.01285029  43.67614544  98.62011145\n",
      "  55.29106264  71.40264918  13.74301119  91.49575015  63.44692978\n",
      " 106.518884   114.66449955  87.12929274  51.69206799  56.95875263\n",
      "  17.09600872  87.80671854  82.69229925  88.88718021  26.16332221\n",
      "  44.30942113  53.53435338]\n",
      "59-th iteration, loss: 0.003555278707065439, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.504455407789e-06\n",
      "59-th iteration, new layer inserted. now 59 layers\n",
      "[6.27525090e+01 1.11970004e+02 5.58361441e+01 8.83442245e+01\n",
      " 5.58987618e+01 1.08020897e+02 6.40611617e+01 1.02025545e+02\n",
      " 5.33404704e+01 8.98185315e+01 6.74107429e+01 9.92486183e+01\n",
      " 5.76664985e+01 1.57005142e+02 5.55649841e+01 0.00000000e+00\n",
      " 7.10542736e-15 9.50432840e-06 5.55649841e+01 8.38564260e+01\n",
      " 2.97048662e+01 7.25227050e+01 9.34493881e+01 9.34687969e+01\n",
      " 1.15581084e+02 9.49708227e+01 1.16032570e+02 1.04069291e+01\n",
      " 3.61428511e+01 1.32824545e+02 1.07288192e+02 1.14715518e+02\n",
      " 9.65944445e+01 1.30971470e+02 8.12037876e+01 1.27268138e+02\n",
      " 1.01034284e+02 1.02127664e+02 5.76914671e+01 1.80128523e+01\n",
      " 4.36761475e+01 9.86201135e+01 5.52910646e+01 7.14026510e+01\n",
      " 1.37430147e+01 9.14957507e+01 6.34469314e+01 1.06518886e+02\n",
      " 1.14664501e+02 8.71292927e+01 5.16920679e+01 5.69587538e+01\n",
      " 1.70960083e+01 8.78067188e+01 8.26922996e+01 8.88871801e+01\n",
      " 2.61633214e+01 4.43094231e+01 5.35343530e+01]\n",
      "60-th iteration, loss: 0.003555278427236905, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.504086031534968e-06\n",
      "60-th iteration, new layer inserted. now 59 layers\n",
      "[6.27525092e+01 1.11970005e+02 5.58361441e+01 8.83442247e+01\n",
      " 5.58987619e+01 1.08020897e+02 6.40611623e+01 1.02025546e+02\n",
      " 5.33404709e+01 8.98185319e+01 6.74107413e+01 9.92486172e+01\n",
      " 5.76664973e+01 1.57005141e+02 5.55649874e+01 9.50401840e-06\n",
      " 3.30926102e-06 1.90083468e-05 5.55649874e+01 8.38564232e+01\n",
      " 2.97048679e+01 7.25227090e+01 9.34493874e+01 9.34688021e+01\n",
      " 1.15581085e+02 9.49708257e+01 1.16032574e+02 1.04069329e+01\n",
      " 3.61428532e+01 1.32824543e+02 1.07288195e+02 1.14715514e+02\n",
      " 9.65944453e+01 1.30971476e+02 8.12037896e+01 1.27268141e+02\n",
      " 1.01034287e+02 1.02127668e+02 5.76914730e+01 1.80128542e+01\n",
      " 4.36761496e+01 9.86201155e+01 5.52910665e+01 7.14026528e+01\n",
      " 1.37430182e+01 9.14957512e+01 6.34469330e+01 1.06518889e+02\n",
      " 1.14664501e+02 8.71292926e+01 5.16920679e+01 5.69587551e+01\n",
      " 1.70960079e+01 8.78067191e+01 8.26923000e+01 8.88871799e+01\n",
      " 2.61633207e+01 4.43094250e+01 5.35343526e+01]\n",
      "61-th iteration, loss: 0.0035552781479241888, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.503716483850975e-06\n",
      "61-th iteration, new layer inserted. now 61 layers\n",
      "[6.27525093e+01 1.11970005e+02 5.58361442e+01 8.83442249e+01\n",
      " 5.58987621e+01 1.08020898e+02 6.40611630e+01 1.02025546e+02\n",
      " 5.33404713e+01 8.98185322e+01 6.74107398e+01 9.92486162e+01\n",
      " 5.76664962e+01 1.57005140e+02 5.55649907e+01 1.90076670e-05\n",
      " 6.61804005e-06 0.00000000e+00 8.47032947e-22 2.85119962e-05\n",
      " 5.55649907e+01 8.38564203e+01 2.97048696e+01 7.25227130e+01\n",
      " 9.34493868e+01 9.34688073e+01 1.15581085e+02 9.49708287e+01\n",
      " 1.16032577e+02 1.04069367e+01 3.61428554e+01 1.32824542e+02\n",
      " 1.07288197e+02 1.14715510e+02 9.65944460e+01 1.30971481e+02\n",
      " 8.12037916e+01 1.27268144e+02 1.01034291e+02 1.02127673e+02\n",
      " 5.76914789e+01 1.80128562e+01 4.36761517e+01 9.86201176e+01\n",
      " 5.52910684e+01 7.14026546e+01 1.37430217e+01 9.14957517e+01\n",
      " 6.34469346e+01 1.06518891e+02 1.14664502e+02 8.71292926e+01\n",
      " 5.16920678e+01 5.69587563e+01 1.70960075e+01 8.78067194e+01\n",
      " 8.26923004e+01 8.88871798e+01 2.61633199e+01 4.43094270e+01\n",
      " 5.35343522e+01]\n",
      "62-th iteration, loss: 0.0035552778121667487, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.503138315977587e-06\n",
      "62-th iteration, new layer inserted. now 61 layers\n",
      "[6.27525095e+01 1.11970005e+02 5.58361442e+01 8.83442250e+01\n",
      " 5.58987623e+01 1.08020898e+02 6.40611637e+01 1.02025547e+02\n",
      " 5.33404718e+01 8.98185325e+01 6.74107382e+01 9.92486152e+01\n",
      " 5.76664950e+01 1.57005139e+02 5.55649940e+01 2.85107961e-05\n",
      " 9.92617015e-06 9.50313058e-06 3.30813010e-06 3.80151268e-05\n",
      " 5.55649940e+01 8.38564174e+01 2.97048713e+01 7.25227170e+01\n",
      " 9.34493861e+01 9.34688126e+01 1.15581085e+02 9.49708316e+01\n",
      " 1.16032580e+02 1.04069405e+01 3.61428575e+01 1.32824540e+02\n",
      " 1.07288200e+02 1.14715507e+02 9.65944467e+01 1.30971487e+02\n",
      " 8.12037935e+01 1.27268147e+02 1.01034295e+02 1.02127677e+02\n",
      " 5.76914847e+01 1.80128582e+01 4.36761537e+01 9.86201196e+01\n",
      " 5.52910703e+01 7.14026565e+01 1.37430252e+01 9.14957522e+01\n",
      " 6.34469362e+01 1.06518893e+02 1.14664503e+02 8.71292925e+01\n",
      " 5.16920677e+01 5.69587575e+01 1.70960072e+01 8.78067197e+01\n",
      " 8.26923008e+01 8.88871797e+01 2.61633191e+01 4.43094289e+01\n",
      " 5.35343518e+01]\n",
      "63-th iteration, loss: 0.003555277476932236, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.502560035186276e-06\n",
      "63-th iteration, new layer inserted. now 63 layers\n",
      "[6.27525097e+01 1.11970006e+02 5.58361443e+01 8.83442252e+01\n",
      " 5.58987624e+01 1.08020898e+02 6.40611644e+01 1.02025547e+02\n",
      " 5.33404722e+01 8.98185328e+01 6.74107367e+01 9.92486141e+01\n",
      " 5.76664938e+01 1.57005139e+02 5.55649973e+01 3.80133458e-05\n",
      " 1.32335810e-05 1.90056826e-05 6.61553870e-06 0.00000000e+00\n",
      " 8.47032947e-22 4.75176796e-05 5.55649973e+01 8.38564146e+01\n",
      " 2.97048730e+01 7.25227210e+01 9.34493854e+01 9.34688178e+01\n",
      " 1.15581085e+02 9.49708346e+01 1.16032584e+02 1.04069443e+01\n",
      " 3.61428596e+01 1.32824538e+02 1.07288203e+02 1.14715503e+02\n",
      " 9.65944475e+01 1.30971492e+02 8.12037955e+01 1.27268151e+02\n",
      " 1.01034298e+02 1.02127681e+02 5.76914906e+01 1.80128601e+01\n",
      " 4.36761557e+01 9.86201216e+01 5.52910722e+01 7.14026583e+01\n",
      " 1.37430287e+01 9.14957527e+01 6.34469378e+01 1.06518896e+02\n",
      " 1.14664504e+02 8.71292925e+01 5.16920677e+01 5.69587587e+01\n",
      " 1.70960068e+01 8.78067200e+01 8.26923012e+01 8.88871795e+01\n",
      " 2.61633183e+01 4.43094309e+01 5.35343514e+01]\n",
      "64-th iteration, loss: 0.0035552770852818015, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.501773250582878e-06\n",
      "64-th iteration, new layer inserted. now 65 layers\n",
      "[6.27525099e+01 1.11970006e+02 5.58361443e+01 8.83442254e+01\n",
      " 5.58987626e+01 1.08020899e+02 6.40611650e+01 1.02025548e+02\n",
      " 5.33404726e+01 8.98185332e+01 6.74107351e+01 9.92486131e+01\n",
      " 5.76664927e+01 1.57005138e+02 5.55650006e+01 4.75151666e-05\n",
      " 1.65401056e-05 2.85075064e-05 9.92205895e-06 9.50182534e-06\n",
      " 3.30652025e-06 0.00000000e+00 4.23516474e-22 5.70195050e-05\n",
      " 5.55650006e+01 8.38564117e+01 2.97048747e+01 7.25227250e+01\n",
      " 9.34493847e+01 9.34688230e+01 1.15581086e+02 9.49708376e+01\n",
      " 1.16032587e+02 1.04069480e+01 3.61428617e+01 1.32824536e+02\n",
      " 1.07288205e+02 1.14715499e+02 9.65944482e+01 1.30971498e+02\n",
      " 8.12037974e+01 1.27268154e+02 1.01034302e+02 1.02127685e+02\n",
      " 5.76914964e+01 1.80128621e+01 4.36761578e+01 9.86201236e+01\n",
      " 5.52910741e+01 7.14026601e+01 1.37430322e+01 9.14957532e+01\n",
      " 6.34469394e+01 1.06518898e+02 1.14664505e+02 8.71292924e+01\n",
      " 5.16920676e+01 5.69587599e+01 1.70960064e+01 8.78067203e+01\n",
      " 8.26923016e+01 8.88871794e+01 2.61633176e+01 4.43094328e+01\n",
      " 5.35343510e+01]\n",
      "65-th iteration, loss: 0.003555276637248268, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.500778078124973e-06\n",
      "65-th iteration, new layer inserted. now 65 layers\n",
      "[6.27525100e+01 1.11970006e+02 5.58361444e+01 8.83442256e+01\n",
      " 5.58987627e+01 1.08020899e+02 6.40611657e+01 1.02025548e+02\n",
      " 5.33404731e+01 8.98185335e+01 6.74107336e+01 9.92486121e+01\n",
      " 5.76664915e+01 1.57005137e+02 5.55650039e+01 5.70160491e-05\n",
      " 1.98455070e-05 3.80083928e-05 1.32274537e-05 1.90027140e-05\n",
      " 6.61191283e-06 9.50088943e-06 3.30539258e-06 6.65203944e-05\n",
      " 5.55650039e+01 8.38564088e+01 2.97048763e+01 7.25227290e+01\n",
      " 9.34493840e+01 9.34688282e+01 1.15581086e+02 9.49708406e+01\n",
      " 1.16032591e+02 1.04069518e+01 3.61428637e+01 1.32824535e+02\n",
      " 1.07288208e+02 1.14715495e+02 9.65944489e+01 1.30971503e+02\n",
      " 8.12037993e+01 1.27268157e+02 1.01034306e+02 1.02127689e+02\n",
      " 5.76915023e+01 1.80128640e+01 4.36761598e+01 9.86201256e+01\n",
      " 5.52910760e+01 7.14026619e+01 1.37430357e+01 9.14957537e+01\n",
      " 6.34469410e+01 1.06518900e+02 1.14664506e+02 8.71292924e+01\n",
      " 5.16920675e+01 5.69587611e+01 1.70960060e+01 8.78067205e+01\n",
      " 8.26923020e+01 8.88871793e+01 2.61633168e+01 4.43094348e+01\n",
      " 5.35343506e+01]\n",
      "66-th iteration, loss: 0.003555276189774698, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.499782956335518e-06\n",
      "66-th iteration, new layer inserted. now 65 layers\n",
      "[6.27525102e+01 1.11970006e+02 5.58361444e+01 8.83442258e+01\n",
      " 5.58987629e+01 1.08020900e+02 6.40611664e+01 1.02025548e+02\n",
      " 5.33404735e+01 8.98185338e+01 6.74107320e+01 9.92486110e+01\n",
      " 5.76664903e+01 1.57005136e+02 5.55650072e+01 6.65159339e-05\n",
      " 2.31497150e-05 4.75082822e-05 1.65316529e-05 2.85026065e-05\n",
      " 9.91610759e-06 1.90007834e-05 6.60958513e-06 7.60202892e-05\n",
      " 5.55650072e+01 8.38564060e+01 2.97048780e+01 7.25227330e+01\n",
      " 9.34493833e+01 9.34688334e+01 1.15581086e+02 9.49708435e+01\n",
      " 1.16032594e+02 1.04069556e+01 3.61428658e+01 1.32824533e+02\n",
      " 1.07288210e+02 1.14715491e+02 9.65944496e+01 1.30971508e+02\n",
      " 8.12038013e+01 1.27268161e+02 1.01034309e+02 1.02127693e+02\n",
      " 5.76915081e+01 1.80128659e+01 4.36761618e+01 9.86201276e+01\n",
      " 5.52910778e+01 7.14026637e+01 1.37430391e+01 9.14957542e+01\n",
      " 6.34469425e+01 1.06518903e+02 1.14664507e+02 8.71292923e+01\n",
      " 5.16920675e+01 5.69587623e+01 1.70960056e+01 8.78067208e+01\n",
      " 8.26923024e+01 8.88871791e+01 2.61633160e+01 4.43094367e+01\n",
      " 5.35343502e+01]\n",
      "67-th iteration, loss: 0.0035552757428575117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.498787885858238e-06\n",
      "67-th iteration, new layer inserted. now 65 layers\n",
      "[6.27525104e+01 1.11970007e+02 5.58361445e+01 8.83442260e+01\n",
      " 5.58987631e+01 1.08020900e+02 6.40611671e+01 1.02025549e+02\n",
      " 5.33404740e+01 8.98185342e+01 6.74107305e+01 9.92486100e+01\n",
      " 5.76664892e+01 1.57005135e+02 5.55650105e+01 7.60148210e-05\n",
      " 2.64527299e-05 5.70071747e-05 1.98346567e-05 3.80015028e-05\n",
      " 1.32191048e-05 2.84996821e-05 9.91257789e-06 8.55191894e-05\n",
      " 5.55650105e+01 8.38564031e+01 2.97048797e+01 7.25227370e+01\n",
      " 9.34493826e+01 9.34688386e+01 1.15581086e+02 9.49708465e+01\n",
      " 1.16032597e+02 1.04069593e+01 3.61428679e+01 1.32824531e+02\n",
      " 1.07288213e+02 1.14715487e+02 9.65944503e+01 1.30971514e+02\n",
      " 8.12038032e+01 1.27268164e+02 1.01034313e+02 1.02127698e+02\n",
      " 5.76915139e+01 1.80128679e+01 4.36761638e+01 9.86201296e+01\n",
      " 5.52910796e+01 7.14026655e+01 1.37430426e+01 9.14957546e+01\n",
      " 6.34469441e+01 1.06518905e+02 1.14664508e+02 8.71292923e+01\n",
      " 5.16920674e+01 5.69587635e+01 1.70960052e+01 8.78067211e+01\n",
      " 8.26923028e+01 8.88871790e+01 2.61633152e+01 4.43094387e+01\n",
      " 5.35343498e+01]\n",
      "68-th iteration, loss: 0.0035552752964931396, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.497792867333666e-06\n",
      "68-th iteration, new layer inserted. now 65 layers\n",
      "[6.27525106e+01 1.11970007e+02 5.58361445e+01 8.83442261e+01\n",
      " 5.58987632e+01 1.08020900e+02 6.40611677e+01 1.02025549e+02\n",
      " 5.33404744e+01 8.98185345e+01 6.74107289e+01 9.92486090e+01\n",
      " 5.76664880e+01 1.57005134e+02 5.55650138e+01 8.55127105e-05\n",
      " 2.97545518e-05 6.65050704e-05 2.31364654e-05 4.74994031e-05\n",
      " 1.65209046e-05 3.79975854e-05 1.32143711e-05 9.50170950e-05\n",
      " 5.55650138e+01 8.38564002e+01 2.97048814e+01 7.25227410e+01\n",
      " 9.34493819e+01 9.34688438e+01 1.15581087e+02 9.49708495e+01\n",
      " 1.16032601e+02 1.04069631e+01 3.61428699e+01 1.32824530e+02\n",
      " 1.07288215e+02 1.14715483e+02 9.65944510e+01 1.30971519e+02\n",
      " 8.12038051e+01 1.27268167e+02 1.01034317e+02 1.02127702e+02\n",
      " 5.76915197e+01 1.80128698e+01 4.36761658e+01 9.86201316e+01\n",
      " 5.52910815e+01 7.14026672e+01 1.37430460e+01 9.14957551e+01\n",
      " 6.34469457e+01 1.06518907e+02 1.14664509e+02 8.71292922e+01\n",
      " 5.16920673e+01 5.69587647e+01 1.70960048e+01 8.78067214e+01\n",
      " 8.26923031e+01 8.88871789e+01 2.61633145e+01 4.43094406e+01\n",
      " 5.35343494e+01]\n",
      "69-th iteration, loss: 0.0035552748506780373, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.496797901436153e-06\n",
      "69-th iteration, new layer inserted. now 67 layers\n",
      "[6.27525108e+01 1.11970007e+02 5.58361446e+01 8.83442263e+01\n",
      " 5.58987634e+01 1.08020901e+02 6.40611684e+01 1.02025550e+02\n",
      " 5.33404749e+01 8.98185348e+01 6.74107274e+01 9.92486080e+01\n",
      " 5.76664868e+01 1.57005134e+02 5.55650171e+01 9.50096025e-05\n",
      " 3.30551810e-05 7.60019692e-05 2.64370792e-05 5.69963073e-05\n",
      " 1.98215074e-05 4.74944935e-05 1.65149651e-05 0.00000000e+00\n",
      " 1.69406589e-21 1.04514006e-04 5.55650171e+01 8.38563974e+01\n",
      " 2.97048831e+01 7.25227450e+01 9.34493812e+01 9.34688490e+01\n",
      " 1.15581087e+02 9.49708524e+01 1.16032604e+02 1.04069668e+01\n",
      " 3.61428720e+01 1.32824528e+02 1.07288218e+02 1.14715480e+02\n",
      " 9.65944516e+01 1.30971525e+02 8.12038070e+01 1.27268170e+02\n",
      " 1.01034320e+02 1.02127706e+02 5.76915255e+01 1.80128717e+01\n",
      " 4.36761677e+01 9.86201335e+01 5.52910833e+01 7.14026690e+01\n",
      " 1.37430495e+01 9.14957556e+01 6.34469473e+01 1.06518910e+02\n",
      " 1.14664510e+02 8.71292921e+01 5.16920672e+01 5.69587660e+01\n",
      " 1.70960044e+01 8.78067217e+01 8.26923035e+01 8.88871787e+01\n",
      " 2.61633137e+01 4.43094426e+01 5.35343490e+01]\n",
      "70-th iteration, loss: 0.0035552743485682673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.495594846882729e-06\n",
      "70-th iteration, new layer inserted. now 69 layers\n",
      "[6.27525109e+01 1.11970007e+02 5.58361446e+01 8.83442265e+01\n",
      " 5.58987636e+01 1.08020901e+02 6.40611691e+01 1.02025550e+02\n",
      " 5.33404753e+01 8.98185352e+01 6.74107258e+01 9.92486069e+01\n",
      " 5.76664856e+01 1.57005133e+02 5.55650204e+01 1.04505347e-04\n",
      " 3.63544511e-05 8.54977219e-05 2.97363316e-05 6.64920661e-05\n",
      " 2.31207466e-05 5.69902569e-05 1.98141932e-05 9.49576724e-06\n",
      " 3.29922818e-06 0.00000000e+00 4.23516474e-22 1.14009773e-04\n",
      " 5.55650204e+01 8.38563945e+01 2.97048848e+01 7.25227490e+01\n",
      " 9.34493805e+01 9.34688542e+01 1.15581087e+02 9.49708554e+01\n",
      " 1.16032607e+02 1.04069706e+01 3.61428740e+01 1.32824526e+02\n",
      " 1.07288220e+02 1.14715476e+02 9.65944523e+01 1.30971530e+02\n",
      " 8.12038089e+01 1.27268173e+02 1.01034324e+02 1.02127710e+02\n",
      " 5.76915313e+01 1.80128737e+01 4.36761697e+01 9.86201355e+01\n",
      " 5.52910851e+01 7.14026708e+01 1.37430529e+01 9.14957561e+01\n",
      " 6.34469488e+01 1.06518912e+02 1.14664511e+02 8.71292921e+01\n",
      " 5.16920671e+01 5.69587672e+01 1.70960040e+01 8.78067220e+01\n",
      " 8.26923039e+01 8.88871786e+01 2.61633129e+01 4.43094445e+01\n",
      " 5.35343486e+01]\n",
      "71-th iteration, loss: 0.0035552737902155114, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.494183866799908e-06\n",
      "71-th iteration, new layer inserted. now 71 layers\n",
      "[6.27525111e+01 1.11970008e+02 5.58361447e+01 8.83442267e+01\n",
      " 5.58987637e+01 1.08020902e+02 6.40611698e+01 1.02025551e+02\n",
      " 5.33404757e+01 8.98185355e+01 6.74107243e+01 9.92486059e+01\n",
      " 5.76664845e+01 1.57005132e+02 5.55650237e+01 1.13999737e-04\n",
      " 3.96521255e-05 9.49921196e-05 3.30339861e-05 7.59864707e-05\n",
      " 2.64183856e-05 6.64846669e-05 2.31118190e-05 1.89901818e-05\n",
      " 6.59685179e-06 9.49441536e-06 3.29762361e-06 0.00000000e+00\n",
      " 4.23516474e-22 1.23504189e-04 5.55650237e+01 8.38563916e+01\n",
      " 2.97048864e+01 7.25227530e+01 9.34493798e+01 9.34688594e+01\n",
      " 1.15581087e+02 9.49708583e+01 1.16032611e+02 1.04069743e+01\n",
      " 3.61428760e+01 1.32824524e+02 1.07288223e+02 1.14715472e+02\n",
      " 9.65944529e+01 1.30971535e+02 8.12038107e+01 1.27268177e+02\n",
      " 1.01034328e+02 1.02127714e+02 5.76915370e+01 1.80128756e+01\n",
      " 4.36761716e+01 9.86201375e+01 5.52910869e+01 7.14026726e+01\n",
      " 1.37430564e+01 9.14957566e+01 6.34469504e+01 1.06518914e+02\n",
      " 1.14664512e+02 8.71292920e+01 5.16920671e+01 5.69587684e+01\n",
      " 1.70960036e+01 8.78067222e+01 8.26923043e+01 8.88871785e+01\n",
      " 2.61633121e+01 4.43094465e+01 5.35343482e+01]\n",
      "72-th iteration, loss: 0.0035552731756807584, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.492565147863788e-06\n",
      "72-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525113e+01 1.11970008e+02 5.58361447e+01 8.83442269e+01\n",
      " 5.58987639e+01 1.08020902e+02 6.40611704e+01 1.02025551e+02\n",
      " 5.33404762e+01 8.98185358e+01 6.74107227e+01 9.92486049e+01\n",
      " 5.76664833e+01 1.57005131e+02 5.55650270e+01 1.23492562e-04\n",
      " 4.29479677e-05 1.04484954e-04 3.63298063e-05 8.54793126e-05\n",
      " 2.97141881e-05 7.59775149e-05 2.64076061e-05 2.84830352e-05\n",
      " 9.89263443e-06 1.89872703e-05 6.59340404e-06 9.49285568e-06\n",
      " 3.29578044e-06 0.00000000e+00 4.23516474e-22 1.32997044e-04\n",
      " 5.55650270e+01 8.38563888e+01 2.97048881e+01 7.25227570e+01\n",
      " 9.34493791e+01 9.34688646e+01 1.15581087e+02 9.49708612e+01\n",
      " 1.16032614e+02 1.04069781e+01 3.61428780e+01 1.32824523e+02\n",
      " 1.07288225e+02 1.14715468e+02 9.65944536e+01 1.30971541e+02\n",
      " 8.12038126e+01 1.27268180e+02 1.01034331e+02 1.02127718e+02\n",
      " 5.76915428e+01 1.80128775e+01 4.36761736e+01 9.86201394e+01\n",
      " 5.52910887e+01 7.14026744e+01 1.37430598e+01 9.14957571e+01\n",
      " 6.34469519e+01 1.06518917e+02 1.14664513e+02 8.71292919e+01\n",
      " 5.16920670e+01 5.69587696e+01 1.70960032e+01 8.78067225e+01\n",
      " 8.26923047e+01 8.88871783e+01 2.61633114e+01 4.43094484e+01\n",
      " 5.35343478e+01]\n",
      "73-th iteration, loss: 0.0035552725050342435, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.490738900351487e-06\n",
      "73-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525115e+01 1.11970008e+02 5.58361448e+01 8.83442270e+01\n",
      " 5.58987640e+01 1.08020902e+02 6.40611711e+01 1.02025552e+02\n",
      " 5.33404766e+01 8.98185361e+01 6.74107212e+01 9.92486038e+01\n",
      " 5.76664821e+01 1.57005130e+02 5.55650303e+01 1.32983614e-04\n",
      " 4.62417416e-05 1.13976016e-04 3.96235559e-05 9.49703834e-05\n",
      " 3.30079179e-05 8.54685926e-05 2.97013182e-05 3.79741190e-05\n",
      " 1.31863399e-05 2.84783564e-05 9.88710515e-06 1.89839433e-05\n",
      " 6.58947933e-06 9.49108841e-06 3.29369890e-06 1.42488133e-04\n",
      " 5.55650303e+01 8.38563859e+01 2.97048898e+01 7.25227610e+01\n",
      " 9.34493783e+01 9.34688698e+01 1.15581088e+02 9.49708642e+01\n",
      " 1.16032617e+02 1.04069818e+01 3.61428800e+01 1.32824521e+02\n",
      " 1.07288228e+02 1.14715464e+02 9.65944542e+01 1.30971546e+02\n",
      " 8.12038144e+01 1.27268183e+02 1.01034335e+02 1.02127722e+02\n",
      " 5.76915485e+01 1.80128794e+01 4.36761755e+01 9.86201413e+01\n",
      " 5.52910905e+01 7.14026761e+01 1.37430633e+01 9.14957575e+01\n",
      " 6.34469535e+01 1.06518919e+02 1.14664514e+02 8.71292919e+01\n",
      " 5.16920669e+01 5.69587708e+01 1.70960028e+01 8.78067228e+01\n",
      " 8.26923050e+01 8.88871782e+01 2.61633106e+01 4.43094504e+01\n",
      " 5.35343474e+01]\n",
      "74-th iteration, loss: 0.003555271835089665, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.488913220039958e-06\n",
      "74-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525117e+01 1.11970009e+02 5.58361448e+01 8.83442272e+01\n",
      " 5.58987642e+01 1.08020903e+02 6.40611718e+01 1.02025552e+02\n",
      " 5.33404771e+01 8.98185365e+01 6.74107196e+01 9.92486028e+01\n",
      " 5.76664809e+01 1.57005130e+02 5.55650336e+01 1.42472835e-04\n",
      " 4.95333778e-05 1.23465248e-04 4.29151656e-05 1.04459624e-04\n",
      " 3.62995055e-05 9.49578411e-05 3.29928860e-05 4.74633744e-05\n",
      " 1.64778989e-05 3.79676148e-05 1.31786575e-05 2.84732041e-05\n",
      " 9.88102728e-06 1.89803507e-05 6.58524464e-06 1.51977396e-04\n",
      " 5.55650336e+01 8.38563830e+01 2.97048915e+01 7.25227649e+01\n",
      " 9.34493776e+01 9.34688750e+01 1.15581088e+02 9.49708671e+01\n",
      " 1.16032620e+02 1.04069855e+01 3.61428820e+01 1.32824519e+02\n",
      " 1.07288230e+02 1.14715460e+02 9.65944548e+01 1.30971552e+02\n",
      " 8.12038163e+01 1.27268186e+02 1.01034338e+02 1.02127726e+02\n",
      " 5.76915543e+01 1.80128813e+01 4.36761774e+01 9.86201433e+01\n",
      " 5.52910922e+01 7.14026779e+01 1.37430667e+01 9.14957580e+01\n",
      " 6.34469551e+01 1.06518921e+02 1.14664515e+02 8.71292918e+01\n",
      " 5.16920668e+01 5.69587720e+01 1.70960023e+01 8.78067231e+01\n",
      " 8.26923054e+01 8.88871780e+01 2.61633098e+01 4.43094523e+01\n",
      " 5.35343470e+01]\n",
      "75-th iteration, loss: 0.003555271165843514, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.487088107269118e-06\n",
      "75-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525118e+01 1.11970009e+02 5.58361449e+01 8.83442274e+01\n",
      " 5.58987644e+01 1.08020903e+02 6.40611725e+01 1.02025553e+02\n",
      " 5.33404775e+01 8.98185368e+01 6.74107181e+01 9.92486018e+01\n",
      " 5.76664798e+01 1.57005129e+02 5.55650369e+01 1.51960225e-04\n",
      " 5.28228771e-05 1.32952649e-04 4.62046362e-05 1.13947036e-04\n",
      " 3.95889519e-05 1.04445261e-04 3.62823103e-05 5.69508019e-05\n",
      " 1.97673122e-05 4.74550461e-05 1.64680620e-05 3.79606384e-05\n",
      " 1.31704251e-05 2.84677873e-05 9.87463807e-06 1.61464834e-04\n",
      " 5.55650369e+01 8.38563801e+01 2.97048932e+01 7.25227689e+01\n",
      " 9.34493769e+01 9.34688802e+01 1.15581088e+02 9.49708700e+01\n",
      " 1.16032624e+02 1.04069893e+01 3.61428840e+01 1.32824517e+02\n",
      " 1.07288232e+02 1.14715456e+02 9.65944554e+01 1.30971557e+02\n",
      " 8.12038181e+01 1.27268190e+02 1.01034342e+02 1.02127730e+02\n",
      " 5.76915600e+01 1.80128832e+01 4.36761793e+01 9.86201452e+01\n",
      " 5.52910940e+01 7.14026797e+01 1.37430701e+01 9.14957585e+01\n",
      " 6.34469566e+01 1.06518923e+02 1.14664516e+02 8.71292917e+01\n",
      " 5.16920667e+01 5.69587732e+01 1.70960019e+01 8.78067234e+01\n",
      " 8.26923058e+01 8.88871779e+01 2.61633091e+01 4.43094543e+01\n",
      " 5.35343466e+01]\n",
      "76-th iteration, loss: 0.0035552704972923303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.485263562380953e-06\n",
      "76-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525120e+01 1.11970009e+02 5.58361449e+01 8.83442276e+01\n",
      " 5.58987645e+01 1.08020904e+02 6.40611731e+01 1.02025553e+02\n",
      " 5.33404780e+01 8.98185371e+01 6.74107165e+01 9.92486007e+01\n",
      " 5.76664786e+01 1.57005128e+02 5.55650402e+01 1.61445784e-04\n",
      " 5.61102403e-05 1.42438221e-04 4.94919686e-05 1.23432618e-04\n",
      " 4.28762578e-05 1.13930853e-04 3.95695920e-05 6.64364021e-05\n",
      " 2.30545806e-05 5.69406509e-05 1.97553194e-05 4.74462470e-05\n",
      " 1.64576737e-05 3.79533990e-05 1.31618800e-05 1.70950448e-04\n",
      " 5.55650402e+01 8.38563773e+01 2.97048948e+01 7.25227729e+01\n",
      " 9.34493762e+01 9.34688854e+01 1.15581088e+02 9.49708729e+01\n",
      " 1.16032627e+02 1.04069930e+01 3.61428860e+01 1.32824515e+02\n",
      " 1.07288235e+02 1.14715452e+02 9.65944560e+01 1.30971562e+02\n",
      " 8.12038199e+01 1.27268193e+02 1.01034345e+02 1.02127734e+02\n",
      " 5.76915657e+01 1.80128851e+01 4.36761812e+01 9.86201471e+01\n",
      " 5.52910957e+01 7.14026814e+01 1.37430735e+01 9.14957590e+01\n",
      " 6.34469581e+01 1.06518926e+02 1.14664516e+02 8.71292916e+01\n",
      " 5.16920666e+01 5.69587744e+01 1.70960015e+01 8.78067236e+01\n",
      " 8.26923062e+01 8.88871778e+01 2.61633083e+01 4.43094562e+01\n",
      " 5.35343462e+01]\n",
      "77-th iteration, loss: 0.0035552698294325723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.483439585743315e-06\n",
      "77-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525122e+01 1.11970009e+02 5.58361450e+01 8.83442278e+01\n",
      " 5.58987647e+01 1.08020904e+02 6.40611738e+01 1.02025553e+02\n",
      " 5.33404784e+01 8.98185375e+01 6.74107149e+01 9.92485997e+01\n",
      " 5.76664774e+01 1.57005127e+02 5.55650435e+01 1.70929514e-04\n",
      " 5.93954684e-05 1.51921964e-04 5.27771636e-05 1.32916372e-04\n",
      " 4.61614241e-05 1.23414617e-04 4.28547318e-05 7.59201755e-05\n",
      " 2.63397050e-05 6.64244297e-05 2.30404306e-05 5.69300304e-05\n",
      " 1.97427739e-05 4.74371862e-05 1.64469714e-05 1.80434238e-04\n",
      " 5.55650435e+01 8.38563744e+01 2.97048965e+01 7.25227769e+01\n",
      " 9.34493754e+01 9.34688905e+01 1.15581088e+02 9.49708759e+01\n",
      " 1.16032630e+02 1.04069967e+01 3.61428879e+01 1.32824514e+02\n",
      " 1.07288237e+02 1.14715448e+02 9.65944566e+01 1.30971568e+02\n",
      " 8.12038218e+01 1.27268196e+02 1.01034349e+02 1.02127738e+02\n",
      " 5.76915714e+01 1.80128870e+01 4.36761831e+01 9.86201490e+01\n",
      " 5.52910974e+01 7.14026832e+01 1.37430769e+01 9.14957594e+01\n",
      " 6.34469597e+01 1.06518928e+02 1.14664517e+02 8.71292916e+01\n",
      " 5.16920665e+01 5.69587756e+01 1.70960011e+01 8.78067239e+01\n",
      " 8.26923066e+01 8.88871776e+01 2.61633075e+01 4.43094582e+01\n",
      " 5.35343458e+01]\n",
      "78-th iteration, loss: 0.003555269162260836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.481616177664584e-06\n",
      "78-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525124e+01 1.11970010e+02 5.58361450e+01 8.83442279e+01\n",
      " 5.58987649e+01 1.08020904e+02 6.40611745e+01 1.02025554e+02\n",
      " 5.33404788e+01 8.98185378e+01 6.74107134e+01 9.92485986e+01\n",
      " 5.76664762e+01 1.57005126e+02 5.55650467e+01 1.80411415e-04\n",
      " 6.26785622e-05 1.61403878e-04 5.60602220e-05 1.42398299e-04\n",
      " 4.94444517e-05 1.32896554e-04 4.61377307e-05 8.54021228e-05\n",
      " 2.96226863e-05 7.59063831e-05 2.63233964e-05 6.64119891e-05\n",
      " 2.30257265e-05 5.69191495e-05 1.97299130e-05 1.89916205e-04\n",
      " 5.55650467e+01 8.38563715e+01 2.97048982e+01 7.25227809e+01\n",
      " 9.34493747e+01 9.34688957e+01 1.15581088e+02 9.49708788e+01\n",
      " 1.16032634e+02 1.04070004e+01 3.61428899e+01 1.32824512e+02\n",
      " 1.07288240e+02 1.14715444e+02 9.65944572e+01 1.30971573e+02\n",
      " 8.12038236e+01 1.27268199e+02 1.01034353e+02 1.02127742e+02\n",
      " 5.76915770e+01 1.80128889e+01 4.36761850e+01 9.86201509e+01\n",
      " 5.52910991e+01 7.14026850e+01 1.37430803e+01 9.14957599e+01\n",
      " 6.34469612e+01 1.06518930e+02 1.14664518e+02 8.71292915e+01\n",
      " 5.16920664e+01 5.69587768e+01 1.70960007e+01 8.78067242e+01\n",
      " 8.26923069e+01 8.88871775e+01 2.61633067e+01 4.43094601e+01\n",
      " 5.35343454e+01]\n",
      "79-th iteration, loss: 0.003555268495773751, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.479793338500186e-06\n",
      "79-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525125e+01 1.11970010e+02 5.58361451e+01 8.83442281e+01\n",
      " 5.58987650e+01 1.08020905e+02 6.40611752e+01 1.02025554e+02\n",
      " 5.33404793e+01 8.98185381e+01 6.74107118e+01 9.92485976e+01\n",
      " 5.76664750e+01 1.57005125e+02 5.55650500e+01 1.89891487e-04\n",
      " 6.59595224e-05 1.70883965e-04 5.93411448e-05 1.51878398e-04\n",
      " 5.27253414e-05 1.42376665e-04 4.94185895e-05 9.48822444e-05\n",
      " 3.29035252e-05 8.53865116e-05 2.96042177e-05 7.58921238e-05\n",
      " 2.63065324e-05 6.63992895e-05 2.30107057e-05 1.99396350e-04\n",
      " 5.55650500e+01 8.38563686e+01 2.97048998e+01 7.25227849e+01\n",
      " 9.34493740e+01 9.34689009e+01 1.15581089e+02 9.49708817e+01\n",
      " 1.16032637e+02 1.04070041e+01 3.61428918e+01 1.32824510e+02\n",
      " 1.07288242e+02 1.14715440e+02 9.65944577e+01 1.30971578e+02\n",
      " 8.12038254e+01 1.27268202e+02 1.01034356e+02 1.02127746e+02\n",
      " 5.76915827e+01 1.80128908e+01 4.36761869e+01 9.86201528e+01\n",
      " 5.52911008e+01 7.14026867e+01 1.37430837e+01 9.14957604e+01\n",
      " 6.34469628e+01 1.06518933e+02 1.14664519e+02 8.71292914e+01\n",
      " 5.16920663e+01 5.69587780e+01 1.70960003e+01 8.78067245e+01\n",
      " 8.26923073e+01 8.88871774e+01 2.61633060e+01 4.43094621e+01\n",
      " 5.35343450e+01]\n",
      "80-th iteration, loss: 0.0035552678299678453, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.477971068589063e-06\n",
      "80-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525127e+01 1.11970010e+02 5.58361452e+01 8.83442283e+01\n",
      " 5.58987652e+01 1.08020905e+02 6.40611758e+01 1.02025555e+02\n",
      " 5.33404797e+01 8.98185384e+01 6.74107103e+01 9.92485966e+01\n",
      " 5.76664738e+01 1.57005125e+02 5.55650533e+01 1.99369730e-04\n",
      " 6.92383500e-05 1.80362224e-04 6.26199326e-05 1.61356671e-04\n",
      " 5.60040939e-05 1.51854950e-04 5.26973090e-05 1.04360541e-04\n",
      " 3.61822227e-05 9.48648159e-05 3.28828953e-05 8.53704349e-05\n",
      " 2.95851924e-05 7.58776068e-05 2.62893502e-05 2.08874673e-04\n",
      " 5.55650533e+01 8.38563657e+01 2.97049015e+01 7.25227888e+01\n",
      " 9.34493732e+01 9.34689061e+01 1.15581089e+02 9.49708846e+01\n",
      " 1.16032640e+02 1.04070078e+01 3.61428938e+01 1.32824508e+02\n",
      " 1.07288245e+02 1.14715436e+02 9.65944583e+01 1.30971584e+02\n",
      " 8.12038272e+01 1.27268205e+02 1.01034360e+02 1.02127751e+02\n",
      " 5.76915884e+01 1.80128927e+01 4.36761887e+01 9.86201547e+01\n",
      " 5.52911025e+01 7.14026885e+01 1.37430871e+01 9.14957608e+01\n",
      " 6.34469643e+01 1.06518935e+02 1.14664520e+02 8.71292913e+01\n",
      " 5.16920662e+01 5.69587792e+01 1.70959999e+01 8.78067248e+01\n",
      " 8.26923077e+01 8.88871772e+01 2.61633052e+01 4.43094641e+01\n",
      " 5.35343446e+01]\n",
      "81-th iteration, loss: 0.0035552671648397937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.476149368284167e-06\n",
      "81-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525129e+01 1.11970011e+02 5.58361452e+01 8.83442285e+01\n",
      " 5.58987654e+01 1.08020906e+02 6.40611765e+01 1.02025555e+02\n",
      " 5.33404802e+01 8.98185388e+01 6.74107087e+01 9.92485955e+01\n",
      " 5.76664727e+01 1.57005124e+02 5.55650566e+01 2.08846147e-04\n",
      " 7.25150457e-05 1.89838656e-04 6.58965865e-05 1.70833118e-04\n",
      " 5.92807103e-05 1.61331410e-04 5.59738901e-05 1.13837013e-04\n",
      " 3.94587795e-05 1.04341296e-04 3.61594301e-05 9.48469231e-05\n",
      " 3.28617074e-05 8.53541019e-05 2.95658476e-05 2.18351174e-04\n",
      " 5.55650566e+01 8.38563629e+01 2.97049031e+01 7.25227928e+01\n",
      " 9.34493725e+01 9.34689113e+01 1.15581089e+02 9.49708875e+01\n",
      " 1.16032643e+02 1.04070115e+01 3.61428957e+01 1.32824506e+02\n",
      " 1.07288247e+02 1.14715433e+02 9.65944589e+01 1.30971589e+02\n",
      " 8.12038289e+01 1.27268209e+02 1.01034363e+02 1.02127755e+02\n",
      " 5.76915940e+01 1.80128945e+01 4.36761906e+01 9.86201566e+01\n",
      " 5.52911042e+01 7.14026902e+01 1.37430905e+01 9.14957613e+01\n",
      " 6.34469658e+01 1.06518937e+02 1.14664521e+02 8.71292912e+01\n",
      " 5.16920661e+01 5.69587804e+01 1.70959995e+01 8.78067250e+01\n",
      " 8.26923081e+01 8.88871771e+01 2.61633044e+01 4.43094660e+01\n",
      " 5.35343442e+01]\n",
      "82-th iteration, loss: 0.0035552665003862967, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.47432823789789e-06\n",
      "82-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525131e+01 1.11970011e+02 5.58361453e+01 8.83442287e+01\n",
      " 5.58987655e+01 1.08020906e+02 6.40611772e+01 1.02025556e+02\n",
      " 5.33404806e+01 8.98185391e+01 6.74107071e+01 9.92485945e+01\n",
      " 5.76664715e+01 1.57005123e+02 5.55650598e+01 2.18320736e-04\n",
      " 7.57896105e-05 1.99313262e-04 6.91711072e-05 1.80307740e-04\n",
      " 6.25551913e-05 1.70806045e-04 5.92483336e-05 1.23311661e-04\n",
      " 4.27331966e-05 1.13815954e-04 3.94338230e-05 1.04321589e-04\n",
      " 3.61360781e-05 9.48287753e-05 3.28401986e-05 2.27825854e-04\n",
      " 5.55650598e+01 8.38563600e+01 2.97049048e+01 7.25227968e+01\n",
      " 9.34493717e+01 9.34689164e+01 1.15581089e+02 9.49708904e+01\n",
      " 1.16032646e+02 1.04070152e+01 3.61428976e+01 1.32824504e+02\n",
      " 1.07288249e+02 1.14715429e+02 9.65944594e+01 1.30971594e+02\n",
      " 8.12038307e+01 1.27268212e+02 1.01034367e+02 1.02127759e+02\n",
      " 5.76915997e+01 1.80128964e+01 4.36761924e+01 9.86201585e+01\n",
      " 5.52911059e+01 7.14026919e+01 1.37430939e+01 9.14957618e+01\n",
      " 6.34469673e+01 1.06518939e+02 1.14664522e+02 8.71292912e+01\n",
      " 5.16920660e+01 5.69587816e+01 1.70959990e+01 8.78067253e+01\n",
      " 8.26923085e+01 8.88871770e+01 2.61633036e+01 4.43094680e+01\n",
      " 5.35343438e+01]\n",
      "83-th iteration, loss: 0.0035552658366039642, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.472507677763693e-06\n",
      "83-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525133e+01 1.11970011e+02 5.58361453e+01 8.83442289e+01\n",
      " 5.58987657e+01 1.08020906e+02 6.40611779e+01 1.02025556e+02\n",
      " 5.33404810e+01 8.98185394e+01 6.74107056e+01 9.92485935e+01\n",
      " 5.76664703e+01 1.57005122e+02 5.55650631e+01 2.27793499e-04\n",
      " 7.90620452e-05 2.08786043e-04 7.24434955e-05 1.89780536e-04\n",
      " 6.58275377e-05 1.80278857e-04 6.25206404e-05 1.32784486e-04\n",
      " 4.60054747e-05 1.23288789e-04 4.27060746e-05 1.13794433e-04\n",
      " 3.94083056e-05 1.04301628e-04 3.61124040e-05 2.37298714e-04\n",
      " 5.55650631e+01 8.38563571e+01 2.97049065e+01 7.25228008e+01\n",
      " 9.34493710e+01 9.34689216e+01 1.15581089e+02 9.49708933e+01\n",
      " 1.16032650e+02 1.04070189e+01 3.61428995e+01 1.32824502e+02\n",
      " 1.07288252e+02 1.14715425e+02 9.65944599e+01 1.30971599e+02\n",
      " 8.12038325e+01 1.27268215e+02 1.01034370e+02 1.02127763e+02\n",
      " 5.76916053e+01 1.80128983e+01 4.36761942e+01 9.86201603e+01\n",
      " 5.52911076e+01 7.14026937e+01 1.37430972e+01 9.14957622e+01\n",
      " 6.34469689e+01 1.06518942e+02 1.14664523e+02 8.71292911e+01\n",
      " 5.16920659e+01 5.69587828e+01 1.70959986e+01 8.78067256e+01\n",
      " 8.26923088e+01 8.88871768e+01 2.61633029e+01 4.43094699e+01\n",
      " 5.35343434e+01]\n",
      "84-th iteration, loss: 0.00355526517348951, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.470687688234207e-06\n",
      "84-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525134e+01 1.11970011e+02 5.58361454e+01 8.83442290e+01\n",
      " 5.58987658e+01 1.08020907e+02 6.40611785e+01 1.02025557e+02\n",
      " 5.33404815e+01 8.98185397e+01 6.74107040e+01 9.92485924e+01\n",
      " 5.76664691e+01 1.57005121e+02 5.55650664e+01 2.37264436e-04\n",
      " 8.23323505e-05 2.18256998e-04 7.57137524e-05 1.99251509e-04\n",
      " 6.90977505e-05 1.89749844e-04 6.57908112e-05 1.42255489e-04\n",
      " 4.92756147e-05 1.32759802e-04 4.59761860e-05 1.23265456e-04\n",
      " 4.26783905e-05 1.13772660e-04 3.93824647e-05 2.46769754e-04\n",
      " 5.55650664e+01 8.38563542e+01 2.97049081e+01 7.25228048e+01\n",
      " 9.34493702e+01 9.34689268e+01 1.15581089e+02 9.49708961e+01\n",
      " 1.16032653e+02 1.04070226e+01 3.61429014e+01 1.32824501e+02\n",
      " 1.07288254e+02 1.14715421e+02 9.65944604e+01 1.30971605e+02\n",
      " 8.12038342e+01 1.27268218e+02 1.01034374e+02 1.02127767e+02\n",
      " 5.76916109e+01 1.80129001e+01 4.36761961e+01 9.86201622e+01\n",
      " 5.52911092e+01 7.14026954e+01 1.37431006e+01 9.14957627e+01\n",
      " 6.34469704e+01 1.06518944e+02 1.14664523e+02 8.71292910e+01\n",
      " 5.16920658e+01 5.69587840e+01 1.70959982e+01 8.78067259e+01\n",
      " 8.26923092e+01 8.88871767e+01 2.61633021e+01 4.43094719e+01\n",
      " 5.35343430e+01]\n",
      "85-th iteration, loss: 0.0035552645110396956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.468868269625831e-06\n",
      "85-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525136e+01 1.11970012e+02 5.58361454e+01 8.83442292e+01\n",
      " 5.58987660e+01 1.08020907e+02 6.40611792e+01 1.02025557e+02\n",
      " 5.33404819e+01 8.98185401e+01 6.74107024e+01 9.92485914e+01\n",
      " 5.76664679e+01 1.57005120e+02 5.55650697e+01 2.46733548e-04\n",
      " 8.56005275e-05 2.27726129e-04 7.89818786e-05 2.08720657e-04\n",
      " 7.23658304e-05 1.99219009e-04 6.90588471e-05 1.51724669e-04\n",
      " 5.25436175e-05 1.42228993e-04 4.92441580e-05 1.32734658e-04\n",
      " 4.59463338e-05 1.23241872e-04 4.26503816e-05 2.56238976e-04\n",
      " 5.55650697e+01 8.38563513e+01 2.97049098e+01 7.25228087e+01\n",
      " 9.34493695e+01 9.34689319e+01 1.15581089e+02 9.49708990e+01\n",
      " 1.16032656e+02 1.04070263e+01 3.61429033e+01 1.32824499e+02\n",
      " 1.07288256e+02 1.14715417e+02 9.65944610e+01 1.30971610e+02\n",
      " 8.12038360e+01 1.27268221e+02 1.01034377e+02 1.02127771e+02\n",
      " 5.76916165e+01 1.80129020e+01 4.36761979e+01 9.86201641e+01\n",
      " 5.52911108e+01 7.14026971e+01 1.37431040e+01 9.14957631e+01\n",
      " 6.34469719e+01 1.06518946e+02 1.14664524e+02 8.71292909e+01\n",
      " 5.16920657e+01 5.69587852e+01 1.70959978e+01 8.78067261e+01\n",
      " 8.26923096e+01 8.88871765e+01 2.61633013e+01 4.43094738e+01\n",
      " 5.35343426e+01]\n",
      "86-th iteration, loss: 0.0035552638492512786, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.467049422279914e-06\n",
      "86-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525138e+01 1.11970012e+02 5.58361455e+01 8.83442294e+01\n",
      " 5.58987662e+01 1.08020908e+02 6.40611799e+01 1.02025557e+02\n",
      " 5.33404823e+01 8.98185404e+01 6.74107009e+01 9.92485903e+01\n",
      " 5.76664667e+01 1.57005120e+02 5.55650729e+01 2.56200835e-04\n",
      " 8.88665768e-05 2.37193437e-04 8.22478750e-05 2.18187983e-04\n",
      " 7.56317783e-05 2.08686351e-04 7.23247487e-05 1.61192027e-04\n",
      " 5.58094839e-05 1.51696364e-04 5.25099913e-05 1.42202040e-04\n",
      " 4.92121363e-05 1.32709264e-04 4.59161554e-05 2.65706378e-04\n",
      " 5.55650729e+01 8.38563484e+01 2.97049114e+01 7.25228127e+01\n",
      " 9.34493687e+01 9.34689371e+01 1.15581090e+02 9.49709019e+01\n",
      " 1.16032659e+02 1.04070300e+01 3.61429051e+01 1.32824497e+02\n",
      " 1.07288259e+02 1.14715413e+02 9.65944615e+01 1.30971615e+02\n",
      " 8.12038377e+01 1.27268224e+02 1.01034380e+02 1.02127775e+02\n",
      " 5.76916221e+01 1.80129039e+01 4.36761997e+01 9.86201659e+01\n",
      " 5.52911125e+01 7.14026989e+01 1.37431073e+01 9.14957636e+01\n",
      " 6.34469734e+01 1.06518948e+02 1.14664525e+02 8.71292908e+01\n",
      " 5.16920656e+01 5.69587864e+01 1.70959974e+01 8.78067264e+01\n",
      " 8.26923100e+01 8.88871764e+01 2.61633006e+01 4.43094758e+01\n",
      " 5.35343422e+01]\n",
      "87-th iteration, loss: 0.0035552631881209376, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.465231146511162e-06\n",
      "87-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525140e+01 1.11970012e+02 5.58361455e+01 8.83442296e+01\n",
      " 5.58987663e+01 1.08020908e+02 6.40611806e+01 1.02025558e+02\n",
      " 5.33404828e+01 8.98185407e+01 6.74106993e+01 9.92485893e+01\n",
      " 5.76664655e+01 1.57005119e+02 5.55650762e+01 2.65666298e-04\n",
      " 9.21304994e-05 2.46658920e-04 8.55117424e-05 2.27653486e-04\n",
      " 7.88955951e-05 2.18151872e-04 7.55885169e-05 1.70657564e-04\n",
      " 5.90732147e-05 1.61161914e-04 5.57736869e-05 1.51667602e-04\n",
      " 5.24757989e-05 1.42174838e-04 4.91797872e-05 2.75171963e-04\n",
      " 5.55650762e+01 8.38563456e+01 2.97049131e+01 7.25228167e+01\n",
      " 9.34493680e+01 9.34689423e+01 1.15581090e+02 9.49709048e+01\n",
      " 1.16032662e+02 1.04070336e+01 3.61429070e+01 1.32824495e+02\n",
      " 1.07288261e+02 1.14715409e+02 9.65944620e+01 1.30971621e+02\n",
      " 8.12038394e+01 1.27268227e+02 1.01034384e+02 1.02127779e+02\n",
      " 5.76916277e+01 1.80129057e+01 4.36762015e+01 9.86201677e+01\n",
      " 5.52911141e+01 7.14027006e+01 1.37431107e+01 9.14957640e+01\n",
      " 6.34469749e+01 1.06518951e+02 1.14664526e+02 8.71292907e+01\n",
      " 5.16920655e+01 5.69587876e+01 1.70959969e+01 8.78067267e+01\n",
      " 8.26923103e+01 8.88871763e+01 2.61632998e+01 4.43094777e+01\n",
      " 5.35343418e+01]\n",
      "88-th iteration, loss: 0.003555262527645613, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.463413442650752e-06\n",
      "88-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525141e+01 1.11970013e+02 5.58361456e+01 8.83442298e+01\n",
      " 5.58987665e+01 1.08020908e+02 6.40611812e+01 1.02025558e+02\n",
      " 5.33404832e+01 8.98185410e+01 6.74106977e+01 9.92485883e+01\n",
      " 5.76664643e+01 1.57005118e+02 5.55650795e+01 2.75129938e-04\n",
      " 9.53922960e-05 2.56122582e-04 8.87734818e-05 2.37117167e-04\n",
      " 8.21572815e-05 2.27615571e-04 7.88501527e-05 1.80121281e-04\n",
      " 6.23348108e-05 1.70625645e-04 5.90352455e-05 1.61131346e-04\n",
      " 5.57373223e-05 1.51638594e-04 5.24412776e-05 2.84635730e-04\n",
      " 5.55650794e+01 8.38563427e+01 2.97049147e+01 7.25228206e+01\n",
      " 9.34493672e+01 9.34689474e+01 1.15581090e+02 9.49709076e+01\n",
      " 1.16032666e+02 1.04070373e+01 3.61429088e+01 1.32824493e+02\n",
      " 1.07288263e+02 1.14715405e+02 9.65944624e+01 1.30971626e+02\n",
      " 8.12038411e+01 1.27268230e+02 1.01034387e+02 1.02127783e+02\n",
      " 5.76916332e+01 1.80129076e+01 4.36762032e+01 9.86201696e+01\n",
      " 5.52911157e+01 7.14027023e+01 1.37431140e+01 9.14957645e+01\n",
      " 6.34469764e+01 1.06518953e+02 1.14664527e+02 8.71292906e+01\n",
      " 5.16920654e+01 5.69587888e+01 1.70959965e+01 8.78067270e+01\n",
      " 8.26923107e+01 8.88871761e+01 2.61632990e+01 4.43094797e+01\n",
      " 5.35343414e+01]\n",
      "89-th iteration, loss: 0.0035552618678220188, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.461596311038706e-06\n",
      "89-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525143e+01 1.11970013e+02 5.58361456e+01 8.83442300e+01\n",
      " 5.58987667e+01 1.08020909e+02 6.40611819e+01 1.02025559e+02\n",
      " 5.33404837e+01 8.98185414e+01 6.74106962e+01 9.92485872e+01\n",
      " 5.76664631e+01 1.57005117e+02 5.55650827e+01 2.84591755e-04\n",
      " 9.86519676e-05 2.65584420e-04 9.20330938e-05 2.46579027e-04\n",
      " 8.54168384e-05 2.37077450e-04 8.21096568e-05 1.89583178e-04\n",
      " 6.55942730e-05 1.80087556e-04 6.22946681e-05 1.70593271e-04\n",
      " 5.89967075e-05 1.61100532e-04 5.57006275e-05 2.94097680e-04\n",
      " 5.55650827e+01 8.38563398e+01 2.97049164e+01 7.25228246e+01\n",
      " 9.34493664e+01 9.34689526e+01 1.15581090e+02 9.49709105e+01\n",
      " 1.16032669e+02 1.04070410e+01 3.61429107e+01 1.32824491e+02\n",
      " 1.07288265e+02 1.14715401e+02 9.65944629e+01 1.30971631e+02\n",
      " 8.12038428e+01 1.27268233e+02 1.01034391e+02 1.02127787e+02\n",
      " 5.76916388e+01 1.80129094e+01 4.36762050e+01 9.86201714e+01\n",
      " 5.52911173e+01 7.14027040e+01 1.37431173e+01 9.14957649e+01\n",
      " 6.34469779e+01 1.06518955e+02 1.14664528e+02 8.71292905e+01\n",
      " 5.16920653e+01 5.69587900e+01 1.70959961e+01 8.78067273e+01\n",
      " 8.26923111e+01 8.88871760e+01 2.61632982e+01 4.43094816e+01\n",
      " 5.35343410e+01]\n",
      "90-th iteration, loss: 0.003555261208647016, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.45977975198199e-06\n",
      "90-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525145e+01 1.11970013e+02 5.58361457e+01 8.83442301e+01\n",
      " 5.58987668e+01 1.08020909e+02 6.40611826e+01 1.02025559e+02\n",
      " 5.33404841e+01 8.98185417e+01 6.74106946e+01 9.92485862e+01\n",
      " 5.76664619e+01 1.57005116e+02 5.55650860e+01 2.94051749e-04\n",
      " 1.01909515e-04 2.75044438e-04 9.52905794e-05 2.56039065e-04\n",
      " 8.86742668e-05 2.46537508e-04 8.53670300e-05 1.99043256e-04\n",
      " 6.88516022e-05 1.89547649e-04 6.55519555e-05 1.80053379e-04\n",
      " 6.22539552e-05 1.70560653e-04 5.89578378e-05 3.03557814e-04\n",
      " 5.55650860e+01 8.38563369e+01 2.97049180e+01 7.25228286e+01\n",
      " 9.34493657e+01 9.34689577e+01 1.15581090e+02 9.49709133e+01\n",
      " 1.16032672e+02 1.04070446e+01 3.61429125e+01 1.32824489e+02\n",
      " 1.07288268e+02 1.14715397e+02 9.65944634e+01 1.30971636e+02\n",
      " 8.12038445e+01 1.27268237e+02 1.01034394e+02 1.02127791e+02\n",
      " 5.76916443e+01 1.80129112e+01 4.36762068e+01 9.86201732e+01\n",
      " 5.52911189e+01 7.14027058e+01 1.37431207e+01 9.14957654e+01\n",
      " 6.34469794e+01 1.06518957e+02 1.14664529e+02 8.71292904e+01\n",
      " 5.16920652e+01 5.69587912e+01 1.70959957e+01 8.78067275e+01\n",
      " 8.26923114e+01 8.88871759e+01 2.61632975e+01 4.43094836e+01\n",
      " 5.35343406e+01]\n",
      "91-th iteration, loss: 0.00355526055011747, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.45796376581587e-06\n",
      "91-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525147e+01 1.11970013e+02 5.58361457e+01 8.83442303e+01\n",
      " 5.58987670e+01 1.08020910e+02 6.40611833e+01 1.02025560e+02\n",
      " 5.33404845e+01 8.98185420e+01 6.74106930e+01 9.92485851e+01\n",
      " 5.76664607e+01 1.57005115e+02 5.55650892e+01 3.03509922e-04\n",
      " 1.05164939e-04 2.84502634e-04 9.85459395e-05 2.65497284e-04\n",
      " 9.19295673e-05 2.55995748e-04 8.86222732e-05 2.08501515e-04\n",
      " 7.21067992e-05 1.99005924e-04 6.88071084e-05 1.89511669e-04\n",
      " 6.55090663e-05 1.80018958e-04 6.22129093e-05 3.13016133e-04\n",
      " 5.55650892e+01 8.38563340e+01 2.97049197e+01 7.25228326e+01\n",
      " 9.34493649e+01 9.34689629e+01 1.15581090e+02 9.49709162e+01\n",
      " 1.16032675e+02 1.04070483e+01 3.61429143e+01 1.32824487e+02\n",
      " 1.07288270e+02 1.14715393e+02 9.65944638e+01 1.30971642e+02\n",
      " 8.12038462e+01 1.27268240e+02 1.01034398e+02 1.02127794e+02\n",
      " 5.76916499e+01 1.80129131e+01 4.36762085e+01 9.86201750e+01\n",
      " 5.52911205e+01 7.14027075e+01 1.37431240e+01 9.14957658e+01\n",
      " 6.34469809e+01 1.06518960e+02 1.14664529e+02 8.71292903e+01\n",
      " 5.16920651e+01 5.69587924e+01 1.70959952e+01 8.78067278e+01\n",
      " 8.26923118e+01 8.88871757e+01 2.61632967e+01 4.43094855e+01\n",
      " 5.35343402e+01]\n",
      "92-th iteration, loss: 0.0035552598922302596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.456148352855363e-06\n",
      "92-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525148e+01 1.11970014e+02 5.58361458e+01 8.83442305e+01\n",
      " 5.58987672e+01 1.08020910e+02 6.40611839e+01 1.02025560e+02\n",
      " 5.33404850e+01 8.98185423e+01 6.74106914e+01 9.92485841e+01\n",
      " 5.76664595e+01 1.57005114e+02 5.55650925e+01 3.12966273e-04\n",
      " 1.08418240e-04 2.93959010e-04 1.01799175e-04 2.74953683e-04\n",
      " 9.51827409e-05 2.65452168e-04 9.18753874e-05 2.17957956e-04\n",
      " 7.53598649e-05 2.08462381e-04 7.20601279e-05 1.98968142e-04\n",
      " 6.87620418e-05 1.89475447e-04 6.54658429e-05 3.22472636e-04\n",
      " 5.55650925e+01 8.38563311e+01 2.97049213e+01 7.25228365e+01\n",
      " 9.34493641e+01 9.34689681e+01 1.15581090e+02 9.49709190e+01\n",
      " 1.16032678e+02 1.04070519e+01 3.61429161e+01 1.32824485e+02\n",
      " 1.07288272e+02 1.14715389e+02 9.65944643e+01 1.30971647e+02\n",
      " 8.12038479e+01 1.27268243e+02 1.01034401e+02 1.02127798e+02\n",
      " 5.76916554e+01 1.80129149e+01 4.36762102e+01 9.86201768e+01\n",
      " 5.52911220e+01 7.14027092e+01 1.37431273e+01 9.14957663e+01\n",
      " 6.34469824e+01 1.06518962e+02 1.14664530e+02 8.71292902e+01\n",
      " 5.16920650e+01 5.69587935e+01 1.70959948e+01 8.78067281e+01\n",
      " 8.26923122e+01 8.88871756e+01 2.61632959e+01 4.43094875e+01\n",
      " 5.35343398e+01]\n",
      "93-th iteration, loss: 0.003555259234982302, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.454333513408696e-06\n",
      "93-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525150e+01 1.11970014e+02 5.58361458e+01 8.83442307e+01\n",
      " 5.58987673e+01 1.08020910e+02 6.40611846e+01 1.02025561e+02\n",
      " 5.33404854e+01 8.98185426e+01 6.74106899e+01 9.92485830e+01\n",
      " 5.76664583e+01 1.57005114e+02 5.55650957e+01 3.22420804e-04\n",
      " 1.11669420e-04 3.03413566e-04 1.05050286e-04 2.84408262e-04\n",
      " 9.84337884e-05 2.74906770e-04 9.51263732e-05 2.27412579e-04\n",
      " 7.86108001e-05 2.17917022e-04 7.53110146e-05 2.08422800e-04\n",
      " 7.20128823e-05 1.98930120e-04 6.87166395e-05 3.31927324e-04\n",
      " 5.55650957e+01 8.38563282e+01 2.97049229e+01 7.25228405e+01\n",
      " 9.34493633e+01 9.34689732e+01 1.15581090e+02 9.49709219e+01\n",
      " 1.16032681e+02 1.04070556e+01 3.61429179e+01 1.32824483e+02\n",
      " 1.07288275e+02 1.14715385e+02 9.65944647e+01 1.30971652e+02\n",
      " 8.12038496e+01 1.27268246e+02 1.01034404e+02 1.02127802e+02\n",
      " 5.76916609e+01 1.80129167e+01 4.36762120e+01 9.86201786e+01\n",
      " 5.52911236e+01 7.14027109e+01 1.37431307e+01 9.14957667e+01\n",
      " 6.34469839e+01 1.06518964e+02 1.14664531e+02 8.71292901e+01\n",
      " 5.16920648e+01 5.69587947e+01 1.70959944e+01 8.78067284e+01\n",
      " 8.26923126e+01 8.88871754e+01 2.61632952e+01 4.43094894e+01\n",
      " 5.35343394e+01]\n",
      "94-th iteration, loss: 0.0035552585783705185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.45251924781429e-06\n",
      "94-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525152e+01 1.11970014e+02 5.58361459e+01 8.83442309e+01\n",
      " 5.58987675e+01 1.08020911e+02 6.40611853e+01 1.02025561e+02\n",
      " 5.33404858e+01 8.98185430e+01 6.74106883e+01 9.92485820e+01\n",
      " 5.76664571e+01 1.57005113e+02 5.55650990e+01 3.31873515e-04\n",
      " 1.14918479e-04 3.12866303e-04 1.08299275e-04 2.93861023e-04\n",
      " 1.01682711e-04 2.84359554e-04 9.83752316e-05 2.36865385e-04\n",
      " 8.18596056e-05 2.27369846e-04 7.85597695e-05 2.17875642e-04\n",
      " 7.52615888e-05 2.08382979e-04 7.19652997e-05 3.41380199e-04\n",
      " 5.55650990e+01 8.38563253e+01 2.97049246e+01 7.25228445e+01\n",
      " 9.34493626e+01 9.34689784e+01 1.15581090e+02 9.49709247e+01\n",
      " 1.16032685e+02 1.04070592e+01 3.61429197e+01 1.32824481e+02\n",
      " 1.07288277e+02 1.14715381e+02 9.65944652e+01 1.30971657e+02\n",
      " 8.12038512e+01 1.27268249e+02 1.01034408e+02 1.02127806e+02\n",
      " 5.76916664e+01 1.80129186e+01 4.36762137e+01 9.86201804e+01\n",
      " 5.52911251e+01 7.14027126e+01 1.37431340e+01 9.14957671e+01\n",
      " 6.34469854e+01 1.06518966e+02 1.14664532e+02 8.71292900e+01\n",
      " 5.16920647e+01 5.69587959e+01 1.70959940e+01 8.78067286e+01\n",
      " 8.26923129e+01 8.88871753e+01 2.61632944e+01 4.43094914e+01\n",
      " 5.35343390e+01]\n",
      "95-th iteration, loss: 0.003555257922391885, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.450705556376735e-06\n",
      "95-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525154e+01 1.11970014e+02 5.58361460e+01 8.83442311e+01\n",
      " 5.58987676e+01 1.08020911e+02 6.40611860e+01 1.02025561e+02\n",
      " 5.33404863e+01 8.98185433e+01 6.74106867e+01 9.92485810e+01\n",
      " 5.76664559e+01 1.57005112e+02 5.55651022e+01 3.41324406e-04\n",
      " 1.18165418e-04 3.22317221e-04 1.11546141e-04 3.03311967e-04\n",
      " 1.04929509e-04 2.93810520e-04 1.01621963e-04 2.46316375e-04\n",
      " 8.51062823e-05 2.36820855e-04 8.18063934e-05 2.27326668e-04\n",
      " 7.85081621e-05 2.17834023e-04 7.52118246e-05 3.50831260e-04\n",
      " 5.55651022e+01 8.38563224e+01 2.97049262e+01 7.25228484e+01\n",
      " 9.34493618e+01 9.34689835e+01 1.15581090e+02 9.49709275e+01\n",
      " 1.16032688e+02 1.04070629e+01 3.61429215e+01 1.32824479e+02\n",
      " 1.07288279e+02 1.14715377e+02 9.65944656e+01 1.30971662e+02\n",
      " 8.12038529e+01 1.27268252e+02 1.01034411e+02 1.02127810e+02\n",
      " 5.76916719e+01 1.80129204e+01 4.36762154e+01 9.86201822e+01\n",
      " 5.52911267e+01 7.14027143e+01 1.37431373e+01 9.14957676e+01\n",
      " 6.34469868e+01 1.06518969e+02 1.14664533e+02 8.71292899e+01\n",
      " 5.16920646e+01 5.69587971e+01 1.70959935e+01 8.78067289e+01\n",
      " 8.26923133e+01 8.88871752e+01 2.61632936e+01 4.43094934e+01\n",
      " 5.35343386e+01]\n",
      "96-th iteration, loss: 0.003555257267043293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.448892439415497e-06\n",
      "96-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525155e+01 1.11970015e+02 5.58361460e+01 8.83442312e+01\n",
      " 5.58987678e+01 1.08020911e+02 6.40611866e+01 1.02025562e+02\n",
      " 5.33404867e+01 8.98185436e+01 6.74106851e+01 9.92485799e+01\n",
      " 5.76664547e+01 1.57005111e+02 5.55651055e+01 3.50773479e-04\n",
      " 1.21410238e-04 3.31766321e-04 1.14790885e-04 3.12761092e-04\n",
      " 1.08174183e-04 3.03259671e-04 1.04866569e-04 2.55765548e-04\n",
      " 8.83508311e-05 2.46270049e-04 8.50508872e-05 2.36775881e-04\n",
      " 8.17526030e-05 2.27283254e-04 7.84562150e-05 3.60280508e-04\n",
      " 5.55651055e+01 8.38563195e+01 2.97049278e+01 7.25228524e+01\n",
      " 9.34493610e+01 9.34689887e+01 1.15581090e+02 9.49709304e+01\n",
      " 1.16032691e+02 1.04070665e+01 3.61429233e+01 1.32824477e+02\n",
      " 1.07288281e+02 1.14715373e+02 9.65944660e+01 1.30971668e+02\n",
      " 8.12038545e+01 1.27268255e+02 1.01034415e+02 1.02127814e+02\n",
      " 5.76916774e+01 1.80129222e+01 4.36762171e+01 9.86201840e+01\n",
      " 5.52911282e+01 7.14027160e+01 1.37431406e+01 9.14957680e+01\n",
      " 6.34469883e+01 1.06518971e+02 1.14664534e+02 8.71292898e+01\n",
      " 5.16920645e+01 5.69587983e+01 1.70959931e+01 8.78067292e+01\n",
      " 8.26923137e+01 8.88871750e+01 2.61632928e+01 4.43094953e+01\n",
      " 5.35343382e+01]\n",
      "97-th iteration, loss: 0.0035552566123218327, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.447079897245408e-06\n",
      "97-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525157e+01 1.11970015e+02 5.58361461e+01 8.83442314e+01\n",
      " 5.58987680e+01 1.08020912e+02 6.40611873e+01 1.02025562e+02\n",
      " 5.33404871e+01 8.98185439e+01 6.74106836e+01 9.92485789e+01\n",
      " 5.76664535e+01 1.57005110e+02 5.55651087e+01 3.60220733e-04\n",
      " 1.24652939e-04 3.41213603e-04 1.18033510e-04 3.22208401e-04\n",
      " 1.11416734e-04 3.12707005e-04 1.08109050e-04 2.65212907e-04\n",
      " 9.15932527e-05 2.55717428e-04 8.82932516e-05 2.46223280e-04\n",
      " 8.49949124e-05 2.36730672e-04 8.16984716e-05 3.69727944e-04\n",
      " 5.55651087e+01 8.38563166e+01 2.97049295e+01 7.25228563e+01\n",
      " 9.34493602e+01 9.34689938e+01 1.15581090e+02 9.49709332e+01\n",
      " 1.16032694e+02 1.04070701e+01 3.61429251e+01 1.32824475e+02\n",
      " 1.07288283e+02 1.14715368e+02 9.65944664e+01 1.30971673e+02\n",
      " 8.12038562e+01 1.27268258e+02 1.01034418e+02 1.02127818e+02\n",
      " 5.76916828e+01 1.80129240e+01 4.36762188e+01 9.86201857e+01\n",
      " 5.52911297e+01 7.14027177e+01 1.37431439e+01 9.14957685e+01\n",
      " 6.34469898e+01 1.06518973e+02 1.14664534e+02 8.71292897e+01\n",
      " 5.16920643e+01 5.69587995e+01 1.70959927e+01 8.78067295e+01\n",
      " 8.26923140e+01 8.88871749e+01 2.61632921e+01 4.43094973e+01\n",
      " 5.35343378e+01]\n",
      "98-th iteration, loss: 0.0035552559582244505, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.445267930178612e-06\n",
      "98-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525159e+01 1.11970015e+02 5.58361461e+01 8.83442316e+01\n",
      " 5.58987681e+01 1.08020912e+02 6.40611880e+01 1.02025563e+02\n",
      " 5.33404876e+01 8.98185442e+01 6.74106820e+01 9.92485778e+01\n",
      " 5.76664523e+01 1.57005109e+02 5.55651120e+01 3.69666169e-04\n",
      " 1.27893523e-04 3.50659068e-04 1.21274014e-04 3.31653894e-04\n",
      " 1.14657164e-04 3.22152523e-04 1.11349407e-04 2.74658451e-04\n",
      " 9.48335481e-05 2.65162993e-04 9.15334876e-05 2.55668865e-04\n",
      " 8.82350912e-05 2.46176277e-04 8.49385954e-05 3.79173569e-04\n",
      " 5.55651119e+01 8.38563137e+01 2.97049311e+01 7.25228603e+01\n",
      " 9.34493594e+01 9.34689989e+01 1.15581090e+02 9.49709360e+01\n",
      " 1.16032697e+02 1.04070738e+01 3.61429268e+01 1.32824473e+02\n",
      " 1.07288286e+02 1.14715364e+02 9.65944668e+01 1.30971678e+02\n",
      " 8.12038578e+01 1.27268261e+02 1.01034421e+02 1.02127822e+02\n",
      " 5.76916883e+01 1.80129258e+01 4.36762205e+01 9.86201875e+01\n",
      " 5.52911312e+01 7.14027194e+01 1.37431472e+01 9.14957689e+01\n",
      " 6.34469913e+01 1.06518975e+02 1.14664535e+02 8.71292896e+01\n",
      " 5.16920642e+01 5.69588007e+01 1.70959922e+01 8.78067297e+01\n",
      " 8.26923144e+01 8.88871747e+01 2.61632913e+01 4.43094992e+01\n",
      " 5.35343374e+01]\n",
      "99-th iteration, loss: 0.003555255304748176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.443456538520752e-06\n",
      "99-th iteration, new layer inserted. now 73 layers\n",
      "[6.27525161e+01 1.11970016e+02 5.58361462e+01 8.83442318e+01\n",
      " 5.58987683e+01 1.08020913e+02 6.40611886e+01 1.02025563e+02\n",
      " 5.33404880e+01 8.98185446e+01 6.74106804e+01 9.92485768e+01\n",
      " 5.76664511e+01 1.57005108e+02 5.55651152e+01 3.79109788e-04\n",
      " 1.31131990e-04 3.60102717e-04 1.24512400e-04 3.41097571e-04\n",
      " 1.17895473e-04 3.31596227e-04 1.14587641e-04 2.84102180e-04\n",
      " 9.80717181e-05 2.74606744e-04 9.47715959e-05 2.65112638e-04\n",
      " 9.14731401e-05 2.55620070e-04 8.81765871e-05 3.88617382e-04\n",
      " 5.55651152e+01 8.38563108e+01 2.97049327e+01 7.25228643e+01\n",
      " 9.34493586e+01 9.34690041e+01 1.15581091e+02 9.49709388e+01\n",
      " 1.16032700e+02 1.04070774e+01 3.61429286e+01 1.32824471e+02\n",
      " 1.07288288e+02 1.14715360e+02 9.65944672e+01 1.30971683e+02\n",
      " 8.12038594e+01 1.27268264e+02 1.01034425e+02 1.02127826e+02\n",
      " 5.76916937e+01 1.80129276e+01 4.36762221e+01 9.86201893e+01\n",
      " 5.52911327e+01 7.14027210e+01 1.37431505e+01 9.14957693e+01\n",
      " 6.34469927e+01 1.06518978e+02 1.14664536e+02 8.71292895e+01\n",
      " 5.16920641e+01 5.69588019e+01 1.70959918e+01 8.78067300e+01\n",
      " 8.26923148e+01 8.88871746e+01 2.61632905e+01 4.43095012e+01\n",
      " 5.35343370e+01]\n",
      "0-th iteration, loss: 0.7704066124377217, 18 gd steps\n",
      "insert gradient: -0.520912552453571\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  77.72251444    0.         3147.76183466]\n",
      "1-th iteration, loss: 0.5743535913392861, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.02244886e+01 1.38093305e+00 0.00000000e+00 1.11855577e+02\n",
      " 3.12376037e+03]\n",
      "2-th iteration, loss: 0.5701580976258396, 34 gd steps\n",
      "insert gradient: -0.2420511638546506\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.99237557  110.21618812  152.35506597    0.         2970.92378643]\n",
      "3-th iteration, loss: 0.45815136756696556, 22 gd steps\n",
      "insert gradient: -0.14361943335565747\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  69.5348836   101.41064205   61.7325562     0.           54.60956895\n",
      "   81.60535082 2949.9215755 ]\n",
      "4-th iteration, loss: 0.4367102993318311, 29 gd steps\n",
      "insert gradient: -0.12477077007514706\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[  71.50559851  110.33989261   59.75786877   30.05092717   38.82437555\n",
      "   93.75206694 2937.7205755 ]\n",
      "5-th iteration, loss: 0.4190440345313347, 135 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  72.95640757  124.68547705   55.38323237   75.72449854   27.36740576\n",
      "   84.24080545  166.02166621    0.         2739.35749248]\n",
      "6-th iteration, loss: 0.3796943025174112, 27 gd steps\n",
      "insert gradient: -0.026220295481137832\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  64.98589513  132.37712051   55.73185165   85.6333479    19.91917072\n",
      "   84.82013475  142.1311621    70.97989817 2318.94401271    0.\n",
      "  403.29461091]\n",
      "7-th iteration, loss: 0.36195774983140927, 21 gd steps\n",
      "insert gradient: -0.07551725937176249\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  65.56833401  123.0587727    56.52892269   69.35880121   28.96642616\n",
      "   79.95561973  124.34505879  108.27977528 2310.75887389   21.7945075\n",
      "  289.41100534    0.          108.529127  ]\n",
      "8-th iteration, loss: 0.27091234435758965, 55 gd steps\n",
      "insert gradient: -0.1121071224240652\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  76.52773751  104.55148074   44.2261849    62.92774999   47.22354644\n",
      "   98.23264451  103.18926991  117.78884413 2298.95799205  112.76151234\n",
      "  112.49340552    0.          112.49340552   98.84740046   85.24750336]\n",
      "9-th iteration, loss: 0.23742251644188653, 39 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  75.76375242  106.10263611   45.04461772   62.58931515   49.08372766\n",
      "   98.47613731  105.64157578  113.87252677  143.68977261    0.\n",
      " 2155.34658908  117.65641541   82.75468388   78.54640086   89.50044569\n",
      "   88.46038838   88.65901631]\n",
      "10-th iteration, loss: 0.21347208628349312, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  66.58455278  106.40553821   53.07856744   61.00273065   46.29012003\n",
      "  113.33898123  101.52724129  102.67110997  129.17742394   44.06359479\n",
      "  153.78075598    0.         1999.14982776  103.14416562   91.56441772\n",
      "   86.2110501    89.4519739    91.74148334   92.16076144]\n",
      "11-th iteration, loss: 0.18309872258276652, 52 gd steps\n",
      "insert gradient: -0.02508001796132489\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  63.36410339  108.96006115   53.59452842   83.52242996   47.91355974\n",
      "   95.10052382  108.18449744  108.17717766  105.16805399  102.93227153\n",
      "  131.8800465     0.           26.3760093    18.51410653 1966.25952797\n",
      "   90.90441227  101.11719206   91.57185988  101.42362698   96.29733995\n",
      "   96.72509591]\n",
      "12-th iteration, loss: 0.166277722970774, 189 gd steps\n",
      "insert gradient: -0.018342437172344028\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  63.86287937  106.83777561   55.19202866   86.45009406   51.12234367\n",
      "   92.39841764  111.40844945   98.72646291  103.56731436  120.75538767\n",
      "  108.56898602  126.91287483 1305.86654775    0.          652.93327388\n",
      "   95.82589146   94.67356531   91.24765759   96.8025268    93.4540142\n",
      "   96.48357155]\n",
      "13-th iteration, loss: 0.15523246764314694, 187 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  66.95344663  109.06119876   52.51570959   80.84981384   50.45134822\n",
      "   95.5739807   119.64956962   94.802822    122.02927871  107.14589772\n",
      "   95.41045462  116.80516917  117.35007968    0.         1173.5007968\n",
      "   48.20177374  640.80904594   92.71323742   97.3856267    92.9172932\n",
      "   94.08755409   96.72808727   95.41061312]\n",
      "14-th iteration, loss: 0.12786351261336004, 61 gd steps\n",
      "insert gradient: -0.06776498094463784\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  52.74188325   98.25922988   53.70669259   97.17986706   60.96891351\n",
      "  101.22960342  107.58444968  101.6031884   112.62731847  115.02960526\n",
      "   95.21113574  118.17949349  112.41629325   60.89924403 1025.28598997\n",
      "    0.          113.92066555   95.52951289  638.20664412   59.83807442\n",
      "  100.35729022   71.40227953  101.33016354   78.58518371   94.88240027]\n",
      "15-th iteration, loss: 0.10756359648576805, 53 gd steps\n",
      "insert gradient: -0.006053404016098026\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  56.68954076   99.44414449   55.80551157   96.05150967   56.37664854\n",
      "  102.35354191  116.34528013  101.34880814  111.90377212   98.50216352\n",
      "  108.62193466  100.49079338   99.19378806  101.73071916 1002.92284192\n",
      "   50.94606236   85.95800035  127.28017352  629.81044277   64.78235153\n",
      "   98.37019433   71.8319125    94.26153977   77.78024133   90.69871897]\n",
      "16-th iteration, loss: 0.10688881125195739, 23 gd steps\n",
      "insert gradient: -0.03614103119230419\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[ 55.55542353  99.1320287   56.33577646  96.5607877   56.48843602\n",
      " 102.18585875 116.44976344 100.98521988 111.4243681   98.21533625\n",
      " 109.06776748 100.390449    99.00017938 101.60791351 334.23482176\n",
      "   0.         668.46964351  51.4122212   86.40756042 127.03541613\n",
      " 629.61051973  65.05917551  98.26143012  72.13499089  94.34062154\n",
      "  77.98275164  90.75530973]\n",
      "17-th iteration, loss: 0.10298489555560354, 24 gd steps\n",
      "insert gradient: -0.013613671074512843\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[ 55.49806486  94.84905512  56.37860035  99.1648176   55.20848611\n",
      " 100.72509296 122.88619759  93.88176354 113.38076043  93.88986429\n",
      " 108.05685711 101.38976568  92.88831463 111.54178978 324.46603267\n",
      "   6.09466497 670.91005074  56.23604449  87.37885371 126.45561282\n",
      " 313.60470672   0.         313.60470672  77.68088936  90.42190411\n",
      "  81.11665074  90.86019971  84.35739555  89.62400465]\n",
      "18-th iteration, loss: 0.09012520727137632, 415 gd steps\n",
      "insert gradient: -0.03216396002108346\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[ 52.3201267   89.02534791  46.40018048  92.92349039  66.44823651\n",
      "  98.20105249 120.08054693 107.62377985 120.92417555  93.77160428\n",
      " 108.87828121  94.318334    95.80147592  88.91278344 865.55018315\n",
      "   0.         123.65002616  91.76062784  94.53829872 107.56195612\n",
      " 298.52473665  22.96963869 306.04393023  92.39186558  88.26910062\n",
      "  93.91681596  80.04147866  99.60445466  84.56620168]\n",
      "19-th iteration, loss: 0.0678345942332816, 65 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[ 51.93239789  89.91406735  43.43662964  89.55654382  62.64048734\n",
      "  98.0280543  128.35853495 115.01614781 112.31407345  93.87224806\n",
      " 110.0238552   91.40438131 106.21168826  80.46162024 214.10316623\n",
      "   0.         642.30949869  51.01678846  86.83543258 102.12778294\n",
      "  81.60498282 119.69146501 291.04566489  40.06474593 311.40983117\n",
      "  85.93217087  88.02642941  89.13384417  83.09525492  97.8718112\n",
      "  84.50715267]\n",
      "20-th iteration, loss: 0.05398765057484901, 68 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 33 layers\n",
      "[ 61.17280781  95.82022768  43.84422549  81.40881781  54.87473333\n",
      "  98.29769924 147.3020655   90.22309053 109.29892914 102.37533581\n",
      " 107.63741058  97.52615848 106.94268496  83.64033875 200.88891073\n",
      "  31.99135194 603.68965493 112.46743601  77.24570298  95.79210639\n",
      "  86.54340242  96.37133636 300.55750734  54.10158819 302.97451815\n",
      "  80.9409906   94.60150125  93.26195934  65.15862349  15.46176621\n",
      "   0.          92.77059728  89.55512642]\n",
      "21-th iteration, loss: 0.052347556566873545, 36 gd steps\n",
      "insert gradient: -0.01996883896456108\n",
      "21-th iteration, new layer inserted. now 33 layers\n",
      "[ 62.5852093   96.28373473  44.07946582  80.62454899  52.15714805\n",
      "  99.28482623 147.49784297  93.35424393 109.86205812 100.74068093\n",
      " 108.84370066  95.81289607 105.49813378  84.60442273 198.59940868\n",
      "  35.13331193 602.88034958  63.67394398   0.          47.75545799\n",
      "  76.91400805  96.59447224  87.39122435  96.68773757 301.16910348\n",
      "  54.77767976 301.07557036  80.43818397  95.04732713  92.89446106\n",
      "  65.71502999 109.5943182   90.88151114]\n",
      "22-th iteration, loss: 0.05232893003143931, 6 gd steps\n",
      "insert gradient: -0.004164745822242764\n",
      "22-th iteration, new layer inserted. now 35 layers\n",
      "[6.25844565e+01 9.62835447e+01 4.40795472e+01 8.06247510e+01\n",
      " 5.21583321e+01 9.92859242e+01 1.47499915e+02 9.33558892e+01\n",
      " 1.09862833e+02 1.00741336e+02 1.08846915e+02 9.58138150e+01\n",
      " 1.05498723e+02 8.46069391e+01 1.41861717e+02 0.00000000e+00\n",
      " 5.67446868e+01 3.51414887e+01 6.02890370e+02 6.36808885e+01\n",
      " 1.45602938e-02 4.77624025e+01 7.69259058e+01 9.65991546e+01\n",
      " 8.74010380e+01 9.66912519e+01 3.01172067e+02 5.47788482e+01\n",
      " 3.01075408e+02 8.04380017e+01 9.50471406e+01 9.28939647e+01\n",
      " 6.57139161e+01 1.09593916e+02 9.08813598e+01]\n",
      "23-th iteration, loss: 0.05048217293560143, 98 gd steps\n",
      "insert gradient: -0.0018526418779780862\n",
      "23-th iteration, new layer inserted. now 35 layers\n",
      "[ 61.96438102  96.32100719  44.16696886  80.50783192  52.27592807\n",
      "  99.38348725 146.94077013  94.84510123 109.01455847 100.22827668\n",
      " 110.20556579  95.8756903  103.48835718  84.92143394 138.16643208\n",
      "   6.28839328  53.74854814  37.83068581 344.76821197   0.\n",
      " 258.57615898 112.73756124  76.96424636  96.77372958  86.46479218\n",
      "  96.90695822 300.39859314  55.1603292  299.8534885   80.55721536\n",
      "  95.72513268  92.77253431  64.88499849 109.39505458  90.8537474 ]\n",
      "24-th iteration, loss: 0.04580920116163627, 19 gd steps\n",
      "insert gradient: -0.0034183065994619287\n",
      "24-th iteration, new layer inserted. now 37 layers\n",
      "[5.80290792e+01 9.63791462e+01 4.63394772e+01 8.04818187e+01\n",
      " 5.27851224e+01 1.01092736e+02 1.43580971e+02 9.98463121e+01\n",
      " 1.08132918e+02 9.93005182e+01 1.11556930e+02 9.72749189e+01\n",
      " 1.00762146e+02 9.04677866e+01 1.29275928e+02 0.00000000e+00\n",
      " 2.13162821e-14 2.46654594e+01 4.27595747e+01 4.97665504e+01\n",
      " 3.43455530e+02 1.29825404e+01 2.51753314e+02 1.05267141e+02\n",
      " 7.90241586e+01 1.01864661e+02 8.09168968e+01 1.02796929e+02\n",
      " 3.01592982e+02 5.85384948e+01 2.95910988e+02 8.20366902e+01\n",
      " 9.28287896e+01 9.42888066e+01 6.51077383e+01 1.10909688e+02\n",
      " 8.94444071e+01]\n",
      "25-th iteration, loss: 0.045040624536728344, 30 gd steps\n",
      "insert gradient: -0.0022598977829305805\n",
      "25-th iteration, new layer inserted. now 37 layers\n",
      "[ 58.14752664  96.0489059   46.2834711   80.80935385  52.81590165\n",
      " 100.56478406 143.18752746  99.91761038 107.78435993  99.52473063\n",
      " 111.76890272  97.60367503 100.73013704  91.11148081 128.09729715\n",
      "  26.9727431   41.36534944  51.18185287 343.62475442  15.74189567\n",
      " 125.12733003   0.         125.12733003 105.20558665  78.57556246\n",
      " 101.66862553  81.17925159 103.5678055  301.23766422  58.53585837\n",
      " 295.01211038  82.26257191  93.38068461  94.55844255  64.59902189\n",
      " 111.16291828  89.4446877 ]\n",
      "26-th iteration, loss: 0.03992282531585374, 35 gd steps\n",
      "insert gradient: -0.0019033236700755924\n",
      "26-th iteration, new layer inserted. now 39 layers\n",
      "[ 55.70688643  94.14210338  47.19080508  82.73784779  52.70481039\n",
      "  98.77487402  72.20432087   0.          72.20432087 101.33728663\n",
      " 106.06643754 100.55591572 113.74504645  98.54719368  99.54543503\n",
      "  93.3005895  130.02824591  28.60492706  32.32640547  59.00885855\n",
      " 342.10757607  24.84849389 120.43994626  17.7876477  116.7501935\n",
      " 101.17499096  80.02770329 101.90769296  79.39697579 105.45086696\n",
      " 301.46470557  58.63019133 293.14604199  82.56766431  94.29739891\n",
      "  94.73748131  62.87054611 111.67675342  88.40892208]\n",
      "27-th iteration, loss: 0.031144901073417733, 34 gd steps\n",
      "insert gradient: -0.0013510080787284477\n",
      "27-th iteration, new layer inserted. now 39 layers\n",
      "[ 54.26089186  92.7060684   43.23904946  81.10063202  55.22064509\n",
      " 103.46498874  67.75856658  24.14132068  62.812763    98.1417722\n",
      " 107.02921193 101.95076344 110.26302261 100.66759399  98.11028474\n",
      "  96.02706176 136.14272977  31.2020663   19.60258624  76.56547991\n",
      " 330.40751399  41.52696408 113.64886934  37.95808551 111.90486563\n",
      "  91.50038763  82.72952739 103.15542734  75.94818701 111.84250169\n",
      " 302.61195276  62.30575672 287.72430075  81.90877688  98.24110557\n",
      "  95.19750611  57.6400573  114.03099395  87.14874638]\n",
      "28-th iteration, loss: 0.02491911074029383, 25 gd steps\n",
      "insert gradient: -0.0024194504336935703\n",
      "28-th iteration, new layer inserted. now 41 layers\n",
      "[ 54.0023654   95.16964729  44.31871489  76.30917707  58.21992571\n",
      " 106.26035168  63.0391065   47.56874419  49.03561912  95.71305969\n",
      " 111.30384126 103.33175382 112.20903642 101.3649022   95.64496315\n",
      " 102.13617446 133.2136758   30.62059831  12.03758661  88.28234396\n",
      " 318.86061028  71.08882561 102.73166069  68.14543421 107.48914492\n",
      "  78.45032666  82.83493478 105.44285801  74.35407845 119.86361671\n",
      " 300.14124772  67.57595228 285.7737581   79.87282869 101.45195904\n",
      "  93.40783623  53.42495977  93.0763588    0.          23.2690897\n",
      "  87.020756  ]\n",
      "29-th iteration, loss: 0.022390100237491627, 22 gd steps\n",
      "insert gradient: -0.0007686287273006197\n",
      "29-th iteration, new layer inserted. now 43 layers\n",
      "[5.41303245e+01 9.51554204e+01 4.41974204e+01 7.61229621e+01\n",
      " 5.74588633e+01 1.05761941e+02 6.25764567e+01 4.78531912e+01\n",
      " 4.93546462e+01 9.58105707e+01 1.11251355e+02 1.03373731e+02\n",
      " 1.12195914e+02 1.01347341e+02 9.52950330e+01 1.02078898e+02\n",
      " 1.33124167e+02 3.05560819e+01 1.20530275e+01 8.82852881e+01\n",
      " 3.18734946e+02 7.12151251e+01 1.02659071e+02 6.82964794e+01\n",
      " 1.07266844e+02 7.84732265e+01 8.31856390e+01 1.05383543e+02\n",
      " 7.42722168e+01 1.20128037e+02 3.00357706e+02 6.77529805e+01\n",
      " 2.85984143e+02 7.99061025e+01 1.01450803e+02 9.33868181e+01\n",
      " 5.34586419e+01 7.45296695e+01 0.00000000e+00 1.86324174e+01\n",
      " 2.31239824e-01 2.33549146e+01 8.71698555e+01]\n",
      "30-th iteration, loss: 0.0222745231172416, 7 gd steps\n",
      "insert gradient: -0.005358266674871229\n",
      "30-th iteration, new layer inserted. now 45 layers\n",
      "[5.41322590e+01 9.51559785e+01 4.41976525e+01 7.61222203e+01\n",
      " 5.74544359e+01 1.05758604e+02 6.25726145e+01 4.78547043e+01\n",
      " 4.93549405e+01 9.58096146e+01 1.11246860e+02 1.03371036e+02\n",
      " 1.12188233e+02 1.01342426e+02 9.52809689e+01 1.02071838e+02\n",
      " 1.33104270e+02 3.05394759e+01 1.20225288e+01 8.82736376e+01\n",
      " 3.18716674e+02 7.12047563e+01 1.02630215e+02 6.82843092e+01\n",
      " 0.00000000e+00 3.55271368e-15 1.07234629e+02 7.84608658e+01\n",
      " 8.31650227e+01 1.05373896e+02 7.42575007e+01 1.20123452e+02\n",
      " 3.00352752e+02 6.77520622e+01 2.85983394e+02 7.99056052e+01\n",
      " 1.01449434e+02 9.33861867e+01 5.34585451e+01 7.45303264e+01\n",
      " 2.00507087e-03 1.86330743e+01 2.33033868e-01 2.33555783e+01\n",
      " 8.71706584e+01]\n",
      "31-th iteration, loss: 0.022214457281103783, 19 gd steps\n",
      "insert gradient: -0.0012893702205792673\n",
      "31-th iteration, new layer inserted. now 47 layers\n",
      "[5.41677377e+01 9.51703040e+01 4.42162407e+01 7.61244093e+01\n",
      " 5.74244329e+01 1.05735356e+02 6.25644683e+01 4.79085517e+01\n",
      " 0.00000000e+00 3.55271368e-15 4.94120063e+01 9.58259316e+01\n",
      " 1.11234578e+02 1.03373284e+02 1.12166892e+02 1.01334178e+02\n",
      " 9.52202944e+01 1.02061921e+02 1.33098948e+02 3.05356943e+01\n",
      " 1.20194571e+01 8.82778313e+01 3.18705078e+02 7.12343730e+01\n",
      " 1.02651637e+02 6.83148899e+01 6.48465436e-03 3.05549637e-02\n",
      " 1.07223452e+02 7.84604927e+01 8.31750826e+01 1.05355744e+02\n",
      " 7.42346088e+01 1.20143537e+02 3.00374698e+02 6.77748698e+01\n",
      " 2.86009619e+02 7.99109618e+01 1.01448754e+02 9.33838298e+01\n",
      " 5.34640930e+01 7.45420504e+01 3.25058381e-02 1.86447967e+01\n",
      " 2.60993438e-01 2.33673918e+01 8.71859318e+01]\n",
      "32-th iteration, loss: 0.02124070274283381, 42 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "32-th iteration, new layer inserted. now 49 layers\n",
      "[5.53343100e+01 9.54207716e+01 4.43118837e+01 7.60737034e+01\n",
      " 5.67271286e+01 1.05050650e+02 6.17156541e+01 4.91766574e+01\n",
      " 6.79408406e-01 1.19409870e+00 5.01794480e+01 9.60779515e+01\n",
      " 1.10902274e+02 1.03309517e+02 1.11227058e+02 1.01365533e+02\n",
      " 9.52218391e+01 1.02539448e+02 1.33042722e+02 3.01117696e+01\n",
      " 1.18304623e+01 8.85965094e+01 3.18163058e+02 7.28103357e+01\n",
      " 1.02896083e+02 6.96019502e+01 5.22854236e-03 1.31757209e+00\n",
      " 1.05929815e+02 7.82638422e+01 8.34199158e+01 1.04772420e+02\n",
      " 7.36663677e+01 1.20788553e+02 3.00592624e+02 6.84650013e+01\n",
      " 1.43098157e+02 0.00000000e+00 1.43098157e+02 7.99759372e+01\n",
      " 1.01727350e+02 9.32055690e+01 5.34072944e+01 7.47862255e+01\n",
      " 1.64521388e-01 1.88842254e+01 6.72177986e-01 2.36024494e+01\n",
      " 8.75600384e+01]\n",
      "33-th iteration, loss: 0.020279366562165703, 48 gd steps\n",
      "insert gradient: -0.012598757842674828\n",
      "33-th iteration, new layer inserted. now 49 layers\n",
      "[5.57348112e+01 9.50838051e+01 4.39776330e+01 7.62505361e+01\n",
      " 5.71826941e+01 1.04759902e+02 6.06093633e+01 5.08581587e+01\n",
      " 1.11393775e-01 2.68518979e+00 5.02792496e+01 9.62168556e+01\n",
      " 1.11078951e+02 1.03145333e+02 1.10279633e+02 1.01335549e+02\n",
      " 9.61644792e+01 1.03750471e+02 1.32886816e+02 2.90280993e+01\n",
      " 1.14521697e+01 8.92702778e+01 3.17131753e+02 7.59478397e+01\n",
      " 1.02482086e+02 7.21201780e+01 1.29423652e-03 3.83573304e+00\n",
      " 0.00000000e+00 2.22044605e-16 1.03879397e+02 7.81821985e+01\n",
      " 8.36456064e+01 1.03752013e+02 7.28170314e+01 1.21820068e+02\n",
      " 3.00447916e+02 6.96478624e+01 1.42545582e+02 2.99359357e+00\n",
      " 1.42533726e+02 7.98906046e+01 1.01884909e+02 9.26992033e+01\n",
      " 5.32473483e+01 9.43676435e+01 6.30638859e-01 2.39362752e+01\n",
      " 8.78235278e+01]\n",
      "34-th iteration, loss: 0.02023908539483881, 7 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 51 layers\n",
      "[5.57348716e+01 9.50837607e+01 4.39777342e+01 7.62507124e+01\n",
      " 5.71830714e+01 1.04760034e+02 6.06095754e+01 5.08589275e+01\n",
      " 1.11922651e-01 2.68594755e+00 5.02800533e+01 9.62174345e+01\n",
      " 1.11080313e+02 1.03146034e+02 1.10280986e+02 1.01336258e+02\n",
      " 9.61660838e+01 1.03751943e+02 1.32890302e+02 2.90300561e+01\n",
      " 1.14559193e+01 8.92725334e+01 3.17135931e+02 7.59521980e+01\n",
      " 1.02491598e+02 7.21259404e+01 1.40317004e-02 3.84149545e+00\n",
      " 1.28899748e-02 5.76240938e-03 1.03892287e+02 7.81874984e+01\n",
      " 8.36550766e+01 1.03755338e+02 7.28212475e+01 6.09113049e+01\n",
      " 0.00000000e+00 6.09113049e+01 3.00450551e+02 6.96491242e+01\n",
      " 1.42546478e+02 2.99499235e+00 1.42534611e+02 7.98909561e+01\n",
      " 1.01885444e+02 9.26992852e+01 5.32475401e+01 9.43677589e+01\n",
      " 6.30462289e-01 2.39363888e+01 8.78234935e+01]\n",
      "35-th iteration, loss: 0.019841409286305896, 89 gd steps\n",
      "insert gradient: -0.0010521098399428122\n",
      "35-th iteration, new layer inserted. now 49 layers\n",
      "[5.57651961e+01 9.49283698e+01 4.39708353e+01 7.63923274e+01\n",
      " 5.73836735e+01 1.04591123e+02 6.00772446e+01 5.48025663e+01\n",
      " 5.02808023e+01 9.62557793e+01 1.11206795e+02 1.03041311e+02\n",
      " 1.09980308e+02 1.01172676e+02 9.63338891e+01 1.04207916e+02\n",
      " 1.32813239e+02 2.86054889e+01 1.14014050e+01 8.95835508e+01\n",
      " 3.16718449e+02 7.72352514e+01 1.01916463e+02 7.30911064e+01\n",
      " 1.33388915e-03 4.80661280e+00 8.79024452e-03 9.70743577e-01\n",
      " 1.03171012e+02 7.82295814e+01 8.34589452e+01 1.03291312e+02\n",
      " 7.23578112e+01 6.13620528e+01 5.89285626e-02 6.13628921e+01\n",
      " 3.00385487e+02 7.02436342e+01 1.42373485e+02 4.31948938e+00\n",
      " 1.42358740e+02 7.98980072e+01 1.01794749e+02 9.24817764e+01\n",
      " 5.33415142e+01 9.45613211e+01 3.11064125e-01 2.41296111e+01\n",
      " 8.76740224e+01]\n",
      "36-th iteration, loss: 0.01966777274350169, 57 gd steps\n",
      "insert gradient: -0.0024925639157450472\n",
      "36-th iteration, new layer inserted. now 45 layers\n",
      "[5.57650478e+01 9.48554035e+01 4.39806157e+01 7.64675117e+01\n",
      " 5.74933955e+01 1.04524755e+02 5.98633877e+01 5.50982722e+01\n",
      " 5.03019125e+01 9.62788794e+01 1.11266538e+02 1.02999589e+02\n",
      " 1.09895177e+02 1.01093071e+02 9.63900344e+01 1.04406593e+02\n",
      " 1.32777705e+02 2.84461431e+01 1.14268637e+01 8.97372700e+01\n",
      " 3.16553169e+02 7.78004163e+01 1.01588718e+02 8.00762589e+01\n",
      " 1.02881306e+02 7.82723244e+01 8.33605808e+01 1.03097929e+02\n",
      " 7.21050310e+01 6.15612517e+01 9.36194369e-02 6.15627979e+01\n",
      " 3.00292881e+02 7.04869476e+01 1.42225858e+02 4.94102820e+00\n",
      " 1.42210198e+02 7.98859065e+01 1.01728193e+02 9.23726136e+01\n",
      " 5.33992352e+01 9.46728393e+01 1.76479821e-01 2.42419424e+01\n",
      " 8.75706882e+01]\n",
      "37-th iteration, loss: 0.019664009186053672, 10 gd steps\n",
      "insert gradient: -0.00037860562809560895\n",
      "37-th iteration, new layer inserted. now 47 layers\n",
      "[5.57650579e+01 9.48538339e+01 4.39808046e+01 7.64690242e+01\n",
      " 5.74954802e+01 1.04523160e+02 5.98585643e+01 5.51044583e+01\n",
      " 5.03019653e+01 9.62789937e+01 1.11266874e+02 1.02998023e+02\n",
      " 1.09891177e+02 1.01090567e+02 9.63897385e+01 1.04410961e+02\n",
      " 1.32779414e+02 2.84446729e+01 1.14302744e+01 8.97422093e+01\n",
      " 3.16553465e+02 7.78156196e+01 1.01589940e+02 8.00891908e+01\n",
      " 1.02886571e+02 7.82778007e+01 8.33662038e+01 1.03096624e+02\n",
      " 7.21020455e+01 6.15672381e+01 9.95675787e-02 6.15688038e+01\n",
      " 3.00292237e+02 7.04925674e+01 1.42222483e+02 0.00000000e+00\n",
      " 7.10542736e-15 4.95479374e+00 1.42206785e+02 7.98855258e+01\n",
      " 1.01726566e+02 9.23700237e+01 5.34001699e+01 9.46753245e+01\n",
      " 1.73853230e-01 2.42444453e+01 8.75680976e+01]\n",
      "38-th iteration, loss: 0.019659278616645418, 12 gd steps\n",
      "insert gradient: -0.0006217277486816044\n",
      "38-th iteration, new layer inserted. now 45 layers\n",
      "[5.57651309e+01 9.48523311e+01 4.39810663e+01 7.64705392e+01\n",
      " 5.74974458e+01 1.04521521e+02 5.98537717e+01 5.51106027e+01\n",
      " 5.03019534e+01 9.62789491e+01 1.11266626e+02 1.02996271e+02\n",
      " 1.09887119e+02 1.01087769e+02 9.63882921e+01 1.04414175e+02\n",
      " 1.32777471e+02 2.84407068e+01 1.14291782e+01 8.97448765e+01\n",
      " 3.16549370e+02 7.78271984e+01 1.01582226e+02 8.00971462e+01\n",
      " 1.02879878e+02 7.82782740e+01 8.33623629e+01 1.03091867e+02\n",
      " 7.20946508e+01 6.15705349e+01 9.83458096e-02 6.15721188e+01\n",
      " 3.00288389e+02 7.04970104e+01 1.42217999e+02 4.98039239e+00\n",
      " 1.42202324e+02 7.98847516e+01 1.01724345e+02 9.23672473e+01\n",
      " 5.34008919e+01 9.46777297e+01 1.71230609e-01 2.42468682e+01\n",
      " 8.75654617e+01]\n",
      "39-th iteration, loss: 0.018662920449718644, 33 gd steps\n",
      "insert gradient: -0.000524566835089426\n",
      "39-th iteration, new layer inserted. now 45 layers\n",
      "[5.59555177e+01 9.42145451e+01 4.38932407e+01 7.69672904e+01\n",
      " 5.85274394e+01 1.04103431e+02 5.83397962e+01 5.79179149e+01\n",
      " 5.04255949e+01 9.62633103e+01 1.11208418e+02 1.02562679e+02\n",
      " 1.09493687e+02 1.00316331e+02 9.64698057e+01 1.05943784e+02\n",
      " 1.32109429e+02 2.73696835e+01 1.16601992e+01 9.11899225e+01\n",
      " 3.15172948e+02 8.29709870e+01 9.98816920e+01 8.41331873e+01\n",
      " 1.01476662e+02 7.92273859e+01 8.31607022e+01 1.01974443e+02\n",
      " 7.10743629e+01 1.26353160e+02 2.99024392e+02 7.26506778e+01\n",
      " 1.41078049e+02 9.97948198e+00 8.46698533e+01 0.00000000e+00\n",
      " 5.64465689e+01 7.98067743e+01 1.01048212e+02 9.11415001e+01\n",
      " 5.34941228e+01 9.57450136e+01 3.16942952e-04 2.53181194e+01\n",
      " 8.65139192e+01]\n",
      "40-th iteration, loss: 0.016577422825176415, 44 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 45 layers\n",
      "[ 55.03457174  91.28690862  42.87583507  79.65568449  60.0501604\n",
      " 103.16494393  54.71257834  68.22191404  48.74774782  96.1087181\n",
      " 109.02369074 105.83966733 108.1016556  103.054841    92.57415016\n",
      " 110.13646281 128.2086379   26.12130209  12.89567932  19.54448586\n",
      "   0.          78.17794343 311.09248347  98.91841099  94.09922215\n",
      "  95.25069595  96.46847299  83.08332259  84.62720921  93.32136876\n",
      "  74.29840539 126.75379767 295.77205179  79.70653601 140.16417149\n",
      "  17.91146206  78.33928572   6.63725967  54.1525099   82.3716588\n",
      " 101.83583952  86.95676887  52.10681254 127.36387913  85.77248044]\n",
      "41-th iteration, loss: 0.016535306858070843, 20 gd steps\n",
      "insert gradient: -0.00011965507213886601\n",
      "41-th iteration, new layer inserted. now 47 layers\n",
      "[5.50595353e+01 9.12972450e+01 4.28897264e+01 7.96608513e+01\n",
      " 6.00403384e+01 1.03158733e+02 5.47079689e+01 6.82321922e+01\n",
      " 4.87504387e+01 9.61073947e+01 1.09002483e+02 1.05837656e+02\n",
      " 1.08073391e+02 1.03049678e+02 9.25557984e+01 1.10140309e+02\n",
      " 1.28225906e+02 2.61609577e+01 1.29488878e+01 1.95643321e+01\n",
      " 3.35089771e-02 7.81978622e+01 3.11044846e+02 9.89177793e+01\n",
      " 9.40601506e+01 9.52461754e+01 9.64388227e+01 8.30971238e+01\n",
      " 8.47000582e+01 9.33113370e+01 7.42809595e+01 1.26766005e+02\n",
      " 2.95778937e+02 7.97183299e+01 1.40150643e+02 1.79089311e+01\n",
      " 6.26560388e+01 0.00000000e+00 1.56640097e+01 6.65543976e+00\n",
      " 5.41507879e+01 8.23721690e+01 1.01827548e+02 8.69456865e+01\n",
      " 5.20970762e+01 1.27365401e+02 8.57617591e+01]\n",
      "42-th iteration, loss: 0.01647388367762255, 29 gd steps\n",
      "insert gradient: -0.0014420403737270208\n",
      "42-th iteration, new layer inserted. now 45 layers\n",
      "[ 55.33403266  91.3873724   42.99446275  79.70306727  59.86313499\n",
      " 103.0590143   54.63266947  68.40807614  48.85282172  96.15962523\n",
      " 108.91582181 105.95473008 108.02276875 103.0775046   92.27341876\n",
      " 110.22983116 128.22233254  26.21327827  13.07086508  98.07146883\n",
      " 310.76300105  99.14275906  93.93833989  95.39590994  96.35413681\n",
      "  83.22536417  84.90508388  93.04413367  74.24134244 126.6049305\n",
      " 295.63223264  79.86340597 140.08677248  17.97030187  62.51478293\n",
      "   0.31992866  15.52100011   6.96072994  54.23724471  82.44252757\n",
      " 101.84299355  86.83794055  52.00749829 127.39076682  85.63139212]\n",
      "43-th iteration, loss: 0.016310014762491862, 32 gd steps\n",
      "insert gradient: -0.00039451452076203153\n",
      "43-th iteration, new layer inserted. now 45 layers\n",
      "[ 55.67660388  91.3045993   42.89589969  79.9174828   59.83368153\n",
      " 103.0376263   54.43282408  68.98649694  48.67915933  96.15287177\n",
      " 108.51942496 106.49044108 107.97786846 103.48399133  91.71395251\n",
      " 110.91241028 127.98710581  25.8726944   13.36491138  98.73123557\n",
      " 310.31964202 100.3976722   93.4317718   95.98905275  95.93800119\n",
      "  83.67377493  85.29190988  91.83230878  75.06188865 125.77712085\n",
      " 295.34435428  80.60954519 139.96200324  18.48036814  61.71781055\n",
      "   1.10278658  14.67660194   7.90842399  54.35006865  82.78111085\n",
      " 102.17102309  86.49160941  51.86050013 127.63885431  85.36856837]\n",
      "44-th iteration, loss: 0.016302268190172456, 16 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "44-th iteration, new layer inserted. now 47 layers\n",
      "[ 55.67714595  91.30440791  42.89648445  79.9191325   59.83651557\n",
      " 103.03883978  54.43124482  68.98554535  48.67110549  96.14788634\n",
      " 108.50799205 106.48602458 107.96289201 103.4822557   91.71365928\n",
      " 110.91646177 127.99617933  25.88411023  13.38583122  98.73956439\n",
      " 124.1280035    0.         186.19200525 100.4000955   93.42353402\n",
      "  95.9861147   95.92367746  83.6730275   85.297903    91.82717185\n",
      "  75.06380702 125.77729984 295.34775375  80.61356292 139.96134263\n",
      "  18.48198011  61.71428188   1.10610524  14.67260746   7.9128584\n",
      "  54.35032811  82.78194541 102.17068363  86.48969939  51.8596441\n",
      " 127.64016827  85.36817263]\n",
      "45-th iteration, loss: 0.016298203415446653, 13 gd steps\n",
      "insert gradient: -0.0002966837693122575\n",
      "45-th iteration, new layer inserted. now 47 layers\n",
      "[5.56788380e+01 9.13045749e+01 4.28989653e+01 7.99231873e+01\n",
      " 5.98421014e+01 1.03041955e+02 5.44327998e+01 6.89896650e+01\n",
      " 4.86664281e+01 9.61448436e+01 1.08498821e+02 1.06486420e+02\n",
      " 1.07954335e+02 1.03482587e+02 9.17054184e+01 1.10921112e+02\n",
      " 1.27995626e+02 2.58847264e+01 1.33936557e+01 9.87453708e+01\n",
      " 1.24118840e+02 3.13865017e-02 1.86182840e+02 1.00407488e+02\n",
      " 9.34175762e+01 9.59895173e+01 9.59163628e+01 8.36769623e+01\n",
      " 8.53094017e+01 9.18166925e+01 7.50626928e+01 1.25771267e+02\n",
      " 2.95345980e+02 8.06188440e+01 1.39956835e+02 1.84843547e+01\n",
      " 6.17062391e+01 1.11322589e+00 1.46636330e+01 7.92159779e+00\n",
      " 5.43503569e+01 8.27837603e+01 1.02170906e+02 8.64862256e+01\n",
      " 5.18583325e+01 1.27643014e+02 8.53672812e+01]\n",
      "46-th iteration, loss: 0.014683821278449055, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "46-th iteration, new layer inserted. now 49 layers\n",
      "[ 55.43155182  90.52517738  44.28378932  81.3954223   58.18378922\n",
      " 103.9426004   53.93230085  76.90954972  44.58158336  93.35407512\n",
      " 107.89351389 109.77414186 106.16838838 106.80120436  86.72574485\n",
      " 117.35086959 126.55218301  27.61414097  16.01199428 107.39602946\n",
      " 115.54301844  12.93779443 175.05392773 112.952616    89.71944718\n",
      " 107.07627954  89.93224073  91.46809351  89.8083331   76.93990564\n",
      "  41.13319229   0.          41.13319229 111.76622171 295.45390938\n",
      "  84.81957415 139.49392674  24.29627128  52.98836856   8.58149776\n",
      "   4.80727986  12.60900898  58.95529741  89.51376559 101.84444748\n",
      "  83.89258476  50.52218746 129.95302165  84.7123078 ]\n",
      "47-th iteration, loss: 0.01463231079545091, 15 gd steps\n",
      "insert gradient: -0.001094744529885136\n",
      "47-th iteration, new layer inserted. now 49 layers\n",
      "[5.54496270e+01 9.05171977e+01 4.42233301e+01 8.13476344e+01\n",
      " 5.80854690e+01 1.03882055e+02 5.38468218e+01 7.68901973e+01\n",
      " 4.46057781e+01 9.33493163e+01 1.07853754e+02 1.09779468e+02\n",
      " 1.06224383e+02 1.06814499e+02 8.66134797e+01 1.17345213e+02\n",
      " 1.26478936e+02 2.75767967e+01 1.60041874e+01 1.07388074e+02\n",
      " 1.15641829e+02 1.31228263e+01 1.75155007e+02 1.12936439e+02\n",
      " 8.97744780e+01 1.07064587e+02 8.99131258e+01 9.14690314e+01\n",
      " 8.96923125e+01 7.69153453e+01 4.11553641e+01 2.61082456e-02\n",
      " 4.11553056e+01 1.11613148e+02 2.95410015e+02 8.48260227e+01\n",
      " 1.39623801e+02 2.43934993e+01 5.30497386e+01 8.61855615e+00\n",
      " 4.83819315e+00 1.26589524e+01 5.89857638e+01 8.95156893e+01\n",
      " 1.01860950e+02 8.39241041e+01 5.06120974e+01 1.30011119e+02\n",
      " 8.47920130e+01]\n",
      "48-th iteration, loss: 0.014623390307719059, 12 gd steps\n",
      "insert gradient: -0.0001143955242688773\n",
      "48-th iteration, new layer inserted. now 49 layers\n",
      "[5.54596988e+01 9.05215570e+01 4.42249422e+01 8.13490098e+01\n",
      " 5.80950582e+01 1.03885829e+02 5.38463411e+01 7.68884535e+01\n",
      " 4.46088879e+01 9.33442488e+01 1.07834216e+02 1.09772747e+02\n",
      " 1.06221576e+02 1.06816925e+02 8.66108882e+01 1.17346512e+02\n",
      " 1.26484741e+02 2.76051578e+01 1.60398867e+01 1.07391747e+02\n",
      " 1.15630683e+02 1.31196158e+01 1.75124691e+02 1.12916238e+02\n",
      " 8.97447685e+01 1.07050224e+02 8.99062320e+01 9.14801963e+01\n",
      " 8.97390335e+01 7.69317259e+01 4.11823771e+01 3.23918275e-02\n",
      " 4.11822525e+01 1.11604182e+02 2.95417227e+02 8.48294748e+01\n",
      " 1.39650414e+02 2.44161869e+01 5.30607321e+01 8.62821677e+00\n",
      " 4.84084531e+00 1.26718486e+01 5.89895485e+01 8.95143245e+01\n",
      " 1.01861479e+02 8.39303507e+01 5.06319334e+01 1.30024465e+02\n",
      " 8.48095459e+01]\n",
      "49-th iteration, loss: 0.014620477939734934, 6 gd steps\n",
      "insert gradient: -0.0003655952837460945\n",
      "49-th iteration, new layer inserted. now 51 layers\n",
      "[5.54599524e+01 9.05216544e+01 4.42249223e+01 8.13489794e+01\n",
      " 5.80950819e+01 1.03885787e+02 5.38461492e+01 7.68883543e+01\n",
      " 4.46088759e+01 9.33440268e+01 1.07833436e+02 1.09772372e+02\n",
      " 1.06221003e+02 1.06816503e+02 8.66092518e+01 1.17345901e+02\n",
      " 1.26482961e+02 2.76042772e+01 1.60383786e+01 1.07390820e+02\n",
      " 1.15628612e+02 1.31186177e+01 1.75122166e+02 1.12914327e+02\n",
      " 8.97406655e+01 1.07047779e+02 0.00000000e+00 7.10542736e-15\n",
      " 8.99009332e+01 9.14781729e+01 8.97349408e+01 7.69300477e+01\n",
      " 4.11792219e+01 2.93146639e-02 4.11790966e+01 1.11602221e+02\n",
      " 2.95415034e+02 8.48286771e+01 1.39649996e+02 2.44161622e+01\n",
      " 5.30603247e+01 8.62827528e+00 4.84029715e+00 1.26719287e+01\n",
      " 5.89892210e+01 8.95141582e+01 1.01861306e+02 8.39304636e+01\n",
      " 5.06324184e+01 1.30024821e+02 8.48099757e+01]\n",
      "50-th iteration, loss: 0.014371703225350519, 38 gd steps\n",
      "insert gradient: -0.0005288080747196467\n",
      "50-th iteration, new layer inserted. now 51 layers\n",
      "[5.67729208e+01 9.11625355e+01 4.41096429e+01 8.11799717e+01\n",
      " 5.86170887e+01 1.04057216e+02 5.30958156e+01 7.62889940e+01\n",
      " 4.51827608e+01 9.32864178e+01 1.07194670e+02 1.09791929e+02\n",
      " 1.06440403e+02 1.07396925e+02 8.53574538e+01 1.17711326e+02\n",
      " 1.25411966e+02 2.84103444e+01 1.73192660e+01 1.07527204e+02\n",
      " 1.16013628e+02 1.48962211e+01 1.73324634e+02 1.12565959e+02\n",
      " 8.93587162e+01 1.07202345e+02 1.10763565e-03 2.01220733e-01\n",
      " 8.98595578e+01 9.26270140e+01 8.97626849e+01 7.70691992e+01\n",
      " 4.17739195e+01 3.52124796e-02 4.17735413e+01 1.08152908e+02\n",
      " 2.95032269e+02 8.39314445e+01 1.40205154e+02 2.67763392e+01\n",
      " 5.19969427e+01 1.02933290e+01 2.87185437e+00 1.43926837e+01\n",
      " 5.90408870e+01 8.94713406e+01 1.01380363e+02 8.42860288e+01\n",
      " 5.14879415e+01 1.30583008e+02 8.53124773e+01]\n",
      "51-th iteration, loss: 0.014363132654822681, 13 gd steps\n",
      "insert gradient: -0.00012272931148507612\n",
      "51-th iteration, new layer inserted. now 51 layers\n",
      "[5.67448945e+01 9.11500389e+01 4.40777395e+01 8.11513189e+01\n",
      " 5.85497371e+01 1.04021730e+02 5.30525422e+01 7.62693840e+01\n",
      " 4.51727966e+01 9.32815567e+01 1.07191695e+02 1.09800227e+02\n",
      " 1.06470588e+02 1.07419615e+02 8.53679332e+01 1.17730067e+02\n",
      " 1.25387872e+02 2.83994387e+01 1.73140875e+01 1.07525734e+02\n",
      " 1.16017263e+02 1.49376611e+01 1.73298153e+02 1.12580112e+02\n",
      " 8.93572488e+01 1.07207054e+02 1.38322564e-02 2.05873904e-01\n",
      " 8.98754258e+01 9.26335177e+01 8.96960342e+01 7.70614078e+01\n",
      " 4.18103738e+01 4.29274233e-02 4.18099048e+01 1.08094517e+02\n",
      " 2.95027695e+02 8.39161350e+01 1.40202372e+02 2.68191302e+01\n",
      " 5.19658094e+01 1.03124362e+01 2.82341745e+00 1.44119447e+01\n",
      " 5.90312166e+01 8.94716218e+01 1.01373894e+02 8.42917232e+01\n",
      " 5.14907524e+01 1.30580787e+02 8.53099389e+01]\n",
      "52-th iteration, loss: 0.014282560239041023, 31 gd steps\n",
      "insert gradient: -0.00046407632546438066\n",
      "52-th iteration, new layer inserted. now 49 layers\n",
      "[5.67299551e+01 9.13453492e+01 4.42442276e+01 8.10940333e+01\n",
      " 5.84624628e+01 1.04022094e+02 5.30670472e+01 7.61066666e+01\n",
      " 4.52024512e+01 9.32158616e+01 1.07012005e+02 1.09889025e+02\n",
      " 1.06442618e+02 1.07758335e+02 8.50003297e+01 1.18008242e+02\n",
      " 1.24937852e+02 2.85211042e+01 1.78701963e+01 1.07556868e+02\n",
      " 1.16188026e+02 1.55517064e+01 1.72470348e+02 1.12738817e+02\n",
      " 8.92271584e+01 1.07825200e+02 0.00000000e+00 7.10542736e-15\n",
      " 8.97621969e+01 9.29984202e+01 8.96617138e+01 7.70048176e+01\n",
      " 8.40628235e+01 1.06774864e+02 2.95222341e+02 8.37259444e+01\n",
      " 1.40380383e+02 2.78053991e+01 5.12787087e+01 1.09172790e+01\n",
      " 1.80417522e+00 1.49979295e+01 5.90819410e+01 8.96111229e+01\n",
      " 1.01251945e+02 8.44719834e+01 5.15787361e+01 1.30528957e+02\n",
      " 8.51629489e+01]\n",
      "53-th iteration, loss: 0.014128357158581812, 37 gd steps\n",
      "insert gradient: -0.002182626695825098\n",
      "53-th iteration, new layer inserted. now 45 layers\n",
      "[ 56.94052725  91.80319588  44.51355047  80.88282549  58.26658687\n",
      " 104.04741266  53.22839504  75.78628296  45.20676764  93.08598191\n",
      " 106.80175736 110.18592819 106.15237146 108.56473746  84.29184629\n",
      " 118.54414551 124.0121024   28.71042882  19.00199564 107.54783956\n",
      " 116.773624    16.83430284 170.61927809 113.21435357  89.00909846\n",
      " 108.71963579  89.49929409  93.78759254  89.43729569  76.88329856\n",
      "  84.79199911 104.04719531 295.6349671   83.40424823 140.68083063\n",
      "  29.91169647  49.68204867  28.4108351   59.25683922  90.00696843\n",
      " 100.95206558  84.94408471  51.83307237 130.39370352  84.83569686]\n",
      "54-th iteration, loss: 0.014115992769063133, 22 gd steps\n",
      "insert gradient: -0.0004605357605988438\n",
      "54-th iteration, new layer inserted. now 45 layers\n",
      "[ 56.9397464   91.81282658  44.52387956  80.87932191  58.25608918\n",
      " 104.04266727  53.22852293  75.77898819  45.20351576  93.07900511\n",
      " 106.7903093  110.19334346 106.1496396  108.58591976  84.27556394\n",
      " 118.56118118 123.98873615  28.71254389  19.04557722 107.55031157\n",
      " 116.80928824  16.89734297 170.59223644 113.23469683  88.98948828\n",
      " 108.71759064  89.48842288  93.80391374  89.43259779  76.89124524\n",
      "  84.8459039  103.99863401 295.6755629   83.40083156 140.68912969\n",
      "  29.96464357  49.62369442  28.43665011  59.25229446  90.01357499\n",
      " 100.93247331  84.95491832  51.83993817 130.38998207  84.82650248]\n",
      "55-th iteration, loss: 0.013205957602994135, 113 gd steps\n",
      "insert gradient: -2.6836263550905988e-05\n",
      "55-th iteration, new layer inserted. now 47 layers\n",
      "[ 56.27555283  93.07101659  46.91416828  79.98205065  57.83860988\n",
      " 102.31137052  54.54222808  74.80179266  45.25593669  91.83797805\n",
      " 106.16021236 113.88102691 101.97091462 118.25839292  79.92366026\n",
      " 114.33748819 118.48129274  32.08078282  28.09389273 100.20404555\n",
      " 121.20938563  30.44335351 158.73315258 115.27754201  90.0154718\n",
      " 111.59540269  86.81885891 101.55101948  86.85847247  78.07263023\n",
      "  90.1018702   83.89923277 297.62496679  85.9532324  139.51190175\n",
      "  46.02431982  32.20223014  45.07811693  58.54642366  93.05200994\n",
      "  97.40648216  89.71499598  53.83744169 103.6873489    0.\n",
      "  25.92183722  81.67103681]\n",
      "56-th iteration, loss: 0.013179421533777236, 8 gd steps\n",
      "insert gradient: -0.0011653441672355993\n",
      "56-th iteration, new layer inserted. now 49 layers\n",
      "[5.62760582e+01 9.30712810e+01 4.69141996e+01 7.99818404e+01\n",
      " 5.78377236e+01 1.02310648e+02 5.45403507e+01 7.48003804e+01\n",
      " 4.52529264e+01 9.18358788e+01 1.06155748e+02 1.13877772e+02\n",
      " 1.01962859e+02 1.18254103e+02 7.99156699e+01 1.14334160e+02\n",
      " 1.18474151e+02 3.20765032e+01 2.80876823e+01 1.00200503e+02\n",
      " 1.21201053e+02 3.04385822e+01 1.58722660e+02 1.15270130e+02\n",
      " 8.99988324e+01 1.11584724e+02 0.00000000e+00 7.10542736e-15\n",
      " 8.67958948e+01 1.01540765e+02 8.68379140e+01 7.80648151e+01\n",
      " 9.00884890e+01 8.38944446e+01 2.97618711e+02 8.59510408e+01\n",
      " 1.39509694e+02 4.60232337e+01 3.22003077e+01 4.50784155e+01\n",
      " 5.85465364e+01 9.30522168e+01 9.74063426e+01 8.97150611e+01\n",
      " 5.38374573e+01 1.03687498e+02 2.42727972e-04 2.59219864e+01\n",
      " 8.16710043e+01]\n",
      "57-th iteration, loss: 0.013175061062891541, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "57-th iteration, new layer inserted. now 51 layers\n",
      "[5.62811046e+01 9.30746193e+01 4.69183022e+01 7.99837325e+01\n",
      " 5.78393248e+01 1.02311774e+02 5.45414287e+01 7.48020793e+01\n",
      " 4.52560508e+01 9.18368485e+01 1.06155396e+02 1.13879507e+02\n",
      " 1.01962330e+02 1.18250448e+02 7.99034362e+01 1.14330585e+02\n",
      " 1.18467247e+02 3.20719430e+01 2.80863401e+01 1.00200699e+02\n",
      " 1.21201906e+02 3.04363268e+01 3.96790853e+01 0.00000000e+00\n",
      " 1.19037256e+02 1.15270411e+02 9.00095968e+01 1.11591796e+02\n",
      " 9.11829946e-03 7.06494649e-03 8.68050176e+01 1.01543518e+02\n",
      " 8.68240486e+01 7.80602550e+01 9.00774585e+01 8.38921415e+01\n",
      " 2.97616163e+02 8.59509092e+01 1.39511990e+02 4.60266215e+01\n",
      " 3.22036237e+01 4.50872560e+01 5.85557008e+01 9.30568686e+01\n",
      " 9.74101342e+01 8.97171878e+01 5.38388894e+01 1.03688777e+02\n",
      " 1.84200872e-03 2.59232658e+01 8.16709485e+01]\n",
      "58-th iteration, loss: 0.01299446748851442, 23 gd steps\n",
      "insert gradient: -0.007269061311140518\n",
      "58-th iteration, new layer inserted. now 51 layers\n",
      "[ 56.19174926  93.7795795   47.03185826  79.85945711  57.92966541\n",
      " 102.84672007  53.79416698  74.67745423  45.2124725   91.61064573\n",
      " 105.52660141 115.41410491 101.91993099 118.01406628  79.77881106\n",
      " 112.82598258 117.93988701  31.38565046  30.18469395 100.93523846\n",
      " 120.63665871  34.79237705  36.51637338   4.83659005 116.2242994\n",
      " 115.28571338  89.73819694  84.2840308    0.          28.09467693\n",
      "  86.31537192 102.29593289  86.39942634  77.97952416  91.23449569\n",
      "  83.23211516 296.81693381  85.84781741 138.75195583  48.74271923\n",
      "  29.75181414  48.00570149  59.20891232  93.91566117  96.94799149\n",
      "  90.09869736  53.50123955 104.47795907   0.7933996   26.70676105\n",
      "  80.59792501]\n",
      "59-th iteration, loss: 0.01295180242966148, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "59-th iteration, new layer inserted. now 53 layers\n",
      "[5.61921607e+01 9.37798846e+01 4.70324716e+01 7.98598697e+01\n",
      " 5.79304107e+01 1.02847273e+02 5.37954453e+01 7.46785334e+01\n",
      " 4.52147290e+01 9.16120610e+01 1.05529269e+02 1.15416306e+02\n",
      " 1.01924588e+02 1.18016157e+02 7.97820103e+01 1.12827598e+02\n",
      " 1.17943669e+02 3.13876562e+01 3.01887500e+01 1.00937932e+02\n",
      " 1.20643514e+02 3.47963332e+01 3.65250823e+01 4.84413705e+00\n",
      " 1.16232464e+02 1.15291520e+02 8.97520858e+01 8.42927726e+01\n",
      " 2.00783772e-02 2.81034188e+01 8.63331096e+01 1.02303750e+02\n",
      " 8.64127504e+01 7.79846337e+01 9.12427390e+01 8.32354712e+01\n",
      " 2.96822019e+02 4.29248858e+01 0.00000000e+00 4.29248858e+01\n",
      " 1.38754460e+02 4.87441490e+01 2.97537902e+01 4.80068173e+01\n",
      " 5.92101810e+01 9.39161156e+01 9.69483790e+01 9.00988682e+01\n",
      " 5.35013108e+01 1.04477912e+02 7.93108339e-01 2.67067079e+01\n",
      " 8.05977072e+01]\n",
      "60-th iteration, loss: 0.012938723512593772, 107 gd steps\n",
      "insert gradient: -0.00015187304001372312\n",
      "60-th iteration, new layer inserted. now 53 layers\n",
      "[5.62148551e+01 9.37937991e+01 4.70526736e+01 7.98679556e+01\n",
      " 5.79330675e+01 1.02848425e+02 5.37997350e+01 7.46835277e+01\n",
      " 4.52256547e+01 9.16160066e+01 1.05526045e+02 1.15423235e+02\n",
      " 1.01919721e+02 1.18013305e+02 7.97723179e+01 1.12822528e+02\n",
      " 1.17942408e+02 3.13843400e+01 3.02026299e+01 1.00944244e+02\n",
      " 9.04918408e+01 0.00000000e+00 3.01639469e+01 3.48248735e+01\n",
      " 3.65499249e+01 4.88520830e+00 1.16257240e+02 1.15305095e+02\n",
      " 8.97550394e+01 1.12399997e+02 8.63139023e+01 1.02300829e+02\n",
      " 8.63932689e+01 7.79776405e+01 9.12140935e+01 8.32387354e+01\n",
      " 2.96842637e+02 4.29302902e+01 4.63040808e-02 4.29302915e+01\n",
      " 1.38768352e+02 4.87531629e+01 2.97491621e+01 4.80326633e+01\n",
      " 5.92287623e+01 9.39219000e+01 9.69355586e+01 9.00982710e+01\n",
      " 5.34983697e+01 1.04474229e+02 7.73000911e-01 2.67026724e+01\n",
      " 8.05818267e+01]\n",
      "61-th iteration, loss: 0.012172742662131223, 52 gd steps\n",
      "insert gradient: -0.004283800676079953\n",
      "61-th iteration, new layer inserted. now 53 layers\n",
      "[ 57.64042335  94.69132816  47.37786648  80.00965564  57.95241224\n",
      " 102.58353372  52.09984649  74.47030024  46.27989967  91.31444655\n",
      " 104.11575858 120.05638396 101.32742277 117.62132431  79.41986633\n",
      " 108.35933158 117.26887975  33.84391939  32.91073841  96.84914982\n",
      "  82.51522585  13.35972311  24.41520419  60.26838609  27.62070587\n",
      "  15.37462099 109.67361196 115.3972113   89.41493639  83.10058806\n",
      "   0.          27.70019602  86.18763139 104.70359326  85.74045851\n",
      "  77.53654108  92.50354264  82.18748151 298.79077839  83.52115196\n",
      " 136.0733312   57.0192098   23.83973453  58.17818046  55.47579982\n",
      "  94.44815528  96.37926636  91.79392751  53.71355743 106.45521038\n",
      "   1.13788412  28.53675791  76.91425016]\n",
      "62-th iteration, loss: 0.012141571616381226, 187 gd steps\n",
      "insert gradient: -8.733230453648458e-05\n",
      "62-th iteration, new layer inserted. now 53 layers\n",
      "[5.76166738e+01 9.46836887e+01 4.73748109e+01 8.00071468e+01\n",
      " 5.79400678e+01 1.02575204e+02 5.20967556e+01 7.44701023e+01\n",
      " 4.62717785e+01 9.13129594e+01 1.04116728e+02 1.20058517e+02\n",
      " 1.01325526e+02 1.17614459e+02 7.93968600e+01 1.08348191e+02\n",
      " 1.17242076e+02 3.38270774e+01 3.28967122e+01 9.68465940e+01\n",
      " 8.25061920e+01 1.33691192e+01 2.44112937e+01 6.02949749e+01\n",
      " 2.76238610e+01 1.53854160e+01 1.09681418e+02 1.15412160e+02\n",
      " 8.94553437e+01 1.10820751e+02 8.62180544e+01 1.04708576e+02\n",
      " 8.57241686e+01 7.75422066e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.25363835e+01 8.21968177e+01 2.98793466e+02 8.34993951e+01\n",
      " 1.36046160e+02 5.70132733e+01 2.37743297e+01 5.81504814e+01\n",
      " 5.54501067e+01 9.44375662e+01 9.63577217e+01 9.17867191e+01\n",
      " 5.37047852e+01 1.06453195e+02 1.13333328e+00 2.85345181e+01\n",
      " 7.69055154e+01]\n",
      "63-th iteration, loss: 0.011774073721423363, 29 gd steps\n",
      "insert gradient: -0.0012026361101067958\n",
      "63-th iteration, new layer inserted. now 53 layers\n",
      "[6.02287108e+01 9.44700480e+01 4.70613260e+01 7.99059814e+01\n",
      " 5.87508354e+01 9.95596464e+01 5.14963736e+01 7.51414684e+01\n",
      " 4.72823962e+01 9.23521175e+01 1.03173155e+02 1.22183725e+02\n",
      " 1.00405128e+02 1.17853724e+02 7.93417162e+01 1.06082972e+02\n",
      " 1.17271390e+02 3.50932202e+01 3.32198740e+01 9.74521259e+01\n",
      " 7.34409352e+01 1.81668842e+01 2.13133306e+01 8.62065750e+01\n",
      " 1.99947171e+01 2.23609455e+01 1.07237393e+02 1.16862643e+02\n",
      " 8.89607472e+01 1.09731001e+02 8.64232076e+01 1.03955179e+02\n",
      " 8.53118084e+01 7.83934406e+01 2.54540536e-01 5.96206512e-01\n",
      " 9.30760675e+01 8.10196825e+01 3.01064424e+02 8.03187822e+01\n",
      " 1.32462309e+02 6.40857260e+01 2.11034566e+01 6.19765732e+01\n",
      " 5.52313918e+01 9.45305177e+01 9.59262675e+01 9.25422550e+01\n",
      " 5.35434093e+01 1.07637259e+02 2.02461411e+00 2.94035006e+01\n",
      " 7.28598535e+01]\n",
      "64-th iteration, loss: 0.011696371821600637, 28 gd steps\n",
      "insert gradient: -0.00010220641340708799\n",
      "64-th iteration, new layer inserted. now 53 layers\n",
      "[5.88880643e+01 9.42302422e+01 4.73918342e+01 8.01563839e+01\n",
      " 5.86444388e+01 9.95232202e+01 5.14858397e+01 7.50804478e+01\n",
      " 4.67148606e+01 9.21673403e+01 1.03508549e+02 1.22271965e+02\n",
      " 1.00582088e+02 1.17861374e+02 7.93487217e+01 1.05954204e+02\n",
      " 1.17246134e+02 3.51406458e+01 3.31149718e+01 9.73958480e+01\n",
      " 7.34588610e+01 1.82730282e+01 2.15897474e+01 8.65815032e+01\n",
      " 1.99346166e+01 2.23741856e+01 1.07103373e+02 1.16932492e+02\n",
      " 8.90489979e+01 1.09656775e+02 8.61580960e+01 1.03924405e+02\n",
      " 8.53371205e+01 7.84692741e+01 2.16755933e-01 6.33807733e-01\n",
      " 9.31241568e+01 8.11926315e+01 3.01279367e+02 8.02070472e+01\n",
      " 1.32323424e+02 6.42165167e+01 2.10841205e+01 6.19915709e+01\n",
      " 5.51727981e+01 9.45562990e+01 9.61159942e+01 9.25799620e+01\n",
      " 5.36135920e+01 1.07793935e+02 2.34594703e+00 2.95731591e+01\n",
      " 7.28831574e+01]\n",
      "65-th iteration, loss: 0.011616071094269783, 47 gd steps\n",
      "insert gradient: -0.00047753853120333117\n",
      "65-th iteration, new layer inserted. now 51 layers\n",
      "[ 57.36112075  94.5960009   48.41804289  80.44602696  58.13787485\n",
      "  99.77797946  52.06292983  75.10181349  45.97125347  91.98629133\n",
      " 103.72831947 122.83993478 100.43924627 117.89147669  79.23483805\n",
      " 105.96990504 117.34335138  35.06862858  32.92266318  97.70295667\n",
      "  73.12371557  18.27825104  21.52048037  89.21997015  19.51200557\n",
      "  22.35678226 106.41829978 117.55171855  89.25516292 109.7672381\n",
      "  85.69240475 103.87983714  85.35183245  79.26940512  93.03025776\n",
      "  81.839861   301.738734    79.49693547 132.42945431  65.00556463\n",
      "  20.52307819  62.29713987  55.57504585  94.73839189  95.94847738\n",
      "  92.43663424  53.53678095 108.07632775   2.33424665  29.85983387\n",
      "  72.33002638]\n",
      "66-th iteration, loss: 0.01160920212711686, 14 gd steps\n",
      "insert gradient: -0.0006368254274187895\n",
      "66-th iteration, new layer inserted. now 51 layers\n",
      "[ 57.3531556   94.59598391  48.413708    80.44253509  58.13469862\n",
      "  99.7788715   52.0586478   75.09776284  45.96399003  91.98303786\n",
      " 103.72609185 122.84122355 100.43049686 117.88729698  79.22372844\n",
      " 105.97254263 117.36098271  35.07378358  32.93446149  97.71198912\n",
      "  73.11469889  18.30120383  21.51971881  89.24532532  19.4548088\n",
      "  22.32383865 106.38184638 117.55364915  89.25986202 109.7883998\n",
      "  85.70399008 103.89316791  85.3623292   79.26926644  92.98868551\n",
      "  81.85387118 301.7539812   79.48659319 132.46402162  65.02187678\n",
      "  20.52385843  62.33073512  55.61720453  94.75765151  95.96283136\n",
      "  92.44444628  53.54747273 108.08499036   2.33882246  29.86913613\n",
      "  72.32389513]\n",
      "67-th iteration, loss: 0.011608264141547477, 12 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "67-th iteration, new layer inserted. now 53 layers\n",
      "[ 57.35277318  94.59625052  48.41395928  80.44271876  58.13533929\n",
      "  99.77964563  52.05992869  75.0987598   45.96585296  91.98415894\n",
      " 103.72825209 122.84341125 100.43456297 117.88862776  79.22387398\n",
      " 105.97330175 117.3624133   35.07437945  32.93617166  97.71348155\n",
      "  73.11595858  18.30394783  21.52166232  89.24894847  19.45372261\n",
      "  22.32363881 106.38170687 117.55522921  89.26388334 109.79132163\n",
      "  85.70743269 103.89478351  85.36114123  79.26908934  92.98605674\n",
      "  81.85530573  75.43892352   0.         226.31677055  79.48554222\n",
      " 132.46634039  65.02299721  20.52290537  62.33287222  55.61998235\n",
      "  94.75897053  95.96366344  92.44498202  53.54824939 108.08561404\n",
      "   2.33890553  29.86979671  72.32327042]\n",
      "68-th iteration, loss: 0.011395547891171911, 20 gd steps\n",
      "insert gradient: -0.004388382083705501\n",
      "68-th iteration, new layer inserted. now 55 layers\n",
      "[ 58.14240779  96.03849981  48.75351249  79.65393738  56.87628089\n",
      "  99.91424733  51.71179411  75.59090007  46.47893465  91.19689834\n",
      " 103.37028853 125.63557644  99.62518017 118.59179309  78.67551302\n",
      " 104.32119545 118.2716989   34.47883543  33.11018357  99.65863018\n",
      "  71.08288973  16.25722721  18.77873504 108.51128086  14.33947919\n",
      "  27.22501308 105.20515776 114.25412759  88.33111142  84.14143402\n",
      "   0.          28.04714467  84.77132959 103.62969858  85.58056488\n",
      "  80.23982769  93.85813068  80.7318181   74.16384077   6.8360864\n",
      " 225.02966823  78.92162096 129.10219774  70.58211416  20.59953017\n",
      "  60.61180469  53.34712088  94.47312304  96.79614087  91.57079003\n",
      "  52.84291377 110.19870179   2.58763825  32.05476391  68.56253952]\n",
      "69-th iteration, loss: 0.011376187381782064, 6 gd steps\n",
      "insert gradient: -0.00044777286011576825\n",
      "69-th iteration, new layer inserted. now 57 layers\n",
      "[5.81422040e+01 9.60383633e+01 4.87532318e+01 7.96537162e+01\n",
      " 5.68755860e+01 9.99138219e+01 5.17112900e+01 7.55907711e+01\n",
      " 4.64788713e+01 9.11970474e+01 1.03370893e+02 1.25636029e+02\n",
      " 9.96259002e+01 1.18592257e+02 7.86765046e+01 1.04321754e+02\n",
      " 1.18273585e+02 3.44800865e+01 3.31124323e+01 9.96600163e+01\n",
      " 7.10870793e+01 1.62601635e+01 1.87841330e+01 1.08513917e+02\n",
      " 1.43457678e+01 2.72288141e+01 1.05208917e+02 1.14257569e+02\n",
      " 8.83376076e+01 8.41465644e+01 1.27143868e-02 2.80522751e+01\n",
      " 8.47817062e+01 1.03634824e+02 0.00000000e+00 7.10542736e-15\n",
      " 8.55925837e+01 8.02437035e+01 9.38627757e+01 8.07337801e+01\n",
      " 7.41661525e+01 6.83726216e+00 2.25031350e+02 7.89220225e+01\n",
      " 1.29102424e+02 7.05823990e+01 2.06000479e+01 6.06119316e+01\n",
      " 5.33473612e+01 9.44732671e+01 9.67965224e+01 9.15709043e+01\n",
      " 5.28430450e+01 1.10198825e+02 2.58790875e+00 3.20549082e+01\n",
      " 6.85626025e+01]\n",
      "70-th iteration, loss: 0.011355245780858314, 280 gd steps\n",
      "insert gradient: -0.00010077904736824993\n",
      "70-th iteration, new layer inserted. now 57 layers\n",
      "[5.81175156e+01 9.60237707e+01 4.87284744e+01 7.96369458e+01\n",
      " 5.68205923e+01 9.98809073e+01 5.16723931e+01 7.55776460e+01\n",
      " 4.64584623e+01 9.11937495e+01 7.75317725e+01 0.00000000e+00\n",
      " 2.58439242e+01 1.25644209e+02 9.96409449e+01 1.18599001e+02\n",
      " 7.86848685e+01 1.04319443e+02 1.18274875e+02 3.44949614e+01\n",
      " 3.31250571e+01 9.96565122e+01 7.10946086e+01 1.62468231e+01\n",
      " 1.88155546e+01 1.08520839e+02 1.43613008e+01 2.72222663e+01\n",
      " 1.05187762e+02 1.14267703e+02 8.82933057e+01 1.12187023e+02\n",
      " 8.47550075e+01 1.03638090e+02 5.88600866e-02 3.48251970e-03\n",
      " 8.56514551e+01 8.02548442e+01 9.38325707e+01 8.07325498e+01\n",
      " 7.41406613e+01 6.84783137e+00 2.24979375e+02 7.88927866e+01\n",
      " 1.29062820e+02 7.05707393e+01 2.05634272e+01 6.05932835e+01\n",
      " 5.33400583e+01 9.44736395e+01 9.68043797e+01 9.15732670e+01\n",
      " 5.28473940e+01 1.10205263e+02 2.60142779e+00 3.20625698e+01\n",
      " 6.85649208e+01]\n",
      "71-th iteration, loss: 0.010895975883562664, 42 gd steps\n",
      "insert gradient: -2.6843214996399444e-05\n",
      "71-th iteration, new layer inserted. now 59 layers\n",
      "[5.84511895e+01 9.71423147e+01 4.88515667e+01 8.42538450e+01\n",
      " 5.18841575e+01 9.68512867e+01 5.18458154e+01 7.77251656e+01\n",
      " 4.39110300e+01 9.12684084e+01 7.24017272e+01 1.05524182e+01\n",
      " 2.21030728e+01 1.26739842e+02 9.97220071e+01 1.18450453e+02\n",
      " 8.08094718e+01 1.01144882e+02 1.20136248e+02 3.27681498e+01\n",
      " 3.37162328e+01 9.94726729e+01 7.28821255e+01 1.02966206e+01\n",
      " 1.88572299e+01 1.16822079e+02 9.96259471e+00 2.96580089e+01\n",
      " 1.04517824e+02 1.16169353e+02 8.81153110e+01 1.13077772e+02\n",
      " 8.40388436e+01 1.03843788e+02 1.70859818e-01 7.19478227e-02\n",
      " 8.59581034e+01 7.92080705e+01 9.43440437e+01 7.89914011e+01\n",
      " 7.49405061e+01 1.18930152e+01 2.23224524e+02 8.04097882e+01\n",
      " 1.25421802e+02 7.31131915e+01 2.15916994e+01 5.51912812e+01\n",
      " 5.52066350e+01 9.47002870e+01 9.62672182e+01 9.05245198e+01\n",
      " 5.20031260e+01 1.12118975e+02 1.42052674e+00 0.00000000e+00\n",
      " 1.11022302e-16 3.50055067e+01 6.39423792e+01]\n",
      "72-th iteration, loss: 0.010758481616743082, 24 gd steps\n",
      "insert gradient: -0.0009367397062019443\n",
      "72-th iteration, new layer inserted. now 55 layers\n",
      "[ 58.25590915  97.39135904  48.75265552  83.2232086   51.58480469\n",
      "  97.26327458  51.57043335  77.67952736  44.68939736  91.7231676\n",
      "  71.42116009  11.56646954  21.94452829 128.33847411  99.00079805\n",
      " 117.26516887  81.54730358 100.12546755 120.62211785  31.71980305\n",
      "  34.47929025  99.2954675   73.3268595    8.26542087  18.98757054\n",
      " 118.76673446   8.38757718  30.37993895 104.83680503 115.30365634\n",
      "  88.99352744 112.8854827   83.82415017 103.44514461  86.56309534\n",
      "  78.75305971  94.90022802  78.22074949  75.48442287  13.11988035\n",
      " 222.44400615  80.96413624 123.96329004  73.84077191  21.38793167\n",
      "  53.03222399  56.84161444  95.44436155  95.32703268  90.07525276\n",
      "  51.99439747 113.35776311   0.66096599  38.03216081  62.77017792]\n",
      "73-th iteration, loss: 0.010536086737138784, 64 gd steps\n",
      "insert gradient: -0.006146019377873901\n",
      "73-th iteration, new layer inserted. now 55 layers\n",
      "[ 57.2546595   96.72287847  49.44614162  83.41882993  51.87337063\n",
      "  96.55588046  50.52565106  77.18582975  45.02999229  92.81487839\n",
      "  69.36478597  13.07674229  21.0115432  132.54972448  98.93819672\n",
      " 114.54693827  83.13537438  96.80412652 122.03815124  26.00085822\n",
      "  38.48053438  99.68587314  74.20463262   2.44232878  17.98080672\n",
      " 123.91941575   4.61794135  31.11398847 107.59169493 115.48067053\n",
      "  89.16068044  84.6771704    0.          28.22572347  84.55032865\n",
      " 102.5969078   86.91130654  78.58548507  95.95877321  75.30472512\n",
      "  77.57369256  16.06022929 219.33019156  83.21412918 120.04784969\n",
      "  75.5822808   22.39186292  47.55540025  60.37856887  97.32872899\n",
      "  94.16619418  87.81353246  51.55968845 159.35929868  58.53382961]\n",
      "74-th iteration, loss: 0.01050527433898421, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "74-th iteration, new layer inserted. now 57 layers\n",
      "[5.72547235e+01 9.67228790e+01 4.94460676e+01 8.34188322e+01\n",
      " 5.18735188e+01 9.65560747e+01 5.05261307e+01 7.71861306e+01\n",
      " 4.50306846e+01 9.28154268e+01 6.93661034e+01 1.30778780e+01\n",
      " 2.10129368e+01 1.32550660e+02 9.89395731e+01 1.14547642e+02\n",
      " 8.31371169e+01 9.68051908e+01 1.22040922e+02 2.60019313e+01\n",
      " 3.84829582e+01 9.96879527e+01 7.42103726e+01 2.44731475e+00\n",
      " 1.79866608e+01 1.23923268e+02 4.62427595e+00 3.11183289e+01\n",
      " 1.07596068e+02 1.15484523e+02 8.91714464e+01 8.46843063e+01\n",
      " 1.71991625e-02 2.82328594e+01 8.45647644e+01 1.02603462e+02\n",
      " 8.69222080e+01 7.85894366e+01 9.59646530e+01 7.53068888e+01\n",
      " 7.75762586e+01 1.60610354e+01 1.09666110e+02 0.00000000e+00\n",
      " 1.09666110e+02 8.32146959e+01 1.20047911e+02 7.55824930e+01\n",
      " 2.23922853e+01 4.75552977e+01 6.03786363e+01 9.73287614e+01\n",
      " 9.41662452e+01 8.78135009e+01 5.15595141e+01 1.59359062e+02\n",
      " 5.85336186e+01]\n",
      "75-th iteration, loss: 0.010487863556849688, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "75-th iteration, new layer inserted. now 57 layers\n",
      "[5.72685957e+01 9.67237262e+01 4.94286934e+01 8.34096213e+01\n",
      " 5.18689925e+01 9.65618134e+01 5.05370825e+01 7.71772162e+01\n",
      " 4.50115318e+01 9.28100469e+01 6.93668569e+01 1.30765888e+01\n",
      " 2.10198779e+01 1.32553064e+02 9.89176335e+01 1.14543224e+02\n",
      " 8.31687757e+01 9.68097799e+01 1.22046152e+02 2.59822655e+01\n",
      " 3.84753779e+01 9.96835234e+01 7.42000328e+01 2.43544793e+00\n",
      " 1.79733066e+01 1.23929947e+02 4.58313250e+00 3.11204766e+01\n",
      " 1.07583255e+02 1.15472571e+02 8.91963325e+01 1.12926334e+02\n",
      " 8.45920550e+01 1.02618070e+02 8.69149702e+01 7.85998121e+01\n",
      " 9.60029483e+01 7.53141725e+01 7.75930585e+01 1.60513803e+01\n",
      " 2.74177932e+01 0.00000000e+00 8.22533796e+01 6.09500021e-02\n",
      " 1.09671149e+02 8.32024501e+01 1.19984496e+02 7.55483071e+01\n",
      " 2.22927277e+01 4.74856141e+01 6.03272918e+01 9.73068945e+01\n",
      " 9.41211829e+01 8.77934195e+01 5.15198297e+01 1.59316603e+02\n",
      " 5.84973180e+01]\n",
      "76-th iteration, loss: 0.010334979846464242, 18 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.1207886389017366e-19\n",
      "76-th iteration, new layer inserted. now 59 layers\n",
      "[5.76149714e+01 9.65825891e+01 4.87228287e+01 8.32443815e+01\n",
      " 5.15420439e+01 9.66410731e+01 5.13027569e+01 7.71702514e+01\n",
      " 4.48738269e+01 9.26777160e+01 6.90881035e+01 1.33972331e+01\n",
      " 2.06885917e+01 1.33111193e+02 9.87923869e+01 1.14057939e+02\n",
      " 8.36682770e+01 9.66426212e+01 1.22366702e+02 2.46686327e+01\n",
      " 3.90962962e+01 9.96872143e+01 7.42787510e+01 9.35290018e-01\n",
      " 1.78832166e+01 1.24437931e+02 4.05159609e+00 3.10318551e+01\n",
      " 1.07940623e+02 1.15741000e+02 8.99210043e+01 1.12446047e+02\n",
      " 8.46088041e+01 1.02896659e+02 8.65470484e+01 7.93359762e+01\n",
      " 9.61613558e+01 7.51934095e+01 7.74732709e+01 1.74532012e+01\n",
      " 2.58219877e+01 4.05438535e+00 8.06570240e+01 1.32474600e+00\n",
      " 1.08090316e+02 8.28888235e+01 1.19035746e+02 7.59940955e+01\n",
      " 2.30574573e+01 4.63792584e+01 6.03343667e+01 9.75455472e+01\n",
      " 9.44757017e+01 8.79984980e+01 5.15024594e+01 1.58141202e+02\n",
      " 5.76402015e+01 0.00000000e+00 7.10542736e-15]\n",
      "77-th iteration, loss: 0.010260291396251352, 40 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.88668957623624e-06\n",
      "77-th iteration, new layer inserted. now 59 layers\n",
      "[5.72175957e+01 9.64613957e+01 4.89527979e+01 8.34213660e+01\n",
      " 5.14483112e+01 9.64095073e+01 5.10377620e+01 7.71574909e+01\n",
      " 4.49780923e+01 9.26514174e+01 6.89644482e+01 1.35537342e+01\n",
      " 2.04659109e+01 1.33397244e+02 9.89115127e+01 1.13841623e+02\n",
      " 8.39244502e+01 9.64417757e+01 1.22496042e+02 2.40658284e+01\n",
      " 3.93760308e+01 9.96409998e+01 7.44377266e+01 1.00899219e-02\n",
      " 1.80136533e+01 1.24660540e+02 3.82835782e+00 3.10717873e+01\n",
      " 1.07949847e+02 1.15707470e+02 9.01275307e+01 1.12189210e+02\n",
      " 8.46750229e+01 1.02982481e+02 8.66168884e+01 7.96818570e+01\n",
      " 9.58369156e+01 7.51234301e+01 7.75636760e+01 0.00000000e+00\n",
      " 7.10542736e-15 1.82115418e+01 2.51659245e+01 5.59915268e+00\n",
      " 8.01010023e+01 1.49455689e+00 1.07631124e+02 8.27775245e+01\n",
      " 1.18432943e+02 7.61861890e+01 2.35064625e+01 4.58640789e+01\n",
      " 6.02480340e+01 9.75956958e+01 9.45457310e+01 8.79759308e+01\n",
      " 5.15768787e+01 1.57896656e+02 5.77793809e+01]\n",
      "78-th iteration, loss: 0.01019752259463228, 61 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "78-th iteration, new layer inserted. now 59 layers\n",
      "[ 56.99114492  96.27960173  48.8690189   83.4751137   51.37001245\n",
      "  96.26211923  50.99808097  77.11664507  45.02544746  92.63170508\n",
      "  68.84315776  13.73393697  20.08761101 133.86794765  98.98241408\n",
      " 113.4752526   84.21079955  96.12388412 122.86176742  23.17118028\n",
      "  39.74158343  99.55679087  92.66650777 125.04063559   3.31093699\n",
      "  31.24958236 107.93365756 115.57290834  90.43140247  74.53953952\n",
      "   0.          37.26976976  84.52621552 102.93334906  86.69360506\n",
      "  80.13865683  95.36976121  74.91384307  77.75073405   0.46906775\n",
      "   0.16838214  18.68525175  24.3050559    7.79980141  79.33269447\n",
      "   1.5723103  107.10732583  82.63859389 117.4190351   76.54125857\n",
      "  24.32671963  44.96999199  60.19314143  97.74159779  94.828293\n",
      "  87.82261958  51.53028815 157.55992974  57.95755833]\n",
      "79-th iteration, loss: 0.010193269319840312, 6 gd steps\n",
      "insert gradient: -9.347901063905194e-05\n",
      "79-th iteration, new layer inserted. now 59 layers\n",
      "[5.69911376e+01 9.62796034e+01 4.88690860e+01 8.34752002e+01\n",
      " 5.13701938e+01 9.62622565e+01 5.09984525e+01 7.71169579e+01\n",
      " 4.50260139e+01 9.26319752e+01 6.88435305e+01 1.37344987e+01\n",
      " 2.00878485e+01 1.33868342e+02 9.89829095e+01 1.13475238e+02\n",
      " 8.42100987e+01 9.61238141e+01 1.22861976e+02 2.31711039e+01\n",
      " 3.97421056e+01 9.95574327e+01 9.26686640e+01 1.25041988e+02\n",
      " 3.31345354e+00 3.12510600e+01 1.07935132e+02 1.15574317e+02\n",
      " 9.04351148e+01 7.45420592e+01 4.80322597e-03 3.72722894e+01\n",
      " 8.45315386e+01 1.02935739e+02 8.66980961e+01 8.01402589e+01\n",
      " 9.53718884e+01 7.49147363e+01 7.77520870e+01 4.69684900e-01\n",
      " 1.69724175e-01 1.86858728e+01 2.43059984e+01 7.80095647e+00\n",
      " 7.93336428e+01 1.57304341e+00 1.07108300e+02 8.26389210e+01\n",
      " 1.17419283e+02 7.65415033e+01 2.43272559e+01 4.49701094e+01\n",
      " 6.01933696e+01 9.77417127e+01 9.48284997e+01 8.78226576e+01\n",
      " 5.15303358e+01 1.57559928e+02 5.79576043e+01]\n",
      "80-th iteration, loss: 0.010190967220615322, 35 gd steps\n",
      "insert gradient: -6.660567754685673e-05\n",
      "80-th iteration, new layer inserted. now 59 layers\n",
      "[5.69910426e+01 9.62796231e+01 4.88703351e+01 8.34767478e+01\n",
      " 5.13732271e+01 9.62643623e+01 5.10040424e+01 7.71213239e+01\n",
      " 4.50331307e+01 9.26345076e+01 6.88446450e+01 1.37386479e+01\n",
      " 2.00862248e+01 1.33871727e+02 9.89870802e+01 1.13473240e+02\n",
      " 8.41960513e+01 9.61182485e+01 1.22852404e+02 2.31644300e+01\n",
      " 3.97381779e+01 9.95554747e+01 9.26685671e+01 1.25040489e+02\n",
      " 3.31250948e+00 3.12491055e+01 1.07932289e+02 1.15574292e+02\n",
      " 9.04329509e+01 1.11813069e+02 8.45344720e+01 1.02936440e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.67084660e+01 8.01435929e+01\n",
      " 9.53716944e+01 7.49148836e+01 7.77554537e+01 4.70641946e-01\n",
      " 1.72944790e-01 1.86868832e+01 2.43047728e+01 7.80729261e+00\n",
      " 7.93331397e+01 1.57476421e+00 1.07108609e+02 8.26387617e+01\n",
      " 1.17416504e+02 7.65418509e+01 2.43280460e+01 4.49671888e+01\n",
      " 6.01921034e+01 9.77415374e+01 9.48287928e+01 8.78218906e+01\n",
      " 5.15298262e+01 1.57559154e+02 5.79581840e+01]\n",
      "81-th iteration, loss: 0.010074890278543518, 34 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "81-th iteration, new layer inserted. now 61 layers\n",
      "[5.67576508e+01 9.58296286e+01 4.86338206e+01 8.35885460e+01\n",
      " 5.14247523e+01 9.60550317e+01 5.09856313e+01 7.67644922e+01\n",
      " 4.50510371e+01 9.27717529e+01 6.89255184e+01 1.41786351e+01\n",
      " 1.92473787e+01 1.35332532e+02 9.91713049e+01 1.12420561e+02\n",
      " 8.47914429e+01 9.51429786e+01 1.23688874e+02 2.08989019e+01\n",
      " 4.07520672e+01 9.90833770e+01 9.25040527e+01 1.25506488e+02\n",
      " 3.01567449e+00 3.12791177e+01 1.07983106e+02 1.15396983e+02\n",
      " 9.08295233e+01 1.10402718e+02 8.47035877e+01 1.02470335e+02\n",
      " 6.72137341e-03 1.83243551e-03 8.65448329e+01 8.19742996e+01\n",
      " 9.49449361e+01 7.43455346e+01 7.83698800e+01 1.31328492e+00\n",
      " 6.05059926e-01 1.96094521e+01 2.21379392e+01 1.21691899e+01\n",
      " 5.19136802e+01 0.00000000e+00 2.59568401e+01 5.19310251e-01\n",
      " 1.06721235e+02 8.29057231e+01 1.15874523e+02 7.78057136e+01\n",
      " 2.70486628e+01 4.19270209e+01 5.95792129e+01 9.78068685e+01\n",
      " 9.56053595e+01 8.67937234e+01 5.10002276e+01 1.56506737e+02\n",
      " 5.91140954e+01]\n",
      "82-th iteration, loss: 0.010035204490179504, 38 gd steps\n",
      "insert gradient: -0.00043038612477973407\n",
      "82-th iteration, new layer inserted. now 59 layers\n",
      "[5.66546193e+01 9.57347269e+01 4.86205335e+01 8.35613329e+01\n",
      " 5.14627926e+01 9.60627587e+01 5.10108950e+01 7.66096617e+01\n",
      " 4.50105745e+01 9.27603086e+01 6.89564839e+01 1.43139065e+01\n",
      " 1.89877751e+01 1.35783609e+02 9.92083909e+01 1.12057083e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.49034880e+01 9.48680199e+01\n",
      " 1.24039519e+02 2.03145196e+01 4.09627883e+01 9.89203587e+01\n",
      " 9.23921401e+01 1.25644802e+02 2.86251349e+00 3.13236078e+01\n",
      " 1.08085325e+02 1.15151420e+02 9.10693253e+01 1.09986534e+02\n",
      " 8.49386963e+01 1.02414195e+02 8.63511038e+01 8.22093745e+01\n",
      " 9.46490322e+01 7.41242008e+01 7.87047657e+01 1.56062296e+00\n",
      " 7.89211767e-01 1.99237464e+01 2.11931696e+01 1.35348836e+01\n",
      " 5.16485199e+01 1.29036157e+00 1.32217048e+02 8.28753088e+01\n",
      " 1.15381279e+02 7.79416878e+01 2.77967532e+01 4.11057898e+01\n",
      " 5.95232625e+01 9.78015170e+01 9.57732036e+01 8.64699423e+01\n",
      " 5.09721687e+01 1.56180775e+02 5.94263327e+01]\n",
      "83-th iteration, loss: 0.010030441808514352, 24 gd steps\n",
      "insert gradient: -0.00030947142943290993\n",
      "83-th iteration, new layer inserted. now 59 layers\n",
      "[5.66544821e+01 9.57332243e+01 4.86174085e+01 8.35579975e+01\n",
      " 5.14578990e+01 9.60592245e+01 5.10029730e+01 7.66013960e+01\n",
      " 4.50007923e+01 9.27560589e+01 6.89514830e+01 1.43071202e+01\n",
      " 1.89807748e+01 1.35784264e+02 9.91981266e+01 1.12053637e+02\n",
      " 8.49510653e+01 9.48703141e+01 1.24054687e+02 2.03131990e+01\n",
      " 4.09684421e+01 9.89160630e+01 9.23780688e+01 1.25638822e+02\n",
      " 2.83582393e+00 3.13156312e+01 1.08076278e+02 1.15139633e+02\n",
      " 9.10595516e+01 1.09975676e+02 0.00000000e+00 1.42108547e-14\n",
      " 8.49398105e+01 1.02414483e+02 8.63671776e+01 8.22173860e+01\n",
      " 9.46570424e+01 7.41254535e+01 7.87226674e+01 1.55934739e+00\n",
      " 8.04791993e-01 1.99236858e+01 2.11871744e+01 1.35657517e+01\n",
      " 5.16494109e+01 1.31199968e+00 1.32214424e+02 8.28722827e+01\n",
      " 1.15367908e+02 7.79374574e+01 2.77941792e+01 4.10857446e+01\n",
      " 5.95114280e+01 9.77959347e+01 9.57688458e+01 8.64621743e+01\n",
      " 5.09680582e+01 1.56175014e+02 5.94287978e+01]\n",
      "84-th iteration, loss: 0.009904969328086766, 41 gd steps\n",
      "insert gradient: -0.0012682179020216421\n",
      "84-th iteration, new layer inserted. now 59 layers\n",
      "[5.67398390e+01 9.56441112e+01 4.85294283e+01 8.33448677e+01\n",
      " 5.14673847e+01 9.60393738e+01 5.09615706e+01 7.62541914e+01\n",
      " 4.51710476e+01 9.28106868e+01 6.90350106e+01 1.53125723e+01\n",
      " 1.82172969e+01 1.37132711e+02 9.89011085e+01 1.11126865e+02\n",
      " 8.52002696e+01 9.44673220e+01 1.25197381e+02 1.86258648e+01\n",
      " 4.12279394e+01 9.87696525e+01 9.18950110e+01 1.26323477e+02\n",
      " 1.59536376e+00 3.17709042e+01 1.09044241e+02 1.13955157e+02\n",
      " 9.23016688e+01 1.08600647e+02 2.64478471e-01 1.22720831e-03\n",
      " 8.52042919e+01 1.02713079e+02 8.52446509e+01 8.29410591e+01\n",
      " 9.40147763e+01 7.39482043e+01 8.00483679e+01 1.66877347e+00\n",
      " 1.59000890e+00 2.03703681e+01 1.76560906e+01 1.73795590e+01\n",
      " 5.08268364e+01 5.29634231e+00 1.30513361e+02 8.23762220e+01\n",
      " 1.14711985e+02 7.79923954e+01 2.95203049e+01 3.89760821e+01\n",
      " 5.92379278e+01 9.75680077e+01 9.61851798e+01 8.56105386e+01\n",
      " 5.10779102e+01 1.55546874e+02 6.00697685e+01]\n",
      "85-th iteration, loss: 0.009896229573218899, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "85-th iteration, new layer inserted. now 61 layers\n",
      "[5.67366433e+01 9.56403247e+01 4.85203942e+01 8.33373810e+01\n",
      " 5.14561622e+01 9.60322628e+01 5.09466514e+01 7.62415721e+01\n",
      " 4.51554599e+01 9.28049945e+01 6.90308112e+01 1.53099363e+01\n",
      " 1.82112746e+01 1.37135570e+02 9.88896301e+01 1.11127099e+02\n",
      " 8.52404815e+01 9.44758347e+01 1.25224698e+02 1.86341319e+01\n",
      " 4.12384700e+01 9.87653253e+01 9.18691426e+01 1.26311642e+02\n",
      " 1.55630224e+00 3.17570024e+01 1.09036311e+02 1.13940865e+02\n",
      " 9.22889517e+01 1.08592814e+02 2.82119236e-01 6.83724875e-04\n",
      " 8.52219553e+01 1.02724538e+02 8.53020644e+01 8.29562608e+01\n",
      " 9.39999276e+01 7.39477121e+01 8.00627345e+01 1.66989213e+00\n",
      " 1.59894715e+00 2.03767788e+01 1.76148150e+01 1.73984098e+01\n",
      " 5.08268467e+01 5.33600535e+00 1.30502873e+02 8.23704571e+01\n",
      " 7.64690058e+01 0.00000000e+00 3.82345029e+01 7.79872243e+01\n",
      " 2.95213520e+01 3.89621692e+01 5.92318871e+01 9.75639790e+01\n",
      " 9.61841969e+01 8.56057680e+01 5.10789776e+01 1.55544167e+02\n",
      " 6.00712458e+01]\n",
      "86-th iteration, loss: 0.009800979085238755, 31 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "86-th iteration, new layer inserted. now 61 layers\n",
      "[ 56.92640598  95.66995913  48.40853492  83.14175742  51.45476244\n",
      "  96.09323447  50.99013591  76.07222018  45.16470487  92.79645521\n",
      "  69.0391225   16.36662548  17.62168513 138.11740062  98.5134728\n",
      " 110.55594573  85.24255404  94.38261514 126.13865323  17.75169208\n",
      "  41.03817372  98.84051125  91.40981439 127.32616261   0.2146192\n",
      "  32.73339253 109.8104263  112.6531439   93.39077271  71.9813788\n",
      "   0.          35.9906894   85.48550146 103.28250084  84.28818888\n",
      "  83.45394414  93.43459678  74.22705923  81.17201198   1.54047566\n",
      "   2.38439148  20.61495418  15.07020497  19.46202896  50.43403661\n",
      "   7.88689247 129.39451949  81.79254344  75.64347602   2.16880339\n",
      "  37.50551283  77.83602099  30.21186875  37.72749871  59.13899782\n",
      "  97.41263168  96.46929333  85.20903809  51.23228798 155.37463586\n",
      "  60.19029082]\n",
      "87-th iteration, loss: 0.009769806656644078, 6 gd steps\n",
      "insert gradient: -0.0005392451440055609\n",
      "87-th iteration, new layer inserted. now 63 layers\n",
      "[5.69263941e+01 9.56699794e+01 4.84086537e+01 8.31418911e+01\n",
      " 5.14551515e+01 9.60935694e+01 5.09909479e+01 7.60728613e+01\n",
      " 4.51661452e+01 9.27974864e+01 6.90414120e+01 1.63682586e+01\n",
      " 1.76240536e+01 1.38119021e+02 9.85164162e+01 1.10557603e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.52468138e+01 9.43845315e+01\n",
      " 1.26142893e+02 1.77539039e+01 4.10415425e+01 9.88428804e+01\n",
      " 9.14155298e+01 1.27330219e+02 2.20754831e-01 3.27374706e+01\n",
      " 1.09814752e+02 1.12656703e+02 9.34009084e+01 7.19877028e+01\n",
      " 1.50106223e-02 3.59970134e+01 8.54984658e+01 1.03288394e+02\n",
      " 8.42979367e+01 8.34574797e+01 9.34395567e+01 7.42290411e+01\n",
      " 8.11746572e+01 1.54157174e+00 2.38697202e+00 2.06161604e+01\n",
      " 1.50718454e+01 1.94637324e+01 5.04358174e+01 7.88802840e+00\n",
      " 1.29396194e+02 8.17930946e+01 7.56440207e+01 2.16927790e+00\n",
      " 3.75060761e+01 7.78364323e+01 3.02128398e+01 3.77278864e+01\n",
      " 5.91394800e+01 9.74128395e+01 9.64697247e+01 8.52091540e+01\n",
      " 5.12324388e+01 1.55374718e+02 6.01903574e+01]\n",
      "88-th iteration, loss: 0.009765079364559528, 9 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.05680131274798e-06\n",
      "88-th iteration, new layer inserted. now 63 layers\n",
      "[5.69260466e+01 9.56697892e+01 4.84084522e+01 8.31418614e+01\n",
      " 5.14556473e+01 9.60941151e+01 5.09921536e+01 7.60735918e+01\n",
      " 4.51682499e+01 9.27992956e+01 6.90458579e+01 1.63706965e+01\n",
      " 1.76286972e+01 1.38122314e+02 9.85228175e+01 1.10561984e+02\n",
      " 1.44937805e-02 4.38093806e-03 8.52613078e+01 9.43890341e+01\n",
      " 1.26150935e+02 1.77585536e+01 4.10455247e+01 9.88432977e+01\n",
      " 9.14120053e+01 1.27329313e+02 2.13799446e-01 3.27365285e+01\n",
      " 1.09813907e+02 1.12655985e+02 9.34036294e+01 1.07988510e+02\n",
      " 8.55038858e+01 1.03291465e+02 8.43048859e+01 8.34597668e+01\n",
      " 9.34407403e+01 7.42298411e+01 8.11755329e+01 1.54212924e+00\n",
      " 2.38756720e+00 2.06171635e+01 1.50696836e+01 1.94650483e+01\n",
      " 5.04363664e+01 0.00000000e+00 7.10542736e-15 7.89023141e+00\n",
      " 1.29396314e+02 8.17929044e+01 7.56435381e+01 2.17035170e+00\n",
      " 3.75058029e+01 7.78363560e+01 3.02132807e+01 3.77273602e+01\n",
      " 5.91394441e+01 9.74127450e+01 9.64701120e+01 8.52089805e+01\n",
      " 5.12326277e+01 1.55374903e+02 6.01905376e+01]\n",
      "89-th iteration, loss: 0.009763155473091107, 11 gd steps\n",
      "insert gradient: -4.215487363696983e-05\n",
      "89-th iteration, new layer inserted. now 65 layers\n",
      "[5.69248281e+01 9.56688710e+01 4.84065935e+01 8.31405255e+01\n",
      " 5.14540795e+01 9.60933831e+01 5.09906710e+01 7.60723496e+01\n",
      " 4.51669176e+01 9.27986661e+01 6.90449436e+01 1.63727637e+01\n",
      " 1.76267384e+01 1.38123773e+02 9.85202851e+01 1.10560528e+02\n",
      " 1.59487194e-02 2.89860909e-03 8.52627701e+01 9.43887717e+01\n",
      " 1.26151740e+02 1.77553829e+01 4.10410074e+01 9.88396336e+01\n",
      " 9.13980361e+01 1.27324201e+02 1.91501672e-01 3.27313202e+01\n",
      " 1.09806682e+02 1.12650906e+02 9.34060971e+01 1.07990690e+02\n",
      " 8.55134054e+01 1.03297147e+02 8.43159969e+01 8.34634411e+01\n",
      " 9.34384496e+01 7.42306424e+01 8.11770067e+01 1.54238181e+00\n",
      " 2.38819338e+00 2.06187936e+01 1.50610416e+01 0.00000000e+00\n",
      " 1.77635684e-15 1.94690076e+01 5.04354861e+01 4.73629840e-03\n",
      " 6.35394111e-07 7.89496771e+00 1.29393708e+02 8.17912314e+01\n",
      " 7.56403346e+01 2.17305743e+00 3.75032679e+01 7.78354896e+01\n",
      " 3.02138555e+01 3.77250065e+01 5.91390340e+01 9.74122841e+01\n",
      " 9.64708919e+01 8.52082014e+01 5.12331181e+01 1.55375299e+02\n",
      " 6.01909901e+01]\n",
      "90-th iteration, loss: 0.009734978665849442, 28 gd steps\n",
      "insert gradient: -9.344280414209313e-05\n",
      "90-th iteration, new layer inserted. now 65 layers\n",
      "[5.69066952e+01 9.56585314e+01 4.83785540e+01 8.30988475e+01\n",
      " 5.14328476e+01 9.60892116e+01 5.09791492e+01 7.60594284e+01\n",
      " 4.52099686e+01 9.28147857e+01 6.90436680e+01 1.66134181e+01\n",
      " 1.75045761e+01 1.38308645e+02 9.84263714e+01 1.10473634e+02\n",
      " 2.50827978e-02 1.78998425e-03 8.52719510e+01 9.44003657e+01\n",
      " 1.26349185e+02 1.75810127e+01 4.09667498e+01 9.88594771e+01\n",
      " 9.11788364e+01 1.27565697e+02 1.63329050e-03 3.29729028e+01\n",
      " 1.09879210e+02 1.12359395e+02 9.36377928e+01 1.07959852e+02\n",
      " 8.55138647e+01 1.03449688e+02 8.40809097e+01 8.35216152e+01\n",
      " 9.32498067e+01 7.43028792e+01 8.13394921e+01 1.53773667e+00\n",
      " 2.50366750e+00 2.06958077e+01 1.45737519e+01 3.63769840e-01\n",
      " 2.90343288e-03 1.98327775e+01 5.03264096e+01 2.43233844e-01\n",
      " 4.14029922e-03 8.13346545e+00 1.29159357e+02 8.16716516e+01\n",
      " 7.54594214e+01 2.42482416e+00 3.73789071e+01 7.78094343e+01\n",
      " 3.03453255e+01 3.75828517e+01 5.91565587e+01 9.73913259e+01\n",
      " 9.65515384e+01 8.51337867e+01 5.12417127e+01 1.55394686e+02\n",
      " 6.02095527e+01]\n",
      "91-th iteration, loss: 0.009719779111084778, 25 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.961901809818178e-06\n",
      "91-th iteration, new layer inserted. now 59 layers\n",
      "[5.68946359e+01 9.56514080e+01 4.83643046e+01 8.30786320e+01\n",
      " 5.14246171e+01 9.60845991e+01 5.09636628e+01 7.60480109e+01\n",
      " 4.52294639e+01 9.28231050e+01 6.90331428e+01 1.67345751e+01\n",
      " 1.74276920e+01 1.38400285e+02 9.83761513e+01 1.10428739e+02\n",
      " 3.68938544e-02 6.33831015e-04 8.52837826e+01 9.44157626e+01\n",
      " 1.26472507e+02 1.74857660e+01 4.09245506e+01 9.88641564e+01\n",
      " 9.10030904e+01 1.60738416e+02 1.09888642e+02 1.12188039e+02\n",
      " 9.37438933e+01 1.07938647e+02 8.54935320e+01 1.03537523e+02\n",
      " 8.39902331e+01 8.35683821e+01 9.31991764e+01 7.43545326e+01\n",
      " 8.14312525e+01 1.52591386e+00 2.57291235e+00 2.07259952e+01\n",
      " 1.43287583e+01 2.05946098e+01 5.02532935e+01 8.57785001e+00\n",
      " 1.29018147e+02 8.15990210e+01 7.53585797e+01 2.55598663e+00\n",
      " 3.73120310e+01 7.77880438e+01 3.03950985e+01 3.74945566e+01\n",
      " 5.91384156e+01 9.73729659e+01 9.66144794e+01 8.50901718e+01\n",
      " 5.12469108e+01 1.55425411e+02 6.02282845e+01]\n",
      "92-th iteration, loss: 0.009719410887678201, 13 gd steps\n",
      "insert gradient: -8.027383978564075e-05\n",
      "92-th iteration, new layer inserted. now 61 layers\n",
      "[5.68943907e+01 9.56512854e+01 4.83641836e+01 8.30785011e+01\n",
      " 5.14248282e+01 9.60847276e+01 5.09637359e+01 7.60479616e+01\n",
      " 4.52297461e+01 9.28232044e+01 6.90329126e+01 1.67358546e+01\n",
      " 1.74265671e+01 1.38401379e+02 9.83756931e+01 1.10428258e+02\n",
      " 3.71823001e-02 1.13904868e-04 8.52840716e+01 9.44157390e+01\n",
      " 1.26473333e+02 1.74843564e+01 4.09233819e+01 9.88635282e+01\n",
      " 9.09991036e+01 1.60738388e+02 1.09886935e+02 1.12184965e+02\n",
      " 9.37433738e+01 1.07937734e+02 0.00000000e+00 1.42108547e-14\n",
      " 8.54929913e+01 1.03538395e+02 8.39909950e+01 8.35690140e+01\n",
      " 9.31980845e+01 7.43547138e+01 8.14323055e+01 1.52442756e+00\n",
      " 2.57368216e+00 2.07250723e+01 1.43248326e+01 2.05965882e+01\n",
      " 5.02510216e+01 8.57883325e+00 1.29014736e+02 8.15973878e+01\n",
      " 7.53559211e+01 2.55683948e+00 3.73097595e+01 7.77870962e+01\n",
      " 3.03944651e+01 3.74924481e+01 5.91369842e+01 9.73722973e+01\n",
      " 9.66149504e+01 8.50894928e+01 5.12469649e+01 1.55425910e+02\n",
      " 6.02286171e+01]\n",
      "93-th iteration, loss: 0.009433422983804491, 30 gd steps\n",
      "insert gradient: -0.0007795801733507972\n",
      "93-th iteration, new layer inserted. now 59 layers\n",
      "[5.69878862e+01 9.56639553e+01 4.79283558e+01 8.24235022e+01\n",
      " 5.14829805e+01 9.63641904e+01 5.06948835e+01 7.52976358e+01\n",
      " 4.54446337e+01 9.34968720e+01 6.91249228e+01 2.01046834e+01\n",
      " 1.54473297e+01 1.41074962e+02 9.74796420e+01 1.09432700e+02\n",
      " 8.52689417e+01 9.52926623e+01 1.29757302e+02 1.43409200e+01\n",
      " 3.97494354e+01 9.99924613e+01 8.79911919e+01 1.64439056e+02\n",
      " 1.10339754e+02 1.07280602e+02 9.53732089e+01 1.07420771e+02\n",
      " 0.00000000e+00 1.42108547e-14 8.48581010e+01 1.06615563e+02\n",
      " 8.17728029e+01 8.45653933e+01 9.28687773e+01 7.56637824e+01\n",
      " 8.29578346e+01 1.92776043e+00 3.74092094e+00 2.20214559e+01\n",
      " 9.47309683e+00 2.54258690e+01 4.90057452e+01 1.19438474e+01\n",
      " 1.26885835e+02 8.01962623e+01 7.31884055e+01 5.85203559e+00\n",
      " 3.66679316e+01 7.78586621e+01 3.27275728e+01 3.52073084e+01\n",
      " 5.78581722e+01 9.63416848e+01 9.82038746e+01 8.34795736e+01\n",
      " 5.09384134e+01 1.56098064e+02 6.07390831e+01]\n",
      "94-th iteration, loss: 0.009139906727068272, 50 gd steps\n",
      "insert gradient: -4.510572140537077e-05\n",
      "94-th iteration, new layer inserted. now 61 layers\n",
      "[5.71775169e+01 9.57650169e+01 4.75930557e+01 8.18166937e+01\n",
      " 5.14240032e+01 9.66756434e+01 5.07851568e+01 7.48701816e+01\n",
      " 0.00000000e+00 7.10542736e-15 4.56122580e+01 9.43328867e+01\n",
      " 6.94270582e+01 2.33024618e+01 1.28726052e+01 1.43758971e+02\n",
      " 9.68559674e+01 1.09444869e+02 8.48928696e+01 9.62561941e+01\n",
      " 1.31220566e+02 1.19807340e+01 3.94510698e+01 1.02174569e+02\n",
      " 8.52379865e+01 1.68524261e+02 1.10336457e+02 1.04030562e+02\n",
      " 9.60958127e+01 1.07098941e+02 7.24373111e-03 3.61897861e-02\n",
      " 8.37626888e+01 1.09171782e+02 8.08795794e+01 8.44479519e+01\n",
      " 9.34223565e+01 7.64290900e+01 8.31956996e+01 3.03692995e+00\n",
      " 3.93901943e+00 2.35269592e+01 5.92738240e+00 2.87430798e+01\n",
      " 4.76952918e+01 1.44988298e+01 1.26817273e+02 7.89561344e+01\n",
      " 6.98726388e+01 9.44190924e+00 3.66879697e+01 7.78541092e+01\n",
      " 3.52970878e+01 3.40936392e+01 5.54301725e+01 9.47150548e+01\n",
      " 9.97114709e+01 8.19743851e+01 5.07559091e+01 1.56608629e+02\n",
      " 6.17233137e+01]\n",
      "95-th iteration, loss: 0.008919397348270096, 36 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "95-th iteration, new layer inserted. now 59 layers\n",
      "[ 57.18754734  95.72703618  47.35606666  81.59592507  51.40572755\n",
      "  96.53770302  50.33475109  74.66897803  46.40266807  95.00759443\n",
      "  70.05247713  24.96201118  11.73730689 145.15028914  96.47921667\n",
      " 109.8213959   84.28865789  96.84807912 131.99650356  10.63988159\n",
      "  39.25230118 103.34439494  84.17488608 169.64436116 110.32688539\n",
      " 103.69741583  96.00563544 107.83226946  82.78253472  36.7164221\n",
      "   0.          73.43284419  80.84737426  84.31123409  93.46602933\n",
      "  76.60809721  82.87614131   4.17525135   3.72707963  24.63583211\n",
      "   5.13838594  30.11701757  47.7340426   14.94660753 126.71430891\n",
      "  78.51405542  67.93872945  11.29530083  36.87336937  78.0124628\n",
      "  36.58757419  34.66858591  52.92211306  93.2275318  100.3070908\n",
      "  81.48606205  50.5941576  156.83454641  62.25692461]\n",
      "96-th iteration, loss: 0.00890616828886498, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "96-th iteration, new layer inserted. now 61 layers\n",
      "[5.71889481e+01 9.57281486e+01 4.73583214e+01 8.15971131e+01\n",
      " 5.14073501e+01 9.65382825e+01 5.03361684e+01 7.46705174e+01\n",
      " 4.64057825e+01 9.50094067e+01 7.00544650e+01 2.49667923e+01\n",
      " 1.17373850e+01 1.45153548e+02 9.64802058e+01 1.09821510e+02\n",
      " 8.42840016e+01 9.68493328e+01 1.32000781e+02 1.06377773e+01\n",
      " 3.92550647e+01 1.03350676e+02 8.41886003e+01 5.65521847e+01\n",
      " 0.00000000e+00 1.13104369e+02 1.10336337e+02 1.03705008e+02\n",
      " 9.60237590e+01 1.07841639e+02 8.27873646e+01 3.67228927e+01\n",
      " 8.67397026e-03 7.34393143e+01 8.08369567e+01 8.43091424e+01\n",
      " 9.34665904e+01 7.66047690e+01 8.28567513e+01 4.17269670e+00\n",
      " 3.70859158e+00 2.46321181e+01 5.12965233e+00 3.01117202e+01\n",
      " 4.77264132e+01 1.49391062e+01 1.26710397e+02 7.85132817e+01\n",
      " 6.79372626e+01 1.12957878e+01 3.68734669e+01 7.80134548e+01\n",
      " 3.65908058e+01 3.46709932e+01 5.29226652e+01 9.32274406e+01\n",
      " 1.00309464e+02 8.14862405e+01 5.05942351e+01 1.56834800e+02\n",
      " 6.22569671e+01]\n",
      "97-th iteration, loss: 0.0088945579749378, 403 gd steps\n",
      "insert gradient: -5.138983307722681e-05\n",
      "97-th iteration, new layer inserted. now 61 layers\n",
      "[5.72008284e+01 9.57386373e+01 4.73809722e+01 8.16098178e+01\n",
      " 5.14282585e+01 9.65465321e+01 5.03504462e+01 7.46819305e+01\n",
      " 4.64289603e+01 9.50230483e+01 7.00638656e+01 0.00000000e+00\n",
      " 7.10542736e-15 2.49978682e+01 1.17212171e+01 1.45175454e+02\n",
      " 9.64763893e+01 1.09818735e+02 8.42274301e+01 9.68479302e+01\n",
      " 1.32004223e+02 1.06068768e+01 3.92535914e+01 1.03376341e+02\n",
      " 8.42245717e+01 5.65853249e+01 5.47412626e-02 1.13137476e+02\n",
      " 1.10357470e+02 1.03717708e+02 9.60328485e+01 1.07847389e+02\n",
      " 8.27406016e+01 1.10163887e+02 8.07724046e+01 8.42934135e+01\n",
      " 9.34624260e+01 7.66037526e+01 8.28331225e+01 4.19493464e+00\n",
      " 3.68789978e+00 2.46515415e+01 5.12899689e+00 3.01284105e+01\n",
      " 4.77418813e+01 1.49394745e+01 1.26730897e+02 7.85208235e+01\n",
      " 6.79360692e+01 1.13203084e+01 3.68876876e+01 7.80210182e+01\n",
      " 3.66084731e+01 3.46830725e+01 5.29150820e+01 9.32197928e+01\n",
      " 1.00320605e+02 8.14834263e+01 5.05904548e+01 1.56835246e+02\n",
      " 6.22561773e+01]\n",
      "98-th iteration, loss: 0.008418205923860382, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "98-th iteration, new layer inserted. now 63 layers\n",
      "[ 57.5179824   95.73794132  47.15104075  81.28723365  51.64721524\n",
      "  95.74660553  48.41699823  73.33629364  46.91108303  97.30714259\n",
      "  72.70068841   3.04125869   2.33644443  28.17535274   7.59366944\n",
      " 148.53097804  92.47680103 110.79020774  82.09670496  99.12415047\n",
      " 135.61041775   6.44093848  38.68583392 107.04231463  81.67388405\n",
      "  57.21777284   0.46228963 113.77352416 109.10384804 104.94853331\n",
      "  95.49647611  36.92274194   0.          73.84548388  79.77970246\n",
      " 114.00137004  79.42974502  84.64566349  92.87236719  77.47668768\n",
      "  83.46821886   5.51324867   4.10828819  26.3912406    1.49772868\n",
      "  32.63655957  46.29396187  17.43418732 127.16105541  78.78934791\n",
      "  64.76999724  16.19241495  35.57855965  78.4707748   39.6879616\n",
      "  35.17460543  47.96137527  90.21029561 102.44853423  80.40240965\n",
      "  49.82801303 158.03006337  62.29094504]\n",
      "99-th iteration, loss: 0.008399356570853438, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "99-th iteration, new layer inserted. now 63 layers\n",
      "[ 57.51291595  95.73401979  47.1472592   81.28744413  51.64892165\n",
      "  95.75142323  48.43651811  73.34847061  46.92814436  97.30999318\n",
      "  72.69347839   3.06143027   2.32562341  28.19752443   7.59491108\n",
      " 148.54333136  92.47208481 110.78441966  82.05392407  99.11579653\n",
      " 135.59845042   6.42154872  38.67081379 107.04505464  81.68506052\n",
      "  57.2262085    0.48311952 113.78235451 109.11375873 104.96604526\n",
      "  95.52633711  73.87006117   0.          36.93503059  79.77556399\n",
      " 114.01458564  79.38159955  84.63381248  92.82977365  77.46245625\n",
      "  83.40676036   5.51780239   4.04358873  26.39560628   1.46589829\n",
      "  32.63823453  46.31087591  17.44479413 127.18949086  78.79905773\n",
      "  64.77188449  16.19662153  35.58180955  78.47225797  39.68302112\n",
      "  35.16985789  47.96088776  90.21037743 102.45116844  80.40365952\n",
      "  49.83138504 158.0298716   62.2929534 ]\n",
      "0-th iteration, loss: 0.768386602784115, 18 gd steps\n",
      "insert gradient: -0.5007025453146909\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  78.25487276    0.         3169.32234658]\n",
      "1-th iteration, loss: 0.5768865183067913, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.92379672e+01 1.34061366e+00 0.00000000e+00 1.08589707e+02\n",
      " 3.14497555e+03]\n",
      "2-th iteration, loss: 0.5710645001872866, 31 gd steps\n",
      "insert gradient: -0.22478409039061018\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.66461302  107.08105334  153.3624352     0.         2990.56748648]\n",
      "3-th iteration, loss: 0.46173477669409524, 18 gd steps\n",
      "insert gradient: -0.21436147193700808\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  67.99420893  112.20623144   59.53629638    0.           52.66672372\n",
      "   88.64617328 2967.34713139]\n",
      "4-th iteration, loss: 0.4362677847581102, 26 gd steps\n",
      "insert gradient: -0.1040295960310507\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[  73.32167708  107.87829683   61.48200533   33.50673925   37.22499787\n",
      "   94.17999988 2957.03262313]\n",
      "5-th iteration, loss: 0.4262605427879085, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  75.69287515  112.84685643   55.38814621   60.49367127   28.94823463\n",
      "   90.94429338  168.73042724    0.         2784.05204947]\n",
      "6-th iteration, loss: 0.3769418138322402, 37 gd steps\n",
      "insert gradient: -0.05104358227605659\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  63.87541734  125.81260023   60.29216269   68.40510925   24.08750418\n",
      "   77.54857776  134.96411883   92.03548772  410.0241884     0.\n",
      " 2357.6390833 ]\n",
      "7-th iteration, loss: 0.3409203477230596, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  70.87578269   10.90787783    0.          109.07877828   32.69362475\n",
      "   67.04750134   59.08570757   92.07044759  100.59084704  118.4863931\n",
      "  395.733952     50.21438452 2344.15645195]\n",
      "8-th iteration, loss: 0.30104539965280463, 41 gd steps\n",
      "insert gradient: -0.17249276950321307\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  82.1563071    93.15067391   38.63424891   72.38290111   51.70250056\n",
      "  111.793531     96.02048802  118.46965933  118.90910359    0.\n",
      "  254.80522197  107.86285795 2326.72314835]\n",
      "9-th iteration, loss: 0.22213481800718227, 48 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  73.81040311  103.27030945   41.85774733   62.91987284   56.09061513\n",
      "  106.82396642   99.07945485  104.30561569   99.7450586    95.6426664\n",
      "  230.83149976   79.31437872  128.96682553    0.         2192.43603405]\n",
      "10-th iteration, loss: 0.19474636072280616, 24 gd steps\n",
      "insert gradient: -0.06654830174956117\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[  75.36073975   94.116512     45.63463049   63.41128924   55.92173136\n",
      "  102.01068255  102.22668837  101.03621653   96.73790132  119.15743891\n",
      "  115.74812848    0.          115.74812848   50.79530272  113.32320267\n",
      "   66.41364606 2173.4181248 ]\n",
      "11-th iteration, loss: 0.1691896549983069, 44 gd steps\n",
      "insert gradient: -0.043699387394233784\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  65.31244459  101.92141357   50.09203394   68.22298094   53.51976709\n",
      "  105.57328839   99.84383722  106.18077709   99.24071102  102.89118562\n",
      "  103.03493563   79.96025826  110.51660307   15.67849804   70.09802322\n",
      "    0.           28.03920929  105.39540006 2162.45000446]\n",
      "12-th iteration, loss: 0.14302533546492474, 37 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  56.74138539  104.79749013   46.43081069   83.77954932   51.24168121\n",
      "  115.8849947    93.84655902  118.56148001   96.17286561  108.82297449\n",
      "   99.23230021   98.12786042  111.70596019   49.16469317   29.12484637\n",
      "   74.95633498    9.57758675    7.84448324    0.           86.28931561\n",
      " 2144.69809616]\n",
      "13-th iteration, loss: 0.13065084208061936, 21 gd steps\n",
      "insert gradient: -0.03309250965794511\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[5.77601448e+01 9.79037320e+01 4.99929986e+01 8.45064047e+01\n",
      " 5.39581304e+01 1.10476419e+02 9.81762649e+01 1.20781743e+02\n",
      " 9.20244942e+01 1.19813191e+02 0.00000000e+00 3.55271368e-14\n",
      " 9.91723854e+01 1.03670079e+02 1.08708273e+02 7.30309516e+01\n",
      " 1.48733772e+01 8.52840991e+01 2.71851658e+01 5.30915603e+01\n",
      " 2.14461942e+03]\n",
      "14-th iteration, loss: 0.11716721967119434, 29 gd steps\n",
      "insert gradient: -0.017642172297749902\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  56.67583708   96.27493809   52.33134482   92.35861558   54.9688968\n",
      "  107.2276009    98.20436873  114.16106312   94.42012402  128.37747036\n",
      "  103.57211441  101.07805307  105.10075331  103.3902918    17.49877625\n",
      "   55.05450173   49.29309764   47.93680731 1432.308722      0.\n",
      "  716.154361  ]\n",
      "15-th iteration, loss: 0.11218371818959333, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  55.31430927   96.77181279   53.42471154   92.49490673   55.39748749\n",
      "  104.20138034  101.08827827  108.21108717   96.31082473  128.17441651\n",
      "  102.94617522  104.22319149  104.77760647  100.61739045   23.09802562\n",
      "   44.81968585   47.67478275   54.32938458  130.5023743     0.\n",
      " 1305.023743     13.26132409  717.70229407]\n",
      "16-th iteration, loss: 0.10865254749121984, 22 gd steps\n",
      "insert gradient: -0.0237673242948091\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  56.44525898   97.90847369   54.44805393   91.95191249   54.06668777\n",
      "  103.02261745  101.69315182  105.87976052   97.68636734  127.12867474\n",
      "  100.50446379  113.50347614  103.65551203  102.50815339   27.59873368\n",
      "   37.78715424   48.84297629   53.36064821   51.83059493    0.\n",
      "   77.74589239   18.1082976  1299.60823756   15.31795636  717.75605324]\n",
      "17-th iteration, loss: 0.09761665050258682, 61 gd steps\n",
      "insert gradient: -0.007062314932067132\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[ 55.65275443  97.00014339  53.10595795  95.0398137   55.92134993\n",
      " 100.58781725 103.36164982 100.99236397 101.17184509 115.41092069\n",
      "  98.52464311 125.37852933 102.53557088 104.00430743  37.77230124\n",
      "  15.81539699  56.80622558  85.16563019  20.01004401  52.63054986\n",
      "  51.52161595  23.66853248 868.66889983   0.         434.33444991\n",
      "  12.27733186 720.51392822]\n",
      "18-th iteration, loss: 0.09670213972367774, 20 gd steps\n",
      "insert gradient: -0.004876665776039755\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[ 55.38411959  95.11246784  54.11744639  94.13039132  55.82274575\n",
      " 101.69715183 102.81697446 101.4663662  100.51026667 115.71249854\n",
      "  97.76545573 126.64880073 102.69354064 103.07572329  38.7840509\n",
      "  15.83510598  55.21520614  86.82806394  19.67373194  53.45950444\n",
      "  51.52739592  25.12211997 649.65067319   0.         216.5502244\n",
      "   8.54983596 431.55774619  11.51505579 720.14326006]\n",
      "19-th iteration, loss: 0.0954999865770385, 69 gd steps\n",
      "insert gradient: -0.005116475781644005\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[ 55.02359978  95.86050773  53.50467255  94.94273412  54.86970503\n",
      "  99.04936078 103.33098376  99.3273656  101.25799619 112.04760531\n",
      "  95.32469562 133.15463481 101.49065301 105.25009993  41.08410073\n",
      "  11.55633605  55.7038768   91.43245835  17.3888575   52.99849795\n",
      "  52.0615352   26.10521515 487.44071681   0.         162.48023894\n",
      "  11.7567928  211.76034876  13.56915849 425.11075753   9.66834728\n",
      " 722.13965382]\n",
      "20-th iteration, loss: 0.0946262022179006, 26 gd steps\n",
      "insert gradient: -0.004828097162886101\n",
      "20-th iteration, new layer inserted. now 33 layers\n",
      "[ 55.74615088  95.86289567  52.97416694  94.54207948  54.83821076\n",
      "  98.49141691 103.80319576  98.48211632 100.85870835 111.5477983\n",
      "  94.2504845  135.0929921  101.42181658 105.68130753  41.33289902\n",
      "  10.28982246  55.63166563  92.50727796  16.68247589  54.00366321\n",
      "  51.98747149  26.59813575 487.2026811   11.75006178 157.12512887\n",
      "  13.86749812 150.16677518   0.          60.06671007  13.80351439\n",
      " 421.86188655   8.10310642 724.16183489]\n",
      "21-th iteration, loss: 0.0940342252956842, 14 gd steps\n",
      "insert gradient: -0.005016680710108457\n",
      "21-th iteration, new layer inserted. now 35 layers\n",
      "[5.58233866e+01 9.63403236e+01 5.36451427e+01 9.38939618e+01\n",
      " 0.00000000e+00 2.13162821e-14 5.36052676e+01 9.82247200e+01\n",
      " 1.03775246e+02 9.83033770e+01 1.01113552e+02 1.10991825e+02\n",
      " 9.41978659e+01 1.35129956e+02 1.01447274e+02 1.06498857e+02\n",
      " 4.07739909e+01 9.81762651e+00 5.62563802e+01 9.27597293e+01\n",
      " 1.65940846e+01 5.42169381e+01 5.17732298e+01 2.56197394e+01\n",
      " 4.86347411e+02 1.41688207e+01 1.54709730e+02 1.42244721e+01\n",
      " 1.47456499e+02 7.73704891e+00 5.71553297e+01 1.57769160e+01\n",
      " 4.21702980e+02 8.56107584e+00 7.25251586e+02]\n",
      "22-th iteration, loss: 0.0937962299885445, 21 gd steps\n",
      "insert gradient: -0.005333051067759969\n",
      "22-th iteration, new layer inserted. now 37 layers\n",
      "[5.47308936e+01 9.58332306e+01 5.36505535e+01 9.45269305e+01\n",
      " 4.08678006e-01 1.42711320e-01 5.44454786e+01 9.81449172e+01\n",
      " 1.03791859e+02 9.80774886e+01 1.00729463e+02 1.10962340e+02\n",
      " 9.42493413e+01 1.35531311e+02 1.01344756e+02 1.06753366e+02\n",
      " 4.07888014e+01 9.60450439e+00 5.63183503e+01 9.27143905e+01\n",
      " 1.62800453e+01 5.41239505e+01 5.16373446e+01 2.52187724e+01\n",
      " 3.24255066e+02 0.00000000e+00 1.62127533e+02 1.49120851e+01\n",
      " 1.54727786e+02 1.43567250e+01 1.47299882e+02 9.17159073e+00\n",
      " 5.61589491e+01 1.60332667e+01 4.21390096e+02 8.37369823e+00\n",
      " 7.25432842e+02]\n",
      "23-th iteration, loss: 0.09186782608773447, 60 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 37 layers\n",
      "[ 54.5177073   96.09151342  53.59554036  95.84690766  54.07405345\n",
      "  94.84016584 104.81028661  95.43105768 101.75632397 108.75404858\n",
      "  93.94171935 136.98316065  99.56776359 111.0419801   41.34784202\n",
      "   6.75210896  58.51861068  92.22854082  15.84703193  54.34232054\n",
      "  52.91445111  24.76893218  54.29605471   0.         271.48027357\n",
      "  12.92700266 150.41449433  18.53916814 148.88592341  22.98237707\n",
      " 145.19658627  25.35808243  38.68378301  29.1254197  417.48429733\n",
      "   8.99094383 726.57710066]\n",
      "24-th iteration, loss: 0.08824596014121298, 66 gd steps\n",
      "insert gradient: -0.003917922817712273\n",
      "24-th iteration, new layer inserted. now 37 layers\n",
      "[ 54.48436237  95.45038981  54.05725631  94.35291674  54.7704415\n",
      "  95.04794539 104.34663877  94.88074711 102.46566574 105.27491265\n",
      "  94.679956   134.31054372  81.02606558   0.          16.20521312\n",
      " 117.82875673 106.10945255  96.14013763  19.11193111  41.14213658\n",
      "  58.20504251  44.52243504  33.39995481  31.26294721 255.4102139\n",
      "  13.5121836  148.82162972  17.49260159 150.96232877  22.6327095\n",
      " 143.47126998  36.22080685  32.98351176  33.59290911 416.38305366\n",
      "   7.99669026 726.50411869]\n",
      "25-th iteration, loss: 0.08783566222407399, 20 gd steps\n",
      "insert gradient: -0.003127152855948258\n",
      "25-th iteration, new layer inserted. now 39 layers\n",
      "[ 54.77138059  96.14026046  52.8694865   96.19585038  54.13305738\n",
      "  95.36548329 104.33729493  95.11360393 102.35346851 104.82162869\n",
      "  95.07273081 132.48052059  80.01875878   3.83228565  15.41279233\n",
      " 117.48377598 105.70988485  96.39092089  20.02292724  39.13975036\n",
      "  58.18493844  48.16418219  30.52289863  35.02172912 254.04654321\n",
      "  13.29285656 149.14992555  17.58080991 150.94100492  22.7287919\n",
      " 142.64451274  38.52709742  31.74553279  34.72077043 416.57326995\n",
      "   7.76335537 605.44565993   0.         121.08913199]\n",
      "26-th iteration, loss: 0.08747356749623664, 19 gd steps\n",
      "insert gradient: -0.010284420273367971\n",
      "26-th iteration, new layer inserted. now 41 layers\n",
      "[ 54.65796746  95.90965763  54.42194993  94.73598844  53.69585954\n",
      "  95.21046783 104.33824303  95.19485076 102.30707007 104.88299012\n",
      "  95.28211394 131.81149455  79.5110648    4.68032484  15.32927216\n",
      " 117.31056047 105.77813787  96.69060263  20.29126265  38.24111039\n",
      "  58.33115977  49.6093653   29.27190477  36.77849049 253.44057445\n",
      "  13.22494238 149.23092344  17.48407831 151.44827843  22.38072673\n",
      " 142.52397402  39.54424944  31.05265972  35.21094712 416.16330812\n",
      "   6.78183708 604.71168404   3.89272406  96.55694845   0.\n",
      "  24.13923711]\n",
      "27-th iteration, loss: 0.0836428591985612, 57 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "27-th iteration, new layer inserted. now 41 layers\n",
      "[ 54.40650591  95.57519665  54.07609723  94.98072308  53.95049208\n",
      "  95.10258312 104.412247    95.1025913  102.35871219 105.33690697\n",
      "  95.80266806 129.73368699  77.73545569   7.31757073  15.91191666\n",
      "  23.25909992   0.          93.03639967 105.44867608  97.67948653\n",
      "  22.06304942  34.99614429  58.54490442  55.20782479  25.13727134\n",
      "  43.57786071 254.11068015  11.16377599 150.10319796  16.84788419\n",
      " 150.94475484  21.7478863  141.23226435  43.11556606  28.75812797\n",
      "  37.3155884  413.04841782   3.10069535 691.02387335  24.91827835\n",
      "  19.83896488]\n",
      "28-th iteration, loss: 0.08341198609702329, 689 gd steps\n",
      "insert gradient: -0.0027470903387716264\n",
      "28-th iteration, new layer inserted. now 41 layers\n",
      "[ 54.27378608  95.74541717  53.95431094  95.24604372  54.0355681\n",
      "  95.12089234 104.54191044  95.08307439 102.46918404 105.22127491\n",
      "  95.96278099 129.49026954  77.0923372    8.0915281   16.17373282\n",
      " 115.71502421 105.48786543  98.02410601  22.63806273  33.60642665\n",
      "  58.6669965   57.45900716  23.7674548   45.4192116  253.91413775\n",
      "  10.87115894 150.12272442  16.76508904 151.10324242  21.58854338\n",
      " 141.03531858  44.5055329   27.9155434   38.17912008 412.39307116\n",
      "   2.54393081 551.85538912   0.         137.96384728  27.29343236\n",
      "  18.34925   ]\n",
      "29-th iteration, loss: 0.08254509916863291, 78 gd steps\n",
      "insert gradient: -0.002588088513741689\n",
      "29-th iteration, new layer inserted. now 43 layers\n",
      "[ 54.14390367  95.57343289  53.98276797  95.30355745  54.01891704\n",
      "  94.65908001 104.63459667  94.4661744  102.54511061 104.36723195\n",
      "  95.73731848 130.70943649  74.1036017   10.1862563   17.58655512\n",
      " 113.24442281 105.26309464  99.70897991  24.96628833  27.13797704\n",
      "  59.60319588  70.59424583  17.21722779  53.40518102 251.50401479\n",
      "  10.23691124 150.8361428   15.16256112 122.53635538   0.\n",
      "  30.63408885  20.89456936 138.76248916  50.70990412  24.24495307\n",
      "  43.12007228 408.71898789   1.96057626 545.84935985   8.66871409\n",
      " 137.90488096  32.63021069  16.522344  ]\n",
      "30-th iteration, loss: 0.08206949693125562, 18 gd steps\n",
      "insert gradient: -0.0030253277839433065\n",
      "30-th iteration, new layer inserted. now 43 layers\n",
      "[ 54.63670504  95.51543406  54.07983752  94.9495326   53.20400038\n",
      "  94.41845099 104.56083358  93.89970643 102.57575455 104.17643118\n",
      "  95.2787094  131.42834839  72.99175798  11.38296375  17.70015045\n",
      " 112.28250149 105.193718   100.33919197  25.61571194  25.51563569\n",
      "  59.47230622  74.99312359  15.53521948  55.86273213 249.23530441\n",
      "   8.7274787  151.05475102  15.14748344 119.85240044   8.69870219\n",
      "  28.73323326  26.71853638 133.03458677  53.15151556  22.91095593\n",
      "  43.8585363  407.60380866   1.58848583 543.97228552   8.93616818\n",
      " 138.45026841  32.81714723  16.47365517]\n",
      "31-th iteration, loss: 0.0819480915029876, 21 gd steps\n",
      "insert gradient: -0.0022704330271233974\n",
      "31-th iteration, new layer inserted. now 45 layers\n",
      "[ 53.89183509  95.4091059   54.0007607   95.21275544  53.99843206\n",
      "  94.48694474 104.61086768  93.93550131 102.38212131 104.21986623\n",
      "  95.06774591 131.92962372  72.92971471  11.78364638  17.55075309\n",
      " 112.20960221 104.95819431 100.26249738  25.65012797  25.33514302\n",
      "  59.28871072  75.24283411  15.18185194  55.88009497 248.90197123\n",
      "   8.55035964  90.50814904   0.          60.33876602  14.65100677\n",
      " 119.50838276   9.72661706  29.0445229   27.2223325  132.49396112\n",
      "  53.39081     22.73036628  43.95649488 407.50474546   1.50319574\n",
      " 543.80564904   8.99330173 138.50939674  32.80137345  16.54800155]\n",
      "32-th iteration, loss: 0.08058631429424921, 84 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "32-th iteration, new layer inserted. now 47 layers\n",
      "[ 53.9345036   94.86377941  54.30540086  94.60588111  53.20631858\n",
      "  93.26376507 104.75007328  92.51036615 101.99626889 103.55281602\n",
      "  93.84841035 134.12610833  71.11603031  15.0456154   17.14896936\n",
      " 109.52858688 104.61620855 102.33327357  27.14111594  20.91311697\n",
      "  60.30359256  82.23961508  13.04373542  56.49231323 249.15628101\n",
      "   3.59234389  87.82053765  13.54538848  44.06271423  17.59982512\n",
      " 110.85379168  23.97211725  29.29214042  42.37061106 128.79370866\n",
      "  58.04982241  21.43903675  44.72609025 163.2388477    0.\n",
      " 244.85827155   1.7211652  543.64772849   8.34401788 139.17769886\n",
      "  32.52951004  16.28243579]\n",
      "33-th iteration, loss: 0.08042209942126552, 19 gd steps\n",
      "insert gradient: -0.002964742756081107\n",
      "33-th iteration, new layer inserted. now 47 layers\n",
      "[ 53.90258567  95.13315263  53.71164626  94.62598458  53.66932985\n",
      "  93.28559727 103.86851195  92.66160651 101.92569998 103.57941176\n",
      "  93.59408601 134.50368268  71.09709968  15.37942504  16.92956561\n",
      " 109.40710041 104.58414313 102.59334803  27.20856627  20.57533744\n",
      "  60.40932936  82.7297269   12.90425695  56.3738899  249.11823264\n",
      "   3.11210207  87.7684496   14.42759085  43.05050191  18.46384569\n",
      " 110.90206157  25.12987547  28.00369648  43.40248043 128.29122677\n",
      "  58.32990105  21.32100229  44.52276629 163.24506077   2.08491897\n",
      " 245.03487968   1.68354795 543.81273461   8.2031974  139.09920734\n",
      "  32.53626122  16.25988954]\n",
      "34-th iteration, loss: 0.08026444366562345, 35 gd steps\n",
      "insert gradient: -0.0020603356435487928\n",
      "34-th iteration, new layer inserted. now 49 layers\n",
      "[5.35810266e+01 9.48244553e+01 5.37583840e+01 9.45806925e+01\n",
      " 5.36399973e+01 9.33121325e+01 1.04046628e+02 9.27227925e+01\n",
      " 1.01799010e+02 1.03490347e+02 9.33636187e+01 1.34898043e+02\n",
      " 7.09075129e+01 1.59130373e+01 1.68260537e+01 1.09041289e+02\n",
      " 1.04402357e+02 1.02787817e+02 2.74022773e+01 2.01942551e+01\n",
      " 6.03571148e+01 8.32563546e+01 1.27224567e+01 5.62107483e+01\n",
      " 2.49075182e+02 2.52285474e+00 8.77315662e+01 1.56132169e+01\n",
      " 4.16081852e+01 1.95033886e+01 1.10938946e+02 0.00000000e+00\n",
      " 7.10542736e-15 2.66872223e+01 2.68220637e+01 4.48833974e+01\n",
      " 1.27575185e+02 5.86380985e+01 2.12195773e+01 4.41272954e+01\n",
      " 1.62291817e+02 2.97446306e+00 2.45157273e+02 1.49576730e+00\n",
      " 5.43966508e+02 8.11829434e+00 1.38928799e+02 3.25249510e+01\n",
      " 1.62979832e+01]\n",
      "35-th iteration, loss: 0.07959257935334746, 79 gd steps\n",
      "insert gradient: -0.0021686135063294157\n",
      "35-th iteration, new layer inserted. now 49 layers\n",
      "[5.33155727e+01 9.45359361e+01 5.37736676e+01 9.42942002e+01\n",
      " 5.35312211e+01 9.26852586e+01 1.03883283e+02 9.19811854e+01\n",
      " 1.01355392e+02 1.03030071e+02 9.22847004e+01 1.37176506e+02\n",
      " 6.93503858e+01 1.88408806e+01 1.66101814e+01 1.05648172e+02\n",
      " 7.80900200e+01 0.00000000e+00 2.60300067e+01 1.04716066e+02\n",
      " 2.84397393e+01 1.77984907e+01 6.01472294e+01 8.70379378e+01\n",
      " 1.20160219e+01 5.49121689e+01 3.36072054e+02 2.34354749e+01\n",
      " 3.38850379e+01 2.57028829e+01 1.08579802e+02 5.17589833e+00\n",
      " 1.03969221e-02 3.18647604e+01 2.02735815e+01 5.65232453e+01\n",
      " 1.23227670e+02 6.05666456e+01 2.07912470e+01 4.31885507e+01\n",
      " 1.54977253e+02 4.77650056e+00 2.48019542e+02 8.34979949e-01\n",
      " 5.47256698e+02 8.04661655e+00 1.38819083e+02 3.22915425e+01\n",
      " 1.63142049e+01]\n",
      "36-th iteration, loss: 0.07933361609178798, 18 gd steps\n",
      "insert gradient: -0.0027237069038109417\n",
      "36-th iteration, new layer inserted. now 47 layers\n",
      "[ 53.37868476  94.68517822  54.07597758  93.96549406  52.76336912\n",
      "  92.20788773 103.41335341  92.21897783 101.31970952 103.29861786\n",
      "  91.74736725 136.97678916  68.99828336  19.57224944  16.20877428\n",
      " 105.49056447  76.09340214   5.12141612  24.59649399 104.78565524\n",
      "  30.61652858  15.79775827  59.85783677  88.048378    12.34943295\n",
      "  53.63253563 335.59883379  24.83503192  32.21051917  26.81235088\n",
      " 107.8770479   40.02337325  18.60557311  59.96217091 122.27064526\n",
      "  61.19653703  20.6248835   43.29506653 152.06637772   5.40380163\n",
      " 249.04092602   0.64555803 548.4857812    8.43765647 138.78206359\n",
      "  32.36099779  16.18259161]\n",
      "37-th iteration, loss: 0.07893653268744263, 92 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "37-th iteration, new layer inserted. now 49 layers\n",
      "[5.30965919e+01 9.42079939e+01 5.36818166e+01 9.39270810e+01\n",
      " 5.32195334e+01 9.17642035e+01 1.03603236e+02 9.17599517e+01\n",
      " 1.00828815e+02 1.03447450e+02 9.12891639e+01 1.36580503e+02\n",
      " 6.91829080e+01 2.06107980e+01 1.48882604e+01 1.07429402e+02\n",
      " 7.20534821e+01 8.68028828e+00 2.45888676e+01 1.05505982e+02\n",
      " 3.36865722e+01 1.22245426e+01 5.99038431e+01 8.96618669e+01\n",
      " 1.27270030e+01 5.09385079e+01 3.33902848e+02 2.62473916e+01\n",
      " 3.01507544e+01 2.84698454e+01 1.07912637e+02 4.47399959e+01\n",
      " 1.59248873e+01 6.54661548e+01 1.20086837e+02 6.25013777e+01\n",
      " 1.99006086e+01 4.44828840e+01 1.46835990e+02 7.03117359e+00\n",
      " 6.28709078e+01 0.00000000e+00 1.88612723e+02 4.13590497e-01\n",
      " 5.51307281e+02 9.02183498e+00 1.38442310e+02 3.27524354e+01\n",
      " 1.59661931e+01]\n",
      "38-th iteration, loss: 0.07847007077370304, 75 gd steps\n",
      "insert gradient: -0.0013455819276101337\n",
      "38-th iteration, new layer inserted. now 49 layers\n",
      "[ 52.90525901  93.99371047  53.66566545  93.82825751  53.02999138\n",
      "  91.07630262 103.43525249  91.13834984  75.38790176   0.\n",
      "  25.12930059 103.10434004  90.6409286  136.71653609  69.46053976\n",
      "  21.40143381  13.47698376 109.93759943  68.80779293  11.40628749\n",
      "  24.15440385 106.75616882  36.03456871   8.94461533  60.46423445\n",
      "  91.06785366  13.05953201  48.85174674 330.80413824  25.54814312\n",
      "  28.85461419  28.92770011 109.57756306  50.68011432  13.20794231\n",
      "  71.1769866  117.89200446  64.13562328  18.99985284  46.87930442\n",
      " 143.14127181  10.6232565   58.85167721   5.26913514 740.719535\n",
      "   9.02894346 137.44980829  32.67647675  15.6834857 ]\n",
      "39-th iteration, loss: 0.07786212567264318, 27 gd steps\n",
      "insert gradient: -0.001126135133369335\n",
      "39-th iteration, new layer inserted. now 51 layers\n",
      "[ 52.48269518  93.82350269  54.07937417  94.07693416  52.60455783\n",
      "  90.89979313 103.62566398  91.0093907   71.93693511   5.3847044\n",
      "  24.62945176 100.75500619  90.23028338 137.53702858  68.8264939\n",
      "  23.53263527  12.61534995 111.6168059   67.15894277  14.08668803\n",
      "  22.72819118 107.71825131  38.88331792   5.62695306  60.9560819\n",
      "  91.81435605  13.19912419  47.1626819  325.37470257  24.08825873\n",
      "  28.94275102  30.28913433 112.93744497  56.12812481  11.12283838\n",
      "  75.18080972 116.74658674  65.52094203  18.07307304  48.9551192\n",
      " 141.43213203  13.51999988  40.22945998   0.          13.40981999\n",
      "   7.96170229 742.69408523   9.2242258  137.33789596  32.66039701\n",
      "  15.42474593]\n",
      "40-th iteration, loss: 0.07704764755365849, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.81883244  93.58709562  53.42968711  93.51041708  52.61931052\n",
      "  90.7808583  103.30632033  90.65855241  68.86106067   7.81480274\n",
      "  25.05222074 100.56887882  88.76525626 139.20668081  68.83363268\n",
      "  25.61518098  11.06294338 112.47934106  66.91799981  16.05684248\n",
      "  20.83234678 108.19310943  40.64216772   3.13738496  61.57921141\n",
      "  92.05443222  13.49980573  45.82098678 160.14357562   0.\n",
      " 160.14357562  24.42986153  29.52926898  32.94189499 114.94266877\n",
      "  59.05116866  10.04430743  76.366432   115.97553719  65.73733355\n",
      "  17.45932469  50.03504966 140.07001814  17.01693982  37.84791592\n",
      "   3.54043604  11.09699756   9.24045183 742.97529453   9.04766071\n",
      " 136.65248869  32.67375933  15.09566224]\n",
      "41-th iteration, loss: 0.07650606516696563, 15 gd steps\n",
      "insert gradient: -0.0023577634652889516\n",
      "41-th iteration, new layer inserted. now 53 layers\n",
      "[5.44286028e+01 9.32713352e+01 5.18956869e+01 9.29627399e+01\n",
      " 5.27329933e+01 9.07662524e+01 1.02431095e+02 9.03355066e+01\n",
      " 6.59243353e+01 1.01395366e+01 2.50313310e+01 1.00073989e+02\n",
      " 8.70956295e+01 1.41447027e+02 6.83919745e+01 2.80755229e+01\n",
      " 9.66187292e+00 1.12908977e+02 6.64170362e+01 1.88466316e+01\n",
      " 1.88711233e+01 1.08267744e+02 4.31204918e+01 2.33459184e-01\n",
      " 6.29664552e+01 9.22496609e+01 1.38470677e+01 4.48392313e+01\n",
      " 1.55316227e+02 6.46556300e+00 1.55590023e+02 2.86062257e+01\n",
      " 3.06319723e+01 3.75463654e+01 1.16474575e+02 6.18551530e+01\n",
      " 8.58986858e+00 7.65691856e+01 1.14774507e+02 6.56108930e+01\n",
      " 1.71829545e+01 5.12886357e+01 1.38114924e+02 2.09997194e+01\n",
      " 3.58558476e+01 5.71802395e+00 9.08361588e+00 1.07314373e+01\n",
      " 7.42659712e+02 8.74869988e+00 1.35614080e+02 3.27917644e+01\n",
      " 1.47150191e+01]\n",
      "42-th iteration, loss: 0.07394303064314843, 101 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.1955826   92.42265124  52.22077777  92.63476785  52.61395977\n",
      "  91.76019442 100.73783717  91.9782822   64.58961526  13.70218446\n",
      "  21.33684151 101.00685202  81.04967516  37.05834153   0.\n",
      " 111.17502459  66.80847697  35.27080789   6.62048635 113.75971801\n",
      "  64.79130217  25.77158242  15.84432595 106.04885206 107.95010653\n",
      "  92.1669991   14.95145566  42.31091252 155.24457159   8.38600339\n",
      " 151.57863626  36.83401809  25.27057911  45.91555757 114.71906399\n",
      "  67.47169309   7.72213196  74.48363485 114.49189337  63.48560871\n",
      "  16.85389356  53.9111346  134.31460092  28.09014513  32.81371331\n",
      "   8.95665863   5.3696261   14.0121175  741.08225979   8.24060903\n",
      " 133.84295132  33.00738476  14.28804251]\n",
      "43-th iteration, loss: 0.0725773773450713, 20 gd steps\n",
      "insert gradient: -0.006686406292693062\n",
      "43-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.27985169  92.11236439  51.1117019   92.29866275  52.8742001\n",
      "  91.94375997  98.86845018  94.90172522  66.68451199  17.39309203\n",
      "  15.52882497 105.22755487  73.80068384  38.51131455   1.36639311\n",
      " 112.73106058  64.80702727  40.50922152   5.09812421 113.97627007\n",
      "  63.78962107  29.86838416  14.91698848 103.34126056 107.81953975\n",
      "  92.04758884  14.92324635  40.40713944 159.43645801   7.80230188\n",
      " 148.3176861   42.69456922  20.78709051  51.68012767 111.75891986\n",
      "  70.9319912    7.15929161  72.34794255 114.55316086  61.74387426\n",
      "  16.53714113  55.55904274 131.29805849  33.24382006  30.42702828\n",
      "  11.09853467   2.62343538  16.15145089 739.49741786   7.50950306\n",
      " 132.40343369  32.8184802   13.54851857]\n",
      "44-th iteration, loss: 0.07211928288514097, 18 gd steps\n",
      "insert gradient: -0.002188430206652768\n",
      "44-th iteration, new layer inserted. now 55 layers\n",
      "[5.11706015e+01 9.20024796e+01 5.27786883e+01 9.21202806e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.11575898e+01 9.14029715e+01\n",
      " 9.91706352e+01 9.44566607e+01 6.64535766e+01 1.84708687e+01\n",
      " 1.43145404e+01 1.06142385e+02 7.31944502e+01 4.00793390e+01\n",
      " 5.21438181e-01 1.14134039e+02 6.49442211e+01 4.22040183e+01\n",
      " 4.22002177e+00 1.14295624e+02 6.29948780e+01 3.08802987e+01\n",
      " 1.48512741e+01 1.02412313e+02 1.07090107e+02 9.16449608e+01\n",
      " 1.52455985e+01 4.03011216e+01 1.59968650e+02 8.00169552e+00\n",
      " 1.46910089e+02 4.39111839e+01 2.05921394e+01 5.25115434e+01\n",
      " 1.11511576e+02 7.14601951e+01 7.59021754e+00 7.18614817e+01\n",
      " 1.14545431e+02 6.14560976e+01 1.68725774e+01 5.58283882e+01\n",
      " 1.30670124e+02 3.45965038e+01 3.03921887e+01 1.17882883e+01\n",
      " 2.17578350e+00 1.68792412e+01 7.39473525e+02 7.29879214e+00\n",
      " 1.32297060e+02 3.30257129e+01 1.37624652e+01]\n",
      "45-th iteration, loss: 0.07145938260482146, 48 gd steps\n",
      "insert gradient: -0.0010760010321746963\n",
      "45-th iteration, new layer inserted. now 51 layers\n",
      "[ 51.109853    91.64770506  51.62576808  92.23021119  53.19745548\n",
      "  91.57867009  98.33724969  94.37750187  66.78558049  20.60819527\n",
      "  12.00469761 108.10234981  71.20414874 159.95305327  63.62579106\n",
      "  46.25485279   2.89378074 115.35989028  61.5672856   33.57627651\n",
      "  14.62860404 100.34067605 107.21168564  90.80064339  15.53676925\n",
      "  39.74163075 161.90018822   8.63888636 143.8956178   46.37192135\n",
      "  19.44836635  54.0990191  111.19348581  72.37847959   7.76010587\n",
      "  70.41841722 114.54776807  60.55568046  16.9046029   56.16846145\n",
      " 129.28292212  37.16446625  29.63065398  12.64299191   0.76395153\n",
      "  17.76207791 738.96441559   6.82605467 131.81477856  33.06521937\n",
      "  13.68832214]\n",
      "46-th iteration, loss: 0.07139725715523138, 12 gd steps\n",
      "insert gradient: -0.002548420924515564\n",
      "46-th iteration, new layer inserted. now 53 layers\n",
      "[5.12988281e+01 9.17171539e+01 5.16578827e+01 9.21556018e+01\n",
      " 5.29522789e+01 9.15367955e+01 9.83331986e+01 9.43930106e+01\n",
      " 6.67854701e+01 2.06619121e+01 1.19365702e+01 1.08132863e+02\n",
      " 7.10420278e+01 1.59867560e+02 6.35399339e+01 4.63028002e+01\n",
      " 2.83595558e+00 1.15366551e+02 6.15535996e+01 3.36296295e+01\n",
      " 1.46161252e+01 1.00306709e+02 1.07216702e+02 9.07823285e+01\n",
      " 1.55313223e+01 3.97325380e+01 1.21442805e+02 0.00000000e+00\n",
      " 4.04809351e+01 8.66895799e+00 1.43823517e+02 4.64069540e+01\n",
      " 1.94101484e+01 5.41047622e+01 1.11158699e+02 7.23863337e+01\n",
      " 7.76312954e+00 7.03922180e+01 1.14548343e+02 6.05464475e+01\n",
      " 1.69167452e+01 5.61751016e+01 1.29271074e+02 3.72074315e+01\n",
      " 2.96168419e+01 1.26517377e+01 7.37248600e-01 1.77712249e+01\n",
      " 7.38944724e+02 6.80161753e+00 1.31798589e+02 3.30633672e+01\n",
      " 1.36912785e+01]\n",
      "47-th iteration, loss: 0.07096029820028514, 21 gd steps\n",
      "insert gradient: -0.0062765012740487085\n",
      "47-th iteration, new layer inserted. now 51 layers\n",
      "[ 51.09780822  91.61469136  51.9341866   92.14921428  52.40413295\n",
      "  91.50413829  98.42971803  94.82040576  67.45732851  22.13262556\n",
      "  10.40085112 109.48145024  69.44849347 159.74437344  63.1136663\n",
      "  48.28575447   1.97012019 116.18192659  61.32617549  35.25261314\n",
      "  14.157218    99.22269642 107.16153159  90.20610479  15.56606626\n",
      "  39.15658818 121.70779098   1.65925248  40.81007219   9.72483367\n",
      " 141.27523353  47.28590557  18.65694435  54.6843801  110.50159279\n",
      "  72.7452597    7.99658782  69.76935015 115.10681084  60.30890639\n",
      "  17.06209436  56.33083328 128.7964541   38.55651823  29.22346876\n",
      "  31.46526608 738.69802351   6.4502263  131.52838861  32.93615911\n",
      "  13.57119879]\n",
      "48-th iteration, loss: 0.07094274748475064, 12 gd steps\n",
      "insert gradient: -0.0011970655562531973\n",
      "48-th iteration, new layer inserted. now 53 layers\n",
      "[ 51.15115403  91.63162554  51.93153213  92.13015728  52.35339929\n",
      "  91.48119786  98.39070345  94.8120742   67.4604285   22.14910153\n",
      "  10.38604305 109.49550588  69.44431561 159.75638147  63.11777364\n",
      "  48.3107002    1.97278079 116.19698292  61.33232375  35.28031526\n",
      "  14.17897307  99.21902727  80.38170057   0.          26.79390019\n",
      "  90.20431329  15.57759904  39.15895363 121.71638811   1.68258534\n",
      "  40.82153103   9.74795161 141.25369328  47.3025759   18.66709345\n",
      "  54.69214388 110.49609905  72.74725455   7.9971196   69.7624394\n",
      " 115.10801629  60.30687969  17.06533977  56.33157961 128.79007899\n",
      "  38.56891288  29.22256866  31.47077161 738.69408933   6.44388502\n",
      " 131.52404637  32.93379471  13.57098698]\n",
      "49-th iteration, loss: 0.07039013562625408, 18 gd steps\n",
      "insert gradient: -0.012337808469545217\n",
      "49-th iteration, new layer inserted. now 53 layers\n",
      "[ 51.70970471  91.73334921  51.9340902   91.84852483  51.84901329\n",
      "  91.27708718  98.42219875  95.12277139  68.09431668  23.69392619\n",
      "   8.85962735 110.87735079  68.14731562 160.44260891  62.87597666\n",
      "  50.26276317   0.9361216  117.21057649  60.77139743  37.22381376\n",
      "  13.65685061  97.90518308  79.8784893    2.55917582  26.38598873\n",
      "  89.51009546  16.22557944  38.34985749 121.60468125   2.97823381\n",
      "  40.9801473   11.64469894 138.19820499  48.40976227  18.20305665\n",
      "  55.34942692 110.00392061  73.05351623   8.16724772  69.03832045\n",
      " 115.55297919  59.9793733   17.20701561  56.4548303  128.13753708\n",
      "  39.70392182  28.64426009  32.01933792 738.3016928    6.05178773\n",
      " 131.10650108  32.71049685  13.47696131]\n",
      "50-th iteration, loss: 0.06875515266113154, 37 gd steps\n",
      "insert gradient: -0.003875043987973308\n",
      "50-th iteration, new layer inserted. now 51 layers\n",
      "[ 50.15539583  90.60613989  51.94351796  91.7438695   53.56944108\n",
      "  90.97682702  97.02561739  97.05789772  69.46896463  32.20647661\n",
      "   2.44144482 117.43407742  60.65774551 163.93321139  61.44782436\n",
      " 176.60326444  59.72936605  47.46008186  10.15902833  93.86077347\n",
      "  74.89184225  12.13459372  26.86944268  87.42241444  19.75693008\n",
      "  32.51925091 119.32946907   8.89076005  40.11229124  20.85955181\n",
      " 128.55061022  54.7482256   15.58743363  57.89740286 109.61302962\n",
      "  74.42136493   8.5289427   66.35238347 118.73914522  58.5032133\n",
      "  17.70318976  57.70281102 126.24646662  43.97695537  26.29772965\n",
      "  34.41961937 735.80456572   4.958729   129.38832334  31.14804131\n",
      "  13.66745909]\n",
      "51-th iteration, loss: 0.06857594341134234, 19 gd steps\n",
      "insert gradient: -0.010566021419402577\n",
      "51-th iteration, new layer inserted. now 51 layers\n",
      "[ 50.4820159   90.75845158  52.07495494  91.72820107  53.42880299\n",
      "  90.94478266  96.97463556  97.11250535  69.60565973  32.29827905\n",
      "   2.42248259 117.52547397  60.81934692 164.01730223  61.44044304\n",
      " 176.44756232  59.66150022  47.47140263  10.16967481  93.8449359\n",
      "  74.77607924  12.11145977  26.75570142  87.37880011  19.70607357\n",
      "  32.49250536 119.28701473   8.92481391  40.07090963  20.88941257\n",
      " 128.49666969  54.76770055  15.56434797  57.88330305 109.56195606\n",
      "  74.42090382   8.56964573  66.35263214 118.73440523  58.49631598\n",
      "  17.70868697  57.70546    126.2198035   43.9917005   26.29946128\n",
      "  34.41727937 735.76343162   4.93519347 129.35647087  31.13091052\n",
      "  13.67160488]\n",
      "52-th iteration, loss: 0.06792894028972189, 28 gd steps\n",
      "insert gradient: -0.0019737166830521096\n",
      "52-th iteration, new layer inserted. now 53 layers\n",
      "[4.97696564e+01 0.00000000e+00 3.55271368e-15 9.07619131e+01\n",
      " 5.20786700e+01 9.16052499e+01 5.28477146e+01 9.08301507e+01\n",
      " 9.63904646e+01 9.76392846e+01 6.89357233e+01 3.42360831e+01\n",
      " 8.03029979e-01 1.19165190e+02 6.11210408e+01 1.65848350e+02\n",
      " 6.01867265e+01 1.73133609e+02 5.87284927e+01 4.88047362e+01\n",
      " 1.01327926e+01 9.36242844e+01 7.26597296e+01 1.41524006e+01\n",
      " 2.63366315e+01 8.70292411e+01 2.01225879e+01 3.13756037e+01\n",
      " 1.18743958e+02 1.05994403e+01 3.96281493e+01 2.30156625e+01\n",
      " 1.27531095e+02 5.61228872e+01 1.52150554e+01 5.82658896e+01\n",
      " 1.08879045e+02 7.45407169e+01 8.61412141e+00 6.60381525e+01\n",
      " 1.19132129e+02 5.81473516e+01 1.73763248e+01 5.78572278e+01\n",
      " 1.25274164e+02 4.44227526e+01 2.57831198e+01 3.43853886e+01\n",
      " 7.34176123e+02 4.44878601e+00 1.28422775e+02 3.05904907e+01\n",
      " 1.40197988e+01]\n",
      "53-th iteration, loss: 0.06781208487271173, 11 gd steps\n",
      "insert gradient: -0.014262308671076343\n",
      "53-th iteration, new layer inserted. now 55 layers\n",
      "[4.99019418e+01 4.13870550e-02 1.31712105e-01 9.08050210e+01\n",
      " 5.20845848e+01 9.15779954e+01 5.27770217e+01 9.08035678e+01\n",
      " 9.63435440e+01 9.76484131e+01 6.89760132e+01 3.42648869e+01\n",
      " 8.06815016e-01 1.19193462e+02 6.11527115e+01 1.65868432e+02\n",
      " 6.02100291e+01 1.73133779e+02 5.87393250e+01 4.88149390e+01\n",
      " 1.01183185e+01 9.36198525e+01 7.26228388e+01 1.41487420e+01\n",
      " 2.63090099e+01 8.70076886e+01 2.00642148e+01 3.13341514e+01\n",
      " 1.18689424e+02 0.00000000e+00 7.10542736e-15 1.05787198e+01\n",
      " 3.95710499e+01 2.30084946e+01 1.27488659e+02 5.61143011e+01\n",
      " 1.51694893e+01 5.82508964e+01 1.08848812e+02 7.45327317e+01\n",
      " 8.59996586e+00 6.60286726e+01 1.19122069e+02 5.81432516e+01\n",
      " 1.73771562e+01 5.78582165e+01 1.25262309e+02 4.44269318e+01\n",
      " 2.57831094e+01 3.43823224e+01 7.34153228e+02 4.43747469e+00\n",
      " 1.28406184e+02 3.05839584e+01 1.40222086e+01]\n",
      "54-th iteration, loss: 0.06760662394032642, 17 gd steps\n",
      "insert gradient: -0.007503076791851286\n",
      "54-th iteration, new layer inserted. now 53 layers\n",
      "[5.04018185e+01 1.15598791e-01 6.16848338e-01 9.09311934e+01\n",
      " 5.19909089e+01 9.13985642e+01 5.24012090e+01 9.06895754e+01\n",
      " 9.62211652e+01 9.77098318e+01 6.91476006e+01 3.43989878e+01\n",
      " 7.96248084e-01 1.19324150e+02 6.12804462e+01 1.65969436e+02\n",
      " 6.03102938e+01 1.73134566e+02 5.88060374e+01 4.88877880e+01\n",
      " 1.01058270e+01 9.36190242e+01 7.25098321e+01 1.41709114e+01\n",
      " 2.62474976e+01 8.69638390e+01 1.99731459e+01 3.12767420e+01\n",
      " 1.18661678e+02 1.08219099e+01 3.95436680e+01 2.30990818e+01\n",
      " 1.27414820e+02 5.61471086e+01 1.51085128e+01 5.82318134e+01\n",
      " 1.08752456e+02 7.45188809e+01 8.59295775e+00 6.60006580e+01\n",
      " 1.19096742e+02 5.81280218e+01 1.73827505e+01 5.78640150e+01\n",
      " 1.25205442e+02 4.44436665e+01 2.57668622e+01 3.43632066e+01\n",
      " 7.34039278e+02 4.38791865e+00 1.28324991e+02 3.05546164e+01\n",
      " 1.40390249e+01]\n",
      "55-th iteration, loss: 0.06758512801410235, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "55-th iteration, new layer inserted. now 55 layers\n",
      "[5.04383521e+01 1.93931648e-02 0.00000000e+00 5.81794943e-02\n",
      " 6.46380046e-01 9.09336408e+01 5.19601208e+01 9.13660082e+01\n",
      " 5.23432803e+01 9.06745547e+01 9.62140434e+01 9.77209562e+01\n",
      " 6.91594706e+01 3.44190532e+01 7.76147348e-01 1.19343159e+02\n",
      " 6.12858753e+01 1.65984087e+02 6.03173315e+01 1.73128522e+02\n",
      " 5.88155160e+01 4.89046749e+01 1.01041977e+01 9.36188117e+01\n",
      " 7.24909849e+01 1.41838494e+01 2.62429872e+01 8.69605382e+01\n",
      " 1.99743980e+01 3.12744091e+01 1.18673297e+02 1.08622078e+01\n",
      " 3.95570899e+01 2.31300668e+01 1.27409291e+02 5.61611242e+01\n",
      " 1.51098127e+01 5.82332253e+01 1.08735437e+02 7.45173806e+01\n",
      " 8.59275543e+00 6.59948025e+01 1.19092587e+02 5.81230782e+01\n",
      " 1.73784708e+01 5.78639958e+01 1.25192266e+02 4.44459091e+01\n",
      " 2.57590133e+01 3.43583658e+01 7.34014405e+02 4.37906243e+00\n",
      " 1.28307743e+02 3.05491792e+01 1.40440728e+01]\n",
      "56-th iteration, loss: 0.06719755729382612, 33 gd steps\n",
      "insert gradient: -0.0007583637137309299\n",
      "56-th iteration, new layer inserted. now 51 layers\n",
      "[ 50.64941852  91.04423397  52.04860711  91.11091471  51.96475443\n",
      "  90.44469948  95.71354588  98.18721785  68.92696707 155.83129517\n",
      "  60.40374557 166.31499188  59.87368639 172.38422577  58.96176721\n",
      "  49.96935193   9.53857916  93.40550233  70.99722963  15.27525011\n",
      "  26.06084142  86.64773724  19.92832693  30.3722394  118.32185744\n",
      "  12.34872725  39.20953377  24.8284141  126.59946606  57.02697114\n",
      "  14.93866471  58.47182117  81.12259658   0.          27.04086553\n",
      "  74.5435797    8.60352099  65.59132267 118.88726261  57.78871082\n",
      "  17.14441314  57.94723332 124.27472314  44.6275422   25.43732529\n",
      "  34.16318025 732.23446824   3.86639672 127.16160868  30.19234596\n",
      "  14.43898036]\n",
      "57-th iteration, loss: 0.06654481087848621, 22 gd steps\n",
      "insert gradient: -0.009231842098278034\n",
      "57-th iteration, new layer inserted. now 53 layers\n",
      "[5.10458187e+01 9.07683983e+01 5.22512891e+01 9.07399172e+01\n",
      " 5.18907891e+01 9.01060184e+01 9.50257943e+01 9.94068241e+01\n",
      " 6.78326409e+01 1.57118095e+02 5.93295531e+01 1.67076988e+02\n",
      " 5.99598311e+01 1.71398981e+02 6.00969745e+01 5.25050681e+01\n",
      " 8.43090138e+00 9.33428664e+01 6.81237093e+01 1.76283695e+01\n",
      " 2.55928780e+01 8.61132492e+01 1.98359265e+01 2.83767276e+01\n",
      " 1.17323631e+02 0.00000000e+00 7.10542736e-15 1.58879318e+01\n",
      " 3.81025871e+01 2.84437967e+01 1.25094655e+02 5.92823784e+01\n",
      " 1.40860996e+01 5.88027004e+01 8.03499619e+01 2.71983829e+00\n",
      " 2.64056625e+01 7.52628701e+01 8.99771160e+00 6.43166369e+01\n",
      " 1.18949886e+02 5.75684157e+01 1.70264425e+01 5.83313979e+01\n",
      " 1.23018310e+02 4.53882320e+01 2.51704725e+01 3.40058995e+01\n",
      " 7.28364962e+02 3.40220887e+00 1.25060890e+02 2.96820039e+01\n",
      " 1.50298544e+01]\n",
      "58-th iteration, loss: 0.06649282820868117, 13 gd steps\n",
      "insert gradient: -0.00028783898294231105\n",
      "58-th iteration, new layer inserted. now 55 layers\n",
      "[5.10282927e+01 9.07072728e+01 5.20605411e+01 9.06488772e+01\n",
      " 5.17920204e+01 9.00930465e+01 9.50612201e+01 9.94240850e+01\n",
      " 6.78133682e+01 1.57127308e+02 5.93247349e+01 1.67078539e+02\n",
      " 5.99593194e+01 1.71385192e+02 6.01073626e+01 5.25299199e+01\n",
      " 8.42625831e+00 9.33482911e+01 6.81138300e+01 1.76569559e+01\n",
      " 2.55937504e+01 8.61143788e+01 1.98412206e+01 2.83689903e+01\n",
      " 1.17337735e+02 5.49892144e-02 1.54993076e-02 1.59429627e+01\n",
      " 3.81117951e+01 2.84844098e+01 1.25090155e+02 5.93049495e+01\n",
      " 1.40784016e+01 5.88023056e+01 8.03336773e+01 2.73378284e+00\n",
      " 2.63939639e+01 7.52669811e+01 8.98700742e+00 6.42980423e+01\n",
      " 1.18950848e+02 5.75661702e+01 1.70169989e+01 5.83322161e+01\n",
      " 1.23011207e+02 4.53929135e+01 2.51629373e+01 3.40044880e+01\n",
      " 7.28337323e+02 0.00000000e+00 5.68434189e-14 3.41579212e+00\n",
      " 1.25049469e+02 2.96832542e+01 1.50294039e+01]\n",
      "59-th iteration, loss: 0.0622838665294512, 57 gd steps\n",
      "insert gradient: -0.0033857189144151085\n",
      "59-th iteration, new layer inserted. now 51 layers\n",
      "[ 50.45073607  89.15968178  49.98678817  88.78021157  50.2130015\n",
      "  88.60849324  92.32249465 100.85850187  60.75888497 158.23213828\n",
      "  56.08447511 164.81719425  58.62080799 164.83982685  62.30664607\n",
      "  57.55136471   3.91357854  94.98168403  65.15848154  22.40995556\n",
      "  18.65872072  84.39892186  12.38120038  21.58489752 114.99125769\n",
      "  29.37914623  33.17997156  36.59111571 121.02697853  64.0909607\n",
      "  11.27694668  59.39094482  77.56737944   4.34526095  25.27236374\n",
      "  77.55289434   9.16226781  60.00332985 118.17788645  57.65434086\n",
      "  15.74576947  59.14127273 120.43849345  46.77960664  23.72333502\n",
      "  34.30406152 722.88785148   3.47454949 124.14337682  29.79034814\n",
      "  14.41354825]\n",
      "60-th iteration, loss: 0.06131502434053784, 14 gd steps\n",
      "insert gradient: -0.0009842692024413153\n",
      "60-th iteration, new layer inserted. now 53 layers\n",
      "[4.94490593e+01 8.85653907e+01 5.01355302e+01 8.84121135e+01\n",
      " 5.00799478e+01 8.77808950e+01 9.10968156e+01 1.00847556e+02\n",
      " 5.95136517e+01 1.57132711e+02 0.00000000e+00 1.42108547e-14\n",
      " 5.51804251e+01 1.63692241e+02 5.79887021e+01 1.62658377e+02\n",
      " 6.22812856e+01 5.84116668e+01 3.03872707e+00 9.55642372e+01\n",
      " 6.48442702e+01 2.44162596e+01 1.67649256e+01 8.40513729e+01\n",
      " 9.53351089e+00 1.97092790e+01 1.13999340e+02 3.14116298e+01\n",
      " 3.29136738e+01 3.72271404e+01 1.20037780e+02 6.45648607e+01\n",
      " 1.09351677e+01 6.01693018e+01 7.74381461e+01 3.81107259e+00\n",
      " 2.52450237e+01 7.80602011e+01 9.03730541e+00 5.96451503e+01\n",
      " 1.17820592e+02 5.77852825e+01 1.52678933e+01 5.92899575e+01\n",
      " 1.19623187e+02 4.66606018e+01 2.31929075e+01 3.43642494e+01\n",
      " 7.22073820e+02 3.01747715e+00 1.23822157e+02 2.94612924e+01\n",
      " 1.42474461e+01]\n",
      "61-th iteration, loss: 0.05822341383772525, 96 gd steps\n",
      "insert gradient: -0.0025217305682341245\n",
      "61-th iteration, new layer inserted. now 49 layers\n",
      "[4.87757303e+01 8.71228415e+01 4.84863994e+01 8.62338862e+01\n",
      " 4.86258179e+01 8.56421952e+01 8.83844920e+01 1.00801881e+02\n",
      " 5.73665521e+01 1.54895210e+02 6.81733590e-02 3.79870058e-02\n",
      " 5.42537042e+01 1.58628431e+02 5.87163009e+01 1.54853380e+02\n",
      " 6.30884101e+01 1.62149682e+02 6.37348376e+01 3.69735324e+01\n",
      " 8.74350272e+00 1.02560322e+02 1.15516444e+02 4.15397614e+01\n",
      " 2.61067275e+01 4.30696645e+01 1.15896387e+02 6.63276365e+01\n",
      " 8.92219653e+00 6.52803187e+01 7.67152655e+01 1.91677961e+00\n",
      " 2.51028624e+01 7.96670094e+01 8.00970976e+00 5.86826510e+01\n",
      " 1.14021321e+02 5.88107444e+01 1.31411336e+01 6.08271752e+01\n",
      " 1.13819638e+02 4.80178427e+01 2.12118132e+01 3.46775104e+01\n",
      " 7.19328601e+02 3.83471356e+00 1.23490594e+02 2.94651435e+01\n",
      " 1.38349593e+01]\n",
      "62-th iteration, loss: 0.056950311281348115, 43 gd steps\n",
      "insert gradient: -0.0007658945447191434\n",
      "62-th iteration, new layer inserted. now 49 layers\n",
      "[ 48.37816183  86.44944846  47.74057307  85.02938326  47.59154154\n",
      "  85.07184994  65.04561424   0.          21.68187141 100.52058007\n",
      "  56.42325167 152.52720443  53.29322047 155.47306598  59.16064236\n",
      " 152.9583372   63.17434507 157.41530775  64.20720341  39.07799439\n",
      "   7.05200759 102.96604892 113.63959233  44.76986022  22.57423985\n",
      "  48.08348859 112.61310375  68.26878791   7.75114498  67.05214225\n",
      "  75.30888317   3.50747252  24.28026576  80.95722058   7.81962828\n",
      "  57.08062173 110.47800071  59.68681781  11.336798    62.26476719\n",
      " 109.65257824  48.60438776  19.85511895  35.46122695 718.05783445\n",
      "   3.37542647 123.06227459  28.18899728  14.13223959]\n",
      "63-th iteration, loss: 0.05609688867657573, 23 gd steps\n",
      "insert gradient: -0.0019438473070993317\n",
      "63-th iteration, new layer inserted. now 49 layers\n",
      "[ 48.36254474  85.99252071  47.01109621  84.59899612  47.73340103\n",
      "  85.08254252  63.53301232   3.33558848  20.28134727  99.48386055\n",
      "  55.17701691 152.3459764   52.42023825 154.1313075   58.77532979\n",
      " 151.73809141  63.44811953 155.64113872  65.53619544  40.254579\n",
      "   5.79992673 103.82783154 113.12785013  45.85899337  21.34625888\n",
      "  49.53388463 110.19147711  69.22100323   6.09565449  67.08348978\n",
      "  73.37966058   4.18034562  22.05612446  81.62186854   6.93380399\n",
      "  56.83526977 109.83286788  60.46250874  11.60626836  64.12730789\n",
      " 110.0266465   50.01563981  19.88363202  36.81208428 719.05789538\n",
      "   3.96876671 124.06189607  27.97714178  14.72278378]\n",
      "64-th iteration, loss: 0.05396577762251543, 76 gd steps\n",
      "insert gradient: -0.0010034019904094427\n",
      "64-th iteration, new layer inserted. now 51 layers\n",
      "[4.69016604e+01 8.48052516e+01 4.70790960e+01 8.40061562e+01\n",
      " 4.62009950e+01 8.28319791e+01 5.87500996e+01 1.03627875e+01\n",
      " 1.94666599e+01 9.49794493e+01 5.17606830e+01 1.53867340e+02\n",
      " 5.01555783e+01 1.52667344e+02 5.65201317e+01 1.48743913e+02\n",
      " 6.23339146e+01 1.50682240e+02 6.72523437e+01 4.15203010e+01\n",
      " 3.83405762e+00 1.05235885e+02 1.09330644e+02 4.86999835e+01\n",
      " 1.77606510e+01 5.44539049e+01 1.03757804e+02 7.23690374e+01\n",
      " 3.97476340e+00 6.88111087e+01 7.28464619e+01 1.64667321e+00\n",
      " 2.10434963e+01 8.46933994e+01 6.56934536e+00 5.83903985e+01\n",
      " 1.10786602e+02 6.40359894e+01 1.00130192e+01 6.74521744e+01\n",
      " 1.09357234e+02 5.49869335e+01 1.80638471e+01 3.98944425e+01\n",
      " 0.00000000e+00 3.55271368e-15 7.27868615e+02 5.91832764e+00\n",
      " 1.30469912e+02 2.76249625e+01 1.48733490e+01]\n",
      "65-th iteration, loss: 0.05394737295617572, 18 gd steps\n",
      "insert gradient: -0.0035970186175239296\n",
      "65-th iteration, new layer inserted. now 51 layers\n",
      "[4.68785694e+01 8.47958593e+01 4.70669676e+01 8.40152730e+01\n",
      " 4.62301377e+01 8.28383063e+01 5.87198758e+01 1.03620644e+01\n",
      " 1.94411534e+01 9.49599584e+01 5.17422555e+01 1.53839338e+02\n",
      " 5.01183045e+01 1.52630215e+02 5.64810216e+01 1.48707637e+02\n",
      " 6.23114190e+01 1.50653194e+02 6.72488607e+01 4.15221553e+01\n",
      " 3.81941547e+00 1.05237796e+02 1.09325864e+02 4.87090055e+01\n",
      " 1.77396255e+01 5.44628089e+01 1.03734984e+02 7.23690259e+01\n",
      " 3.95285743e+00 6.88153334e+01 7.28442600e+01 1.62044931e+00\n",
      " 2.10416852e+01 8.46993170e+01 6.57802623e+00 5.83972404e+01\n",
      " 1.10789938e+02 6.40460559e+01 1.00194996e+01 6.74611996e+01\n",
      " 1.09356726e+02 5.50141520e+01 1.80915352e+01 3.99111011e+01\n",
      " 5.11027555e-02 1.66403332e-02 7.27919724e+02 5.93569820e+00\n",
      " 1.30500912e+02 2.76391574e+01 1.48897534e+01]\n",
      "66-th iteration, loss: 0.05391389124494861, 13 gd steps\n",
      "insert gradient: -0.004166777816815888\n",
      "66-th iteration, new layer inserted. now 51 layers\n",
      "[4.68070438e+01 8.47566865e+01 4.69994170e+01 8.40130347e+01\n",
      " 4.62625881e+01 8.28462703e+01 5.86643097e+01 1.03745115e+01\n",
      " 1.94030772e+01 9.49221505e+01 5.17142882e+01 1.53787795e+02\n",
      " 5.00528577e+01 1.52561013e+02 5.64099794e+01 1.48635501e+02\n",
      " 6.22726538e+01 1.50592670e+02 6.72477885e+01 4.15299178e+01\n",
      " 3.79593865e+00 1.05245243e+02 1.09316575e+02 4.87308353e+01\n",
      " 1.76931034e+01 5.44848128e+01 1.03687301e+02 7.23766997e+01\n",
      " 3.93387535e+00 6.88343886e+01 7.28546427e+01 1.56753166e+00\n",
      " 2.10540944e+01 8.47163612e+01 6.60775911e+00 5.84159191e+01\n",
      " 1.10801349e+02 6.40688734e+01 1.00298056e+01 6.74801050e+01\n",
      " 1.09355795e+02 5.50728116e+01 1.81386743e+01 3.99426506e+01\n",
      " 1.66880428e-01 4.80095395e-02 7.28035559e+02 5.97387760e+00\n",
      " 1.30570529e+02 2.76696957e+01 1.49253711e+01]\n",
      "67-th iteration, loss: 0.05345892456398032, 38 gd steps\n",
      "insert gradient: -0.0008454579490155558\n",
      "67-th iteration, new layer inserted. now 51 layers\n",
      "[4.74878806e+01 8.48352612e+01 4.65315983e+01 8.36229126e+01\n",
      " 4.54001826e+01 8.29549176e+01 5.71441715e+01 1.14568962e+01\n",
      " 1.95849745e+01 9.35579541e+01 5.22610301e+01 1.52468672e+02\n",
      " 5.01062527e+01 1.51635164e+02 5.61412608e+01 1.47334194e+02\n",
      " 6.22734437e+01 1.48924093e+02 6.78614901e+01 4.24703117e+01\n",
      " 3.32999380e+00 1.05930686e+02 1.08657189e+02 5.00615506e+01\n",
      " 1.61322003e+01 5.59865977e+01 1.00609854e+02 7.24860797e+01\n",
      " 3.55430805e+00 7.02386416e+01 7.39503289e+01 7.56530146e-03\n",
      " 2.24021790e+01 8.50879542e+01 6.84515210e+00 5.91120942e+01\n",
      " 1.11159367e+02 6.49514877e+01 9.63946750e+00 6.79033723e+01\n",
      " 1.08383730e+02 5.69781559e+01 1.75167916e+01 3.97538845e+01\n",
      " 4.53201105e+00 5.73934710e-05 7.32408587e+02 5.97034724e+00\n",
      " 1.32424348e+02 2.78497406e+01 1.53733063e+01]\n",
      "68-th iteration, loss: 0.05330576583510408, 18 gd steps\n",
      "insert gradient: -0.0013724816532203828\n",
      "68-th iteration, new layer inserted. now 47 layers\n",
      "[ 46.1865364   84.35018106  46.34317582  83.85699378  46.27067656\n",
      "  83.10514632  56.9107979   11.3917423   19.47865637  93.30290533\n",
      "  52.09577188 152.31470084  49.91830919 151.4152269   55.8925509\n",
      " 147.10976848  62.08057531 148.60375905  67.8023467   42.64799782\n",
      "   3.03556751 106.03511121 108.50894934  50.12009724  15.91870311\n",
      "  56.28930192 100.79785797  72.53046141   3.76320163  70.43722682\n",
      "  95.87790911  84.98255877   6.69617775  59.07985048 110.96474369\n",
      "  64.97200328   9.63640474  67.94773824 108.2251108   57.22121695\n",
      "  17.66063494  39.76051845 737.75371054   6.11426887 132.51127174\n",
      "  27.94830242  15.52648374]\n",
      "69-th iteration, loss: 0.05317209470422403, 54 gd steps\n",
      "insert gradient: -0.000911281911048934\n",
      "69-th iteration, new layer inserted. now 47 layers\n",
      "[ 46.81214538  84.46734035  46.09423488  83.32351278  45.25257869\n",
      "  82.85945529  56.58663143  11.03628265  19.82345966  92.78530105\n",
      "  52.58269542 151.32436338  50.04008143 150.61849625  55.86353129\n",
      " 146.63574183  61.93632    147.64739508  67.73484261  43.65838167\n",
      "   2.52888159 106.59005291 107.6437637   50.71395031  15.07456615\n",
      "  58.1615196  100.66028473  72.18931961   3.60053674  71.13117372\n",
      "  95.66162375  84.49162886   7.13019053  59.41761795 110.84329789\n",
      "  65.36146472   9.72803342  68.0582791  108.1078927   58.66025159\n",
      "  17.73054634  39.04880034 739.91557234   7.20338231 132.67982802\n",
      "  28.706526    15.88686861]\n",
      "70-th iteration, loss: 0.0531317589624543, 22 gd steps\n",
      "insert gradient: -0.005709094211103901\n",
      "70-th iteration, new layer inserted. now 49 layers\n",
      "[4.68331971e+01 8.44218837e+01 4.59142561e+01 8.31749147e+01\n",
      " 4.49996091e+01 8.28371704e+01 5.66009671e+01 1.06255924e+01\n",
      " 1.99304496e+01 9.27663866e+01 5.27763242e+01 1.50815609e+02\n",
      " 5.00661761e+01 1.50195879e+02 5.59223494e+01 1.46376500e+02\n",
      " 6.19273343e+01 1.47349665e+02 6.77638561e+01 4.38726223e+01\n",
      " 2.33379794e+00 1.06679857e+02 1.07409849e+02 5.08485533e+01\n",
      " 1.48359634e+01 5.88584270e+01 1.00520134e+02 7.20189680e+01\n",
      " 0.00000000e+00 7.10542736e-15 3.54811809e+00 7.12883527e+01\n",
      " 9.56495025e+01 8.42294564e+01 7.25950792e+00 5.95237170e+01\n",
      " 1.10916857e+02 6.55138330e+01 9.80136448e+00 6.80772932e+01\n",
      " 1.08321912e+02 5.91572047e+01 1.78693939e+01 3.87253398e+01\n",
      " 7.40449830e+02 7.62716385e+00 1.32610211e+02 2.90679484e+01\n",
      " 1.60439989e+01]\n",
      "71-th iteration, loss: 0.053130262819270184, 5 gd steps\n",
      "insert gradient: -0.0014987451400676043\n",
      "71-th iteration, new layer inserted. now 51 layers\n",
      "[4.68318729e+01 8.44213605e+01 4.59140332e+01 8.31753848e+01\n",
      " 0.00000000e+00 7.10542736e-15 4.50010887e+01 8.28375774e+01\n",
      " 5.66010317e+01 1.06252912e+01 1.99304627e+01 9.27663959e+01\n",
      " 5.27763683e+01 1.50815295e+02 5.00660745e+01 1.50195552e+02\n",
      " 5.59223329e+01 1.46376338e+02 6.19274431e+01 1.47349592e+02\n",
      " 6.77642117e+01 4.38728643e+01 2.33416081e+00 1.06680067e+02\n",
      " 1.07410420e+02 5.08491101e+01 1.48376376e+01 5.88595088e+01\n",
      " 1.00521746e+02 7.20201289e+01 4.26629242e-03 1.16095959e-03\n",
      " 3.55238438e+00 7.12896407e+01 9.56511868e+01 8.42301068e+01\n",
      " 7.26199302e+00 5.95244978e+01 1.10917752e+02 6.55143173e+01\n",
      " 9.80251926e+00 6.80776438e+01 1.08322349e+02 5.91575272e+01\n",
      " 1.78696234e+01 3.87252078e+01 7.40450082e+02 7.62741203e+00\n",
      " 1.32610255e+02 2.90681411e+01 1.60440542e+01]\n",
      "72-th iteration, loss: 0.0518434681374867, 46 gd steps\n",
      "insert gradient: -0.04193529713800149\n",
      "72-th iteration, new layer inserted. now 51 layers\n",
      "[4.60224933e+01 8.39884063e+01 4.52494435e+01 8.28835496e+01\n",
      " 6.03040370e-01 1.09923918e-01 4.48692026e+01 8.29821238e+01\n",
      " 5.72715808e+01 8.78865224e+00 1.88982671e+01 9.44912255e+01\n",
      " 5.42472177e+01 1.46060529e+02 5.12284131e+01 1.44978904e+02\n",
      " 5.78327362e+01 1.42555084e+02 6.31656738e+01 1.44802899e+02\n",
      " 6.88726902e+01 4.27428378e+01 1.93182873e+00 1.05574164e+02\n",
      " 1.06442669e+02 5.16985670e+01 1.48044934e+01 6.35536660e+01\n",
      " 1.00078876e+02 7.11598425e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.74735326e+00 7.11676172e+01 9.54199398e+01 8.21513250e+01\n",
      " 8.51915880e+00 6.04237790e+01 1.12601962e+02 6.70535552e+01\n",
      " 1.11993861e+01 6.75634177e+01 1.10888493e+02 6.11550590e+01\n",
      " 1.94690643e+01 3.65793221e+01 7.40020359e+02 1.04085986e+01\n",
      " 1.31554386e+02 3.13960005e+01 1.67167178e+01]\n",
      "73-th iteration, loss: 0.0517845260536509, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "73-th iteration, new layer inserted. now 53 layers\n",
      "[4.60222667e+01 8.39881834e+01 4.52491882e+01 8.28829199e+01\n",
      " 6.01716219e-01 1.08235458e-01 4.48680710e+01 8.29818381e+01\n",
      " 5.72719741e+01 8.78933477e+00 1.88980632e+01 9.44922479e+01\n",
      " 5.42497988e+01 1.46061620e+02 5.12319660e+01 1.44980531e+02\n",
      " 5.78374441e+01 1.42558096e+02 6.31707195e+01 1.44806485e+02\n",
      " 6.88780591e+01 4.27464147e+01 1.94425351e+00 1.05577641e+02\n",
      " 1.06449461e+02 5.17034316e+01 1.48164106e+01 6.35588560e+01\n",
      " 1.00088893e+02 7.11676526e+01 2.70730304e-02 7.81016805e-03\n",
      " 2.77442629e+00 7.11755941e+01 9.54311025e+01 8.21553024e+01\n",
      " 8.53127977e+00 6.04274534e+01 1.12605810e+02 6.70553283e+01\n",
      " 1.12032465e+01 6.75643353e+01 5.54450579e+01 0.00000000e+00\n",
      " 5.54450579e+01 6.11558069e+01 1.94704458e+01 3.65793719e+01\n",
      " 7.40020425e+02 1.04092066e+01 1.31554455e+02 3.13963210e+01\n",
      " 1.67166979e+01]\n",
      "74-th iteration, loss: 0.046619752979282926, 65 gd steps\n",
      "insert gradient: -0.012829015093966665\n",
      "74-th iteration, new layer inserted. now 53 layers\n",
      "[4.94936106e+01 8.44160129e+01 4.38820022e+01 8.28262430e+01\n",
      " 4.79915522e+01 8.31294593e+01 5.68758857e+01 2.57502251e+01\n",
      " 6.45405917e+00 1.02488472e+02 5.13452960e+01 1.32577821e+02\n",
      " 6.50040278e+01 1.23190851e+02 6.80624336e+01 1.34398848e+02\n",
      " 6.63937650e+01 1.46955231e+02 6.74930502e+01 3.76471096e+01\n",
      " 6.95650988e+00 1.02255689e+02 1.10507154e+02 5.50853655e+01\n",
      " 1.37546618e+01 6.87532145e+01 9.80571276e+01 7.17803226e+01\n",
      " 5.55460356e-02 6.19895498e-01 0.00000000e+00 5.55111512e-17\n",
      " 1.29204941e+00 7.17116645e+01 9.70032172e+01 7.87835036e+01\n",
      " 1.00732506e+01 6.21134945e+01 1.13545108e+02 6.87689102e+01\n",
      " 1.26418035e+01 6.40890496e+01 5.45187167e+01 7.43183832e+00\n",
      " 4.86399126e+01 5.90586648e+01 2.19325926e+01 3.56969378e+01\n",
      " 7.37818106e+02 1.18878959e+01 1.29000295e+02 3.24638384e+01\n",
      " 1.56698723e+01]\n",
      "75-th iteration, loss: 0.04272049093697698, 25 gd steps\n",
      "insert gradient: -0.019797844935851218\n",
      "75-th iteration, new layer inserted. now 47 layers\n",
      "[ 49.67887228  85.33248803  43.48153803  83.70957192  47.99808069\n",
      "  84.71960996  55.73842504 146.04165731  52.04275186 120.94343874\n",
      "  72.90266579 116.95797513  69.58577006 135.32221194  65.47341891\n",
      " 150.02530275  65.91426409  38.05064601   7.98226032 102.16785625\n",
      " 112.12916216  56.19652975  13.12238191  69.91783487  96.61575658\n",
      "  73.87067821   0.98040487  72.32033161  97.79036112  78.43141203\n",
      "  10.01194889  62.35536807 114.01629143  69.62120117  12.33163286\n",
      "  63.48969183  55.55127269   9.36951204  45.35392701  58.70668686\n",
      "  21.93841469  35.18457408 739.25503382  11.29803249 128.77663016\n",
      "  31.63054632  14.95288553]\n",
      "76-th iteration, loss: 0.039000625760679156, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "76-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.95853163  86.7232273   47.34493068  84.10864826  46.48179143\n",
      "  86.23878868  50.63661563 141.53159543  64.43844298  95.66653945\n",
      "  82.35742332 110.10055162  69.59745394 140.44050586  63.04908246\n",
      " 155.19035065  62.84401026  44.27163172   7.3757663  100.02388338\n",
      " 112.91365625  60.58364163   9.1529484   79.40092161  92.4429065\n",
      " 151.12749329  99.59322597  79.42463652  10.35017741  61.75307393\n",
      " 112.94563749  78.62019793  11.8009881   54.64716507  59.92931246\n",
      "  27.25498866  26.11018065  70.25756774  25.65523342  26.44191734\n",
      " 149.71786629   0.         598.87146514  10.00652026 129.43835866\n",
      "  32.82477273  14.75228464]\n",
      "77-th iteration, loss: 0.037210748672277305, 50 gd steps\n",
      "insert gradient: -0.004157492604037191\n",
      "77-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.41528985  86.13224619  46.44322962  81.84098795  45.24103146\n",
      "  84.68779667  49.69410019 133.50954704  68.98732805  92.43034687\n",
      "  81.11280817 112.11454644  65.18795631 141.35240983  61.04001603\n",
      " 152.54873939  67.69657054  52.29667228   3.73128807 100.83301031\n",
      " 112.569959    63.34966863   4.32432278  87.19378573  82.61097264\n",
      " 152.61209275 104.99226518  78.20992365   9.21024879  66.10612164\n",
      " 109.14545808  88.15210195  15.12622928  35.60534342  61.79707144\n",
      "  45.21717764  15.74293979  82.34740311  26.79120324  25.85263947\n",
      " 137.65773214  10.16361992 597.64339128   6.97566181 128.83291908\n",
      "  31.16280993  13.61146911]\n",
      "78-th iteration, loss: 0.03388834218133765, 75 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "78-th iteration, new layer inserted. now 49 layers\n",
      "[ 46.22363004  84.49967479  44.49251296  80.18000378  42.92106191\n",
      "  82.7258231   46.99741342 124.90078327  68.95988545  89.62059799\n",
      "  77.60259116 109.24196682  60.84310445 138.76021372  58.69817695\n",
      " 148.05409528  63.5875244   53.78848641   1.63122882 101.15281124\n",
      " 107.02377854  66.02201233   1.83514784  87.4064643   75.92860376\n",
      "  37.0234077    0.         111.07022311 100.80146633  77.32974289\n",
      "   7.70652028  65.78302967 103.84856935  89.47245977  12.75344854\n",
      "  32.06101664  65.11605995  44.95615028  12.08234814  83.78341975\n",
      "  23.76669428  22.31329858 134.15868272   6.51794918 593.10333736\n",
      "   2.71315596 124.0773517   28.15389852  12.85341849]\n",
      "79-th iteration, loss: 0.03130725943717721, 34 gd steps\n",
      "insert gradient: -0.0036469249452283116\n",
      "79-th iteration, new layer inserted. now 47 layers\n",
      "[4.56855144e+01 8.39054682e+01 4.33166516e+01 7.93235432e+01\n",
      " 4.15211760e+01 8.10491577e+01 4.53700370e+01 1.08276937e+02\n",
      " 7.65009190e+01 8.38464265e+01 7.71871361e+01 1.07879387e+02\n",
      " 5.73343028e+01 1.37507840e+02 5.79300290e+01 1.42557582e+02\n",
      " 6.34873668e+01 5.68971531e+01 7.92625923e-02 1.03021420e+02\n",
      " 1.03145250e+02 1.63436186e+02 6.27965592e+01 2.84576648e+01\n",
      " 7.67722267e+00 1.02096312e+02 9.89332113e+01 7.92064120e+01\n",
      " 6.39245094e+00 6.57933205e+01 9.91267976e+01 9.55668166e+01\n",
      " 7.19954098e+00 2.98439242e+01 7.62734801e+01 4.53696969e+01\n",
      " 8.69311084e+00 8.91188642e+01 2.22075312e+01 2.15098832e+01\n",
      " 1.29282399e+02 8.07224429e+00 5.84037880e+02 2.18811252e+00\n",
      " 1.16990474e+02 2.51453311e+01 1.31031981e+01]\n",
      "80-th iteration, loss: 0.03083216509620806, 17 gd steps\n",
      "insert gradient: -0.0009503495810901469\n",
      "80-th iteration, new layer inserted. now 47 layers\n",
      "[ 46.1473871   84.25205026  43.50203662  79.2419828   41.42341717\n",
      "  81.14700705  44.91387363 106.00170633  75.92490211  84.27822105\n",
      "  76.56766717 107.45081931  56.83922021 136.93910521  57.6328378\n",
      " 142.00203473  62.81872875 159.0652534  102.03884736  97.55369122\n",
      "   0.          65.03579415  62.81023282  28.45633569   7.00808956\n",
      " 102.05560546  97.86495137  79.2749477    6.08741621  65.73186068\n",
      "  98.28780918  95.96991009   6.5532859   30.24718211  76.50674523\n",
      "  45.40597834   8.266309    89.26200934  21.78098343  20.97356715\n",
      " 128.7308395    8.03245112 583.36104719   2.08909514 116.40568844\n",
      "  25.25496577  12.86981818]\n",
      "81-th iteration, loss: 0.030384259753012913, 18 gd steps\n",
      "insert gradient: -0.0021513837389703096\n",
      "81-th iteration, new layer inserted. now 47 layers\n",
      "[ 45.9438845   84.11801256  43.02789131  78.88853163  41.31528589\n",
      "  81.25686997  45.01145932 102.13468715  76.29765293  85.31686093\n",
      "  75.68830671 106.68654592  56.10842698 136.07281014  57.60786575\n",
      " 140.87760918  62.61114376 158.33173196 100.93446037  96.63783719\n",
      "   0.          64.4252248   63.96808888  28.88861174   6.10528141\n",
      " 102.52037324  96.39922305  79.79339055   5.57125989  65.91332227\n",
      "  96.64890864  97.19320045   5.22453143  31.41164432  77.57447451\n",
      "  45.62440154   7.40956093  89.80055787  21.06398366  20.02205616\n",
      " 127.77240626   8.18772079 582.21133914   2.35798763 115.4595997\n",
      "  25.57635768  12.64813921]\n",
      "82-th iteration, loss: 0.028835813817328575, 44 gd steps\n",
      "insert gradient: -0.003119338762831348\n",
      "82-th iteration, new layer inserted. now 45 layers\n",
      "[4.46201579e+01 8.32118176e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.18601124e+01 7.77565044e+01 4.00423392e+01 8.08270244e+01\n",
      " 4.62422211e+01 9.03369009e+01 7.80995827e+01 8.63359076e+01\n",
      " 7.48949015e+01 1.03670220e+02 5.60882738e+01 1.34711526e+02\n",
      " 5.67846602e+01 1.37683612e+02 6.24392557e+01 1.56701447e+02\n",
      " 9.90839547e+01 1.56663999e+02 6.85081297e+01 3.24677879e+01\n",
      " 1.46883000e+00 1.06026693e+02 9.26919293e+01 8.51734786e+01\n",
      " 2.13297502e+00 6.75987991e+01 8.75312356e+01 1.46527867e+02\n",
      " 8.48242837e+01 4.91440620e+01 3.73949094e+00 9.43672778e+01\n",
      " 1.78199665e+01 1.83074344e+01 1.24276627e+02 8.55306500e+00\n",
      " 5.81410077e+02 6.17223013e+00 1.15163879e+02 2.81923436e+01\n",
      " 1.18102773e+01]\n",
      "83-th iteration, loss: 0.02863468191379143, 16 gd steps\n",
      "insert gradient: -0.00047064701353552425\n",
      "83-th iteration, new layer inserted. now 47 layers\n",
      "[4.48720826e+01 8.32884671e+01 9.99140671e-02 4.71262121e-02\n",
      " 4.19337813e+01 7.78142365e+01 4.02263827e+01 8.08024006e+01\n",
      " 4.60018900e+01 9.04451726e+01 7.80596451e+01 8.62301212e+01\n",
      " 7.45552008e+01 1.03519402e+02 5.58056183e+01 1.34518668e+02\n",
      " 5.66436415e+01 1.37583374e+02 6.23710644e+01 1.56551828e+02\n",
      " 9.88067948e+01 1.56297956e+02 6.83008151e+01 3.23814967e+01\n",
      " 1.06140498e+00 1.05945193e+02 9.23432287e+01 8.51936101e+01\n",
      " 1.94097798e+00 6.75690192e+01 8.73937141e+01 1.46483402e+02\n",
      " 8.46645283e+01 4.91009988e+01 3.51397264e+00 9.43535950e+01\n",
      " 1.77216918e+01 1.82943540e+01 9.93371847e+01 0.00000000e+00\n",
      " 2.48342962e+01 8.62482443e+00 5.81399462e+02 6.07810885e+00\n",
      " 1.15095483e+02 2.81612432e+01 1.17933310e+01]\n",
      "84-th iteration, loss: 0.026637986109148054, 289 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "84-th iteration, new layer inserted. now 43 layers\n",
      "[ 44.74151078  82.40831618  41.64224743  76.95636161  40.73747628\n",
      "  79.87411251  44.19227231  90.88894972  77.24657779  86.19520003\n",
      "  72.068727   105.79232634  52.85198246 132.41722228  57.50726923\n",
      " 131.49339012  67.63234043 155.78829534  96.29063669 146.96951856\n",
      "  72.48222944 135.45308107  79.9370319  160.39343657  92.20462205\n",
      " 149.14021778  79.58724873  48.93435975   2.52425133  97.12212092\n",
      "  17.74745182  14.28640476  99.75730934  17.12157487  29.16140865\n",
      "  19.78717394 116.23839804   0.         464.95359215   4.88092343\n",
      " 111.44557771  25.62996798  13.37492913]\n",
      "85-th iteration, loss: 0.025521374450073193, 23 gd steps\n",
      "insert gradient: -0.0025707746444630134\n",
      "85-th iteration, new layer inserted. now 43 layers\n",
      "[4.62280849e+01 8.32304481e+01 4.22043080e+01 7.59624949e+01\n",
      " 0.00000000e+00 3.55271368e-15 4.00063586e+01 7.85904135e+01\n",
      " 4.30300660e+01 8.68808673e+01 7.76049925e+01 8.79873363e+01\n",
      " 6.68265318e+01 1.15225015e+02 5.25337457e+01 1.31380394e+02\n",
      " 5.93872983e+01 1.20205296e+02 7.49392914e+01 1.56821672e+02\n",
      " 9.66379123e+01 1.38286990e+02 7.58263964e+01 1.29997554e+02\n",
      " 8.06741510e+01 1.63489383e+02 9.23117570e+01 1.41265773e+02\n",
      " 7.97347907e+01 1.61429249e+02 9.94736800e+00 2.04967061e+00\n",
      " 1.01539685e+02 4.02792275e+01 1.99571332e+01 3.79560225e+01\n",
      " 1.15869822e+02 1.11089157e+01 4.62278059e+02 4.87663333e+00\n",
      " 1.08221663e+02 2.53874090e+01 1.34444936e+01]\n",
      "86-th iteration, loss: 0.025247382971131552, 13 gd steps\n",
      "insert gradient: -0.001158785979506913\n",
      "86-th iteration, new layer inserted. now 45 layers\n",
      "[4.52281411e+01 8.28020949e+01 4.18352923e+01 7.60783355e+01\n",
      " 3.99420674e-01 3.78102028e-02 4.04435496e+01 7.90123663e+01\n",
      " 4.41063921e+01 8.73408774e+01 7.83079413e+01 8.81164233e+01\n",
      " 6.66128233e+01 1.15321852e+02 5.23872154e+01 1.31191704e+02\n",
      " 5.93515711e+01 1.20076947e+02 7.51388121e+01 1.25612620e+02\n",
      " 0.00000000e+00 3.14031550e+01 9.66912844e+01 1.38182514e+02\n",
      " 7.56880009e+01 1.29865855e+02 8.07271895e+01 1.63621449e+02\n",
      " 9.21671372e+01 1.41037926e+02 7.96079897e+01 1.61130925e+02\n",
      " 9.75387678e+00 1.99423580e+00 1.01302596e+02 4.02601380e+01\n",
      " 1.96395596e+01 3.79638469e+01 1.15737477e+02 1.10609278e+01\n",
      " 4.62192637e+02 4.82993482e+00 1.08166694e+02 2.54275437e+01\n",
      " 1.33711551e+01]\n",
      "87-th iteration, loss: 0.024655652812939244, 17 gd steps\n",
      "insert gradient: -0.0016166516017468192\n",
      "87-th iteration, new layer inserted. now 43 layers\n",
      "[ 45.32651308  83.16090224  41.79097037  76.21559959  40.24710098\n",
      "  79.0352348   44.37721102  88.49713482  77.13612019  89.15555873\n",
      "  64.43849705 119.36947863  52.06141071 128.22507226  62.01973479\n",
      " 117.68457963  77.48130058 124.14195828   4.93106863  30.46018062\n",
      "  92.78016036 134.70373778  75.32383922 129.58730344  82.87845106\n",
      " 165.86936007  91.44973379 139.40581366  80.76223254 159.85605329\n",
      "   9.4007339    3.09648833 100.59020189  42.89388951  18.54157328\n",
      "  40.02890355 114.87073642  10.37656698 460.7221183    4.46704009\n",
      " 107.16450543  25.65536322  12.92137567]\n",
      "88-th iteration, loss: 0.024546043761856044, 17 gd steps\n",
      "insert gradient: -0.0003233270250518558\n",
      "88-th iteration, new layer inserted. now 45 layers\n",
      "[ 45.87748525  83.48900196  42.13396686  76.13213453  39.96833236\n",
      "  79.04907059  44.68579748  88.64024225  77.0814958   89.33483934\n",
      "  63.87802226 119.68151965  52.0372572  127.71143918  62.12265979\n",
      " 117.33770982  77.3948486   99.22564765   0.          24.80641191\n",
      "   5.06027325  30.54170301  92.655505   134.57834071  75.41522056\n",
      " 129.49255337  82.91396644 165.87452087  91.16256104 139.20077644\n",
      "  80.76611999 159.84750734   9.34877462   3.22419312 100.51861011\n",
      "  43.12655507  18.47752602  40.20393611 114.81825694  10.31912679\n",
      " 460.57342129   4.35481341 107.04884249  25.69864684  13.01404461]\n",
      "89-th iteration, loss: 0.023145086025765108, 59 gd steps\n",
      "insert gradient: -0.0002841468470968446\n",
      "89-th iteration, new layer inserted. now 45 layers\n",
      "[ 47.64778137  85.33161094  43.39695075  74.72749563  39.0817842\n",
      "  78.3941543   45.12995845  88.7034158   74.3250469  100.97495028\n",
      "  53.84892354 128.25221234  57.60938306 106.53597341  74.20280386\n",
      " 104.87822517  82.89149825 112.72980657   7.64761474  38.39697271\n",
      "  72.69600015   0.          18.17400004 131.79891512  75.74824125\n",
      " 127.70548834  86.31399983 170.12532416  87.64501331 134.04346157\n",
      "  83.27354308 161.03658414   6.01940497   7.04209991  98.42763969\n",
      "  58.19563412  12.83714867  49.07955087 113.35574743   6.77414047\n",
      " 452.79914506   2.05551396 103.45644089  27.01973628  13.38357484]\n",
      "90-th iteration, loss: 0.023059495743821018, 22 gd steps\n",
      "insert gradient: -0.000524641626953507\n",
      "90-th iteration, new layer inserted. now 45 layers\n",
      "[ 46.8296652   84.97565784  43.18940753  74.85214074  39.65862322\n",
      "  78.56581063  45.1534963   88.57495617  74.5661513  101.07963696\n",
      "  53.88709413 127.58153717  58.3247446  105.09121451  74.7009514\n",
      " 104.77192808  83.29821602 111.37614714   7.99265496  38.33725724\n",
      "  71.79133197   3.39116859  17.21411243 131.28167524  75.96394869\n",
      " 127.45871218  86.30368131 170.25985814  87.74810159 133.94113643\n",
      "  83.30460142 161.00503541   5.55058795   7.01644676  98.45027594\n",
      "  60.04375761  11.9928456   50.20568335 113.64588493   6.80788691\n",
      " 451.97263681   2.21318403 103.33575217  27.03850797  13.20645577]\n",
      "91-th iteration, loss: 0.022984272835062883, 21 gd steps\n",
      "insert gradient: -0.00013451695228232133\n",
      "91-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.20103267  85.3113791   43.39520615  74.71839629  39.36956451\n",
      "  78.51055772  45.22599277  88.55626591  74.21455865 101.36972899\n",
      "  54.04440198 127.17122623  58.76093226 104.2825667   74.8876411\n",
      " 104.65556658  83.29950941 110.41593787   8.25480289  37.76680039\n",
      "  71.45165549   5.01383559  16.70018487 130.61843826  76.40739305\n",
      " 127.10812333  86.09438382 170.26005945  87.93686368 133.97551049\n",
      "  83.24934266 128.81464956   0.          32.20366239   5.18065445\n",
      "   6.92454125  98.64172005  61.57978088  11.44520464  51.18404518\n",
      " 113.92529328   6.99564105 451.27670176   2.22700054 103.27524703\n",
      "  27.07210549  13.28235279]\n",
      "92-th iteration, loss: 0.022794455866649483, 39 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "92-th iteration, new layer inserted. now 49 layers\n",
      "[ 47.3577444   85.56785863  43.71898706  74.89153644  39.45185777\n",
      "  78.42656076  45.25467913  88.20027536  74.17542928 102.85011092\n",
      "  53.67391929 126.53448551  60.32883354 101.33358225  76.17790164\n",
      " 103.67449815  83.76142275 107.57838496  10.4681093   34.01948378\n",
      "  72.14272925  10.62381843  14.65567037 127.47115138  77.94175753\n",
      " 125.19533687  85.74903323 170.40719925  88.87251402 134.4645758\n",
      "  82.80630212 128.27371077   1.71556215  32.17287953   1.1040144\n",
      "   3.49210795 101.48058447  70.86718423   8.03123046  56.95858353\n",
      " 114.59579045   7.95525689 223.53101363   0.         223.53101363\n",
      "   2.09617961 103.27331247  26.84873708  13.34382258]\n",
      "93-th iteration, loss: 0.02263975051467424, 18 gd steps\n",
      "insert gradient: -0.0005956382421020664\n",
      "93-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.20789331  85.29821549  44.61140428  75.24980959  39.26486081\n",
      "  77.97567269  44.9000661   87.99912265  74.51408938 102.78019572\n",
      "  53.96738528 126.19253686  61.04256228 101.5791726   76.34444645\n",
      " 102.91586576  83.87439617 107.20870952  11.82023901  30.77816345\n",
      "  72.73199964  13.71862963  13.4746723  125.28324864  78.96642666\n",
      " 123.87874502  85.65624603 170.43852974  89.57510647 135.04305304\n",
      "  82.89425306 127.50970409   2.22575611  35.2759634  102.56392917\n",
      "  75.06650746   6.46848964  60.05640526 113.36598628   7.99117523\n",
      " 219.04120777   6.12378221 220.40815869   1.60706182 102.94554431\n",
      "  27.50465556  12.82242601]\n",
      "94-th iteration, loss: 0.022549633202384897, 38 gd steps\n",
      "insert gradient: -0.00044379557913494196\n",
      "94-th iteration, new layer inserted. now 49 layers\n",
      "[4.74711009e+01 8.55188797e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.37334768e+01 7.53691241e+01 3.97821801e+01 7.84021257e+01\n",
      " 4.53328005e+01 8.77692194e+01 7.42933580e+01 1.03700579e+02\n",
      " 5.36540919e+01 1.26237217e+02 6.13998691e+01 1.01046261e+02\n",
      " 7.65986234e+01 1.02796016e+02 8.37933879e+01 1.07344952e+02\n",
      " 1.29596438e+01 2.82490115e+01 7.25016356e+01 1.71418491e+01\n",
      " 1.26622180e+01 1.23022143e+02 7.98977537e+01 1.22956977e+02\n",
      " 8.55962894e+01 1.70151223e+02 8.98819978e+01 1.35376791e+02\n",
      " 8.24014557e+01 1.26862607e+02 1.99180348e+00 3.59555853e+01\n",
      " 1.02433620e+02 7.81638063e+01 5.36078033e+00 6.22698591e+01\n",
      " 1.13301913e+02 8.27632411e+00 2.15191405e+02 7.15772507e+00\n",
      " 2.19646128e+02 2.42148520e+00 1.04982455e+02 2.80450574e+01\n",
      " 1.25163670e+01]\n",
      "95-th iteration, loss: 0.022528858661546525, 20 gd steps\n",
      "insert gradient: -0.00010876706848107945\n",
      "95-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.42686095  85.56096603  43.72875679  75.30527335  39.82350421\n",
      "  78.47466194  45.55630922  87.9519573   74.07479872 103.72184723\n",
      "  53.81828468 125.94458132  61.54383544 100.97075131  76.6063803\n",
      " 102.73299817  83.65151564 107.37587442  13.15677002  27.55088174\n",
      "  72.40900503  17.85747682  12.45195976 122.53180799  80.01341141\n",
      " 122.75795448  85.60624276 170.14625606  90.00794193 135.50093681\n",
      "  82.32617975 126.68659235   2.00073042  36.05880115 102.48357762\n",
      "  78.81138757   5.15901405  62.70217723 113.30998368   8.34921328\n",
      " 214.41423688   7.33735792 219.40246918   2.74974408 105.54127986\n",
      "  28.16090443  12.47174132]\n",
      "96-th iteration, loss: 0.022512747797296253, 18 gd steps\n",
      "insert gradient: -7.743294422440275e-05\n",
      "96-th iteration, new layer inserted. now 49 layers\n",
      "[4.76363196e+01 8.54411538e+01 4.35229554e+01 7.53054871e+01\n",
      " 3.99715180e+01 7.85521830e+01 4.57528856e+01 8.79904619e+01\n",
      " 0.00000000e+00 7.10542736e-15 7.40497632e+01 1.03761121e+02\n",
      " 5.37674826e+01 1.25783030e+02 6.17131267e+01 1.00801308e+02\n",
      " 7.66976276e+01 1.02632938e+02 8.35872557e+01 1.07500399e+02\n",
      " 1.35639792e+01 2.66492470e+01 7.23227041e+01 1.89650643e+01\n",
      " 1.22223243e+01 1.21818574e+02 8.02893833e+01 1.22436221e+02\n",
      " 8.56526625e+01 1.70111135e+02 9.01253710e+01 1.35597056e+02\n",
      " 8.23078114e+01 1.26382883e+02 2.02193559e+00 3.61827279e+01\n",
      " 1.02467918e+02 7.97265263e+01 4.82732583e+00 6.32708948e+01\n",
      " 1.13310724e+02 8.42090401e+00 2.13295504e+02 7.53055459e+00\n",
      " 2.18941005e+02 3.08848539e+00 1.06409689e+02 2.83641764e+01\n",
      " 1.23922701e+01]\n",
      "97-th iteration, loss: 0.022378005028562017, 69 gd steps\n",
      "insert gradient: -0.0001636253111503991\n",
      "97-th iteration, new layer inserted. now 51 layers\n",
      "[4.69253871e+01 8.55197900e+01 4.42401503e+01 7.56191190e+01\n",
      " 3.97887248e+01 7.85604344e+01 4.54820631e+01 8.80707670e+01\n",
      " 1.97485052e-02 2.11493575e-02 7.38439371e+01 1.04451790e+02\n",
      " 5.39843078e+01 1.25123083e+02 6.25253167e+01 1.00011586e+02\n",
      " 7.71740480e+01 1.02226571e+02 8.29079699e+01 1.09614407e+02\n",
      " 1.56544218e+01 2.12798330e+01 7.20843162e+01 2.62989827e+01\n",
      " 1.06410926e+01 1.15607393e+02 8.20408865e+01 1.20664106e+02\n",
      " 8.57727377e+01 1.27398975e+02 0.00000000e+00 4.24663250e+01\n",
      " 9.06374944e+01 1.35917019e+02 8.20500998e+01 1.23729709e+02\n",
      " 2.13206068e+00 3.76745321e+01 1.02386699e+02 8.66482529e+01\n",
      " 2.67128048e+00 6.62032944e+01 1.13814427e+02 8.66882813e+00\n",
      " 2.08287366e+02 8.32662349e+00 2.13470517e+02 4.76864483e+00\n",
      " 1.12670841e+02 2.95513197e+01 1.21764723e+01]\n",
      "98-th iteration, loss: 0.02235010187346112, 28 gd steps\n",
      "insert gradient: -0.00021402530513587063\n",
      "98-th iteration, new layer inserted. now 51 layers\n",
      "[4.70353783e+01 8.54114409e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.39664833e+01 7.58691342e+01 3.99276732e+01 7.85828860e+01\n",
      " 4.56021430e+01 8.81007581e+01 7.39417965e+01 1.04380025e+02\n",
      " 5.39898734e+01 1.25223915e+02 6.25534824e+01 1.00091365e+02\n",
      " 7.72074830e+01 1.02242706e+02 8.28131369e+01 1.09880127e+02\n",
      " 1.60885951e+01 1.99555074e+01 7.19636736e+01 2.81241375e+01\n",
      " 1.02554050e+01 1.14901191e+02 8.24117730e+01 1.20469267e+02\n",
      " 8.54264485e+01 1.26357037e+02 9.86033317e-01 4.16469176e+01\n",
      " 9.09232996e+01 1.35819359e+02 8.21270595e+01 1.23015840e+02\n",
      " 2.36644412e+00 3.78984624e+01 1.02522356e+02 8.76130327e+01\n",
      " 2.43812756e+00 6.64029974e+01 1.13729593e+02 8.67185815e+00\n",
      " 2.07855589e+02 8.48659684e+00 2.12080995e+02 5.34924187e+00\n",
      " 1.13644689e+02 2.98140481e+01 1.21676108e+01]\n",
      "99-th iteration, loss: 0.02234467640677634, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "99-th iteration, new layer inserted. now 51 layers\n",
      "[ 47.02159705  85.4546496   44.15195696  75.88426678  39.85354228\n",
      "  78.56530993  45.55856724  88.10536903  73.95515777 104.38459865\n",
      "  54.00545234 125.22823651  62.54961039 100.11126171  77.19619796\n",
      " 102.23380285  82.7866811  109.92838381  16.14882624  19.69890187\n",
      "  71.93155727  28.45682355  10.1598705  114.77510503  82.48729921\n",
      " 120.45455224  85.39309412 126.18626855   1.06476412  41.57891446\n",
      "  90.97983706 135.82036474  82.12883296 122.88105148   2.38948826\n",
      "  37.94096578 102.57400525  87.76843798   2.40611553  66.42911843\n",
      " 113.72265331   8.6693267   51.94905147   0.         155.84715441\n",
      "   8.51203782 211.82688581   5.40290391 113.7976272   29.8600919\n",
      "  12.17596476]\n",
      "0-th iteration, loss: 0.7682094385246233, 18 gd steps\n",
      "insert gradient: -0.4761289849469615\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  78.61929876    0.         3184.08159961]\n",
      "1-th iteration, loss: 0.5773967031668003, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.96250473e+01 1.32186882e+00 0.00000000e+00 1.07071374e+02\n",
      " 3.15805620e+03]\n",
      "2-th iteration, loss: 0.5722261084369512, 36 gd steps\n",
      "insert gradient: -0.2147279638686554\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.51156962  105.396646    153.97786437    0.         3002.56835515]\n",
      "3-th iteration, loss: 0.4588086156870489, 42 gd steps\n",
      "insert gradient: -0.16895002605034828\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  68.20025627  102.99261348   59.6446405     0.           52.76256659\n",
      "   85.64743741 2974.69835392]\n",
      "4-th iteration, loss: 0.42278650798090256, 90 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  72.50121869  123.44622026   56.20882749   74.09667889   26.9511119\n",
      "   85.30296878  168.23019344    0.         2775.79819174]\n",
      "5-th iteration, loss: 0.38694489558190504, 18 gd steps\n",
      "insert gradient: -0.1032400439126015\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[6.98016759e+01 1.35406818e+02 0.00000000e+00 2.84217094e-14\n",
      " 4.85396954e+01 8.73299892e+01 1.97625175e+01 8.72062291e+01\n",
      " 1.41577887e+02 7.39951669e+01 2.75980774e+03]\n",
      "6-th iteration, loss: 0.3640718737928863, 96 gd steps\n",
      "insert gradient: -0.04806424984009524\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  65.2305485   120.65899939   57.03984157   73.45186944   26.79230544\n",
      "   84.78905859  114.16587152   98.41907596  408.47965199    0.\n",
      " 2348.75799896]\n",
      "7-th iteration, loss: 0.2840578593302942, 57 gd steps\n",
      "insert gradient: -0.11835244152432649\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  75.23562032   99.790312     35.99387017   64.46038341   50.1542567\n",
      "  103.10163888   90.99737463  115.35547593  110.67969112    0.\n",
      "  237.17076669  126.75091157 2330.84128823]\n",
      "8-th iteration, loss: 0.21569765679212136, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  57.7915006   113.03891338   46.35529078   54.01670416   46.85439689\n",
      "   99.26148666   95.08151955  106.70975678   88.66322816  103.0815038\n",
      "  187.1946328   124.31107942  129.50542564    0.         2201.59223591]\n",
      "9-th iteration, loss: 0.19503189662219678, 15 gd steps\n",
      "insert gradient: -0.0234297232430191\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[6.63254673e+01 1.03378748e+02 5.44618748e+01 4.68268647e+01\n",
      " 0.00000000e+00 1.15463195e-14 4.27637987e+01 1.01218077e+02\n",
      " 9.89626127e+01 9.72542633e+01 9.14239250e+01 1.12281089e+02\n",
      " 1.82833713e+02 1.35379497e+02 9.77379220e+01 4.97192354e+01\n",
      " 2.19293618e+03]\n",
      "10-th iteration, loss: 0.18597986140071077, 40 gd steps\n",
      "insert gradient: -0.020667137622386804\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[  62.53109174  106.7178891    46.58973293   53.17142061   46.9013159\n",
      "   98.25423592   94.78505335  103.93941247   91.18980432  108.18567239\n",
      "  179.06932502  139.10780382   96.48168453   60.47327651  409.74398275\n",
      "    0.         1775.5572586 ]\n",
      "11-th iteration, loss: 0.1788867909522744, 16 gd steps\n",
      "insert gradient: -0.024295108634915182\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  66.64741142   94.13420417   47.68133584   54.14115167   48.15421327\n",
      "  100.00170127   91.25338625  105.88306085   94.50461749  103.49552169\n",
      "  176.17955275   72.22016106    0.           72.22016106   94.86539386\n",
      "   82.6624088   374.13555313   30.34580171 1777.82038322]\n",
      "12-th iteration, loss: 0.17652889610677402, 24 gd steps\n",
      "insert gradient: -0.01553010945109718\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  64.94524236   99.80216447   45.64274315   56.24690084   47.27847189\n",
      "   98.39693678   93.41605194  104.66213453   93.14676694  105.76746601\n",
      "  131.25932705    0.           43.75310902   67.61989488    3.20122021\n",
      "   67.61899471   93.98400766   88.39123893  371.27295329   33.5428531\n",
      " 1775.78795697]\n",
      "13-th iteration, loss: 0.156986444718732, 63 gd steps\n",
      "insert gradient: -0.01870844602239663\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  58.35207823   98.59872978   47.1539131    57.06320639   47.70918037\n",
      "  104.26711632   89.67558243  108.88220874   90.21656545  107.29898685\n",
      "   99.34910301   67.81361159   16.86381103   88.21469228   13.94226069\n",
      "   46.77094566   84.39194277  138.78474508   97.46415597    0.\n",
      "  259.90441592   38.97020448 1765.12506798]\n",
      "14-th iteration, loss: 0.15081797607195196, 39 gd steps\n",
      "insert gradient: -0.017838300697412095\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  57.68097159   99.72570165   45.8981926    58.34176913   52.29076957\n",
      "  105.77752113   89.78786381  109.938385     89.25609095  110.65159623\n",
      "   96.47088089   81.08271169   15.19050224   85.13589412   20.13612955\n",
      "   40.49188406   75.81641689   45.42767434    0.          105.99790678\n",
      "  102.09821944   17.70213555  245.53906343   39.19331013 1754.36203869]\n",
      "15-th iteration, loss: 0.14485770353248587, 21 gd steps\n",
      "insert gradient: -0.011718330985498824\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[5.77634200e+01 9.81947598e+01 4.23439183e+01 6.76530342e+01\n",
      " 5.24665068e+01 1.11024436e+02 8.76865019e+01 1.14943125e+02\n",
      " 8.69326866e+01 1.17596894e+02 9.32315475e+01 1.00316343e+02\n",
      " 0.00000000e+00 1.77635684e-14 1.21870696e+01 7.47540635e+01\n",
      " 2.58172794e+01 4.00018055e+01 6.47390937e+01 4.71504105e+01\n",
      " 3.53882813e+00 1.08422490e+02 1.07326649e+02 2.90636881e+01\n",
      " 2.34504877e+02 2.96853541e+01 1.74877396e+03]\n",
      "16-th iteration, loss: 0.14316956844461293, 13 gd steps\n",
      "insert gradient: -0.00775597189125371\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  58.49886091   95.36724481   42.69230493   68.31439685   53.66432114\n",
      "  110.51503525   88.24092963  113.4848451    86.82453683  118.47512259\n",
      "   92.66017868  105.50161733   12.35718984   70.87833577   27.43898812\n",
      "   38.86769473   62.77337085   48.80984991    3.10608246  109.78009502\n",
      "  107.74828277   31.69928602  232.54950656   24.77967244 1746.67982975]\n",
      "17-th iteration, loss: 0.13584151545663878, 49 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  57.69701299   95.11433779   43.28735289   71.94958939   54.19820975\n",
      "  112.77832763   87.24616955  115.29846074   87.74164481  119.75398676\n",
      "   92.77528085  116.36693219   18.46149877   45.00409284   41.291243\n",
      "   31.35222348   53.55334659   53.33334469    4.66382584  113.09735525\n",
      "  110.7163915    41.39643646  227.82816985    4.27587575  192.91036872\n",
      "    0.         1543.28294979]\n",
      "18-th iteration, loss: 0.12946608301308035, 31 gd steps\n",
      "insert gradient: -0.012520204091809052\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[  57.77518432   95.10520798   45.27412189   78.32018693   51.11131924\n",
      "  109.3204241    90.21325365  113.6630957    91.30510551  115.77260126\n",
      "   91.66891503  117.9152602    25.50095412   15.24788681   70.18319355\n",
      "   26.55785582   37.39639602   68.5710942     2.62911625  115.46133992\n",
      "   71.55180109    0.           35.77590054   51.81075922  404.71529704\n",
      "   15.59891479 1541.46025936]\n",
      "19-th iteration, loss: 0.12203323436355998, 40 gd steps\n",
      "insert gradient: -0.012939233560070026\n",
      "19-th iteration, new layer inserted. now 27 layers\n",
      "[  57.81211562   95.56747734   47.63951836   78.39358914   51.96115221\n",
      "  109.1817413    92.04502996  113.15514732   92.49244019  113.98093662\n",
      "   94.10749885  114.99079893   31.61371867    7.80930524   68.94046366\n",
      "   46.19559948   22.31298003  206.66090015   62.70728244   28.25760664\n",
      "   19.4321113    77.08727568  129.04552931    0.          258.09105862\n",
      "   12.66807609 1538.26876036]\n",
      "20-th iteration, loss: 0.1160273894715402, 30 gd steps\n",
      "insert gradient: -0.038247265092074\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[  57.99114789   96.08751124   47.37369204   80.05213842   52.45200175\n",
      "  108.61748442   93.10766575  110.22235809   91.39877519  119.56810015\n",
      "   92.95195418  118.23191553  109.37939694   53.42577466   20.19151433\n",
      "  138.53263825    0.           69.26631913   49.01935424   49.62996938\n",
      "   14.25913653   83.16829482  120.85129544   24.8569108   254.71904195\n",
      "    5.50985438 1527.9469081 ]\n",
      "21-th iteration, loss: 0.1104630648159972, 18 gd steps\n",
      "insert gradient: -0.0126583435275127\n",
      "21-th iteration, new layer inserted. now 27 layers\n",
      "[5.80119584e+01 9.45825221e+01 4.60570168e+01 8.21473230e+01\n",
      " 5.56311183e+01 1.06094295e+02 9.18265305e+01 1.12931274e+02\n",
      " 9.05366369e+01 1.18701177e+02 9.26286106e+01 1.20460467e+02\n",
      " 1.04906408e+02 6.91010910e+01 1.10229472e+01 1.30111279e+02\n",
      " 0.00000000e+00 1.42108547e-14 1.34968717e+01 5.91904672e+01\n",
      " 4.69052250e+01 5.62154541e+01 1.19703895e+01 8.72156631e+01\n",
      " 1.21974288e+02 2.88375391e+01 1.77727572e+03]\n",
      "22-th iteration, loss: 0.10776470394858398, 31 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[  55.83344603   94.22244277   48.74856053   83.35998493   52.81871342\n",
      "  106.7873403    92.77531535  111.22953554   91.04122317  119.57339849\n",
      "   92.79956518  121.07309765  103.73101742   78.22134815    6.26944996\n",
      "   28.39954908    0.           99.39842177   19.83924741   53.7660547\n",
      "   45.9419991    59.86752166   11.99792463   88.9384619   120.9681599\n",
      "   31.93386558 1772.03084316]\n",
      "23-th iteration, loss: 0.1061350962008767, 23 gd steps\n",
      "insert gradient: -0.011411391936012285\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[  55.45528732   94.05020003   48.8657722    84.15601627   52.53014153\n",
      "  106.41013093   93.42388845  110.77638858   91.7987906   119.72819753\n",
      "   93.11204372  120.85563558  100.65303896  108.71548869    6.33543831\n",
      "   97.18785676   25.28269504   49.56969697   45.07486868   63.10199592\n",
      "   11.38938571   90.31635407  120.23527319   34.70192499 1768.97000579]\n",
      "24-th iteration, loss: 0.10156724799822539, 156 gd steps\n",
      "insert gradient: -0.0038149925463863837\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[  55.63781087   94.47301608   50.301964     88.16918437   53.32500897\n",
      "  104.30011869   96.48870161  107.33388696   95.25031682  117.01141504\n",
      "   92.31105993  126.16336108   98.9081979   113.00295859   22.6157031\n",
      "   29.55709876   62.69810562   34.9017509    34.50687592   87.62728706\n",
      "    7.6431394    90.81519446   79.2625503     0.           39.63127515\n",
      "   35.61705525 1744.87188538]\n",
      "25-th iteration, loss: 0.10106543551378137, 23 gd steps\n",
      "insert gradient: -0.0018437017795989894\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[  55.67802409   94.32468937   50.37533616   88.71730136   53.7703789\n",
      "  104.45153493   96.53627054  107.0338346    95.42882542  116.23781007\n",
      "   92.86174556  125.00019357   99.00072641  113.42895069   23.22988071\n",
      "   28.61171735   63.02965927   36.44758688   33.46589322   88.64342484\n",
      "    8.3731352    87.72525427   77.74809903    5.23415854   38.24792811\n",
      "   36.95019087 1741.59821374]\n",
      "26-th iteration, loss: 0.09942021185332843, 22 gd steps\n",
      "insert gradient: -0.007228261783360169\n",
      "26-th iteration, new layer inserted. now 29 layers\n",
      "[5.78046689e+01 9.28739912e+01 5.04274244e+01 9.02042883e+01\n",
      " 5.36621346e+01 1.02295401e+02 9.79571452e+01 1.05471660e+02\n",
      " 0.00000000e+00 1.06581410e-14 9.62760718e+01 1.13909524e+02\n",
      " 9.45923739e+01 1.20118138e+02 9.71857254e+01 1.21380657e+02\n",
      " 2.58522464e+01 1.80831860e+01 6.91688148e+01 4.55974855e+01\n",
      " 2.50638651e+01 1.05815852e+02 1.41415711e+01 6.11838875e+01\n",
      " 6.60196877e+01 2.37456385e+01 3.22946833e+01 5.62952287e+01\n",
      " 1.73259442e+03]\n",
      "27-th iteration, loss: 0.09906649403317508, 12 gd steps\n",
      "insert gradient: -0.003145923810773961\n",
      "27-th iteration, new layer inserted. now 31 layers\n",
      "[5.51370998e+01 9.31633602e+01 5.14916567e+01 9.03642437e+01\n",
      " 5.42589260e+01 1.02717093e+02 9.79057977e+01 1.05592211e+02\n",
      " 1.88262049e-02 4.20647356e-02 0.00000000e+00 1.73472348e-18\n",
      " 9.63235847e+01 1.13871869e+02 9.46975952e+01 1.20020513e+02\n",
      " 9.73149355e+01 1.21084425e+02 2.61545814e+01 1.80171114e+01\n",
      " 6.90843188e+01 4.61110074e+01 2.48294022e+01 1.05626535e+02\n",
      " 1.40757823e+01 6.08668980e+01 6.58452819e+01 2.41831760e+01\n",
      " 3.17904858e+01 5.63991318e+01 1.73230891e+03]\n",
      "28-th iteration, loss: 0.09713495494588929, 79 gd steps\n",
      "insert gradient: -0.001050368392891878\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[  54.86988518   94.566511     51.26097586   90.0542269    53.56895544\n",
      "  103.05169831   97.82166773  105.26702059   96.85309864  112.49122989\n",
      "   95.07629654  117.94780236   97.38502009  116.84071664   30.52202125\n",
      "   10.45024525   68.80909318   65.83000245   15.58316778  104.75704652\n",
      "   22.62780164   45.18422507   63.09494127   38.37701205   22.89305719\n",
      "   70.11354589 1725.25337164]\n",
      "29-th iteration, loss: 0.0956034929139573, 107 gd steps\n",
      "insert gradient: -0.0002317299423694817\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[5.44092624e+01 9.42133115e+01 5.14209954e+01 9.07896946e+01\n",
      " 5.35535479e+01 1.01843773e+02 9.81063605e+01 0.00000000e+00\n",
      " 3.55271368e-15 1.03993652e+02 9.71771068e+01 1.11769557e+02\n",
      " 9.51784876e+01 1.16661129e+02 9.74851079e+01 1.12637737e+02\n",
      " 3.33085928e+01 6.57400606e+00 6.50715123e+01 9.46341367e+01\n",
      " 7.47305295e+00 9.58009413e+01 3.39988150e+01 3.21034197e+01\n",
      " 5.52179326e+01 5.61150485e+01 1.26871603e+01 9.23471211e+01\n",
      " 1.71700344e+03]\n",
      "30-th iteration, loss: 0.09552815833520907, 22 gd steps\n",
      "insert gradient: -0.001998148030553583\n",
      "30-th iteration, new layer inserted. now 29 layers\n",
      "[5.44848505e+01 9.41944967e+01 5.14518815e+01 9.06181355e+01\n",
      " 0.00000000e+00 1.06581410e-14 5.32848366e+01 1.02025323e+02\n",
      " 9.80213563e+01 1.03972896e+02 9.72791936e+01 1.11646404e+02\n",
      " 9.53054527e+01 1.16490059e+02 9.75182579e+01 1.12620233e+02\n",
      " 3.35217060e+01 6.66121127e+00 6.44592737e+01 9.63899800e+01\n",
      " 7.68501425e+00 9.27377748e+01 3.51554556e+01 3.15139135e+01\n",
      " 5.40609561e+01 5.79408292e+01 1.17347608e+01 9.52458690e+01\n",
      " 1.71603118e+03]\n",
      "31-th iteration, loss: 0.09551339388110004, 13 gd steps\n",
      "insert gradient: -0.000475055712991197\n",
      "31-th iteration, new layer inserted. now 27 layers\n",
      "[  54.3472328    94.21526067   51.51427563   90.6749987    53.560622\n",
      "  102.04917716   97.99040705  104.02832947   97.17518218  111.63151436\n",
      "   95.25453837  116.47521594   97.50519137  112.63730713   33.56786354\n",
      "    6.68073591   64.46807635   96.48951304    7.64814144   92.51336973\n",
      "   35.2634214    31.5052705    53.98308326   58.01064915   11.60762186\n",
      "   95.39210793 1715.84394005]\n",
      "32-th iteration, loss: 0.09543091727324053, 28 gd steps\n",
      "insert gradient: -0.00228245192866847\n",
      "32-th iteration, new layer inserted. now 27 layers\n",
      "[  54.17917606   94.26463876   51.33867682   90.77539264   53.44496095\n",
      "  102.0816782    97.86939094  104.45485373   97.06042993  111.68495197\n",
      "   95.25950988  116.27072761   97.49860246  112.52067863   33.80204036\n",
      "    6.67784527   63.97958019   98.38510937    7.94834288   88.43680821\n",
      "   36.71953709   30.9131532    52.65798462   59.62552749   10.50378546\n",
      "   98.72200404 1714.79149832]\n",
      "33-th iteration, loss: 0.095347603183181, 38 gd steps\n",
      "insert gradient: -0.0005778467539717023\n",
      "33-th iteration, new layer inserted. now 27 layers\n",
      "[  54.25376467   94.30340754   51.45424748   90.7839113    53.51970278\n",
      "  101.82765661   98.13920563  103.90180603   97.21480977  111.60036994\n",
      "   95.33593054  116.05361264   97.5084592   112.26052354   34.04680789\n",
      "    6.61285689   63.59589565  100.09926959    8.51623571   83.40175889\n",
      "   38.62560665   30.24708315   51.20411405   61.03846668    9.56679905\n",
      "  102.06564263 1713.45830563]\n",
      "34-th iteration, loss: 0.09522935495580458, 87 gd steps\n",
      "insert gradient: -0.0005832763102129879\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[  54.32085046   94.34868701   51.56631559   90.93237089   53.4599806\n",
      "  101.74116407   98.19064386  103.92644465   97.34258406  111.37730448\n",
      "   95.63386703  115.49091663   97.61334176  111.75307599   34.01005992\n",
      "    6.89318408   62.9659138   102.03543553   10.09792854   72.54633304\n",
      "   44.30603155   28.03231865   47.74014586   64.9878271     7.91649828\n",
      "  106.75810048 1710.85370462]\n",
      "35-th iteration, loss: 0.09522364770464119, 25 gd steps\n",
      "insert gradient: -0.0002417751625941858\n",
      "35-th iteration, new layer inserted. now 29 layers\n",
      "[  54.20811991   94.31021634   51.59125852   90.99864969   53.54198964\n",
      "  101.76106677   98.24545012  103.9071545    97.32526135  111.34826974\n",
      "   95.63471777  115.44707963   97.60606837  111.73976743   33.97165607\n",
      "    6.92025703   62.96019452  102.0362069    10.13230204   72.19463461\n",
      "   44.53145013   27.9663872    47.55501188   65.22441835    7.85531658\n",
      "  106.73640994 1496.77991797    0.          213.82570257]\n",
      "36-th iteration, loss: 0.09521104666833537, 17 gd steps\n",
      "insert gradient: -0.013136869248645208\n",
      "36-th iteration, new layer inserted. now 31 layers\n",
      "[5.43767914e+01 9.44540933e+01 5.16065846e+01 9.08898794e+01\n",
      " 5.34354353e+01 1.01767280e+02 9.82075427e+01 1.03957146e+02\n",
      " 9.73680618e+01 1.11387346e+02 9.56837930e+01 1.15276690e+02\n",
      " 9.76594047e+01 1.11526426e+02 3.38529705e+01 7.07618633e+00\n",
      " 6.28854061e+01 1.02067807e+02 1.04731941e+01 7.05058240e+01\n",
      " 4.56140467e+01 2.77102076e+01 4.68196204e+01 6.65402497e+01\n",
      " 7.60541172e+00 1.06730278e+02 1.49659341e+03 6.75424085e-01\n",
      " 1.86550323e+02 0.00000000e+00 2.66500461e+01]\n",
      "37-th iteration, loss: 0.09075548635248237, 36 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "37-th iteration, new layer inserted. now 33 layers\n",
      "[5.41585817e+01 9.42806157e+01 5.15318301e+01 9.09470965e+01\n",
      " 5.32843209e+01 1.01447747e+02 9.80082033e+01 1.03504053e+02\n",
      " 9.72601127e+01 1.12283360e+02 9.51901339e+01 1.17235729e+02\n",
      " 9.71797636e+01 1.14249358e+02 3.40531442e+01 6.39039916e+00\n",
      " 6.34829771e+01 1.03944721e+02 1.03126706e+01 7.04370163e+01\n",
      " 4.68880246e+01 2.70591129e+01 4.61140655e+01 6.95124650e+01\n",
      " 6.79792007e+00 1.07754792e+02 1.49071177e+03 6.79591636e-02\n",
      " 5.15501693e+01 0.00000000e+00 1.28875423e+02 3.00174925e+01\n",
      " 1.69407408e+01]\n",
      "38-th iteration, loss: 0.08881695206420094, 41 gd steps\n",
      "insert gradient: -0.0036857976359075196\n",
      "38-th iteration, new layer inserted. now 35 layers\n",
      "[5.36725963e+01 9.26125964e+01 0.00000000e+00 1.77635684e-14\n",
      " 5.03777760e+01 9.29650000e+01 5.51770875e+01 9.94295331e+01\n",
      " 9.84658422e+01 1.02775223e+02 9.69103931e+01 1.10492088e+02\n",
      " 9.44264859e+01 1.19261040e+02 9.60974467e+01 1.18406288e+02\n",
      " 3.81842626e+01 1.25107455e+00 6.45839322e+01 1.04978630e+02\n",
      " 1.13424238e+01 6.54044418e+01 4.81712604e+01 2.85650568e+01\n",
      " 4.24988615e+01 7.54750146e+01 4.85626925e+00 1.07073072e+02\n",
      " 1.47568347e+03 2.08760983e+01 3.71156532e+01 2.60843128e+01\n",
      " 1.31076164e+02 3.78020360e+01 1.28759152e+01]\n",
      "39-th iteration, loss: 0.087804015142622, 656 gd steps\n",
      "insert gradient: -0.002138616364604886\n",
      "39-th iteration, new layer inserted. now 33 layers\n",
      "[  53.06010624   93.49228044   51.62994079   90.60562628   52.66673259\n",
      "   98.53712407   97.76329578  101.58619386   96.22541734  111.56293404\n",
      "   93.02938477  120.96204111   94.47947783  120.93302927  102.67891702\n",
      "  106.87523816   10.94179689   62.29628715   50.5486759    25.40982222\n",
      "   42.09132437   79.28945839    3.08279955  108.88261711 1050.95032909\n",
      "    0.          420.38013163   31.5458409    30.57883303   36.4114476\n",
      "  127.59296435   41.25216803   12.01282739]\n",
      "40-th iteration, loss: 0.08738215839572382, 61 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 35 layers\n",
      "[ 53.39332766  93.33632512  51.87755738  90.13223078  51.68002592\n",
      "  97.47330214  98.02639365 101.0286231   96.07644251 111.42259949\n",
      "  92.66694158 121.60835295  93.35683    123.27878712 101.23491096\n",
      " 110.05874361  11.24427457  57.11084498  54.55575003  23.05773014\n",
      "  40.84522456  82.16463084   1.53749031 111.89873425 150.36002818\n",
      "   0.         902.16016906   3.63985873 413.46695625  40.25102119\n",
      "  25.08811907  45.61143448 124.86349777  44.35493459  11.50658219]\n",
      "41-th iteration, loss: 0.08584510705958033, 73 gd steps\n",
      "insert gradient: -0.008503741581254668\n",
      "41-th iteration, new layer inserted. now 37 layers\n",
      "[ 53.18030375  93.55835852  51.8033904   90.71812509  52.41096578\n",
      "  97.02225032  98.28347174 100.17319633  96.23810761 110.45037006\n",
      "  92.81112048 122.25160006  93.12236644 123.50760948 101.16355177\n",
      " 109.12759623  11.56988981  56.48472859  54.81594039  23.91812949\n",
      "  40.67955542  82.22846006   1.17646431 111.54549253 117.62334684\n",
      "   0.          23.52466937  12.74238456 906.01616949   5.06042625\n",
      " 412.92324532  41.66557934  24.52571621  47.84878717 125.46884537\n",
      "  45.80759353  11.68979054]\n",
      "42-th iteration, loss: 0.08365611721561106, 50 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 39 layers\n",
      "[5.31391754e+01 9.31868719e+01 5.16891591e+01 9.02201668e+01\n",
      " 5.18698271e+01 9.55001449e+01 9.82097895e+01 9.88496635e+01\n",
      " 9.55884696e+01 1.10404103e+02 9.20817590e+01 1.23330682e+02\n",
      " 9.16279585e+01 1.26827005e+02 1.00788205e+02 1.07730519e+02\n",
      " 1.21692752e+01 5.43115088e+01 5.38037738e+01 2.31526962e+01\n",
      " 4.25778266e+01 8.27888680e+01 2.78305040e-01 1.11431872e+02\n",
      " 1.11696095e+02 1.93971157e+01 1.84589140e+01 1.97437932e+01\n",
      " 1.51137884e+02 0.00000000e+00 7.55689422e+02 3.64588648e+00\n",
      " 4.07736661e+02 4.10819217e+01 2.39839966e+01 4.73427030e+01\n",
      " 1.23503542e+02 4.62939956e+01 1.11860648e+01]\n",
      "43-th iteration, loss: 0.0834541794221756, 14 gd steps\n",
      "insert gradient: -0.008727786999719331\n",
      "43-th iteration, new layer inserted. now 39 layers\n",
      "[5.25807180e+01 9.30322779e+01 5.13908619e+01 9.01883029e+01\n",
      " 5.16860018e+01 9.51779813e+01 9.83880215e+01 9.86853853e+01\n",
      " 9.55374097e+01 1.10404735e+02 9.19441404e+01 1.23549299e+02\n",
      " 9.13804263e+01 1.27275651e+02 1.00593028e+02 1.07497563e+02\n",
      " 1.22592289e+01 5.41045396e+01 5.39308444e+01 2.31589545e+01\n",
      " 4.27058241e+01 8.28280378e+01 4.28047910e-01 1.11428730e+02\n",
      " 1.11002380e+02 2.09223660e+01 1.82515249e+01 2.01200072e+01\n",
      " 1.49926261e+02 3.64061375e+00 7.54629960e+02 3.83336551e+00\n",
      " 4.06402047e+02 4.11129873e+01 2.42899874e+01 4.74014906e+01\n",
      " 1.23573213e+02 4.64331227e+01 1.15053343e+01]\n",
      "44-th iteration, loss: 0.08339045380135673, 9 gd steps\n",
      "insert gradient: -0.003881253691927986\n",
      "44-th iteration, new layer inserted. now 41 layers\n",
      "[5.27699455e+01 9.31689282e+01 5.16365417e+01 9.02971913e+01\n",
      " 5.17965838e+01 9.51852431e+01 9.83222493e+01 9.86596226e+01\n",
      " 9.54694385e+01 1.10398595e+02 9.19282228e+01 1.23577135e+02\n",
      " 9.13976678e+01 1.27315056e+02 1.00615854e+02 1.07492767e+02\n",
      " 1.22625848e+01 5.41009898e+01 5.39165773e+01 2.31383097e+01\n",
      " 4.26822813e+01 8.28146580e+01 3.95715391e-01 1.11414076e+02\n",
      " 1.10982756e+02 2.09402028e+01 1.82323644e+01 2.01172140e+01\n",
      " 1.19914610e+02 0.00000000e+00 2.99786525e+01 3.66212401e+00\n",
      " 7.54611966e+02 3.85011193e+00 4.06387804e+02 4.11235359e+01\n",
      " 2.43077974e+01 4.74075419e+01 1.23579128e+02 4.64369507e+01\n",
      " 1.15084171e+01]\n",
      "45-th iteration, loss: 0.08190190317292313, 46 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 43 layers\n",
      "[ 52.74510816  92.9983819   51.4881773   90.07656413  51.47954164\n",
      "  93.93989439  98.56407883  97.21597953  95.30953783 110.3079742\n",
      "  91.02536994 124.92094275  90.28608028  26.0270178    0.\n",
      " 104.10807122  99.81937366 104.52129292  13.09425302  51.81010925\n",
      "  54.63264541  23.91588365  40.30949289  81.08296042   1.81921595\n",
      " 108.81410573 109.56724146  27.57692837  18.43553687  22.99732428\n",
      " 117.92813841  16.95617855  31.12039632  11.19585756 745.81600935\n",
      "   3.28248693 398.42415556  40.06295268  24.22494065  46.69321859\n",
      " 122.82226712  45.02974846  11.07085714]\n",
      "46-th iteration, loss: 0.07895090245695681, 816 gd steps\n",
      "insert gradient: -0.006492209662005238\n",
      "46-th iteration, new layer inserted. now 43 layers\n",
      "[5.22868536e+01 9.10204193e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.98359905e+01 8.89675768e+01 5.02903500e+01 9.20435323e+01\n",
      " 9.65448553e+01 9.47356906e+01 9.25640786e+01 1.09840572e+02\n",
      " 8.69678648e+01 1.29457491e+02 8.43810357e+01 1.40842313e+02\n",
      " 9.90153795e+01 9.86023012e+01 1.44184558e+01 4.76038083e+01\n",
      " 5.76734973e+01 1.98140353e+01 4.10474454e+01 8.31245963e+01\n",
      " 5.58239750e+00 8.43887386e+01 9.00867837e+01 2.66872082e+01\n",
      " 1.80133530e+01 5.56308235e+01 1.14111857e+02 6.01674484e+01\n",
      " 1.83731424e+01 4.24313194e+01 7.34422615e+02 2.84750285e+00\n",
      " 3.91114614e+02 3.24507772e+01 2.82485091e+01 3.63186388e+01\n",
      " 1.22908402e+02 4.11441578e+01 1.10506012e+01]\n",
      "47-th iteration, loss: 0.07887907105229218, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 45 layers\n",
      "[5.18604801e+01 9.09591493e+01 1.58786067e-01 1.84968069e-02\n",
      " 5.00158953e+01 8.91415901e+01 5.05334832e+01 9.20308041e+01\n",
      " 9.62897701e+01 9.46604967e+01 9.24427901e+01 1.09851963e+02\n",
      " 8.69400792e+01 1.29517641e+02 8.43282533e+01 1.40867915e+02\n",
      " 9.90730711e+01 9.86268673e+01 1.44766320e+01 4.75863369e+01\n",
      " 5.76714281e+01 1.97870070e+01 4.10233979e+01 8.31449026e+01\n",
      " 5.55399650e+00 8.42650181e+01 8.98630011e+01 2.66586595e+01\n",
      " 1.79738659e+01 5.57766754e+01 1.14057023e+02 6.02679619e+01\n",
      " 1.84204937e+01 4.24669965e+01 1.46877449e+02 0.00000000e+00\n",
      " 5.87509796e+02 2.82316718e+00 3.91094639e+02 3.24649322e+01\n",
      " 2.83229883e+01 3.63376659e+01 1.22926489e+02 4.11601695e+01\n",
      " 1.10719113e+01]\n",
      "48-th iteration, loss: 0.07657842556958794, 62 gd steps\n",
      "insert gradient: -0.004166400576691418\n",
      "48-th iteration, new layer inserted. now 43 layers\n",
      "[ 50.62703331  89.52506459  48.88203181  88.182327    49.71033105\n",
      "  91.1852885   93.89408778  93.63009254  90.20900939 109.43283328\n",
      "  85.05576737 128.17846616  83.03305339 138.29246692  97.48097999\n",
      "  99.8970821   13.55206291  45.81058579  59.06431599  17.59037108\n",
      "  40.34006464  85.33081577   4.94835203  80.52393608  85.14360461\n",
      "  28.9059427   16.74898187  62.2104432  111.30218688  64.13121878\n",
      "  16.81669225  42.82097318 140.21605161   4.5174608  582.59739043\n",
      "   2.51350018 386.29039503  30.54849959  27.88996189  34.88848998\n",
      " 120.77531368  39.69624988  10.71990552]\n",
      "49-th iteration, loss: 0.07594776249829452, 14 gd steps\n",
      "insert gradient: -0.0022174017605516034\n",
      "49-th iteration, new layer inserted. now 45 layers\n",
      "[4.96236786e+01 8.90926564e+01 4.98585425e+01 8.79841173e+01\n",
      " 4.94733123e+01 9.04177174e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.31234357e+01 9.29492762e+01 9.00365546e+01 1.08603553e+02\n",
      " 8.47014648e+01 1.26106077e+02 8.29494469e+01 1.35311312e+02\n",
      " 9.66362013e+01 1.00609780e+02 1.29370686e+01 4.52109282e+01\n",
      " 6.02387375e+01 1.69571825e+01 4.01490865e+01 8.61867448e+01\n",
      " 4.65997794e+00 8.01346392e+01 8.52813982e+01 3.03610900e+01\n",
      " 1.57622505e+01 6.43209870e+01 1.10341003e+02 6.53440360e+01\n",
      " 1.63341710e+01 4.34655017e+01 1.38306518e+02 4.69430307e+00\n",
      " 5.80923237e+02 2.95606009e+00 3.84468266e+02 2.96955742e+01\n",
      " 2.73625662e+01 3.42298937e+01 1.19274045e+02 3.85878924e+01\n",
      " 1.00028011e+01]\n",
      "50-th iteration, loss: 0.07352667408691216, 59 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "50-th iteration, new layer inserted. now 45 layers\n",
      "[ 48.68182059  88.04265974  48.54671779  86.57523853  47.46590643\n",
      "  89.51070194  90.94069467  92.91154781  87.59500383 107.72220648\n",
      "  81.93906582 125.81299753  79.78127517  26.98447067   0.\n",
      " 107.93788268  94.99556144 103.64350049  12.7938526   39.89026288\n",
      "  63.18069961  15.70420638  37.81077667  89.21584669   5.60677093\n",
      "  71.4574217   79.36763996  33.85356284  12.48094498  77.66307046\n",
      " 105.71735876  74.12712219  13.44873868  46.12495392 132.22698905\n",
      "   6.85267877 575.80069304   2.84610226 378.37046728  28.26682291\n",
      "  27.95279954  32.35198768 118.73242055  37.94212135  10.72151313]\n",
      "51-th iteration, loss: 0.06840968207974521, 351 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "51-th iteration, new layer inserted. now 45 layers\n",
      "[ 46.9744499   84.79440589  45.11743426  83.40674513  46.43017874\n",
      "  86.36990904  86.78526217  87.23763015  83.08428729 104.17136899\n",
      "  78.24269655 121.55461627  77.93909347 127.83374216  86.74724003\n",
      " 108.32316459   8.50586857  37.33414144  67.29658288  11.27832273\n",
      "  32.87030751  93.02979343   2.55894267  66.43944379  83.58722605\n",
      "  39.1913041    4.84503405  89.73654723  95.81861934  83.25089427\n",
      "   7.56806534  50.14046822 125.71531317   9.26419369 226.74432192\n",
      "   0.         340.11648287   2.85783351 367.32956916  25.40097559\n",
      "  27.70044306  29.29592969 114.38582182  35.39194821  11.57607923]\n",
      "52-th iteration, loss: 0.06742178252765317, 181 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "52-th iteration, new layer inserted. now 45 layers\n",
      "[ 46.4084043   84.1511882   44.53203607  82.67183753  45.99400004\n",
      "  86.26544029  85.65242139  86.58138546  82.30706908 102.84390469\n",
      "  77.87019785 118.80405807  77.81712039 125.32593445  84.96735988\n",
      " 111.32214902   6.53032082  38.69565856  71.43534441   6.06917107\n",
      "  32.32931672 167.13350819  83.03042019  45.17617544   1.00496911\n",
      "  98.30716921  92.59060823  93.70404351   4.94191295  51.73113733\n",
      " 122.42334672   9.35468278  44.77759356   0.         179.11037425\n",
      "   2.19639288 340.58558381   2.82167758 365.31925871  21.54683638\n",
      "  29.65730176  25.98038004 114.34293422  33.77766734  12.30911765]\n",
      "53-th iteration, loss: 0.06497319051420972, 23 gd steps\n",
      "insert gradient: -0.003049585412294185\n",
      "53-th iteration, new layer inserted. now 43 layers\n",
      "[4.50228384e+01 8.28406824e+01 4.55870121e+01 8.18907638e+01\n",
      " 4.40109573e+01 8.38431924e+01 8.39936530e+01 8.60509789e+01\n",
      " 8.11263169e+01 1.02358894e+02 7.69057629e+01 1.17256008e+02\n",
      " 7.65692826e+01 1.25278229e+02 8.42627015e+01 1.12176088e+02\n",
      " 6.46140687e+00 3.86011140e+01 7.29232891e+01 4.19236109e+00\n",
      " 3.17563621e+01 1.63131413e+02 8.18217442e+01 4.68965109e+01\n",
      " 4.29453144e-01 1.01028305e+02 9.40523384e+01 1.02448901e+02\n",
      " 3.94024710e+00 5.34967664e+01 1.17579278e+02 2.59243873e+01\n",
      " 2.87172574e+01 2.24149446e+01 5.18650557e+02 2.57125368e+00\n",
      " 3.65706225e+02 1.65852597e+01 3.31510740e+01 2.03958085e+01\n",
      " 1.17240465e+02 3.13925345e+01 1.36428491e+01]\n",
      "54-th iteration, loss: 0.06329356579911641, 37 gd steps\n",
      "insert gradient: -0.0019899724812935692\n",
      "54-th iteration, new layer inserted. now 43 layers\n",
      "[ 44.74173408  81.8342406   43.05115008  80.74713406  45.02844592\n",
      "  84.38857637  82.68412489  84.0450163   79.5598163  100.44663953\n",
      "  75.57732218 116.52496883  75.50967252 124.32699479  83.80073308\n",
      " 112.20241242   6.58470523  38.68895843  72.36906336   2.67011319\n",
      "  31.29781725 159.96441913  79.98888367 151.16843693  76.48373387\n",
      "   0.          19.12093347 104.36659877   3.35197429  54.52055063\n",
      " 114.16386326  30.21753495  26.84107811  26.76008845 522.10397659\n",
      "   2.42250009 368.81413078  17.06710182  34.80028859  19.96536881\n",
      " 118.74798979  29.74510339  14.29671324]\n",
      "55-th iteration, loss: 0.0578620930105324, 62 gd steps\n",
      "insert gradient: -0.00337240053183386\n",
      "55-th iteration, new layer inserted. now 39 layers\n",
      "[4.39833261e+01 7.96028829e+01 4.13834028e+01 7.62529190e+01\n",
      " 4.26310607e+01 8.19735103e+01 7.78905455e+01 8.37639341e+01\n",
      " 7.63085996e+01 9.39571488e+01 7.26274799e+01 1.10681974e+02\n",
      " 7.23290886e+01 1.20651002e+02 8.14451052e+01 1.14008633e+02\n",
      " 5.34745902e+00 3.87469949e+01 1.01466645e+02 1.47924671e+02\n",
      " 7.69415618e+01 1.52154433e+02 7.72716206e+01 5.96843374e+00\n",
      " 1.78639535e+01 1.09728857e+02 3.82272018e-01 5.80548488e+01\n",
      " 9.94373143e+01 4.77591392e+01 1.67441888e+01 3.70675461e+01\n",
      " 9.16841813e+02 1.67003252e+01 3.15307343e+01 1.82841692e+01\n",
      " 1.16109927e+02 2.71334788e+01 1.31667010e+01]\n",
      "56-th iteration, loss: 0.05719101668143292, 15 gd steps\n",
      "insert gradient: -0.0024605409821023168\n",
      "56-th iteration, new layer inserted. now 37 layers\n",
      "[ 43.44949973  79.31012061  40.99407697  75.78415136  41.73755754\n",
      "  81.57638064  77.45325724  83.79113057  75.96589142  93.72581638\n",
      "  72.07379829 109.92667747  71.77785811 120.0248816   80.70655841\n",
      " 113.89892413   5.39838438  38.26461682 100.60599416 147.09293445\n",
      "  76.62832186 151.58603409  77.40683156   5.38860276  17.55225111\n",
      " 168.88678515  97.91060588  49.28707084  15.81168872  38.86459662\n",
      " 916.79337955  16.84887103  30.86575354  18.49941918 114.41382505\n",
      "  27.34519998  13.00737083]\n",
      "57-th iteration, loss: 0.04886248263393968, 46 gd steps\n",
      "insert gradient: -0.0068574243052640415\n",
      "57-th iteration, new layer inserted. now 39 layers\n",
      "[4.29283372e+01 7.84020486e+01 0.00000000e+00 7.10542736e-15\n",
      " 3.98122974e+01 7.22449684e+01 4.03366196e+01 7.93673896e+01\n",
      " 7.03314796e+01 9.48157177e+01 5.92769675e+01 1.12087901e+02\n",
      " 6.13879079e+01 1.14865094e+02 6.74632162e+01 1.20571837e+02\n",
      " 7.65460007e+01 1.15456433e+02 3.41094157e+00 4.00177078e+01\n",
      " 9.68662470e+01 1.44984838e+02 7.19612331e+01 1.48014536e+02\n",
      " 7.69738697e+01 1.97291793e+00 1.66331570e+01 1.66187099e+02\n",
      " 9.24853916e+01 5.93524741e+01 1.04366084e+01 4.49875429e+01\n",
      " 9.08838343e+02 1.63469882e+01 2.80887720e+01 1.96725283e+01\n",
      " 1.07847032e+02 2.86218483e+01 1.18782142e+01]\n",
      "58-th iteration, loss: 0.04300299510508712, 24 gd steps\n",
      "insert gradient: -0.00913361181392145\n",
      "58-th iteration, new layer inserted. now 39 layers\n",
      "[4.61037347e+01 7.86785529e+01 4.49827357e+01 6.96751466e+01\n",
      " 0.00000000e+00 1.06581410e-14 3.71869587e+01 7.94694533e+01\n",
      " 5.36541075e+01 1.23941435e+02 5.42750392e+01 1.01603305e+02\n",
      " 7.21009858e+01 9.86629588e+01 7.25177800e+01 1.12115845e+02\n",
      " 7.76716399e+01 1.13039010e+02 5.81431734e+00 3.29469070e+01\n",
      " 9.51497496e+01 1.46054478e+02 7.50087045e+01 1.40507418e+02\n",
      " 7.56990459e+01 1.64381972e-02 1.41096398e+01 1.66278644e+02\n",
      " 9.44905144e+01 7.27593013e+01 6.10165850e+00 5.04753777e+01\n",
      " 8.95432524e+02 1.70472166e+01 2.71451132e+01 2.09611932e+01\n",
      " 1.07490062e+02 2.91629132e+01 1.09178745e+01]\n",
      "59-th iteration, loss: 0.04018626766184073, 33 gd steps\n",
      "insert gradient: -0.004495313436540856\n",
      "59-th iteration, new layer inserted. now 35 layers\n",
      "[4.75140453e+01 8.18767322e+01 4.06888596e+01 7.10331624e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.89418493e+01 8.17316085e+01\n",
      " 4.90318248e+01 1.23701370e+02 5.86271659e+01 9.59916177e+01\n",
      " 7.23675006e+01 9.95545608e+01 7.00854487e+01 1.14114164e+02\n",
      " 7.18545510e+01 1.41054411e+02 1.00790186e+02 1.53229409e+02\n",
      " 7.67320427e+01 1.37691689e+02 8.99403764e+01 1.68485956e+02\n",
      " 9.29402339e+01 7.74347177e+01 3.78067462e+00 5.33009969e+01\n",
      " 8.87624651e+02 1.86166503e+01 2.59822690e+01 2.06095197e+01\n",
      " 1.04742243e+02 2.89006908e+01 1.02288433e+01]\n",
      "60-th iteration, loss: 0.03952888983514859, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "60-th iteration, new layer inserted. now 35 layers\n",
      "[ 45.31329304  81.280772    40.79545702  71.43397262  40.45133198\n",
      "  81.73872239  48.64597311 123.10733341  59.37303153  95.05263161\n",
      "  72.21816462  99.82861365  69.18608874 115.0202329   70.48365567\n",
      " 140.63515239 100.69642594 154.60178142  76.42198434 137.16373545\n",
      "  89.41250553 168.46967129  92.32842669  78.45003485   2.92459519\n",
      "  54.01221593 126.5686384    0.         759.41183042  18.68124689\n",
      "  25.59912026  20.56441568 104.02654764  28.94203241  10.41531448]\n",
      "61-th iteration, loss: 0.038391314448819186, 188 gd steps\n",
      "insert gradient: -0.0010020950659031793\n",
      "61-th iteration, new layer inserted. now 35 layers\n",
      "[ 45.0283417   81.19735715  40.23162066  72.993615    40.09725438\n",
      "  80.8537013   48.07923553 118.8424021   63.24902642  88.51549733\n",
      "  73.15005636 101.38552535  66.92968369 119.5109909   67.066802\n",
      " 137.88794323  98.74368182 160.32394578  76.62346258 133.06084388\n",
      "  86.71718948 167.51617712  90.54689256 152.28328148 100.17608093\n",
      "   0.          16.69601349   7.24443383 752.18192933  19.04695309\n",
      "  24.69812582  22.39186655 103.94767337  28.41181399  10.88846656]\n",
      "62-th iteration, loss: 0.03647412928481179, 45 gd steps\n",
      "insert gradient: -0.0008185819245435292\n",
      "62-th iteration, new layer inserted. now 37 layers\n",
      "[4.71616511e+01 8.14022483e+01 3.81480416e+01 7.39880400e+01\n",
      " 4.10682698e+01 8.15348583e+01 4.67769068e+01 1.16891770e+02\n",
      " 6.46127333e+01 8.69166271e+01 0.00000000e+00 3.55271368e-15\n",
      " 7.31337633e+01 1.00301478e+02 6.40844981e+01 1.23219934e+02\n",
      " 6.34480049e+01 1.39398378e+02 9.85360256e+01 1.58547811e+02\n",
      " 7.60770874e+01 1.34202400e+02 8.04740914e+01 1.57854214e+02\n",
      " 9.24826802e+01 1.55379933e+02 9.02750623e+01 2.56734620e+01\n",
      " 2.10556374e+01 2.43029982e+01 7.21921062e+02 2.52060794e+01\n",
      " 2.67577033e+01 2.68735046e+01 1.10256442e+02 3.40781167e+01\n",
      " 1.00569766e+01]\n",
      "63-th iteration, loss: 0.03590736794364747, 21 gd steps\n",
      "insert gradient: -0.000789980408047791\n",
      "63-th iteration, new layer inserted. now 37 layers\n",
      "[ 45.77030666  81.25384129  39.98034715  73.09874743  39.95802571\n",
      "  81.3225346   47.12220685 115.7950816   64.56276218  88.38324929\n",
      "  72.74105163  99.90922185  63.76520238 123.48731365  63.68692971\n",
      " 140.5192385   81.77079611   0.          16.35415922 157.69207401\n",
      "  75.89340809 133.82681984  78.88180092 157.21926991  94.29797364\n",
      " 155.57904427  91.07361386  27.57968622  18.67984115  25.56564783\n",
      " 720.14489588  25.762452    26.66596235  26.75178532 110.21394181\n",
      "  33.9414928    9.83109402]\n",
      "64-th iteration, loss: 0.0347830519855923, 37 gd steps\n",
      "insert gradient: -0.0010802223458543376\n",
      "64-th iteration, new layer inserted. now 37 layers\n",
      "[ 45.48671531  81.28832568  39.91753355  74.15893039  40.26484508\n",
      "  80.80990911  45.99236202 112.11731612  67.55665612  87.09425685\n",
      "  73.56693651 100.11540121  62.23868142 126.27828454  61.29318429\n",
      " 141.70066948  78.96470508   6.27227432  16.56534145 154.99091839\n",
      "  76.01574369 134.6358423   75.11663836 153.3737265   96.34356062\n",
      " 156.35787321  91.04173956  43.26203542  13.70543963  35.51476968\n",
      " 708.17120018  28.47919883  25.62096012  26.39277083 109.80668482\n",
      "  31.67761563   9.98100891]\n",
      "65-th iteration, loss: 0.03400514675052154, 47 gd steps\n",
      "insert gradient: -0.0005986737971167894\n",
      "65-th iteration, new layer inserted. now 39 layers\n",
      "[ 45.26189875  81.92217869  40.45636136  74.33851502  40.02283547\n",
      "  80.73189754  45.51445457 106.61259854  70.1243064   87.54752888\n",
      "  73.76270375 100.09056361  60.92057288 126.57593124  60.35050674\n",
      " 141.3839884   73.39232538   9.21067794  18.20887225 156.51074105\n",
      "  76.83254802 136.07742943  72.99297096 150.56967435  96.11654737\n",
      " 158.60648872  88.88316437  56.7707442    8.88931357  43.49112562\n",
      " 582.19147012   0.         116.43829402  29.44628488  24.52172535\n",
      "  26.11021899 109.75450662  29.47646667  11.22513962]\n",
      "66-th iteration, loss: 0.03256785954149559, 52 gd steps\n",
      "insert gradient: -0.0008173684935478072\n",
      "66-th iteration, new layer inserted. now 41 layers\n",
      "[ 46.5385034   83.58467401  40.98014516  75.03939281  39.54145232\n",
      "  80.55993951  44.93485653  98.10342438  74.69828107  86.07775041\n",
      "  74.00029165 101.94043052  57.56259937 129.60650645  59.89512037\n",
      " 141.51544931  64.98847159  14.0839465   17.31085407 126.86362747\n",
      "   0.          31.71590687  79.73315317 138.43612459  70.36463131\n",
      " 149.86866103  95.81533812 158.33279316  82.79177532  74.06983258\n",
      "   2.70489782  55.67530427 567.31748007   8.07774182 111.15509476\n",
      "  29.46867283  23.08997546  28.55632826 110.40391928  27.08723885\n",
      "  14.54766158]\n",
      "67-th iteration, loss: 0.030749672479452057, 38 gd steps\n",
      "insert gradient: -0.0016116788886134565\n",
      "67-th iteration, new layer inserted. now 41 layers\n",
      "[4.54571767e+01 8.29023211e+01 4.09448109e+01 7.54648560e+01\n",
      " 4.07125718e+01 8.07097389e+01 4.54669916e+01 9.48427283e+01\n",
      " 7.66783685e+01 8.72658388e+01 7.24542644e+01 1.04878973e+02\n",
      " 5.57528440e+01 1.31058743e+02 6.03370941e+01 1.35393210e+02\n",
      " 6.57322419e+01 1.63814301e+01 5.18845743e+00 1.24602378e+02\n",
      " 2.61242755e+00 2.66948268e+01 8.99104684e+01 1.41641219e+02\n",
      " 7.09162716e+01 1.51338656e+02 9.37884439e+01 1.56840624e+02\n",
      " 8.28555430e+01 8.21629066e+01 5.19792710e-01 6.27233392e+01\n",
      " 5.58181840e+02 6.20061905e+00 1.08781621e+02 3.00145688e+01\n",
      " 2.38692972e+01 3.26716558e+01 1.14505837e+02 2.85499581e+01\n",
      " 1.46781220e+01]\n",
      "68-th iteration, loss: 0.029761324231474002, 27 gd steps\n",
      "insert gradient: -0.0008298042964231783\n",
      "68-th iteration, new layer inserted. now 37 layers\n",
      "[4.54029469e+01 8.31193717e+01 4.13666214e+01 7.55978112e+01\n",
      " 4.07614494e+01 8.08452585e+01 4.50356361e+01 9.33293542e+01\n",
      " 7.65928301e+01 8.67191079e+01 7.14112888e+01 1.07580364e+02\n",
      " 5.51012634e+01 1.31020053e+02 6.13164642e+01 1.31392434e+02\n",
      " 7.00117735e+01 1.64983529e+02 9.56062538e+01 1.43152636e+02\n",
      " 7.05602204e+01 1.50308691e+02 9.14957241e+01 1.56893964e+02\n",
      " 8.53745310e+01 8.46679874e+01 2.63267122e-01 6.49811925e+01\n",
      " 5.55358229e+02 5.69870911e+00 1.09371267e+02 3.22115834e+01\n",
      " 2.33972572e+01 3.34197426e+01 1.14324760e+02 2.90754863e+01\n",
      " 1.45222172e+01]\n",
      "69-th iteration, loss: 0.029075107071635, 28 gd steps\n",
      "insert gradient: -0.0005570304309785551\n",
      "69-th iteration, new layer inserted. now 35 layers\n",
      "[ 45.1213859   83.16605019  41.00272501  76.84874061  41.23278024\n",
      "  81.38926901  44.47309921  91.25849903  77.686012    87.09864684\n",
      "  69.00588584 112.6342528   53.40408185 132.07973162  61.71363982\n",
      " 127.58266161  71.95016736 163.7389489   96.98467309 141.37685183\n",
      "  70.27936559 146.61299554  85.56596189 159.50520455  89.79846901\n",
      " 158.29200758 548.78160945   6.50688702 111.09052922  37.71702897\n",
      "  21.05595029  37.95319014 113.38510943  30.02304409  14.13438258]\n",
      "70-th iteration, loss: 0.027563831414435937, 64 gd steps\n",
      "insert gradient: -0.0018938628821874518\n",
      "70-th iteration, new layer inserted. now 37 layers\n",
      "[4.75139633e+01 8.48341097e+01 4.23933651e+01 7.58027843e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.99467674e+01 8.05930808e+01\n",
      " 4.33123058e+01 8.86428560e+01 7.82094918e+01 9.04989396e+01\n",
      " 5.96970448e+01 1.27331196e+02 5.28383041e+01 1.27393218e+02\n",
      " 6.50044360e+01 1.19832359e+02 7.90331992e+01 1.68331164e+02\n",
      " 9.29462528e+01 1.31356323e+02 7.56581393e+01 1.36101677e+02\n",
      " 8.26231467e+01 1.68747998e+02 9.39119988e+01 1.56398497e+02\n",
      " 5.38104757e+02 1.42657993e+01 1.14270478e+02 4.98460822e+01\n",
      " 1.53317611e+01 4.78853054e+01 1.10835658e+02 3.28712234e+01\n",
      " 1.21289698e+01]\n",
      "71-th iteration, loss: 0.027022144271148133, 27 gd steps\n",
      "insert gradient: -0.0014014583439310613\n",
      "71-th iteration, new layer inserted. now 35 layers\n",
      "[ 49.89805656  86.52936191  41.6313342   74.09039043  39.45705013\n",
      "  80.85609306  44.84113141  88.47002778  76.60175231  96.36435089\n",
      "  52.82733498 132.67056423  54.8989268  118.26066723  70.63107095\n",
      " 112.82779021  82.73270569 171.52681433  90.45832045 127.06555425\n",
      "  77.67492168 131.53812543  85.59473789 169.81999834  92.97455932\n",
      " 156.32509956 537.42872464  15.45454381 113.76320512  55.3498551\n",
      "  13.34328033  51.59063876 109.31196148  32.19369081  11.56423864]\n",
      "72-th iteration, loss: 0.02665605721868661, 29 gd steps\n",
      "insert gradient: -0.0008672739732369737\n",
      "72-th iteration, new layer inserted. now 37 layers\n",
      "[ 48.93788335  85.64678149  41.9267378   73.96941573  40.20601753\n",
      "  80.86912534  45.13132116  88.57827504  76.69462536  96.31180123\n",
      "  53.05455143 131.76386107  56.72974778 112.16542151  73.2198596\n",
      " 110.21807829  84.48873568 172.67411702  89.6353699  125.38568669\n",
      "  78.47235081 128.8293058   87.35030589 170.77839708  92.24352982\n",
      " 103.2346744    0.          51.6173372  537.24382667  15.33736415\n",
      " 114.06002815  59.50120729  12.32540589  53.74378758 108.94957888\n",
      "  31.5425239   10.84952832]\n",
      "73-th iteration, loss: 0.02647645616102371, 26 gd steps\n",
      "insert gradient: -0.0005359132863173372\n",
      "73-th iteration, new layer inserted. now 39 layers\n",
      "[ 48.68673332  86.35742602  42.40231837  73.61218314  40.01352795\n",
      "  80.92270352  45.16890192  88.19716742  76.73454181  97.31499507\n",
      "  52.80077635 131.26263771  57.63905778 110.01106493  74.06307461\n",
      " 110.05743747  85.04418005 115.55290132   0.          57.77645066\n",
      "  89.24095523 125.19927163  78.51551817 128.34252373  88.70855295\n",
      " 171.77140778  91.76564359  99.4288679    2.91687495  47.10412497\n",
      " 536.42585934  15.49617709 113.99073312  62.67468345  11.07219971\n",
      "  55.30419763 108.83657233  31.57786833  10.84103943]\n",
      "74-th iteration, loss: 0.026114643481573774, 204 gd steps\n",
      "insert gradient: -0.0006691588819350723\n",
      "74-th iteration, new layer inserted. now 41 layers\n",
      "[ 49.06688699  86.62246361  42.88565177  72.84195117  39.96910586\n",
      "  80.9892876   45.41911709  87.36923287  76.5321331   98.40418384\n",
      "  52.38277998 130.45277888  59.23670901 108.74482907  75.34321595\n",
      " 108.46566714  86.2836973  101.48888185   3.8952694   59.87346135\n",
      "  90.56915122 124.51808238  78.63173634 127.23755452  88.61627384\n",
      " 173.63851687  92.11642898 107.45941795   5.91906885  33.31523728\n",
      " 319.60323943   0.         213.06882629  17.24456269 110.07896117\n",
      "  97.53497447   0.48816684  69.59271572 103.37433604  34.18104941\n",
      "   8.78651839]\n",
      "75-th iteration, loss: 0.025667013678459295, 36 gd steps\n",
      "insert gradient: -0.0005651185640955231\n",
      "75-th iteration, new layer inserted. now 43 layers\n",
      "[4.85464151e+01 8.63746741e+01 4.27888201e+01 7.30024459e+01\n",
      " 4.01524853e+01 8.08080322e+01 4.51360934e+01 8.71558636e+01\n",
      " 7.59352234e+01 9.89430157e+01 5.30232348e+01 1.29319898e+02\n",
      " 5.94791440e+01 1.08713030e+02 7.47147233e+01 1.08308319e+02\n",
      " 8.58503632e+01 9.96001523e+01 4.88547046e+00 5.88886774e+01\n",
      " 9.04781770e+01 1.25582977e+02 7.79733123e+01 1.28894297e+02\n",
      " 8.73725119e+01 1.73496009e+02 9.30262679e+01 1.07937719e+02\n",
      " 5.78257690e+00 3.45140263e+01 1.93209539e+02 0.00000000e+00\n",
      " 1.28806360e+02 1.25833062e+01 2.01649430e+02 1.63403288e+01\n",
      " 1.08692398e+02 9.78483074e+01 3.98560179e-02 6.88027385e+01\n",
      " 1.04389490e+02 3.38233191e+01 1.00140636e+01]\n",
      "76-th iteration, loss: 0.025325168004513742, 999 gd steps\n",
      "insert gradient: -0.0005658261449745754\n",
      "76-th iteration, new layer inserted. now 43 layers\n",
      "[ 48.72695105  85.94549777  42.73214812  73.87678762  40.53110071\n",
      "  80.46532859  45.22279392  86.76804075  75.50448073 100.14750997\n",
      "  53.34507726 127.29489219  61.00764625 108.05009676  75.07151758\n",
      " 108.21374873  85.9754984  100.15632214   5.42375498  55.89454068\n",
      "  90.56368199 127.94116265  77.83775112 128.67183233  86.8567845\n",
      " 173.99559451  94.03606383 108.57043192   5.7251308   35.47709745\n",
      " 194.54618978  10.05967457 123.42381415  16.33268257 154.48860203\n",
      "   0.          38.62215051  16.83921036 108.56762468 166.60078226\n",
      " 105.44792889  33.80044129  10.7088559 ]\n",
      "77-th iteration, loss: 0.02465613633960177, 65 gd steps\n",
      "insert gradient: -0.00036239721718586416\n",
      "77-th iteration, new layer inserted. now 45 layers\n",
      "[ 48.39671007  85.3270621   42.40575694  73.60874722  40.90042712\n",
      "  80.68231582  45.30463384  86.48399634  75.03801921 100.65422851\n",
      "  53.75853061 125.64310717  62.2233952  106.82133338  75.682866\n",
      " 107.09262363  85.78646653  99.74381327   6.50509466  52.56346747\n",
      "  91.18210295 128.96877342  77.381198   127.75071356  86.15876197\n",
      " 173.9705307   94.26542867 107.66961016   6.25603582  34.83724524\n",
      " 196.99417755   9.50630262  98.44147954   0.          24.61036989\n",
      "  14.09147735 143.12689644  20.29433373  30.84105736  30.86596372\n",
      " 107.64217708 166.07634197 105.63753305  35.1504588   11.03250116]\n",
      "78-th iteration, loss: 0.023844278362968957, 28 gd steps\n",
      "insert gradient: -0.0009007412857659779\n",
      "78-th iteration, new layer inserted. now 45 layers\n",
      "[ 48.32136861  84.88994006  41.98227879  72.52004695  40.86236534\n",
      "  80.31564137  45.90865288  85.21516413  74.09017051 101.56075081\n",
      "  53.50524304 123.56393713  63.30446948 104.92811452  76.08035165\n",
      " 105.70669925  85.51368653  98.83000441   7.01677597  51.07308732\n",
      "  92.02625298 126.59143912  76.8790739  126.66226389  85.51131038\n",
      " 174.34319053  92.5758384  106.31821303   5.39305839  33.51661342\n",
      " 195.57275404   2.93091136  98.4869038   18.25289691  30.00763018\n",
      "  23.6773065  129.10467879  27.96697483  26.10807438  39.4028317\n",
      " 104.96251457 168.21881616 107.07215749  34.74295068  11.12471422]\n",
      "79-th iteration, loss: 0.023579575083966697, 18 gd steps\n",
      "insert gradient: -0.001385025979031519\n",
      "79-th iteration, new layer inserted. now 43 layers\n",
      "[ 47.42853517  84.34480956  41.30148667  73.13650132  40.40150723\n",
      "  79.94478822  45.83628194  86.64079341  73.11890118 101.68471981\n",
      "  53.92783778 122.81604189  62.60251185 104.33465491  75.48847062\n",
      " 105.79526701  84.9110719   99.1767203    6.6612737   50.1867817\n",
      "  91.7443578  128.06435401  76.05195682 127.73164314  86.28201915\n",
      " 173.44276015  91.2065573  106.33972537   5.10080534  32.26958351\n",
      " 290.65568969  24.93054964  28.81996438  27.73370185 128.5830209\n",
      "  30.64628814  24.3010774   41.97841243 104.09165111 168.43498707\n",
      " 105.21930528  34.22816432  10.28304649]\n",
      "80-th iteration, loss: 0.023492016286902485, 17 gd steps\n",
      "insert gradient: -0.00030549033060283455\n",
      "80-th iteration, new layer inserted. now 45 layers\n",
      "[ 47.77000544  84.62027143  41.93262485  73.24532488  40.42112534\n",
      "  79.82281923  45.15475346  86.4387985   73.11100774 101.69309134\n",
      "  53.98361602 122.82898174  62.64812959 104.33277765  75.42084763\n",
      " 105.7997262   84.81517275  99.14784884   6.42162557  50.07317687\n",
      "  91.54071303 128.14203787  76.04726043 127.7690145   86.27998674\n",
      " 173.32573046  91.11204692 106.35648282   5.11557756  32.18645142\n",
      " 290.57058472  25.22143861  28.44006799  27.82930534 128.4075045\n",
      "  30.72641781  23.96986574  42.09415073 103.88896643 134.72240386\n",
      "   0.          33.68060097 105.15813698  34.2505188   10.25715552]\n",
      "81-th iteration, loss: 0.023275083359325724, 18 gd steps\n",
      "insert gradient: -0.00020058515951223285\n",
      "81-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.61736839  84.07706618  41.29961103  72.77893021  40.52143159\n",
      "  79.96055413  46.25137542  85.55583059  72.78377318 101.63030632\n",
      "  53.93724012 121.77657037  63.00554751 103.48835926  75.30211406\n",
      " 106.07147635  84.3880006  100.22236095   6.47916413  49.27183078\n",
      "  90.73684627 129.14184504  75.49813818 128.14225922  86.72690687\n",
      " 172.15846341  90.52735898 107.81279905   5.03865224  31.00855294\n",
      " 174.65032122   0.         116.43354748  31.90753228  24.29746968\n",
      "  32.56174867 127.09539047  33.7905216   21.31975714  46.77369702\n",
      " 100.55004339 133.21442826   3.26189793  32.01786426 104.61664271\n",
      "  33.92490798   9.53174485]\n",
      "82-th iteration, loss: 0.023106442783792397, 27 gd steps\n",
      "insert gradient: -0.00041854493637430136\n",
      "82-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.69854687  84.49665971  42.16077125  72.80080911  40.19560212\n",
      "  79.49109235  44.69092426  85.49332717  72.64241944 101.80011155\n",
      "  54.2229382  121.32581499  63.17179372 103.32289942  75.03745275\n",
      " 105.94425421  84.21764822 100.34521238   6.36762588  48.92892468\n",
      "  90.30290134 129.35700993  75.35603094 128.16864659  86.68001706\n",
      " 171.74011812  89.90628912 107.93839959   4.52665337  30.44007598\n",
      " 173.94367296   3.35853875 115.95092644  33.56036705  23.66865698\n",
      "  33.46560111 126.11988432  34.87207097  20.40028962  48.00816104\n",
      "  99.69437542 133.05020988   3.69861134  31.69733462 104.52368183\n",
      "  33.84876045   9.47715262]\n",
      "83-th iteration, loss: 0.022767323707776996, 47 gd steps\n",
      "insert gradient: -0.00048161684666097487\n",
      "83-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.39636902  83.77763695  41.66277139  72.31434395  40.01615213\n",
      "  78.8584699   44.65404371  84.96006272  71.66352835 102.16283907\n",
      "  54.67340503 117.77406244  65.09916009 100.40738105  75.39909449\n",
      " 105.47216354  83.82490299 102.15769921   6.4688519   47.67546517\n",
      "  89.40336969 128.18178404  74.29739033 129.0128233   87.34870725\n",
      " 170.07354861  87.52690907 111.41980966   2.88405028  29.62618279\n",
      " 169.48694779   9.23671133 116.57005221  42.48199139  19.96563377\n",
      "  39.72653785 120.34857453  43.92246325  14.61025806  59.86827466\n",
      "  96.48869394 133.89824683   4.6794897   31.01674184 103.41701959\n",
      "  33.44667477   9.04621619]\n",
      "84-th iteration, loss: 0.02275630143684677, 17 gd steps\n",
      "insert gradient: -0.00028922916930121845\n",
      "84-th iteration, new layer inserted. now 49 layers\n",
      "[ 47.35548175  83.87274084  41.84656008  72.32105833  39.98877212\n",
      "  78.88047082  44.62181902  84.96379594  71.65074118 102.22150688\n",
      "  54.62519179 117.74696253  65.07184213 100.42307228  75.34935925\n",
      " 105.44780244  83.82602161 102.13317218   6.40791916  47.61888819\n",
      "  89.3278688  128.25923888  74.27946157 129.03391544  87.29962463\n",
      " 170.02794423  87.5057842  111.37408224   2.81149325  29.51930025\n",
      " 127.0812992    0.          42.36043307   9.26624413 116.51031129\n",
      "  42.56202225  19.95544363  39.80260918 120.27639237  44.04357271\n",
      "  14.54207828  59.98494454  96.35868208 133.89945279   4.64735353\n",
      "  30.98158555 103.38777796  33.46425456   9.03407622]\n",
      "85-th iteration, loss: 0.022015186831267852, 41 gd steps\n",
      "insert gradient: -0.0005388326431561119\n",
      "85-th iteration, new layer inserted. now 47 layers\n",
      "[ 45.95594903  83.95899868  42.46826781  71.1867163   39.52019688\n",
      "  79.16454999  45.83710865  85.78196516  70.29667714 102.72306123\n",
      "  54.90345523 115.21969917  66.18553965  97.74899896  75.87759101\n",
      " 103.15796767  83.78761902 101.78862083   7.68362639  43.572377\n",
      "  88.46192099 131.4214709   72.42037613 131.30863612  87.60724327\n",
      " 165.89022245  82.78297733 147.36320408 108.0405843   19.86134204\n",
      "  39.22869115  21.6308574  119.24475554  49.43644972  17.72504707\n",
      "  44.55502612 117.03789516  51.93893105  10.47280921  70.85521647\n",
      "  95.39164858 135.20430231   4.51875132  29.45632351 100.12782944\n",
      "  33.68113704   8.82114001]\n",
      "86-th iteration, loss: 0.021524649542858237, 76 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "86-th iteration, new layer inserted. now 49 layers\n",
      "[ 45.89597372  83.29971794  42.18963288  71.50830169  38.59096742\n",
      "  78.52948228  44.92637832  85.45254231  69.15581541 103.55490825\n",
      "  54.41114711 114.62086085  65.78086872  96.65717537  75.44230485\n",
      " 100.14128884  83.07996092 101.09267324   8.56101181  40.21504386\n",
      "  87.86690031  33.15442309   0.          99.46326926  72.81744015\n",
      " 130.53907444  87.14523592 165.06686113  82.56462787 143.32082443\n",
      " 101.0484133   35.20793014  28.18906698  29.74978064 116.71887809\n",
      "  56.73289507  13.15070945  50.96248876 115.10896224  53.1129271\n",
      "   7.65188091  81.62138561  93.88857987 136.74744405   4.20940401\n",
      "  28.91937869  98.48354955  31.84530785   8.02721429]\n",
      "87-th iteration, loss: 0.021142187426852522, 47 gd steps\n",
      "insert gradient: -0.00030507480766730886\n",
      "87-th iteration, new layer inserted. now 49 layers\n",
      "[ 46.32692095  82.4924095   41.4805971   71.51177575  38.66016362\n",
      "  78.18800574  44.34727041  85.00770837  68.60302388 102.08198664\n",
      "  54.12387867 114.06570192  65.27361591  96.2050983   74.98183573\n",
      "  97.68637792  81.38744988  99.90451221   9.80261318  32.2830737\n",
      "  85.53938592  30.35756015   3.87783743  99.24355111  75.48825583\n",
      " 127.28895733  85.40196196 164.55483357  82.90663822 142.88093295\n",
      "  98.16805248  44.47828713  22.42941635  36.12894913 113.96687801\n",
      "  63.77664158   9.56469121  57.52391254 111.52095392  53.76473241\n",
      "   5.89018504  88.18732248  93.24526384 137.26540008   4.11226815\n",
      "  28.1312723   97.54287453  31.13033514   7.91781395]\n",
      "88-th iteration, loss: 0.020428126535967185, 25 gd steps\n",
      "insert gradient: -0.0013411457878919463\n",
      "88-th iteration, new layer inserted. now 49 layers\n",
      "[ 42.33899051  80.32194062  41.19128237  72.19363812  38.25998216\n",
      "  77.74170239  45.91888323  84.7745898   66.69063662 100.4792366\n",
      "  53.56310483 113.10827756  64.78695914  96.1100193   73.66171264\n",
      "  95.02432916  78.64476592  97.56923123   9.81670339  24.81414182\n",
      "  82.042902    30.66268934   6.38144696 100.29187968  76.39731251\n",
      " 124.12612076  82.30125446 163.44833117  82.92585473 142.13794792\n",
      "  96.41739044  47.87302346  20.04578429  37.59544047 112.57731447\n",
      "  65.49943767   7.87114023  60.16128448 108.49490513  53.66174694\n",
      "   4.97571261  89.46808213  91.99589091 136.51563691   4.14469774\n",
      "  26.97428293  96.76279688  30.71159608   8.12620636]\n",
      "89-th iteration, loss: 0.01688380760535045, 46 gd steps\n",
      "insert gradient: -0.00044075639529310835\n",
      "89-th iteration, new layer inserted. now 47 layers\n",
      "[ 42.7404701   78.10659774  40.17447929  70.55824158  38.35623187\n",
      "  74.64476529  42.7832751   81.44556529  61.99830478 102.12389256\n",
      "  52.0294252  101.71788085  66.52634697  95.11332071  70.5517921\n",
      "  95.28977727  67.06693133 116.18129309  86.19507718  39.3851215\n",
      "   1.42327363 108.87712988  76.04023081 124.69301389  71.18264857\n",
      " 156.69106265  85.5506702  140.40698821  85.36532876  55.87394317\n",
      "  12.79668974  43.13245736 111.12763108  67.35215645   4.2311733\n",
      "  68.86609217 101.31784213  56.48238125   3.13710542  92.04003457\n",
      "  88.87856797 132.78922514   4.54438362  22.55979161  94.79580312\n",
      "  28.17997469   9.94810937]\n",
      "90-th iteration, loss: 0.01429001606451199, 43 gd steps\n",
      "insert gradient: -0.0004083973369235619\n",
      "90-th iteration, new layer inserted. now 45 layers\n",
      "[4.43412581e+01 7.98728127e+01 4.19569338e+01 7.19513857e+01\n",
      " 0.00000000e+00 3.55271368e-15 3.72156201e+01 7.52766734e+01\n",
      " 4.27117499e+01 8.04010730e+01 4.77144240e+01 1.21750838e+02\n",
      " 5.61090710e+01 8.53927726e+01 7.16005371e+01 9.81204954e+01\n",
      " 6.32989954e+01 1.13475584e+02 5.66651089e+01 1.18292589e+02\n",
      " 7.17313413e+01 1.63060980e+02 8.48486268e+01 1.25718279e+02\n",
      " 6.44722196e+01 1.36474018e+02 8.73922767e+01 1.55776000e+02\n",
      " 7.97958037e+01 8.60965899e+01 3.63152334e-01 5.23624000e+01\n",
      " 1.06802077e+02 6.76943238e+01 7.89355205e-01 8.71528339e+01\n",
      " 9.33295164e+01 1.67874344e+02 8.59361478e+01 1.25388965e+02\n",
      " 5.38473345e+00 1.57666392e+01 8.88117369e+01 2.29256999e+01\n",
      " 1.47462059e+01]\n",
      "91-th iteration, loss: 0.013686129952786832, 39 gd steps\n",
      "insert gradient: -0.0002049427419822602\n",
      "91-th iteration, new layer inserted. now 43 layers\n",
      "[ 43.96829302  79.60769477  40.35600705  73.04990419  38.13425795\n",
      "  73.39601894  42.04139899  82.96589807  49.3893708  117.27793885\n",
      "  55.66261509  91.32722102  67.23785898 103.85057519  57.72358766\n",
      " 117.33867717  58.23425591 114.44011296  75.82836567 162.29991866\n",
      "  79.2737606  126.93109695  68.82180652 133.35535067  88.47215159\n",
      " 153.61365639  76.32424154 146.08601386 100.96340617  39.30602919\n",
      "   0.          26.20401946   2.35019897  88.10228165  91.42211781\n",
      " 166.27401782  88.25010226 128.95244998   1.50063375  18.46797893\n",
      "  89.6135584   28.8952089   11.11539697]\n",
      "92-th iteration, loss: 0.013548783566837922, 22 gd steps\n",
      "insert gradient: -0.00018627472829772448\n",
      "92-th iteration, new layer inserted. now 39 layers\n",
      "[ 44.16304452  80.10628017  40.45480135  72.73859039  37.99511372\n",
      "  73.83665432  41.77121275  82.01937323  48.62985124 117.91170713\n",
      "  55.98888556  90.17056675  66.83920423 106.21965252  57.00379321\n",
      " 117.81382517  58.76050063 111.72378201  76.19544644 162.02994624\n",
      "  79.40231919 128.30796576  68.89737482 132.32772084  88.20120059\n",
      " 153.54972202  77.01605942 145.34720403 100.1430816   38.95650039\n",
      "   5.10784075 115.69089843  87.77264218 164.06622528  88.9717629\n",
      " 149.37681159  90.89908611  30.4460598   10.78848728]\n",
      "93-th iteration, loss: 0.0135137517180783, 34 gd steps\n",
      "insert gradient: -7.714029355280622e-05\n",
      "93-th iteration, new layer inserted. now 39 layers\n",
      "[ 44.09127643  79.47068167  40.28413771  73.23085949  38.12733213\n",
      "  73.43917845  41.47517025  82.37581451  48.5817272  116.84915086\n",
      "  56.03266035  90.8304158   66.00788219 107.40935463  56.46406749\n",
      " 117.23678339  59.36703007 109.31198376  76.92840941 161.22395434\n",
      "  78.7172168  129.52650759  68.73294501 132.05396149  87.87169974\n",
      " 153.43884195  77.77072332 146.35435379  99.76610378  36.49215529\n",
      "   4.82605166 116.9334684   87.39068626 162.0176626   89.66838493\n",
      " 150.37271663  92.04496931  31.80819001  10.49370025]\n",
      "94-th iteration, loss: 0.013508369860059107, 18 gd steps\n",
      "insert gradient: -0.0001003824007146696\n",
      "94-th iteration, new layer inserted. now 39 layers\n",
      "[ 44.05806798  79.79480991  40.19089955  73.01175399  38.12280027\n",
      "  73.61196358  41.40059266  82.09220199  48.46026402 116.96424718\n",
      "  56.12319572  90.4019197   65.89406018 107.68664245  56.33438231\n",
      " 117.10975377  59.47661677 108.69501563  77.24438569 161.07609145\n",
      "  78.39790086 129.75688159  68.71106293 132.29208188  87.86820694\n",
      " 153.10339683  77.80548538 146.52521448  99.93434826  35.79726977\n",
      "   4.98001816 117.2974969   86.94974813 161.54586211  89.60826121\n",
      " 150.44206401  92.23525905  31.89103747  10.50183522]\n",
      "95-th iteration, loss: 0.01348960806681074, 42 gd steps\n",
      "insert gradient: -8.019740922127388e-05\n",
      "95-th iteration, new layer inserted. now 39 layers\n",
      "[ 44.55428254  81.44758947  38.67557098  73.99710242  37.30976205\n",
      "  74.95186628  41.46674215  81.77235039  47.60904258 115.88789305\n",
      "  56.818483    91.33077841  63.81155011 111.56151053  55.30458751\n",
      " 114.74862598  62.51166821 102.40764106  79.36616445 161.83039867\n",
      "  76.71586179 129.74247045  68.37454507 133.71467913  89.18730934\n",
      " 150.53764375  77.31024825 148.39089103 100.41802119  28.40804803\n",
      "   4.95366082 126.34150496  85.65692588 159.98942354  89.26301204\n",
      " 151.540287    94.31745575  32.02530931  10.90476651]\n",
      "96-th iteration, loss: 0.013457001715280586, 27 gd steps\n",
      "insert gradient: -6.458247669094915e-05\n",
      "96-th iteration, new layer inserted. now 41 layers\n",
      "[ 44.49500946  80.13425446  39.96168997  72.89465493  38.02278799\n",
      "  73.4479214   41.10001189  82.36076937  47.72791727 114.60935933\n",
      "  57.60595456  90.88422023  63.48894479 111.95138015  55.39868514\n",
      " 114.0162833   62.77712149 102.26250975  80.03391773  96.72969851\n",
      "   0.          64.48646567  76.32030936 130.63565712  68.00373423\n",
      " 134.18206544  89.1621727  150.23443007  77.60031052 148.16078763\n",
      " 100.57055235  27.50178349   5.10240547 126.20576825  85.27650384\n",
      " 159.75964101  89.64175522 151.66332231  94.65433257  31.45978082\n",
      "  11.0341529 ]\n",
      "97-th iteration, loss: 0.013175716534266111, 93 gd steps\n",
      "insert gradient: -8.764577878275887e-05\n",
      "97-th iteration, new layer inserted. now 43 layers\n",
      "[ 46.25972615  83.20497026  40.7676802   73.4823728   37.38894323\n",
      "  72.07366734  40.05808491  83.75883802  46.99704515  94.87998751\n",
      "  67.63300392  92.97989244  56.35821067 121.27419554  57.02321413\n",
      " 100.17454908  71.40098985  94.16347324  84.55317992  98.0845182\n",
      "   2.22218587  54.35691924  74.00209507 134.34305021  65.83796248\n",
      " 136.52189248  90.62372344 146.82681276  79.53812724 119.28434698\n",
      "   0.          29.82108675 108.05602185  19.41830732   3.81296698\n",
      " 127.02673735  81.84626095 159.2730165   90.5980637  150.54998743\n",
      "  99.12260418  25.13855981  13.86103368]\n",
      "98-th iteration, loss: 0.013137764670836043, 19 gd steps\n",
      "insert gradient: -8.228391597753103e-05\n",
      "98-th iteration, new layer inserted. now 45 layers\n",
      "[4.60258420e+01 8.29495453e+01 4.09415894e+01 7.35532147e+01\n",
      " 3.76442241e+01 7.18217571e+01 0.00000000e+00 3.55271368e-15\n",
      " 3.99247770e+01 8.32730730e+01 4.67954404e+01 9.50819343e+01\n",
      " 6.75291742e+01 9.32037012e+01 5.63567476e+01 1.20376487e+02\n",
      " 5.77137201e+01 1.00016733e+02 7.16446200e+01 9.47185099e+01\n",
      " 8.40698714e+01 9.98075800e+01 1.77639980e+00 5.34901987e+01\n",
      " 7.42427338e+01 1.34330704e+02 6.57426494e+01 1.36680417e+02\n",
      " 9.02711058e+01 1.47878659e+02 8.09133479e+01 1.17598326e+02\n",
      " 3.12774796e+00 2.78737782e+01 1.08507622e+02 1.86266486e+01\n",
      " 1.93173238e+00 1.27187266e+02 8.06579269e+01 1.59873896e+02\n",
      " 9.03073862e+01 1.50335902e+02 9.94912370e+01 2.34914920e+01\n",
      " 1.43207702e+01]\n",
      "99-th iteration, loss: 0.012201753937728559, 466 gd steps\n",
      "insert gradient: -3.7852557580456886e-05\n",
      "99-th iteration, new layer inserted. now 41 layers\n",
      "[ 47.66162296  84.40104938  40.6978626   70.90165869  38.57001514\n",
      "  77.18835477  40.72107585  81.28136705  44.54987322  88.90538554\n",
      "  69.93548154  95.59593528  55.35501835 119.72084691  59.88826636\n",
      " 101.59186111  69.96408075  98.73177867  71.49484539 133.12662178\n",
      "  91.33092283 142.71946996  66.64286746 131.15534273  75.1868327\n",
      " 164.19798987  92.88122657 118.31247146   4.06546525  28.10261123\n",
      "  86.93878172   0.          21.73469543 141.59426404  75.00022146\n",
      " 157.09907459  95.4379879  148.44713481  91.27538576  26.95130073\n",
      "  16.14359592]\n",
      "0-th iteration, loss: 0.7689279469259274, 18 gd steps\n",
      "insert gradient: -0.46618784130189417\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  79.26531523    0.         3210.24526673]\n",
      "1-th iteration, loss: 0.5783537759444813, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.11023928e+01 1.28431131e+00 0.00000000e+00 1.04029216e+02\n",
      " 3.18558819e+03]\n",
      "2-th iteration, loss: 0.5700914493418382, 558 gd steps\n",
      "insert gradient: -0.22394163347815782\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.95984165  105.46689303  152.69727027    0.         2977.59677034]\n",
      "3-th iteration, loss: 0.46024441303079955, 19 gd steps\n",
      "insert gradient: -0.21484834377557213\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  68.37309073  110.39947206   59.45211072    0.           52.59225179\n",
      "   88.33307696 2955.20048895]\n",
      "4-th iteration, loss: 0.4387623790006111, 22 gd steps\n",
      "insert gradient: -0.1860845040066784\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[  71.80477466  108.68756544   54.34662675   31.73374647   39.38032069\n",
      "   91.38735328 2947.6394919 ]\n",
      "5-th iteration, loss: 0.4230776051528528, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  74.48324265  117.90100039   57.18161643   67.99008978   27.0911842\n",
      "   88.79850687  167.89927952    0.         2770.33811201]\n",
      "6-th iteration, loss: 0.3754308283529641, 48 gd steps\n",
      "insert gradient: -0.04929722202713879\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  66.32887131  128.33567576   56.03252571   72.78630456   25.26532683\n",
      "   81.98584132  133.37628685   89.40280066 2346.58170844    0.\n",
      "  408.10116669]\n",
      "7-th iteration, loss: 0.34553427234519724, 21 gd steps\n",
      "insert gradient: -0.10638025516398576\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  69.60780868  128.41708981   43.65778229   46.64999941   56.74661983\n",
      "   92.23905775  101.14638689  119.26213784 2328.221155     56.44715708\n",
      "  124.33976477    0.          266.44235308]\n",
      "8-th iteration, loss: 0.2693133170077397, 78 gd steps\n",
      "insert gradient: -0.10854760688366726\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  74.43505921  106.99341247   45.61754004   65.63924658   44.62981362\n",
      "   96.05818984  101.6635491   119.01811289 2321.32054595  106.63744404\n",
      "   88.50727651  108.06777661  111.09969671    0.          111.09969671]\n",
      "9-th iteration, loss: 0.23386514011446058, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  71.41948247  110.55127485   53.05451895   51.52443746   47.983249\n",
      "   98.54359019  106.20128214  108.28710055  145.30949545    0.\n",
      " 2179.64243172  104.85244177   90.63645009   95.68891267   96.09009965\n",
      "   75.37669256   89.23140506]\n",
      "10-th iteration, loss: 0.2035763132888209, 24 gd steps\n",
      "insert gradient: -0.04771115143777654\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[  68.00886973  101.21173205   50.4257036    79.48638911   51.53440384\n",
      "   95.59612446  104.53086344  114.60336746  137.4626941    23.91117896\n",
      " 2169.36504763   96.17013667   99.00914817   97.53038215   96.96736154\n",
      "   94.31821791   98.82248603]\n",
      "11-th iteration, loss: 0.201561019295307, 24 gd steps\n",
      "insert gradient: -0.03950840960160566\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  69.58966658  101.27207102   52.1179548    83.64196046   50.20391104\n",
      "   92.51651269  107.68467847  111.86516107  108.086491      0.\n",
      "   29.47813391   29.96908613 2166.52905304   94.18486547  100.86960629\n",
      "   96.79446071   99.38010738   96.07140553  100.0013473 ]\n",
      "12-th iteration, loss: 0.1900006822446976, 137 gd steps\n",
      "insert gradient: -0.024488989999424542\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  71.60374682  104.90658912   54.63609314   84.73467036   50.42841096\n",
      "   93.92472361  112.26210628  107.47151987   79.40823837   10.35622675\n",
      "   18.92000702  119.49095259  716.85826603    0.         1433.71653206\n",
      "   97.2769341    99.99428522   91.32099533  101.1977345    92.46656653\n",
      "  100.77007499]\n",
      "13-th iteration, loss: 0.18917721754411504, 25 gd steps\n",
      "insert gradient: -0.05514543828013375\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  70.71537036  103.84402514   55.33636098   85.79382028   50.21687421\n",
      "   95.35665369  111.74742075  105.47341783   78.23215663    8.08110392\n",
      "   21.75792238  117.12584205  724.31532534    6.1782337  1295.10444757\n",
      "    0.          129.51044476   97.45684558   99.51803576   91.71793938\n",
      "  100.80406404   92.93102005  100.37384199]\n",
      "14-th iteration, loss: 0.15760881927401937, 127 gd steps\n",
      "insert gradient: -0.04566312865672713\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[ 63.02713851 107.09817824  54.25228034  87.63468265  53.47331976\n",
      " 100.06540915 110.63802114 101.24127974  64.38667959  17.57437224\n",
      "  32.12375581 107.9447869  719.22125331  53.46717092 752.42113677\n",
      "   0.         501.61409118  89.02743341  95.97218046  93.8788045\n",
      "  95.31600913  99.59543519  95.56703794 105.58728786  94.68428407]\n",
      "15-th iteration, loss: 0.10466138630531974, 318 gd steps\n",
      "insert gradient: -0.11486913854576619\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[ 68.81335341 113.2901951   57.20452958  90.94440648  47.75266698\n",
      "  85.36921075  54.31052996   0.          67.88816245  92.79959593\n",
      "  36.03510428  37.22044028  60.18935334 107.39713405 693.14156138\n",
      " 116.5180163  684.29512894 107.46997519 483.16576075  92.2017274\n",
      "  86.29718627  95.71171922  97.42368256 103.14606841  83.24008987\n",
      " 102.67522392  86.55914592]\n",
      "16-th iteration, loss: 0.10385687129431137, 10 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "16-th iteration, new layer inserted. now 29 layers\n",
      "[6.88430381e+01 1.13310318e+02 5.72061837e+01 9.09236960e+01\n",
      " 4.77088465e+01 8.53653719e+01 5.44673932e+01 4.84796425e-01\n",
      " 6.80452400e+01 9.28198942e+01 3.59245193e+01 3.69830594e+01\n",
      " 6.00444072e+01 1.07405771e+02 8.66487006e+01 0.00000000e+00\n",
      " 6.06540904e+02 1.16576250e+02 6.84426002e+02 1.07481899e+02\n",
      " 4.83168223e+02 9.21867772e+01 8.63253739e+01 9.57064979e+01\n",
      " 9.73960851e+01 1.03142602e+02 8.32716318e+01 1.02689938e+02\n",
      " 8.65586315e+01]\n",
      "17-th iteration, loss: 0.09007146068888193, 92 gd steps\n",
      "insert gradient: -0.016289232971809807\n",
      "17-th iteration, new layer inserted. now 31 layers\n",
      "[ 67.83289221 112.62931111  70.80640098  89.92427248  42.87712204\n",
      "  91.81100044  44.44766563  23.6848511   48.84733902  98.90235327\n",
      "  46.34813697  31.73888193  53.57815815 113.21917928  70.2003927\n",
      "  44.88750666 440.43522579   0.         146.81174193 114.84481002\n",
      " 692.98068358 114.30257889 475.70159525  98.45045115  80.10429347\n",
      " 106.832197    87.21643099 111.54782787  82.69910689 111.97060617\n",
      "  85.55981433]\n",
      "18-th iteration, loss: 0.08454964321365334, 30 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "18-th iteration, new layer inserted. now 33 layers\n",
      "[ 66.5793303  116.95713814  69.67007664  85.76178933  45.60940068\n",
      "  94.5470714   44.59015947  23.34402932  49.30002577  96.15444612\n",
      "  45.87654031  33.93985082  48.69324854 121.28443463  65.79869806\n",
      "  56.13630145  63.07676916   0.         378.46061493  18.80443788\n",
      " 135.99943733 114.29734238 689.1415353  119.685316   473.4661777\n",
      "  97.67221129  80.86470385 107.82885189  86.01303901 113.02355459\n",
      "  83.75607065 111.33190629  83.36615926]\n",
      "19-th iteration, loss: 0.07735080330852874, 34 gd steps\n",
      "insert gradient: -0.03550060160917077\n",
      "19-th iteration, new layer inserted. now 35 layers\n",
      "[6.26914001e+01 1.17837245e+02 7.35360415e+01 8.80730308e+01\n",
      " 4.24944099e+01 8.71435553e+01 3.92934141e+01 3.01938769e+01\n",
      " 5.27792187e+01 9.59777473e+01 4.09972013e+01 2.98561815e+01\n",
      " 6.24425715e+01 1.18539634e+02 6.02393886e+01 6.77640790e+01\n",
      " 5.81391990e+01 1.49963443e+01 3.68831842e+02 2.83823882e+01\n",
      " 1.33268809e+02 1.18606267e+02 6.82169142e+02 1.31062003e+02\n",
      " 0.00000000e+00 1.42108547e-14 4.68690644e+02 1.05249222e+02\n",
      " 7.89127613e+01 1.08911102e+02 8.55719622e+01 1.12403527e+02\n",
      " 8.40528655e+01 1.12303285e+02 8.16981299e+01]\n",
      "20-th iteration, loss: 0.0773341630047772, 5 gd steps\n",
      "insert gradient: -0.004043356731630127\n",
      "20-th iteration, new layer inserted. now 37 layers\n",
      "[6.26907685e+01 1.17836549e+02 7.35356528e+01 8.80723168e+01\n",
      " 4.24945819e+01 8.71441484e+01 3.92972007e+01 0.00000000e+00\n",
      " 5.32907052e-15 3.02003141e+01 5.27832178e+01 9.59777191e+01\n",
      " 4.09948631e+01 2.98539149e+01 6.24418287e+01 1.18541372e+02\n",
      " 6.02444297e+01 6.77695815e+01 5.81459963e+01 1.50011149e+01\n",
      " 3.68834810e+02 2.83830632e+01 1.33273071e+02 1.18607972e+02\n",
      " 6.82171979e+02 1.31071603e+02 2.20772414e-02 9.59997070e-03\n",
      " 4.68712722e+02 1.05259023e+02 7.89232101e+01 1.08915063e+02\n",
      " 8.55763191e+01 1.12405503e+02 8.40550790e+01 1.12303988e+02\n",
      " 8.16976328e+01]\n",
      "21-th iteration, loss: 0.07658030704556326, 38 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "21-th iteration, new layer inserted. now 37 layers\n",
      "[6.34830765e+01 1.17681548e+02 7.33816253e+01 8.72261692e+01\n",
      " 4.22614164e+01 8.69107547e+01 3.92103047e+01 3.30610838e+01\n",
      " 5.19835821e+01 9.51328657e+01 3.93834436e+01 3.03466189e+01\n",
      " 6.34480446e+01 1.18623011e+02 6.08761613e+01 6.91629512e+01\n",
      " 5.85125926e+01 1.63891516e+01 3.66964700e+02 2.87261307e+01\n",
      " 1.32753029e+02 1.19289444e+02 6.81805312e+02 1.32337722e+02\n",
      " 9.54990201e-03 1.27555029e+00 1.55796470e+02 0.00000000e+00\n",
      " 3.11592941e+02 1.05825768e+02 7.92831509e+01 1.09102645e+02\n",
      " 8.57737176e+01 1.12879507e+02 8.43118253e+01 1.12415910e+02\n",
      " 8.06841546e+01]\n",
      "22-th iteration, loss: 0.07512907177946743, 50 gd steps\n",
      "insert gradient: -0.0277861201124462\n",
      "22-th iteration, new layer inserted. now 37 layers\n",
      "[ 64.0375412  117.69862428  73.31143304  86.83697959  42.87707313\n",
      "  86.73748202  38.76860972  34.20852676  51.78499602  94.72577885\n",
      "  37.81389351  31.16234545  64.21136476 118.213187    61.2857109\n",
      "  70.95901592  58.60319567  18.45694378 363.58188547  28.6145657\n",
      " 133.12915294 119.70972824 681.11820527 134.14560122 154.2224183\n",
      "   6.23972491 207.14812824   0.         103.57406412 106.68179448\n",
      "  78.76052139 109.16055789  85.43570929 113.35909054  83.80378863\n",
      " 112.81354941  80.00095354]\n",
      "23-th iteration, loss: 0.07509258916549684, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 39 layers\n",
      "[6.40395016e+01 1.17698691e+02 7.33128228e+01 8.68378477e+01\n",
      " 4.28865139e+01 8.67387951e+01 3.87711097e+01 3.42204963e+01\n",
      " 5.17898937e+01 9.47260450e+01 3.78071878e+01 3.11703728e+01\n",
      " 6.42181092e+01 1.18210158e+02 6.12906139e+01 7.09779918e+01\n",
      " 5.86136860e+01 1.84743851e+01 1.21188342e+02 0.00000000e+00\n",
      " 2.42376685e+02 2.86119773e+01 1.33143462e+02 1.19713651e+02\n",
      " 6.81103091e+02 1.34137084e+02 1.54199502e+02 6.27071308e+00\n",
      " 2.07136521e+02 8.38889663e-02 1.03562527e+02 1.06684885e+02\n",
      " 7.87580185e+01 1.09162421e+02 8.54373502e+01 1.13362523e+02\n",
      " 8.37987256e+01 1.12816733e+02 7.99991148e+01]\n",
      "24-th iteration, loss: 0.06341606058908338, 42 gd steps\n",
      "insert gradient: -0.15204332272884113\n",
      "24-th iteration, new layer inserted. now 39 layers\n",
      "[ 65.82358544 120.29207205  73.10939448  84.14911677  43.9418498\n",
      "  85.67858114  39.45731123  37.98914128  50.07862604  93.57060177\n",
      "  29.95239895  38.29797274  70.12337032 109.83489586  61.54335491\n",
      "  79.08758846  60.11932354  25.13288329 104.85187282  16.61831477\n",
      " 231.59661868  37.2533308  133.11371811 128.97261599 679.48119594\n",
      " 124.04348384 150.43281219  19.82552487 189.01116235  30.46576638\n",
      "  96.89068686 112.35027631  75.3051899  112.60842641  86.61066063\n",
      " 119.20733388  79.58642888 117.51128112  79.44604022]\n",
      "25-th iteration, loss: 0.06326164547342283, 9 gd steps\n",
      "insert gradient: -0.1854617102035951\n",
      "25-th iteration, new layer inserted. now 39 layers\n",
      "[ 65.8125894  120.28725531  73.1034425   84.15372453  43.96050043\n",
      "  85.69089257  39.4776335   37.99916173  50.09246922  93.58085612\n",
      "  29.97682166  38.30816282  70.121849   109.8351271   61.55332902\n",
      "  79.10789049  60.16342396  25.17996638 104.87735013  16.6309138\n",
      " 231.60338188  37.25417018 133.13909965 128.97563724 679.46282686\n",
      " 124.03354276 150.40506143  19.81312058 188.98944491  30.46613962\n",
      "  96.88165173 112.34933169  75.30296519 112.60820084  86.60860264\n",
      " 119.2065889   79.58159069 117.51160846  79.44629375]\n",
      "26-th iteration, loss: 0.06284777903889804, 7 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.722136942366677e-19\n",
      "26-th iteration, new layer inserted. now 40 layers\n",
      "[ 65.81106698 120.28708901  73.10451621  84.15697801  43.96957121\n",
      "  85.69646825  39.48599102  38.00307662  50.09912522  93.58617663\n",
      "  29.98900126  38.31389352  70.12678742 109.83862521  61.56319517\n",
      "  79.11891091  60.18583899  25.19875747 104.89327185  16.63924347\n",
      " 231.61487697  37.26266941 133.15686435 128.98687277 679.48573799\n",
      " 124.05545521 150.46106638  19.83478853 189.04772719  30.50854634\n",
      "  96.9245968  112.36624723  75.31975868 112.61458852  86.61474585\n",
      " 119.20886169  79.58296781 117.51248425  79.4469521    0.        ]\n",
      "27-th iteration, loss: 0.06275928864515462, 7 gd steps\n",
      "insert gradient: -0.03910679738469919\n",
      "27-th iteration, new layer inserted. now 42 layers\n",
      "[6.58081541e+01 1.20285362e+02 7.31011197e+01 8.41556804e+01\n",
      " 4.39677082e+01 8.56953109e+01 3.94848832e+01 3.80033249e+01\n",
      " 5.00979090e+01 9.35840949e+01 2.99833823e+01 3.83106975e+01\n",
      " 7.01197466e+01 1.09835072e+02 6.15578517e+01 7.91161602e+01\n",
      " 6.01782833e+01 2.51946651e+01 1.04884047e+02 1.66320254e+01\n",
      " 2.31603764e+02 3.72559315e+01 1.33149084e+02 1.28979705e+02\n",
      " 6.79466374e+02 1.24043895e+02 9.02612121e+01 0.00000000e+00\n",
      " 6.01741414e+01 1.98230416e+01 1.89021813e+02 3.04951416e+01\n",
      " 9.69078349e+01 1.12360459e+02 7.53136598e+01 1.12612462e+02\n",
      " 8.66121845e+01 1.19207928e+02 7.95812101e+01 1.17512308e+02\n",
      " 7.94467936e+01 9.13679271e-19]\n",
      "28-th iteration, loss: 0.06274319422467951, 6 gd steps\n",
      "insert gradient: -0.0019382165434687676\n",
      "28-th iteration, new layer inserted. now 41 layers\n",
      "[6.58059193e+01 1.20284388e+02 7.30998726e+01 8.41565665e+01\n",
      " 4.39711168e+01 8.56974265e+01 3.94884144e+01 3.80053416e+01\n",
      " 5.01004714e+01 9.35856160e+01 2.99865400e+01 3.83119583e+01\n",
      " 7.01185922e+01 1.09834879e+02 6.15596121e+01 7.91198034e+01\n",
      " 6.01848382e+01 2.52014940e+01 1.04886795e+02 1.66324251e+01\n",
      " 2.31603408e+02 3.72562680e+01 1.33153150e+02 1.28980965e+02\n",
      " 6.79466253e+02 1.24047029e+02 9.02707339e+01 1.21341863e-02\n",
      " 6.01836631e+01 1.98259282e+01 1.89032303e+02 3.05057525e+01\n",
      " 9.69168476e+01 1.12364470e+02 7.53174531e+01 1.12614018e+02\n",
      " 8.66133903e+01 1.19208384e+02 7.95808142e+01 1.17512589e+02\n",
      " 7.94469909e+01]\n",
      "29-th iteration, loss: 0.06272982596559348, 6 gd steps\n",
      "insert gradient: -0.02797007035426701\n",
      "29-th iteration, new layer inserted. now 43 layers\n",
      "[6.58034026e+01 1.20283093e+02 7.30976955e+01 8.41564981e+01\n",
      " 4.39722517e+01 8.56981671e+01 3.94900606e+01 3.80066213e+01\n",
      " 5.01013968e+01 9.35856178e+01 2.99861251e+01 3.83114788e+01\n",
      " 7.01150116e+01 1.09833181e+02 6.15580024e+01 7.91203569e+01\n",
      " 6.01847832e+01 2.52033745e+01 1.04884104e+02 1.66296116e+01\n",
      " 2.31598172e+02 3.72532314e+01 1.33151493e+02 1.28978074e+02\n",
      " 6.79456582e+02 1.24042260e+02 9.02605520e+01 0.00000000e+00\n",
      " 7.10542736e-15 5.54076205e-03 6.01734792e+01 1.98209104e+01\n",
      " 1.89022552e+02 3.05028541e+01 9.69114930e+01 1.12363016e+02\n",
      " 7.53157502e+01 1.12613534e+02 8.66125268e+01 1.19208078e+02\n",
      " 7.95796801e+01 1.17512621e+02 7.94469950e+01]\n",
      "30-th iteration, loss: 0.06268150836871696, 14 gd steps\n",
      "insert gradient: -0.005042579189039989\n",
      "30-th iteration, new layer inserted. now 43 layers\n",
      "[6.57921208e+01 1.20277706e+02 7.30895161e+01 8.41584122e+01\n",
      " 4.39830507e+01 8.57050052e+01 3.95024576e+01 3.80142952e+01\n",
      " 5.01093668e+01 9.35891800e+01 2.99928995e+01 3.83138427e+01\n",
      " 7.01040797e+01 1.09828507e+02 6.15574433e+01 7.91291919e+01\n",
      " 6.01977992e+01 2.52223246e+01 1.04882660e+02 1.66235748e+01\n",
      " 2.31583942e+02 3.72466296e+01 1.33155305e+02 1.28973794e+02\n",
      " 6.79434018e+02 1.24038170e+02 9.02574862e+01 1.14268524e-02\n",
      " 4.62556575e-03 1.69695030e-02 6.01704028e+01 1.98157190e+01\n",
      " 1.89022659e+02 3.05200196e+01 9.69190448e+01 1.12368678e+02\n",
      " 7.53203486e+01 1.12615957e+02 8.66132367e+01 1.19208399e+02\n",
      " 7.95760097e+01 1.17513367e+02 7.94474739e+01]\n",
      "31-th iteration, loss: 0.06089733938200785, 20 gd steps\n",
      "insert gradient: -0.038230669837990346\n",
      "31-th iteration, new layer inserted. now 41 layers\n",
      "[ 64.91588452 120.33671955  72.54477891  84.29903784  44.64516238\n",
      "  85.96414991  39.89408249  37.96540926  49.80357204  93.64112227\n",
      "  29.83183873  39.21553126  70.38078798 108.50736637  61.10109301\n",
      "  79.84938475  60.81034783  26.34301667 103.28409873  17.37292626\n",
      " 230.56958172  38.07188992 132.85984982 129.30967153 679.28760292\n",
      " 123.89160867  89.40609616   3.292119    58.94695001  21.24393911\n",
      " 185.61395077  33.52914763  96.323577   112.95251254  75.78516838\n",
      " 112.85882716  86.54153404 118.98872762  78.52588304 117.70497845\n",
      "  79.77886488]\n",
      "32-th iteration, loss: 0.06088939705124971, 6 gd steps\n",
      "insert gradient: -0.00017048648932111446\n",
      "32-th iteration, new layer inserted. now 43 layers\n",
      "[6.49155766e+01 1.20336909e+02 7.25447192e+01 8.42992588e+01\n",
      " 4.46458769e+01 8.59645544e+01 3.98944122e+01 3.79651540e+01\n",
      " 4.98034035e+01 9.36413360e+01 2.98321580e+01 3.92157922e+01\n",
      " 7.03802662e+01 1.08506817e+02 6.11012022e+01 7.98501050e+01\n",
      " 6.08111556e+01 2.63445731e+01 1.03283889e+02 1.73731937e+01\n",
      " 2.30569181e+02 3.80733738e+01 1.32860502e+02 1.29311365e+02\n",
      " 6.79292141e+02 1.23895951e+02 8.94170861e+01 3.30364000e+00\n",
      " 5.89574943e+01 2.12486382e+01 1.85624248e+02 3.35387125e+01\n",
      " 9.63317022e+01 1.12955965e+02 7.57882886e+01 1.12859928e+02\n",
      " 8.65424009e+01 1.18989001e+02 7.85258205e+01 1.17705262e+02\n",
      " 0.00000000e+00 7.10542736e-15 7.97792011e+01]\n",
      "33-th iteration, loss: 0.06088424830456265, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "33-th iteration, new layer inserted. now 45 layers\n",
      "[6.49150668e+01 1.20336878e+02 7.25440319e+01 8.42988496e+01\n",
      " 4.46451563e+01 8.59641330e+01 3.98936874e+01 3.79644852e+01\n",
      " 4.98022419e+01 9.36406527e+01 2.98304946e+01 3.92151114e+01\n",
      " 7.03782576e+01 1.08505253e+02 6.10989031e+01 7.98486444e+01\n",
      " 6.08076426e+01 2.63431831e+01 1.03280290e+02 1.73717056e+01\n",
      " 2.30565587e+02 3.80722330e+01 1.32857052e+02 1.29309645e+02\n",
      " 6.79288246e+02 1.23892586e+02 8.94080200e+01 3.29584507e+00\n",
      " 1.17895208e+01 0.00000000e+00 4.71580831e+01 2.12454526e+01\n",
      " 1.85614021e+02 3.35349101e+01 9.63257521e+01 1.12954073e+02\n",
      " 7.57860334e+01 1.12859039e+02 8.65412644e+01 1.18988542e+02\n",
      " 7.85250454e+01 1.17705308e+02 1.48315570e-04 4.55559687e-05\n",
      " 7.97793494e+01]\n",
      "34-th iteration, loss: 0.050856603434809765, 31 gd steps\n",
      "insert gradient: -0.41202856203446203\n",
      "34-th iteration, new layer inserted. now 43 layers\n",
      "[5.83660470e+01 1.26852316e+02 7.18172397e+01 8.49161888e+01\n",
      " 4.40206958e+01 8.56945173e+01 4.21289660e+01 3.87898278e+01\n",
      " 4.87078209e+01 9.50578304e+01 2.68904954e+01 4.66694750e+01\n",
      " 6.67167694e+01 1.05181812e+02 5.92560899e+01 8.75654765e+01\n",
      " 5.75206609e+01 3.46693926e+01 9.40918697e+01 2.88667651e+01\n",
      " 2.26016466e+02 4.74030752e+01 1.28142567e+02 1.36153626e+02\n",
      " 6.74407590e+02 1.22235691e+02 9.67776866e+01 8.54158104e+00\n",
      " 4.92913378e-01 2.88764047e+01 3.07099411e+01 5.21464324e+01\n",
      " 1.55398518e+02 6.54365809e+01 8.88366807e+01 1.14571402e+02\n",
      " 7.51971686e+01 1.06823940e+02 8.94949598e+01 1.21431824e+02\n",
      " 7.45592456e+01 1.19425785e+02 8.48433291e+01]\n",
      "35-th iteration, loss: 0.049760579977349645, 10 gd steps\n",
      "insert gradient: -0.3326611678654005\n",
      "35-th iteration, new layer inserted. now 43 layers\n",
      "[5.83636552e+01 1.26849346e+02 7.18080335e+01 8.49053226e+01\n",
      " 4.39910943e+01 8.56737303e+01 4.20946408e+01 3.87753148e+01\n",
      " 4.86828801e+01 9.50355877e+01 2.68335635e+01 4.66395462e+01\n",
      " 6.66802995e+01 1.05168242e+02 5.92324979e+01 8.75446285e+01\n",
      " 5.74650095e+01 3.46258486e+01 9.40365890e+01 2.88222065e+01\n",
      " 2.25962787e+02 4.73786558e+01 1.28110252e+02 1.36133789e+02\n",
      " 6.74363848e+02 1.22226018e+02 9.67876249e+01 8.55639842e+00\n",
      " 5.07380312e-01 2.88909796e+01 3.07395937e+01 5.21507179e+01\n",
      " 1.55401164e+02 6.54472088e+01 8.88435011e+01 1.14574019e+02\n",
      " 7.51992778e+01 1.06824174e+02 8.94923830e+01 1.21430794e+02\n",
      " 7.45578222e+01 1.19424038e+02 8.48411951e+01]\n",
      "36-th iteration, loss: 0.049000913190053626, 8 gd steps\n",
      "insert gradient: -0.0040504186564911905\n",
      "36-th iteration, new layer inserted. now 45 layers\n",
      "[5.83638407e+01 1.26849499e+02 7.18084113e+01 8.49055650e+01\n",
      " 4.39912429e+01 8.56735274e+01 4.20938440e+01 3.87749398e+01\n",
      " 4.86825209e+01 9.50353281e+01 2.68326351e+01 4.66389690e+01\n",
      " 6.66799658e+01 1.05168808e+02 5.92341384e+01 8.75460079e+01\n",
      " 5.74671311e+01 3.46271270e+01 9.40376109e+01 2.88218835e+01\n",
      " 2.25963191e+02 4.73800305e+01 1.28113188e+02 1.36136555e+02\n",
      " 6.74370098e+02 1.22232994e+02 7.74458880e+01 0.00000000e+00\n",
      " 1.93614720e+01 8.57486314e+00 5.28789250e-01 2.89093272e+01\n",
      " 3.07701926e+01 5.21618490e+01 1.55424309e+02 6.54593088e+01\n",
      " 8.88555371e+01 1.14578591e+02 7.52038966e+01 1.06825861e+02\n",
      " 8.94938894e+01 1.21431336e+02 7.45583264e+01 1.19424090e+02\n",
      " 8.48411191e+01]\n",
      "37-th iteration, loss: 0.04772520160850295, 40 gd steps\n",
      "insert gradient: -0.01662539101367672\n",
      "37-th iteration, new layer inserted. now 43 layers\n",
      "[5.89487505e+01 1.26912004e+02 7.25535932e+01 8.53425139e+01\n",
      " 4.38923460e+01 8.51056275e+01 4.12267088e+01 3.91038347e+01\n",
      " 4.82134810e+01 9.47021055e+01 2.68568947e+01 4.73238399e+01\n",
      " 6.72815813e+01 1.06160785e+02 5.92038526e+01 8.76514914e+01\n",
      " 5.70476244e+01 3.53002291e+01 9.31898770e+01 2.92503498e+01\n",
      " 2.25736375e+02 4.69528120e+01 1.28446316e+02 1.36873674e+02\n",
      " 6.74138493e+02 1.21198658e+02 9.71754942e+01 3.89482311e+01\n",
      " 0.00000000e+00 1.77635684e-15 2.96767882e+01 5.23288966e+01\n",
      " 1.53795728e+02 6.83127936e+01 9.04017230e+01 1.14814159e+02\n",
      " 7.64001362e+01 1.07106431e+02 8.87481485e+01 1.20915904e+02\n",
      " 7.43343994e+01 1.18248932e+02 8.39123913e+01]\n",
      "38-th iteration, loss: 0.04772231465049179, 7 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 45 layers\n",
      "[5.89487994e+01 1.26912012e+02 7.25536819e+01 8.53426149e+01\n",
      " 4.38924919e+01 8.51056539e+01 4.12267159e+01 3.91038824e+01\n",
      " 4.82135053e+01 9.47021465e+01 2.68570913e+01 4.73239682e+01\n",
      " 6.72816435e+01 1.06160876e+02 5.92039867e+01 8.76517281e+01\n",
      " 5.70481096e+01 3.53007093e+01 9.31901529e+01 2.92505881e+01\n",
      " 2.25736626e+02 4.69529512e+01 1.28446792e+02 1.36874019e+02\n",
      " 1.34827804e+02 0.00000000e+00 5.39311215e+02 1.21199010e+02\n",
      " 9.71765821e+01 3.89492942e+01 1.55326133e-03 1.06312565e-03\n",
      " 2.96783415e+01 5.23295882e+01 1.53797030e+02 6.83137211e+01\n",
      " 9.04024829e+01 1.14814391e+02 7.64004789e+01 1.07106556e+02\n",
      " 8.87482086e+01 1.20915888e+02 7.43343931e+01 1.18248833e+02\n",
      " 8.39123270e+01]\n",
      "39-th iteration, loss: 0.04711984104832348, 23 gd steps\n",
      "insert gradient: -0.010459316129381958\n",
      "39-th iteration, new layer inserted. now 45 layers\n",
      "[ 59.07405919 126.80463742  72.61056123  85.53541432  43.97314274\n",
      "  84.87246326  40.85782427  39.22454815  48.03929706  94.56740838\n",
      "  27.07719995  47.75126764  67.392363   106.32864221  59.03300328\n",
      "  87.74829886  56.90266536  35.65112653  92.8302423   29.51406715\n",
      " 225.88038459  46.67646064 127.97287725 136.79450412 134.50972479\n",
      "   1.9490822  431.1410107    0.         107.78525267 120.61405838\n",
      "  97.03104027  39.71464662  29.25750267  52.66670867 153.27292503\n",
      "  69.55712817  90.61615839 114.684456    76.66162253 107.27479976\n",
      "  88.63439127 120.71014462  74.11605741 117.77914119  83.54713029]\n",
      "40-th iteration, loss: 0.03748994906126879, 57 gd steps\n",
      "insert gradient: -0.06348691839492478\n",
      "40-th iteration, new layer inserted. now 47 layers\n",
      "[5.99825927e+01 1.26563465e+02 7.20414138e+01 8.64816034e+01\n",
      " 4.59810749e+01 8.13300462e+01 3.81277146e+01 4.42415072e+01\n",
      " 4.63972621e+01 8.75880696e+01 2.80650900e+01 6.14529705e+01\n",
      " 6.00557434e+01 1.01915245e+02 6.34588258e+01 9.32669704e+01\n",
      " 4.99062380e+01 3.84588174e+01 8.99813727e+01 4.05460151e+01\n",
      " 2.27078057e+02 4.08718940e+01 1.27941057e+02 1.28417631e+02\n",
      " 1.33191404e+02 7.90474854e+00 4.21545717e+02 2.69727379e+01\n",
      " 1.03945310e+02 1.14174158e+02 9.50311854e+01 5.36646587e+01\n",
      " 0.00000000e+00 3.55271368e-15 1.87114442e+01 7.49730512e+01\n",
      " 1.43184965e+02 8.84120169e+01 8.33016685e+01 1.17705398e+02\n",
      " 7.60086441e+01 1.11478072e+02 8.96095845e+01 1.17083329e+02\n",
      " 7.25581170e+01 1.21240037e+02 8.03101737e+01]\n",
      "41-th iteration, loss: 0.03744722774214342, 8 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 49 layers\n",
      "[5.99826014e+01 1.26563432e+02 7.20414265e+01 8.64816729e+01\n",
      " 4.59811505e+01 8.13300449e+01 3.81276630e+01 4.42414968e+01\n",
      " 4.63972813e+01 8.75881315e+01 2.80654200e+01 6.14533005e+01\n",
      " 6.00564542e+01 1.01915822e+02 6.34599264e+01 9.32675397e+01\n",
      " 4.99070801e+01 3.84591884e+01 8.99821936e+01 4.05466864e+01\n",
      " 1.13539496e+02 0.00000000e+00 1.13539496e+02 4.08724841e+01\n",
      " 1.27941690e+02 1.28418104e+02 1.33192574e+02 7.90535604e+00\n",
      " 4.21546817e+02 2.69738398e+01 1.03946516e+02 1.14175429e+02\n",
      " 9.50341994e+01 5.36678769e+01 6.17727062e-03 3.21823647e-03\n",
      " 1.87176215e+01 7.49754352e+01 1.43189574e+02 8.84140833e+01\n",
      " 8.33037213e+01 1.17706190e+02 7.60092351e+01 1.11478369e+02\n",
      " 8.96098721e+01 1.17083360e+02 7.25582949e+01 1.21240082e+02\n",
      " 8.03100865e+01]\n",
      "42-th iteration, loss: 0.0319009046355229, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 49 layers\n",
      "[ 60.73934542 123.57211116  70.718549    88.10722778  44.69330973\n",
      "  78.33479112  33.19228394  45.55453006  48.82025461  87.43020969\n",
      "  28.3295171   64.20147133  65.39180798 105.84132891  60.63439617\n",
      "  91.00533784  48.65861859  44.22531346  85.15010186  45.0088124\n",
      " 111.88995519   0.58099367 112.00089181  46.82448541 128.63134201\n",
      " 125.32888528 132.14402713  12.47474305 416.15532873  40.41202923\n",
      " 100.79278041 113.7824917   85.80626898  58.49016332  18.58403303\n",
      "  19.19151597   0.          57.5745479  141.41957833  95.37683769\n",
      "  86.72561069 118.88913084  71.75391226 114.52917655  89.0679921\n",
      " 115.43156806  75.11658592 121.32062285  77.5938769 ]\n",
      "43-th iteration, loss: 0.03190079040625502, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 51 layers\n",
      "[6.07396973e+01 1.23572207e+02 7.07187600e+01 8.81073295e+01\n",
      " 4.46932007e+01 7.83345947e+01 3.31923684e+01 4.55554194e+01\n",
      " 4.88217331e+01 8.74311900e+01 2.83327199e+01 6.42047021e+01\n",
      " 6.54007376e+01 5.29250946e+01 0.00000000e+00 5.29250946e+01\n",
      " 6.06511211e+01 9.10118349e+01 4.86657010e+01 4.42278147e+01\n",
      " 8.51543750e+01 4.50099139e+01 1.11889830e+02 5.80093882e-01\n",
      " 1.12000844e+02 4.68256696e+01 1.28632843e+02 1.25330238e+02\n",
      " 1.32147401e+02 1.24766082e+01 4.16158890e+02 4.04173108e+01\n",
      " 1.00801418e+02 1.13792411e+02 8.58329053e+01 5.85175176e+01\n",
      " 1.86335576e+01 1.92110017e+01 5.18703200e-02 5.75940336e+01\n",
      " 1.41459482e+02 9.53929143e+01 8.67412160e+01 1.18894996e+02\n",
      " 7.17599058e+01 1.14531614e+02 8.90702885e+01 1.15432258e+02\n",
      " 7.51174046e+01 1.21320766e+02 7.75939531e+01]\n",
      "44-th iteration, loss: 0.03189116723947777, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "44-th iteration, new layer inserted. now 51 layers\n",
      "[6.07397189e+01 1.23571964e+02 7.07181434e+01 8.81066322e+01\n",
      " 4.46912905e+01 7.83333609e+01 3.31910739e+01 4.55554289e+01\n",
      " 4.88211414e+01 8.74304860e+01 2.83321303e+01 6.42055165e+01\n",
      " 6.54039078e+01 5.29291156e+01 1.02030171e-02 5.29291156e+01\n",
      " 6.06580518e+01 9.10130786e+01 4.86646192e+01 4.42265260e+01\n",
      " 8.51528652e+01 4.50084808e+01 1.11885003e+02 5.75853207e-01\n",
      " 1.11996059e+02 4.68230048e+01 1.28628844e+02 1.25326587e+02\n",
      " 1.32137975e+02 1.24728342e+01 4.16148680e+02 4.04107413e+01\n",
      " 1.00788620e+02 1.13781195e+02 8.58041281e+01 5.84897175e+01\n",
      " 1.85826950e+01 1.91913609e+01 0.00000000e+00 5.75740826e+01\n",
      " 1.41419067e+02 9.53772030e+01 8.67248502e+01 1.18888795e+02\n",
      " 7.17537721e+01 1.14529560e+02 8.90680280e+01 1.15431275e+02\n",
      " 7.51165982e+01 1.21320357e+02 7.75936555e+01]\n",
      "45-th iteration, loss: 0.0309979424263961, 320 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 51 layers\n",
      "[6.07507936e+01 1.23569403e+02 7.07278614e+01 8.81151482e+01\n",
      " 4.46911566e+01 7.83252219e+01 3.31801956e+01 4.55706209e+01\n",
      " 4.88364616e+01 8.74326319e+01 2.83488285e+01 6.42298149e+01\n",
      " 6.54311382e+01 1.05937580e+02 6.07080230e+01 9.10184398e+01\n",
      " 4.86540442e+01 4.42426778e+01 8.51104616e+01 4.49992773e+01\n",
      " 1.11778634e+02 4.85136854e-01 1.11891239e+02 4.68295557e+01\n",
      " 3.21590451e+01 0.00000000e+00 9.64771353e+01 1.25333834e+02\n",
      " 1.32127947e+02 1.24951184e+01 4.16107257e+02 4.04544942e+01\n",
      " 1.00764131e+02 1.13779870e+02 8.57947066e+01 5.85169142e+01\n",
      " 1.86021343e+01 1.92150782e+01 7.02491255e-04 5.75978133e+01\n",
      " 1.41477339e+02 9.54212087e+01 8.67357636e+01 1.18892001e+02\n",
      " 7.17698605e+01 1.14551172e+02 8.90766934e+01 1.15425566e+02\n",
      " 7.51213912e+01 1.21313315e+02 7.75897920e+01]\n",
      "46-th iteration, loss: 0.030795831381556043, 39 gd steps\n",
      "insert gradient: -0.005036484205911293\n",
      "46-th iteration, new layer inserted. now 49 layers\n",
      "[6.08037119e+01 1.23521055e+02 7.07497131e+01 8.81583903e+01\n",
      " 4.46621354e+01 7.82467804e+01 3.30422437e+01 4.56256393e+01\n",
      " 4.88910689e+01 8.74161213e+01 2.83711120e+01 6.43724862e+01\n",
      " 6.56303142e+01 1.05955522e+02 6.05885999e+01 9.09880248e+01\n",
      " 4.86521754e+01 4.43758383e+01 8.48549585e+01 4.51668903e+01\n",
      " 1.11649168e+02 5.29264465e-01 1.11765957e+02 4.70264019e+01\n",
      " 3.21926060e+01 2.47981737e-01 9.65104238e+01 1.25359913e+02\n",
      " 1.32160514e+02 1.26328091e+01 4.15813381e+02 4.08068614e+01\n",
      " 1.00556373e+02 1.13709022e+02 8.55472883e+01 5.85466517e+01\n",
      " 1.84612059e+01 7.69311591e+01 1.41681768e+02 9.56681923e+01\n",
      " 8.67242255e+01 1.18880035e+02 7.18464854e+01 1.14702258e+02\n",
      " 8.91042019e+01 1.15363975e+02 7.51348537e+01 1.21256255e+02\n",
      " 7.75531834e+01]\n",
      "47-th iteration, loss: 0.030785908033945174, 17 gd steps\n",
      "insert gradient: -0.0010763691519923451\n",
      "47-th iteration, new layer inserted. now 49 layers\n",
      "[6.08053441e+01 1.23519365e+02 7.07501339e+01 8.81599079e+01\n",
      " 4.46619112e+01 7.82448940e+01 3.30387530e+01 4.56274926e+01\n",
      " 4.88934541e+01 8.74165480e+01 2.83748600e+01 6.43794373e+01\n",
      " 6.56411211e+01 1.05959095e+02 6.05905926e+01 9.09898929e+01\n",
      " 4.86559494e+01 4.43817587e+01 8.48489840e+01 4.51738729e+01\n",
      " 1.11647727e+02 5.33309444e-01 1.11764660e+02 4.70334291e+01\n",
      " 3.21931693e+01 2.55067905e-01 9.65109712e+01 1.25360286e+02\n",
      " 1.32163167e+02 1.26383379e+01 4.15805425e+02 4.08199146e+01\n",
      " 1.00551969e+02 1.13708095e+02 8.55403829e+01 5.85481578e+01\n",
      " 1.84575672e+01 7.69333248e+01 1.41688190e+02 9.56761588e+01\n",
      " 8.67240436e+01 1.18879667e+02 7.18487041e+01 1.14706879e+02\n",
      " 8.91044910e+01 1.15361608e+02 7.51346448e+01 1.21254381e+02\n",
      " 7.75519791e+01]\n",
      "48-th iteration, loss: 0.021889638866148232, 74 gd steps\n",
      "insert gradient: -0.0020504348495228213\n",
      "48-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.1191042  119.99938322  68.92601856  88.15532798  41.32446667\n",
      "  73.77268362  28.74519915  51.91245693  50.91891736  83.92460665\n",
      "  27.11286317  74.45889719  71.77436989 101.25080156  57.78281776\n",
      "  90.20807487  46.68582888  57.79033317  72.69049979  57.95083588\n",
      " 106.85820013   2.60354797 108.47924961  62.18751975  26.81026458\n",
      "  15.9177819   90.34623627 116.50256737 137.48523771  26.16310226\n",
      " 396.33414325  72.06129401  92.4698223  114.17464755  81.84875849\n",
      "  63.33023251  11.63930025  80.64508169 142.32618659 107.97673067\n",
      "  84.79034369 116.80423205  75.57864187 124.95411038  83.69230668\n",
      " 109.7289523   77.32073116 118.49291833  79.42850337]\n",
      "49-th iteration, loss: 0.021779728333381793, 18 gd steps\n",
      "insert gradient: -0.006185876282161031\n",
      "49-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.05011716 119.99367517  68.93913684  88.16142148  41.32739882\n",
      "  73.71055924  28.6635886   51.97330131  50.86535885  83.87483779\n",
      "  27.18155182  74.54894305  71.73489179 101.22786402  57.85235827\n",
      "  90.19618639  46.62540841  57.87868942  72.50161811  58.08004794\n",
      " 106.87711588   2.69425334 108.52382503  62.29361749  26.6344855\n",
      "  15.98853494  90.18369289 116.36607069 137.55030655  26.44248414\n",
      " 396.12011817  72.31334702  92.32524601 114.22564346  81.82021747\n",
      "  63.31113223  11.63075019  80.65809635 142.28812506 108.03944164\n",
      "  84.82505484 116.85362581  75.70564669 125.11554678  83.65279863\n",
      " 109.71943557  77.47052375 118.48655718  79.56507098]\n",
      "50-th iteration, loss: 0.021778005583165658, 5 gd steps\n",
      "insert gradient: -0.0009622524691830856\n",
      "50-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.05001847 119.99371754  68.93932554  88.16157884  41.32775278\n",
      "  73.71064534  28.66364292  51.97350138  50.86554134  83.87501496\n",
      "  27.1822022   74.54929714  71.73490316 101.22758449  57.85257043\n",
      "  90.19645077  46.62607221  57.87940774  72.50229266  58.08087502\n",
      " 106.87807352   2.69516908 108.5248393   62.29448977  26.63553491\n",
      "  15.98958193  90.18463947 116.366658   137.55223083  26.44390512\n",
      " 396.12125673  72.31461547  92.32616725 114.22662442  81.8222868\n",
      "  63.31304337  11.6342989   80.65969193 142.291632   108.04091715\n",
      "  84.82648885 116.85421957  75.70640986 125.11603503  83.6528416\n",
      " 109.71943669  77.47085393 118.48653915  79.56537439]\n",
      "51-th iteration, loss: 0.021776710028050827, 5 gd steps\n",
      "insert gradient: -0.004701907536003369\n",
      "51-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.04989621 119.99373434  68.93944443  88.16166573  41.32793392\n",
      "  73.71062591  28.66355582  51.97362269  50.86553244  83.8750247\n",
      "  27.18247515  74.5494407   71.73446148 101.22705871  57.85229635\n",
      "  90.19644183  46.6262952   57.87991228  72.50265218  58.0815272\n",
      " 106.87869907   2.69580071 108.52551692  62.29511241  26.63618876\n",
      "  15.99044729  90.18517791 116.3668887  137.55324779  26.44494172\n",
      " 396.12147143  72.31512695  92.3255097  114.22609991  81.82043134\n",
      "  63.31106181  11.63067067  80.65819738 142.28810341 108.03973284\n",
      "  84.82524829 116.85381116  75.70616869 125.11615233  83.6525131\n",
      " 109.71929992  77.47104957 118.48647648  79.56564328]\n",
      "52-th iteration, loss: 0.021775681952494335, 5 gd steps\n",
      "insert gradient: -0.0008743161976997505\n",
      "52-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.04977746 119.99375456  68.93957152  88.1617597   41.32812245\n",
      "  73.71060242  28.66345767  51.97375176  50.86553172  83.87502613\n",
      "  27.18270499  74.54957241  71.73404891 101.22658896  57.85213937\n",
      "  90.19651575  46.62664981  57.8804628   72.50304044  58.08218264\n",
      " 106.87938088   2.69646944 108.52625553  62.29581444  26.63696039\n",
      "  15.9913463   90.18584432 116.36725873 137.55465955  26.44614941\n",
      " 396.12218298  72.31619493  92.32611047 114.22686173  81.82199407\n",
      "  63.31250965  11.63337548  80.65942059 142.29076196 108.04088764\n",
      "  84.82635433 116.85428268  75.70680522 125.11659499  83.65250752\n",
      " 109.71928497  77.47136217 118.48645342  79.56594068]\n",
      "53-th iteration, loss: 0.02177484644173087, 5 gd steps\n",
      "insert gradient: -0.0035576545101927144\n",
      "53-th iteration, new layer inserted. now 49 layers\n",
      "[ 64.04964631 119.99375905  68.93965146  88.16180666  41.3281989\n",
      "  73.710514    28.66328465  51.97384215  50.86542933  83.87494152\n",
      "  27.18276669  74.54962883  71.73351552 101.22606531  57.85182751\n",
      "  90.19646455  46.62676517  57.88088681  72.50324934  58.08273244\n",
      " 106.8798489    2.6969551  108.52677755  62.2963445   26.63745683\n",
      "  15.99212181  90.18622887 116.36737676 137.55542299  26.44708187\n",
      " 396.12222297  72.31669869  92.32552536 114.22648531  81.82058418\n",
      "  63.31100504  11.63063601  80.65830026 142.28808456 108.04002518\n",
      "  84.82543218 116.85399432  75.70668071 125.11675643  83.65222064\n",
      " 109.71916539  77.47157279 118.4863965   79.56621233]\n",
      "54-th iteration, loss: 0.01624464247992348, 64 gd steps\n",
      "insert gradient: -0.0029634080947190454\n",
      "54-th iteration, new layer inserted. now 51 layers\n",
      "[5.57179007e+01 1.18575555e+02 7.07562223e+01 8.56430160e+01\n",
      " 4.11392485e+01 7.07680840e+01 2.89715108e+01 6.06642890e+01\n",
      " 4.39034135e+01 7.62395542e+01 2.88305059e+01 8.48081731e+01\n",
      " 7.32496230e+01 9.07154825e+01 6.14186996e+01 8.84857141e+01\n",
      " 4.00589210e+01 7.03254826e+01 6.96474792e+01 7.21135030e+01\n",
      " 1.05248733e+02 1.91351510e+00 1.09806695e+02 7.38295367e+01\n",
      " 1.46399773e+01 3.44916357e+01 8.43623743e+01 1.00625633e+02\n",
      " 1.39989105e+02 5.23324873e+01 3.76677634e+02 1.03381373e+02\n",
      " 8.58431157e+01 1.20816738e+02 8.16536382e+01 5.68966835e+01\n",
      " 0.00000000e+00 3.55271368e-15 6.73805905e+00 7.58114973e+01\n",
      " 1.46216417e+02 1.16140571e+02 8.65142026e+01 1.17991814e+02\n",
      " 8.15005512e+01 1.32833971e+02 6.83770293e+01 1.06667577e+02\n",
      " 8.61801193e+01 1.12751461e+02 8.94111042e+01]\n",
      "55-th iteration, loss: 0.01587266189020298, 48 gd steps\n",
      "insert gradient: -0.001555889102609023\n",
      "55-th iteration, new layer inserted. now 53 layers\n",
      "[5.49779441e+01 1.18182403e+02 7.06933302e+01 8.59147602e+01\n",
      " 4.17482808e+01 7.10613541e+01 2.89332289e+01 6.07371311e+01\n",
      " 4.35588826e+01 7.55468500e+01 2.85127316e+01 8.57871239e+01\n",
      " 7.32060354e+01 9.09536842e+01 6.13667933e+01 8.82896013e+01\n",
      " 4.02813064e+01 7.11118912e+01 6.87715329e+01 7.24276869e+01\n",
      " 1.05431359e+02 6.07795955e-01 1.09825018e+02 7.44550231e+01\n",
      " 1.44619369e+01 3.53146066e+01 8.40737023e+01 1.00796846e+02\n",
      " 1.39859459e+02 5.47960453e+01 3.75704602e+02 1.03115178e+02\n",
      " 8.56536429e+01 1.21326980e+02 8.13185244e+01 5.79960627e+01\n",
      " 3.27597860e-04 1.09671069e+00 6.24476060e+00 7.66025448e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.45186502e+02 1.16347676e+02\n",
      " 8.72086605e+01 1.17545203e+02 8.12334807e+01 1.31435943e+02\n",
      " 6.99071437e+01 1.07610229e+02 8.64227863e+01 1.11657918e+02\n",
      " 8.86813892e+01]\n",
      "56-th iteration, loss: 0.01587238280839989, 4 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "56-th iteration, new layer inserted. now 55 layers\n",
      "[5.49778959e+01 1.18182356e+02 7.06933299e+01 8.59148133e+01\n",
      " 4.17483733e+01 7.10613877e+01 2.89331622e+01 6.07370963e+01\n",
      " 4.35588220e+01 7.55467739e+01 2.85126585e+01 8.57871950e+01\n",
      " 7.32058380e+01 9.09535940e+01 6.13666501e+01 8.82895608e+01\n",
      " 4.02812877e+01 7.11119280e+01 6.87713004e+01 7.24277368e+01\n",
      " 5.27157737e+01 0.00000000e+00 5.27157737e+01 6.07895924e-01\n",
      " 1.09825199e+02 7.44551076e+01 1.44618056e+01 3.53146048e+01\n",
      " 8.40736403e+01 1.00796898e+02 1.39859642e+02 5.47963705e+01\n",
      " 3.75704660e+02 1.03115270e+02 8.56538797e+01 1.21327258e+02\n",
      " 8.13190206e+01 5.79966759e+01 1.28181221e-03 1.09732396e+00\n",
      " 6.24574685e+00 7.66030689e+01 9.27553517e-04 5.24031879e-04\n",
      " 1.45187429e+02 1.16348127e+02 8.72091628e+01 1.17545363e+02\n",
      " 8.12336663e+01 1.31435900e+02 6.99073493e+01 1.07610358e+02\n",
      " 8.64228613e+01 1.11657845e+02 8.86813417e+01]\n",
      "57-th iteration, loss: 0.015872009171983302, 7 gd steps\n",
      "insert gradient: -0.0002818027271270301\n",
      "57-th iteration, new layer inserted. now 55 layers\n",
      "[5.49777415e+01 1.18182203e+02 7.06933123e+01 8.59149640e+01\n",
      " 4.17486264e+01 7.10614730e+01 2.89329335e+01 6.07369777e+01\n",
      " 4.35586050e+01 7.55465134e+01 2.85124057e+01 8.57874167e+01\n",
      " 7.32052350e+01 9.09533209e+01 6.13661970e+01 8.82894082e+01\n",
      " 4.02811684e+01 7.11120137e+01 6.87705273e+01 7.24278609e+01\n",
      " 5.27162841e+01 9.12967215e-04 5.27162841e+01 6.08132050e-01\n",
      " 1.09825688e+02 7.44553097e+01 1.44612795e+01 3.53145547e+01\n",
      " 8.40733213e+01 1.00796940e+02 1.39859895e+02 5.47972555e+01\n",
      " 3.75704508e+02 1.03115274e+02 8.56539435e+01 1.21327475e+02\n",
      " 8.13188419e+01 5.79968775e+01 1.18097092e-03 1.09752544e+00\n",
      " 6.24570679e+00 7.66032001e+01 3.65110141e-04 6.55155653e-04\n",
      " 1.45186493e+02 1.16348111e+02 8.72092818e+01 1.17545322e+02\n",
      " 8.12337020e+01 1.31435563e+02 6.99077842e+01 1.07610684e+02\n",
      " 8.64230183e+01 1.11657586e+02 8.86811706e+01]\n",
      "58-th iteration, loss: 0.015749819894827155, 56 gd steps\n",
      "insert gradient: -0.0028773905679557385\n",
      "58-th iteration, new layer inserted. now 51 layers\n",
      "[5.48183127e+01 1.18012196e+02 7.06624229e+01 8.60533767e+01\n",
      " 4.19679339e+01 7.11608264e+01 2.88123781e+01 6.07020252e+01\n",
      " 4.34404483e+01 7.53143236e+01 2.84347662e+01 8.62123233e+01\n",
      " 7.31252906e+01 9.10358691e+01 6.14198975e+01 8.82118802e+01\n",
      " 4.02191857e+01 7.13266938e+01 6.85384597e+01 7.25892930e+01\n",
      " 5.27876613e+01 6.34589243e-01 5.27815984e+01 2.08668315e-01\n",
      " 1.09874886e+02 7.46336304e+01 1.41500761e+01 3.55962977e+01\n",
      " 8.39941233e+01 1.00801817e+02 1.39728084e+02 5.56712608e+01\n",
      " 3.75473139e+02 1.03108936e+02 8.55803224e+01 1.21530398e+02\n",
      " 8.11562685e+01 5.97432826e+01 5.86607619e+00 7.70965435e+01\n",
      " 1.44913498e+02 1.16491387e+02 8.74755982e+01 1.17496158e+02\n",
      " 8.12148422e+01 1.30991788e+02 7.02401451e+01 1.07895875e+02\n",
      " 8.65022923e+01 1.11323779e+02 8.84347778e+01]\n",
      "59-th iteration, loss: 0.014626079450334242, 28 gd steps\n",
      "insert gradient: -0.008176133520535915\n",
      "59-th iteration, new layer inserted. now 51 layers\n",
      "[ 52.03188643 114.47953533  70.72861459  87.43510392  43.17894147\n",
      "  73.01174679  29.0022007   60.83780934  42.44418549  71.51958784\n",
      "  26.82874463  93.91869651  72.21940874  90.71704044  62.5322443\n",
      "  86.7929717   38.68814541  74.16824574  67.71222769  75.44120111\n",
      "  51.94649815   9.10466908 159.67870497  77.04070956  10.82432483\n",
      "  40.29028332  79.75536341  75.70189286   0.          25.23396429\n",
      " 137.57358731  68.61995933 372.66726319 101.92807762  85.63569982\n",
      " 124.42456134  79.0330685   66.72626052   1.83399453  82.11313805\n",
      " 141.7485645  119.82829686  90.15999206 116.96524769  80.790671\n",
      " 124.67824364  72.45918585 112.2626179   88.79961607 106.86102673\n",
      "  84.93335361]\n",
      "60-th iteration, loss: 0.014013860653105201, 33 gd steps\n",
      "insert gradient: -0.0002544756141350096\n",
      "60-th iteration, new layer inserted. now 51 layers\n",
      "[5.22952089e+01 1.14003002e+02 7.02501964e+01 8.71513349e+01\n",
      " 4.28782558e+01 7.32395477e+01 2.90200054e+01 6.04852209e+01\n",
      " 4.16434808e+01 7.08009598e+01 2.76863047e+01 9.53347269e+01\n",
      " 7.36452225e+01 9.22707409e+01 6.19933751e+01 8.66816532e+01\n",
      " 3.84011975e+01 7.44253666e+01 6.71651901e+01 7.55312303e+01\n",
      " 5.15686909e+01 1.03164775e+01 1.59419362e+02 7.74904107e+01\n",
      " 1.07719797e+01 4.05464493e+01 7.87050657e+01 7.61784097e+01\n",
      " 3.61768840e-03 2.57106103e+01 1.36645101e+02 6.97799608e+01\n",
      " 3.73027174e+02 1.02338793e+02 8.57038529e+01 1.25061122e+02\n",
      " 7.85930577e+01 6.75914199e+01 6.42174574e-01 8.28489349e+01\n",
      " 1.41756788e+02 1.20721625e+02 9.00193934e+01 1.17281066e+02\n",
      " 8.06312929e+01 1.24784884e+02 7.32460979e+01 1.12838593e+02\n",
      " 8.81389192e+01 1.06507280e+02 8.33962713e+01]\n",
      "61-th iteration, loss: 0.013953201648419138, 15 gd steps\n",
      "insert gradient: -0.00033172619773298343\n",
      "61-th iteration, new layer inserted. now 51 layers\n",
      "[5.22953539e+01 1.14000431e+02 7.02494948e+01 8.71558600e+01\n",
      " 4.28959077e+01 7.32548688e+01 2.90403662e+01 6.04893193e+01\n",
      " 4.16488604e+01 7.08089342e+01 2.77149716e+01 9.53414849e+01\n",
      " 7.36258445e+01 9.22652876e+01 6.19257972e+01 8.66610631e+01\n",
      " 3.83788458e+01 7.44172821e+01 6.71306475e+01 7.55241729e+01\n",
      " 5.15511861e+01 1.03103309e+01 1.59395672e+02 7.74906231e+01\n",
      " 1.07698875e+01 4.05443859e+01 7.86918067e+01 7.61845124e+01\n",
      " 2.90846627e-03 2.57167128e+01 1.36647223e+02 6.97922241e+01\n",
      " 3.73044106e+02 1.02344380e+02 8.57035370e+01 1.25060174e+02\n",
      " 7.85828512e+01 6.75920531e+01 5.98396453e-01 8.28492636e+01\n",
      " 1.41768030e+02 1.20718976e+02 9.00120633e+01 1.17276777e+02\n",
      " 8.06175380e+01 1.24780874e+02 7.32406518e+01 1.12836842e+02\n",
      " 8.81320537e+01 1.06504190e+02 8.33828196e+01]\n",
      "62-th iteration, loss: 0.013806249994886461, 22 gd steps\n",
      "insert gradient: -0.008689660534376106\n",
      "62-th iteration, new layer inserted. now 51 layers\n",
      "[5.23002053e+01 1.13789037e+02 6.99682520e+01 8.69591255e+01\n",
      " 4.27596592e+01 7.34710849e+01 2.93965776e+01 6.03414189e+01\n",
      " 4.09764652e+01 7.03362845e+01 2.83985697e+01 9.60286358e+01\n",
      " 7.35996599e+01 9.28136714e+01 6.19133891e+01 8.66734203e+01\n",
      " 3.81298707e+01 7.44140931e+01 6.69280958e+01 7.56021338e+01\n",
      " 5.13922867e+01 1.09319533e+01 1.59188478e+02 7.77563974e+01\n",
      " 1.10130754e+01 4.07617538e+01 7.82936009e+01 1.02204448e+02\n",
      " 1.36055665e+02 7.03297048e+01 3.73264252e+02 1.02558091e+02\n",
      " 8.58545937e+01 1.25431460e+02 7.82405820e+01 6.80931668e+01\n",
      " 4.96639887e-02 6.24936234e+01 0.00000000e+00 2.08312078e+01\n",
      " 1.41753435e+02 1.21001671e+02 8.99582182e+01 1.17410772e+02\n",
      " 8.02264599e+01 1.24878770e+02 7.36941698e+01 1.13142542e+02\n",
      " 8.81254592e+01 1.06417857e+02 8.23429246e+01]\n",
      "63-th iteration, loss: 0.01379468069187384, 6 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0020428774644223e-06\n",
      "63-th iteration, new layer inserted. now 51 layers\n",
      "[5.23001680e+01 1.13788980e+02 6.99681505e+01 8.69590356e+01\n",
      " 4.27594946e+01 7.34710309e+01 2.93965299e+01 6.03413408e+01\n",
      " 4.09761905e+01 7.03360706e+01 2.83983981e+01 9.60285741e+01\n",
      " 7.35993203e+01 9.28135953e+01 6.19129121e+01 8.66732811e+01\n",
      " 3.81295883e+01 7.44138651e+01 6.69273608e+01 7.56018976e+01\n",
      " 5.13920122e+01 1.09319068e+01 1.59188262e+02 7.77562799e+01\n",
      " 1.10127470e+01 4.07615653e+01 7.82931351e+01 1.02204223e+02\n",
      " 1.36055037e+02 7.03296192e+01 3.73264184e+02 1.02558326e+02\n",
      " 8.58553624e+01 1.25432378e+02 7.82429315e+01 6.80957070e+01\n",
      " 5.44978056e-02 6.24961605e+01 6.32049402e-03 2.08337449e+01\n",
      " 1.41759613e+02 1.21004007e+02 8.99605738e+01 1.17411672e+02\n",
      " 8.02272648e+01 1.24879116e+02 7.36945466e+01 1.13142698e+02\n",
      " 8.81255822e+01 1.06417888e+02 8.23427981e+01]\n",
      "64-th iteration, loss: 0.01379159716251097, 7 gd steps\n",
      "insert gradient: -3.894491624943339e-05\n",
      "64-th iteration, new layer inserted. now 53 layers\n",
      "[5.22999326e+01 1.13788611e+02 6.99675135e+01 8.69584892e+01\n",
      " 4.27585838e+01 7.34708361e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.93964585e+01 6.03408762e+01 4.09744676e+01 7.03347997e+01\n",
      " 2.83976715e+01 9.60283052e+01 7.35970634e+01 9.28130498e+01\n",
      " 6.19094469e+01 8.66721452e+01 3.81274068e+01 7.44122849e+01\n",
      " 6.69223969e+01 7.56002884e+01 5.13899189e+01 1.09313691e+01\n",
      " 1.59186553e+02 7.77552916e+01 1.10101403e+01 4.07601396e+01\n",
      " 7.82895561e+01 1.02202271e+02 1.36049683e+02 7.03283713e+01\n",
      " 3.73261596e+02 1.02557566e+02 8.58545335e+01 1.25432470e+02\n",
      " 7.82422486e+01 6.80963087e+01 5.44946422e-02 6.24967586e+01\n",
      " 4.50588556e-03 2.08343430e+01 1.41759616e+02 1.21004225e+02\n",
      " 8.99604932e+01 1.17411786e+02 8.02267155e+01 1.24879238e+02\n",
      " 7.36948702e+01 1.13142936e+02 8.81256060e+01 1.06417818e+02\n",
      " 8.23416969e+01]\n",
      "65-th iteration, loss: 0.013778769708302973, 17 gd steps\n",
      "insert gradient: -0.00018077049070376817\n",
      "65-th iteration, new layer inserted. now 53 layers\n",
      "[5.22954265e+01 1.13778997e+02 6.99570782e+01 8.69520645e+01\n",
      " 4.27618675e+01 7.34888441e+01 2.68787280e-02 1.79677964e-02\n",
      " 2.94233639e+01 6.03327328e+01 4.09393850e+01 7.03195608e+01\n",
      " 2.84516704e+01 9.60645714e+01 7.35929935e+01 9.28335602e+01\n",
      " 6.18923933e+01 8.66753323e+01 3.81189511e+01 7.44034556e+01\n",
      " 6.68639153e+01 7.55912675e+01 5.13795975e+01 1.09573333e+01\n",
      " 1.59183823e+02 7.77631948e+01 1.10097211e+01 4.07572540e+01\n",
      " 7.82524426e+01 1.02200161e+02 1.36015122e+02 7.03463201e+01\n",
      " 3.73264865e+02 1.02566203e+02 8.58682447e+01 1.25453816e+02\n",
      " 7.82254788e+01 6.81183080e+01 5.48709018e-02 6.25186225e+01\n",
      " 6.00502631e-04 2.08562065e+01 1.41736818e+02 1.21012088e+02\n",
      " 8.99589254e+01 1.17419067e+02 8.02105264e+01 1.24887401e+02\n",
      " 7.37180619e+01 1.13156809e+02 8.81317059e+01 1.06415881e+02\n",
      " 8.22964844e+01]\n",
      "66-th iteration, loss: 0.013765632693058973, 23 gd steps\n",
      "insert gradient: -0.00017329667647509643\n",
      "66-th iteration, new layer inserted. now 53 layers\n",
      "[5.22885495e+01 1.13764731e+02 6.99395027e+01 8.69390053e+01\n",
      " 4.27580664e+01 7.35104576e+01 6.00251615e-02 3.93156427e-02\n",
      " 2.94566850e+01 6.03169548e+01 4.08787756e+01 7.02908029e+01\n",
      " 2.85239381e+01 9.61179453e+01 7.35897568e+01 9.28673538e+01\n",
      " 6.18858090e+01 8.66890978e+01 3.81263868e+01 7.44075629e+01\n",
      " 6.68389849e+01 7.55924014e+01 5.13708754e+01 1.10016334e+01\n",
      " 1.59176883e+02 7.77795528e+01 1.10299368e+01 4.07708472e+01\n",
      " 7.82172542e+01 1.02201500e+02 1.35953275e+02 7.03716198e+01\n",
      " 3.73273801e+02 1.02579278e+02 8.58835868e+01 1.25481350e+02\n",
      " 7.82007042e+01 6.81489578e+01 2.80526443e-02 6.25491178e+01\n",
      " 8.78794528e-04 2.08867017e+01 1.41722226e+02 1.21022886e+02\n",
      " 8.99584665e+01 1.17428563e+02 8.01857967e+01 1.24897715e+02\n",
      " 7.37502590e+01 1.13175308e+02 8.81400033e+01 1.06412741e+02\n",
      " 8.22290079e+01]\n",
      "67-th iteration, loss: 0.013586475354653986, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "67-th iteration, new layer inserted. now 51 layers\n",
      "[5.22498504e+01 1.13614995e+02 6.97840674e+01 8.66715144e+01\n",
      " 4.25347058e+01 7.38179323e+01 5.15795983e-01 3.10322226e-01\n",
      " 2.99366509e+01 5.99529812e+01 3.95732805e+01 6.95888663e+01\n",
      " 2.95422712e+01 9.69743330e+01 7.32200750e+01 9.34895161e+01\n",
      " 6.17472428e+01 8.67539601e+01 3.79241026e+01 7.44794947e+01\n",
      " 6.67837115e+01 7.58143705e+01 5.12290997e+01 1.17419809e+01\n",
      " 7.94175286e+01 0.00000000e+00 7.94175286e+01 7.80615383e+01\n",
      " 1.10544405e+01 4.10238069e+01 7.78499616e+01 1.02375069e+02\n",
      " 1.35341049e+02 7.09966915e+01 3.73375120e+02 1.02853738e+02\n",
      " 8.59123928e+01 1.25851543e+02 7.75904352e+01 1.52425544e+02\n",
      " 1.41607267e+02 1.21033959e+02 8.98893220e+01 1.17606430e+02\n",
      " 7.97193045e+01 1.25074763e+02 7.42958402e+01 1.13470408e+02\n",
      " 8.82645520e+01 1.06398935e+02 8.09955259e+01]\n",
      "68-th iteration, loss: 0.012244195154277741, 27 gd steps\n",
      "insert gradient: -0.025716968534919577\n",
      "68-th iteration, new layer inserted. now 53 layers\n",
      "[4.67065786e+01 1.11266677e+02 7.40777979e+01 8.05754407e+01\n",
      " 3.64039729e+01 7.74612501e+01 7.12657259e+00 4.66242603e-05\n",
      " 3.82530892e+01 6.07788103e+01 3.12688727e+01 5.39911220e+01\n",
      " 3.38968720e+01 1.07736201e+02 6.89515348e+01 1.01761078e+02\n",
      " 6.04994666e+01 8.66038447e+01 3.24943437e+01 8.19716462e+01\n",
      " 6.56522592e+01 8.21156500e+01 4.49390500e+01 1.90186980e+01\n",
      " 7.47387759e+01 1.06329033e+01 7.76465731e+01 8.35452001e+01\n",
      " 1.16697025e+01 4.07293402e+01 6.52082515e+01 1.12164756e+02\n",
      " 1.35687879e+02 8.42450071e+01 3.66395432e+02 1.06109871e+02\n",
      " 8.26062681e+01 1.35246221e+02 7.48954164e+01 1.15459834e+02\n",
      " 0.00000000e+00 3.84866113e+01 1.38897005e+02 1.28041094e+02\n",
      " 8.67991597e+01 1.27543853e+02 7.25772020e+01 1.25638882e+02\n",
      " 7.88352261e+01 1.17995436e+02 8.74937989e+01 1.11329531e+02\n",
      " 7.25452621e+01]\n",
      "69-th iteration, loss: 0.011549460296033463, 253 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "69-th iteration, new layer inserted. now 51 layers\n",
      "[ 46.33413132 110.54298457  74.88839314  80.70178732  37.75058942\n",
      "  78.17155784  44.70618784  60.57969037  30.36951618  54.20895737\n",
      "  35.70131307 107.42515324  69.64861782 102.34180841  59.19681749\n",
      "  85.72741911  32.57521023  81.96249944  65.05320528  83.24780457\n",
      "  46.3376618   19.5541362   73.49614421  12.25544881  77.20296938\n",
      "  83.97463458   9.75348728  40.21819873  65.86265694 111.97392279\n",
      " 137.39784415  85.42068102 367.58011715 106.73509278  83.28221966\n",
      " 135.84592316  74.28456817 154.23918357  69.17068607   0.\n",
      "  69.17068607 127.91804907  87.26978376 127.5864469   73.82626921\n",
      " 125.91254918  79.00483077 117.84468963  88.07078569 111.64054155\n",
      "  72.89256243]\n",
      "70-th iteration, loss: 0.011548966719381826, 5 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "70-th iteration, new layer inserted. now 53 layers\n",
      "[4.63341124e+01 1.10542925e+02 7.48884334e+01 8.07017764e+01\n",
      " 3.77506397e+01 7.81715957e+01 4.47061802e+01 6.05797044e+01\n",
      " 3.03694933e+01 5.42089702e+01 3.57014092e+01 1.07425117e+02\n",
      " 6.96486302e+01 1.02341833e+02 5.91967014e+01 8.57273637e+01\n",
      " 3.25752378e+01 8.19625018e+01 6.50531050e+01 8.32478792e+01\n",
      " 4.63377552e+01 1.95541713e+01 7.34960567e+01 1.22555851e+01\n",
      " 7.72029447e+01 8.39746866e+01 9.75340176e+00 4.02181729e+01\n",
      " 6.58627263e+01 1.11973901e+02 6.86989865e+01 0.00000000e+00\n",
      " 6.86989865e+01 8.54207674e+01 3.67580203e+02 1.06735178e+02\n",
      " 8.32824160e+01 1.35846125e+02 7.42849554e+01 1.54239650e+02\n",
      " 6.91717459e+01 1.15274606e-03 6.91717459e+01 1.27918459e+02\n",
      " 8.72702165e+01 1.27586587e+02 7.38264917e+01 1.25912611e+02\n",
      " 7.90048903e+01 1.17844684e+02 8.80708280e+01 1.11640567e+02\n",
      " 7.28925925e+01]\n",
      "71-th iteration, loss: 0.010398390772267176, 26 gd steps\n",
      "insert gradient: -0.0030059222494131614\n",
      "71-th iteration, new layer inserted. now 55 layers\n",
      "[4.88268531e+01 9.80812785e+01 8.33952755e+01 6.62368258e+01\n",
      " 3.80991733e+01 8.46368327e+01 4.38720697e+01 6.52566389e+01\n",
      " 2.81297804e+01 4.77629340e+01 4.36020562e+01 1.01093540e+02\n",
      " 7.02403127e+01 1.05397961e+02 5.79214945e+01 7.59641462e+01\n",
      " 3.57546749e+01 8.96818830e+01 5.95712872e+01 9.38921771e+01\n",
      " 5.14163282e+01 2.53163995e+01 5.08277003e+01 2.21049628e+01\n",
      " 8.19759871e+01 9.43574849e+01 8.82965009e+00 2.93547664e+01\n",
      " 7.37388973e+01 9.38015937e+01 6.85140598e+01 1.91478116e+00\n",
      " 7.10622987e+01 9.10552497e+01 3.67298724e+02 1.07299328e+02\n",
      " 8.11349408e+01 1.35183274e+02 7.50630174e+01 1.57113421e+02\n",
      " 6.66031625e+01 0.00000000e+00 7.10542736e-15 4.41570137e+00\n",
      " 6.52997723e+01 1.28808220e+02 8.74392908e+01 1.26128639e+02\n",
      " 7.80910856e+01 1.25705995e+02 7.87944490e+01 1.20542238e+02\n",
      " 8.70345563e+01 1.16148463e+02 7.11077780e+01]\n",
      "72-th iteration, loss: 0.010363427372363012, 19 gd steps\n",
      "insert gradient: -0.00020339076543322826\n",
      "72-th iteration, new layer inserted. now 55 layers\n",
      "[4.88304811e+01 9.80819724e+01 8.33942378e+01 6.62417635e+01\n",
      " 3.81150461e+01 8.46458991e+01 4.38871311e+01 6.52640803e+01\n",
      " 2.81425906e+01 4.77730383e+01 4.36339862e+01 1.01120460e+02\n",
      " 7.02762535e+01 1.05425530e+02 5.79995561e+01 7.59830424e+01\n",
      " 3.57720827e+01 8.96855316e+01 5.95895300e+01 9.39042681e+01\n",
      " 5.14339371e+01 2.53313799e+01 5.08371842e+01 2.21181289e+01\n",
      " 8.19794398e+01 9.43639195e+01 8.83509242e+00 2.93589487e+01\n",
      " 7.37532405e+01 9.38078328e+01 6.85220278e+01 1.93455961e+00\n",
      " 7.10706430e+01 9.10594983e+01 3.67302594e+02 1.07304065e+02\n",
      " 8.11427876e+01 1.35188580e+02 7.50623758e+01 1.57113854e+02\n",
      " 6.66048447e+01 2.53870579e-03 1.73923562e-03 4.41824102e+00\n",
      " 6.52999973e+01 1.28806914e+02 8.74389110e+01 1.26132478e+02\n",
      " 7.80917174e+01 1.25710836e+02 7.87962013e+01 1.20545332e+02\n",
      " 8.70362823e+01 1.16150802e+02 7.11083792e+01]\n",
      "73-th iteration, loss: 0.010363117954632753, 5 gd steps\n",
      "insert gradient: -0.00024122117679861523\n",
      "73-th iteration, new layer inserted. now 55 layers\n",
      "[4.88305363e+01 9.80819823e+01 8.33942167e+01 6.62418298e+01\n",
      " 3.81152468e+01 8.46459952e+01 4.38872390e+01 6.52641025e+01\n",
      " 2.81425544e+01 4.77730209e+01 4.36340125e+01 1.01120459e+02\n",
      " 7.02761146e+01 1.05425474e+02 5.79994362e+01 7.59829722e+01\n",
      " 3.57719739e+01 8.96854493e+01 5.95893339e+01 9.39042417e+01\n",
      " 5.14339824e+01 2.53314764e+01 5.08371998e+01 2.21182237e+01\n",
      " 8.19793809e+01 9.43639621e+01 8.83511402e+00 2.93589632e+01\n",
      " 7.37533258e+01 9.38078776e+01 6.85220515e+01 1.93478889e+00\n",
      " 7.10706693e+01 9.10595240e+01 3.67302602e+02 1.07304091e+02\n",
      " 8.11427859e+01 1.35188555e+02 7.50620835e+01 1.57113586e+02\n",
      " 6.66042069e+01 1.90404732e-03 1.10140472e-03 4.41760637e+00\n",
      " 6.52993340e+01 1.28806639e+02 8.74386459e+01 1.26132436e+02\n",
      " 7.80916302e+01 1.25710873e+02 7.87961963e+01 1.20545370e+02\n",
      " 8.70363016e+01 1.16150834e+02 7.11083837e+01]\n",
      "74-th iteration, loss: 0.010090928794396064, 25 gd steps\n",
      "insert gradient: -0.0005187102459326368\n",
      "74-th iteration, new layer inserted. now 53 layers\n",
      "[ 49.5100375   97.86206185  82.50979826  66.23054542  38.69920771\n",
      "  84.80261589  44.37849346  65.69927546  27.70640545  47.02471368\n",
      "  44.02498208 101.70273063  70.2862073  105.71335629  57.53323386\n",
      "  75.65527658  35.86504159  89.6532337   59.06569946  94.15650901\n",
      "  52.03815019  26.68813611  50.27415356  24.62764174  80.79657341\n",
      "  94.81744568   7.58686193  28.61788275  74.91019658  94.12152295\n",
      "  67.97690439   4.31083231  70.68937262  90.98221705 366.26419992\n",
      " 107.96467531  82.72096367 136.32596055  74.52125712 156.74814447\n",
      "  66.56857633   5.08809288  64.81651587 128.29406088  86.89683282\n",
      " 127.03252488  77.92325931 126.84532213  78.72477224 121.20017654\n",
      "  86.84137275 116.71151336  71.05186686]\n",
      "75-th iteration, loss: 0.010028806621125822, 19 gd steps\n",
      "insert gradient: -0.0012533065939441914\n",
      "75-th iteration, new layer inserted. now 53 layers\n",
      "[ 49.4939487   97.8061033   82.60726869  66.40883151  39.02495153\n",
      "  84.81330027  44.32263016  65.77808461  27.60033303  46.87310927\n",
      "  44.12379839 101.82444326  70.38421781 105.79340229  57.3753469\n",
      "  75.52430289  35.93487909  89.70979369  59.08188233  94.2155546\n",
      "  52.07620776  27.00927061  49.98114043  25.45652514  80.57442398\n",
      "  94.95409735   7.08592648  28.35326938  75.38456011  94.18585804\n",
      "  67.77181093   4.84371029  70.60113975  91.03235187 366.23604767\n",
      " 108.04296802  82.79156451 136.38940958  74.28712878 156.71082789\n",
      "  66.55141363   5.26866884  64.68927716 128.33309325  86.85600211\n",
      " 127.24431233  77.84526486 126.98408952  78.76721253 121.34652643\n",
      "  86.9119705  116.96213878  71.16101927]\n",
      "76-th iteration, loss: 0.009317181054129521, 27 gd steps\n",
      "insert gradient: -6.751656398651074e-05\n",
      "76-th iteration, new layer inserted. now 53 layers\n",
      "[5.02901494e+01 9.51878270e+01 0.00000000e+00 7.10542736e-15\n",
      " 8.33277757e+01 6.51397988e+01 4.08866562e+01 8.37389907e+01\n",
      " 4.38596597e+01 6.84926345e+01 2.77256380e+01 4.16943898e+01\n",
      " 4.55871910e+01 1.04574330e+02 7.05069959e+01 1.07109804e+02\n",
      " 5.55845907e+01 7.17038513e+01 3.91538537e+01 9.02922518e+01\n",
      " 5.83271365e+01 9.44025029e+01 5.22752839e+01 3.93141837e+01\n",
      " 3.91236669e+01 3.89330669e+01 7.46665045e+01 1.21011397e+02\n",
      " 8.91136698e+01 9.09163690e+01 6.10282738e+01 7.42557621e+00\n",
      " 7.41798302e+01 9.62279290e+01 3.64400262e+02 1.08748650e+02\n",
      " 8.25924288e+01 1.36774111e+02 7.49604088e+01 1.56247892e+02\n",
      " 6.87920427e+01 8.25911737e+00 6.05699013e+01 1.25763729e+02\n",
      " 8.40953896e+01 1.33639487e+02 7.66436149e+01 1.29811882e+02\n",
      " 7.69369160e+01 1.25910781e+02 8.40083465e+01 1.23311930e+02\n",
      " 7.14189527e+01]\n",
      "77-th iteration, loss: 0.009289816045826547, 12 gd steps\n",
      "insert gradient: -0.0010959418641516908\n",
      "77-th iteration, new layer inserted. now 53 layers\n",
      "[5.02901433e+01 9.51881679e+01 2.09434019e-03 3.40923412e-04\n",
      " 8.33298701e+01 6.51414327e+01 4.08892971e+01 8.37396476e+01\n",
      " 4.38588397e+01 6.84923394e+01 2.77240566e+01 4.16914621e+01\n",
      " 4.55753544e+01 1.04562131e+02 7.04893889e+01 1.07095297e+02\n",
      " 5.55473304e+01 7.16941015e+01 3.91450866e+01 9.02901614e+01\n",
      " 5.83187297e+01 9.43975893e+01 5.22690248e+01 3.93119144e+01\n",
      " 3.91200041e+01 3.89326778e+01 7.46650967e+01 1.21010224e+02\n",
      " 8.91119846e+01 9.09157850e+01 6.10276507e+01 7.42654729e+00\n",
      " 7.41796177e+01 9.62276454e+01 3.64399234e+02 1.08748073e+02\n",
      " 8.25910742e+01 1.36772810e+02 7.49575945e+01 1.56244697e+02\n",
      " 6.87859298e+01 8.25314872e+00 6.05634503e+01 1.25761559e+02\n",
      " 8.40927785e+01 1.33639020e+02 7.66431454e+01 1.29811687e+02\n",
      " 7.69371903e+01 1.25911093e+02 8.40087336e+01 1.23312160e+02\n",
      " 7.14187636e+01]\n",
      "78-th iteration, loss: 0.009118131998257527, 39 gd steps\n",
      "insert gradient: -0.002575628503685781\n",
      "78-th iteration, new layer inserted. now 53 layers\n",
      "[4.97402243e+01 9.43025546e+01 8.38544749e+01 6.53268561e+01\n",
      " 4.16636676e+01 8.38093577e+01 4.35815250e+01 6.91321360e+01\n",
      " 2.79353238e+01 4.00918386e+01 4.58874762e+01 1.04591209e+02\n",
      " 7.08209731e+01 1.07440171e+02 5.50633339e+01 7.15827973e+01\n",
      " 3.94364984e+01 9.06616636e+01 5.81770079e+01 9.46797199e+01\n",
      " 5.23724486e+01 4.25571102e+01 3.51866708e+01 4.27524361e+01\n",
      " 7.41040718e+01 1.21106719e+02 8.93086006e+01 9.03926979e+01\n",
      " 5.80449422e+01 1.10538924e+01 7.38823601e+01 9.66510056e+01\n",
      " 3.64373744e+02 1.09405027e+02 8.29794339e+01 1.36065187e+02\n",
      " 7.54643620e+01 1.53759217e+02 6.95435743e+01 9.02531648e+00\n",
      " 0.00000000e+00 8.88178420e-16 5.98416851e+01 1.26530916e+02\n",
      " 8.24778742e+01 1.35603453e+02 7.69645188e+01 1.29847992e+02\n",
      " 7.74127632e+01 1.26791035e+02 8.38401391e+01 1.23956224e+02\n",
      " 7.00594879e+01]\n",
      "79-th iteration, loss: 0.009085924721140714, 36 gd steps\n",
      "insert gradient: -0.0012647695660396943\n",
      "79-th iteration, new layer inserted. now 51 layers\n",
      "[ 49.77483743  94.12533468  83.67192737  65.45102958  42.0032555\n",
      "  83.90699603  43.53709267  69.21927104  27.91968354  39.775082\n",
      "  45.9135016  104.60305312  71.08422643 107.54068026  54.87724831\n",
      "  71.57946042  39.5930195   90.78017209  58.14253882  94.70073014\n",
      "  52.35533624  43.07377742  34.47923351  43.51127611  74.09060874\n",
      " 121.18401277  89.37025328  90.35734971  57.47515784  11.63650333\n",
      "  74.08907394  96.88864939 364.53474509 109.47107995  83.01075807\n",
      " 135.77734073  75.37976424 153.35664762  69.61209016   9.44585229\n",
      "  59.60199382 126.61475608  82.18663328 135.99756651  76.93121359\n",
      " 129.86251238  77.44223136 126.87827373  83.85828859 124.22067322\n",
      "  70.23737145]\n",
      "80-th iteration, loss: 0.009080417188100057, 27 gd steps\n",
      "insert gradient: -6.474991097910019e-05\n",
      "80-th iteration, new layer inserted. now 51 layers\n",
      "[ 49.77535919  94.12539228  83.67280323  65.45204274  42.00483679\n",
      "  83.90751428  43.53728758  69.22025123  27.92093377  39.77341932\n",
      "  45.90681981 104.59565721  71.07337256 107.53115878  54.85307395\n",
      "  71.57311429  39.58732821  90.77861375  58.13619423  94.69731157\n",
      "  52.35140529  43.07369637  34.47636004  43.5127428   74.09030015\n",
      " 121.18395973  89.37045276  90.35763239  57.47494107  11.6389766\n",
      "  74.09067546  96.88972339 364.53601747 109.47169879  83.01186076\n",
      " 135.77732126  75.38072966 153.35659443  69.6141022    9.44810809\n",
      "  59.60324325 126.61566265  82.18678067 135.99899828  76.93169929\n",
      " 129.86282055  77.44268583 126.87854108  83.85842952 124.22126566\n",
      "  70.23761687]\n",
      "81-th iteration, loss: 0.008715919093224419, 24 gd steps\n",
      "insert gradient: -0.008216386372609777\n",
      "81-th iteration, new layer inserted. now 53 layers\n",
      "[4.73138984e+01 8.92922828e+01 8.57768175e+01 6.50131472e+01\n",
      " 4.31583029e+01 8.40975293e+01 4.24563835e+01 7.01509740e+01\n",
      " 2.89028299e+01 3.18748788e+01 4.98717481e+01 1.07214829e+02\n",
      " 7.21327159e+01 1.05957061e+02 5.39214352e+01 6.71904905e+01\n",
      " 4.42999308e+01 9.01333434e+01 5.78166696e+01 9.19156670e+01\n",
      " 4.86463791e+01 6.28118744e+01 2.03227970e+01 5.97970579e+01\n",
      " 7.05119726e+01 1.27081545e+02 8.90504300e+01 9.45721057e+01\n",
      " 4.12856984e+01 2.65950276e+01 7.40531340e+01 1.00121333e+02\n",
      " 3.64520379e+02 1.12485612e+02 8.37798681e+01 1.29855138e+02\n",
      " 7.74509578e+01 1.45308565e+02 7.77356663e+01 1.65585167e+01\n",
      " 0.00000000e+00 1.77635684e-15 4.99318927e+01 1.25262555e+02\n",
      " 7.64556210e+01 1.43328927e+02 7.65875273e+01 1.31369559e+02\n",
      " 7.84971717e+01 1.28153989e+02 8.32945200e+01 1.27031360e+02\n",
      " 7.14944546e+01]\n",
      "82-th iteration, loss: 0.008650230271461647, 27 gd steps\n",
      "insert gradient: -0.00022835876549735969\n",
      "82-th iteration, new layer inserted. now 53 layers\n",
      "[4.76231859e+01 8.93794995e+01 8.57356144e+01 6.50538426e+01\n",
      " 4.32912388e+01 8.41732887e+01 4.25656100e+01 7.01684080e+01\n",
      " 2.88166214e+01 3.17947825e+01 4.98678961e+01 1.07213401e+02\n",
      " 7.21045383e+01 1.05987592e+02 5.39917659e+01 6.72639632e+01\n",
      " 4.43468402e+01 9.01447579e+01 5.77501193e+01 9.19318214e+01\n",
      " 4.87071382e+01 6.27931965e+01 2.01245311e+01 5.98072970e+01\n",
      " 7.04968099e+01 1.27102263e+02 8.90440959e+01 9.46021673e+01\n",
      " 4.12550123e+01 2.66109279e+01 7.40272535e+01 1.00177197e+02\n",
      " 3.64592503e+02 1.12543322e+02 8.38876516e+01 1.29937265e+02\n",
      " 7.75616427e+01 1.45274457e+02 7.77503495e+01 1.65435276e+01\n",
      " 3.52931948e-05 6.18021517e-04 4.98567519e+01 1.25272361e+02\n",
      " 7.65849859e+01 1.43392505e+02 7.66216151e+01 1.31346334e+02\n",
      " 7.83871938e+01 1.28113744e+02 8.32381001e+01 1.27022842e+02\n",
      " 7.15363450e+01]\n",
      "83-th iteration, loss: 0.008632039040505522, 21 gd steps\n",
      "insert gradient: -3.4924523760891543e-05\n",
      "83-th iteration, new layer inserted. now 55 layers\n",
      "[4.80359961e+01 8.94582124e+01 8.55746196e+01 6.50608523e+01\n",
      " 4.33821304e+01 8.42328165e+01 4.26925503e+01 7.02294705e+01\n",
      " 2.87971678e+01 3.16983987e+01 4.98527254e+01 1.07188218e+02\n",
      " 7.20980601e+01 1.06017994e+02 5.40011087e+01 6.73605623e+01\n",
      " 4.43778046e+01 9.01622364e+01 5.76526390e+01 9.19772916e+01\n",
      " 4.88664683e+01 6.28301926e+01 1.99522507e+01 5.98991218e+01\n",
      " 7.05508991e+01 1.27172359e+02 8.90622118e+01 9.46538005e+01\n",
      " 4.12184267e+01 2.66791182e+01 7.39948811e+01 1.00264253e+02\n",
      " 2.73496679e+02 0.00000000e+00 9.11655597e+01 1.12586482e+02\n",
      " 8.39935474e+01 1.30013759e+02 7.76767437e+01 1.45174906e+02\n",
      " 7.77483170e+01 1.65480960e+01 3.16810902e-03 6.07135417e-03\n",
      " 4.97318813e+01 1.25285012e+02 7.67825547e+01 1.43492371e+02\n",
      " 7.66674086e+01 1.31295348e+02 7.81957366e+01 1.28036609e+02\n",
      " 8.31444512e+01 1.27003985e+02 7.16282166e+01]\n",
      "84-th iteration, loss: 0.008613596042623411, 26 gd steps\n",
      "insert gradient: -0.00026381475554872755\n",
      "84-th iteration, new layer inserted. now 53 layers\n",
      "[ 48.32076277  89.49667695  85.44153764  65.06650699  43.43054052\n",
      "  84.25370909  42.74552819  70.27343511  28.79884437  31.61841756\n",
      "  49.81418102 107.14784143  72.11613845 106.04641966  53.97480346\n",
      "  67.44009211  44.37587099  90.16544697  57.54615358  92.01453907\n",
      "  49.00406462  62.87600831  19.84087482  59.99457298  70.57949796\n",
      " 127.21585465  89.03659705  94.66569173  41.14536254  26.75021996\n",
      "  73.94170019 100.31156544 273.45245571   0.37432272  91.12436661\n",
      " 112.56500637  84.02599501 130.03788233  77.74112065 145.06846306\n",
      "  77.72879966  16.61271694  49.62309124 125.29738186  76.9471798\n",
      " 143.58135225  76.71116771 131.25748815  78.06074309 127.98056774\n",
      "  83.09417885 126.99481365  71.72603934]\n",
      "85-th iteration, loss: 0.008613466347618061, 12 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "85-th iteration, new layer inserted. now 55 layers\n",
      "[ 48.32169509  89.49677076  85.44103993  65.06656104  43.43064598\n",
      "  84.25361857  42.74518367  70.27314315  28.79777556  31.61757261\n",
      "  49.81348297 107.14745715  72.1159208  106.04652996  53.97475981\n",
      "  67.44037103  44.37560663  90.16524715  57.54535677  92.01461888\n",
      "  49.00454782  62.87614285  19.84024979  59.99484017  70.57928425\n",
      " 127.21584576  89.0363291   94.66571036  41.1452353   26.75072907\n",
      "  73.94165155 100.31180429 136.72611149   0.         136.72611149\n",
      "   0.3758163   91.12419429 112.56493202  84.02620016 130.038037\n",
      "  77.74155844 145.06825955  77.72921144  16.61340143  49.6232564\n",
      " 125.29765306  76.94802024 143.5817811   76.71140297 131.25740054\n",
      "  78.06034165 127.98040849  83.09407979 126.99480901  71.72644135]\n",
      "86-th iteration, loss: 0.008005129288116418, 40 gd steps\n",
      "insert gradient: -0.00019057864517018624\n",
      "86-th iteration, new layer inserted. now 57 layers\n",
      "[4.89022863e+01 8.68340140e+01 8.64449124e+01 6.46041887e+01\n",
      " 4.38707500e+01 8.37913143e+01 4.31489862e+01 7.09933006e+01\n",
      " 2.95504381e+01 2.88211489e+01 5.05933764e+01 1.06976445e+02\n",
      " 7.14183022e+01 1.06478600e+02 5.39526151e+01 6.82409744e+01\n",
      " 4.36625280e+01 9.05800373e+01 5.72812191e+01 9.23487880e+01\n",
      " 4.91940686e+01 6.25253130e+01 1.91365825e+01 6.49066542e+01\n",
      " 6.82022539e+01 1.30713717e+02 8.80056588e+01 9.35720319e+01\n",
      " 3.59793768e+01 3.68157791e+01 7.11067387e+01 1.00598890e+02\n",
      " 1.24282095e+02 1.54078108e+01 1.29070882e+02 1.23668991e+01\n",
      " 9.05893486e+01 1.11577456e+02 8.84594061e+01 1.28805905e+02\n",
      " 8.03979582e+01 1.33378194e+02 8.23286779e+01 1.99435482e+01\n",
      " 0.00000000e+00 1.77635684e-15 4.50142412e+01 1.24879225e+02\n",
      " 7.97926610e+01 1.43701943e+02 7.61225982e+01 1.30836039e+02\n",
      " 7.73396850e+01 1.31635531e+02 8.50005156e+01 1.26522558e+02\n",
      " 7.45752901e+01]\n",
      "87-th iteration, loss: 0.007933896134140226, 42 gd steps\n",
      "insert gradient: -9.377729548408871e-05\n",
      "87-th iteration, new layer inserted. now 57 layers\n",
      "[4.90333824e+01 8.68725799e+01 8.64850843e+01 6.48700589e+01\n",
      " 4.41798263e+01 8.37442073e+01 4.29950384e+01 7.11253643e+01\n",
      " 2.97310495e+01 2.84318775e+01 5.04569570e+01 1.06975940e+02\n",
      " 7.14059553e+01 1.06640304e+02 5.39629830e+01 6.85452256e+01\n",
      " 4.38687855e+01 9.05173425e+01 5.66309958e+01 9.25794066e+01\n",
      " 4.97028839e+01 6.25530383e+01 1.90376284e+01 6.51732598e+01\n",
      " 6.81261041e+01 1.30574152e+02 8.78638607e+01 9.34584714e+01\n",
      " 3.58587526e+01 3.72793412e+01 7.12171588e+01 1.00680178e+02\n",
      " 1.23923600e+02 1.58357342e+01 1.28731510e+02 1.30385036e+01\n",
      " 9.09153479e+01 1.11593316e+02 8.79905794e+01 1.28728680e+02\n",
      " 8.05447452e+01 1.33383299e+02 8.24423508e+01 1.98816426e+01\n",
      " 1.92636218e-03 2.02301693e-03 4.46937433e+01 1.24894847e+02\n",
      " 8.02131326e+01 1.43902981e+02 7.66550086e+01 1.31052504e+02\n",
      " 7.71957109e+01 1.31662277e+02 8.45410239e+01 1.26446978e+02\n",
      " 7.45152328e+01]\n",
      "88-th iteration, loss: 0.007856464736969255, 60 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "88-th iteration, new layer inserted. now 59 layers\n",
      "[4.90365849e+01 8.68238017e+01 8.63727144e+01 6.53701064e+01\n",
      " 4.45764905e+01 8.35490757e+01 4.27039657e+01 7.14889049e+01\n",
      " 3.01391654e+01 2.76325542e+01 5.03401543e+01 1.07079646e+02\n",
      " 7.13709547e+01 1.06817760e+02 5.37597074e+01 6.89594954e+01\n",
      " 4.40993388e+01 9.04931693e+01 5.59127496e+01 9.30481518e+01\n",
      " 5.04734295e+01 6.24843187e+01 1.89287425e+01 6.57113683e+01\n",
      " 6.80676944e+01 1.30452073e+02 8.78222119e+01 9.31848796e+01\n",
      " 3.55198488e+01 3.81802459e+01 7.12144173e+01 1.00661838e+02\n",
      " 1.23080035e+02 1.65363634e+01 8.52742742e+01 0.00000000e+00\n",
      " 4.26371371e+01 1.43492410e+01 9.15132473e+01 1.11632847e+02\n",
      " 8.74687614e+01 1.28679460e+02 8.10217623e+01 1.33439421e+02\n",
      " 8.27552293e+01 1.98701117e+01 3.08532668e-03 1.44559589e-02\n",
      " 4.40387154e+01 1.24801446e+02 8.07725250e+01 1.43991239e+02\n",
      " 7.72070968e+01 1.31201972e+02 7.69724907e+01 1.31888320e+02\n",
      " 8.40957081e+01 1.26454166e+02 7.46476282e+01]\n",
      "89-th iteration, loss: 0.007856019733957222, 5 gd steps\n",
      "insert gradient: -0.0003208961534766031\n",
      "89-th iteration, new layer inserted. now 61 layers\n",
      "[4.90365773e+01 8.68237918e+01 8.63726855e+01 6.53701575e+01\n",
      " 4.45764958e+01 8.35490098e+01 4.27038022e+01 7.14888153e+01\n",
      " 3.01388805e+01 2.76322997e+01 5.03400092e+01 1.07079622e+02\n",
      " 7.13708724e+01 1.06817808e+02 5.37597811e+01 6.89595273e+01\n",
      " 4.40992423e+01 9.04930516e+01 5.59124660e+01 9.30481399e+01\n",
      " 5.04734369e+01 6.24842453e+01 1.89285570e+01 6.57113404e+01\n",
      " 6.80675047e+01 1.30451943e+02 8.78220464e+01 9.31847650e+01\n",
      " 3.55196923e+01 3.81802983e+01 7.12143081e+01 1.00661793e+02\n",
      " 1.23079843e+02 1.65363717e+01 8.52740539e+01 1.50404489e-04\n",
      " 4.26369168e+01 1.43492949e+01 9.15131629e+01 1.11632745e+02\n",
      " 8.74684771e+01 1.28679236e+02 8.10212245e+01 1.33438861e+02\n",
      " 8.27537559e+01 1.98686813e+01 1.22526674e-03 1.30255341e-02\n",
      " 0.00000000e+00 1.73472348e-18 4.40368550e+01 1.24800754e+02\n",
      " 8.07718769e+01 1.43990961e+02 7.72068544e+01 1.31201870e+02\n",
      " 7.69723650e+01 1.31888314e+02 8.40956444e+01 1.26454162e+02\n",
      " 7.46476405e+01]\n",
      "90-th iteration, loss: 0.007799203885435697, 48 gd steps\n",
      "insert gradient: -0.0007371790246343708\n",
      "90-th iteration, new layer inserted. now 57 layers\n",
      "[ 48.9996019   86.79131319  86.33651115  65.67088248  44.75525347\n",
      "  83.42086853  42.53710464  71.69471316  30.32690998  27.17372297\n",
      "  50.32276856 107.16495003  71.3640701  106.90528388  53.65141994\n",
      "  69.1649662   44.20919042  90.49744774  55.60664068  93.2533423\n",
      "  50.76823784  62.40472936  18.84063986  65.97319305  68.04089547\n",
      " 130.40991184  87.82539624  93.02717261  35.36620047  38.73104319\n",
      "  71.1779134  100.61679546 122.5611033   16.84284709  84.76339245\n",
      "   1.10912213  42.09288587  15.1431152   91.79956759 111.62460599\n",
      "  87.23422848 128.64206472  81.26321995 133.42440165  82.90434934\n",
      "  20.01133992  43.6668249  124.70238186  81.00377485 143.95127614\n",
      "  77.38439168 131.17843456  76.8851586  132.06217662  83.99742235\n",
      " 126.47581107  74.7661543 ]\n",
      "91-th iteration, loss: 0.007798595705810971, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "91-th iteration, new layer inserted. now 59 layers\n",
      "[ 48.99959241  86.79130416  86.33649432  65.67093643  44.75527398\n",
      "  83.42082788  42.53702139  71.69469361  30.32680573  27.17358593\n",
      "  50.32278642 107.16503661  71.36416787 106.90542694  53.65171192\n",
      "  69.16509566  44.20931605  90.49749463  55.60672285  93.25345052\n",
      "  50.76837721  62.40474528  18.84066868  65.97325729  68.04091741\n",
      " 130.40993051  87.82544097  93.02717927  35.3662588   38.73119743\n",
      "  71.17796192 100.61682003 122.56108277  16.84294088  84.76338855\n",
      "   1.10941557  14.03095578   0.          28.06191156  15.14334561\n",
      "  91.79973294 111.62470824  87.23444399 128.64231605  81.26396822\n",
      " 133.42509216  82.90622848  20.01314855  43.66898623 124.70320212\n",
      "  81.00469287 143.9516085   77.38476634 131.17855303  76.88527251\n",
      " 132.06225918  83.99745616 126.47582955  74.76618933]\n",
      "92-th iteration, loss: 0.006348871256085404, 59 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3454232707034333e-06\n",
      "92-th iteration, new layer inserted. now 59 layers\n",
      "[ 48.57740971  83.06007839  85.99305747  67.7776956   45.86241518\n",
      "  82.54236602  41.10197687  71.89231458  31.97135929  21.56042035\n",
      "  53.35094311 108.29278741  70.67555151 105.35197126  53.55407184\n",
      "  70.87492942  46.59414589  90.12634816  54.17419185  91.24750211\n",
      "  51.61502877  64.87912512  13.56542101  69.41627403  68.01896881\n",
      " 142.79661915  88.0947506   89.71593134  28.11190834  58.07474166\n",
      "  68.0681854  100.06291519 108.79674414  16.46657109  76.94483387\n",
      "  21.69618638   0.52183531  26.44374562  10.76548784  48.10754937\n",
      "  96.36457554 106.22772734  84.06080079 129.11863825  86.2095627\n",
      " 130.6746484   83.15486348  29.47723113  35.65023786 122.73433521\n",
      "  82.9990457  142.80779691  77.61258949 129.11682785  74.64508789\n",
      " 137.8809324   86.07998263 122.37622385  75.14328696]\n",
      "93-th iteration, loss: 0.006064024357725084, 35 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "93-th iteration, new layer inserted. now 59 layers\n",
      "[ 48.74378416  82.2240866   84.7586012   70.43994378  46.33805241\n",
      "  81.28526071  41.22689696  72.59732519  32.10925412  20.15340129\n",
      "  53.98371724 107.35328696  71.61307199  69.45442354   0.\n",
      "  34.72721177  53.16414396  72.32061792  46.88430791  89.59017876\n",
      "  54.10748453  90.32384905  52.10345968  66.36160705  11.81455397\n",
      "  70.36760051  66.37025831 148.36887406  88.53759288  86.1263026\n",
      "  25.83428307  63.61283137  68.73006795  99.5479172  102.52522703\n",
      "  18.91195499  78.81309687  66.1103218    3.69567118  60.38913549\n",
      "  92.75936397 106.15225369  84.57505121 128.22159005  86.75063876\n",
      " 131.78403294  81.69470113  35.07598439  32.18904627 123.03022688\n",
      "  82.84752308 142.97720429  77.55765927 131.41877456  73.05018146\n",
      " 138.30462805  87.13810909 122.3202766   74.17290536]\n",
      "94-th iteration, loss: 0.005935529641030701, 45 gd steps\n",
      "insert gradient: -5.158266907331324e-05\n",
      "94-th iteration, new layer inserted. now 59 layers\n",
      "[4.86636308e+01 8.21937438e+01 8.47903083e+01 7.04603615e+01\n",
      " 4.63024523e+01 8.12411205e+01 4.11396090e+01 7.25555826e+01\n",
      " 3.20569990e+01 2.01335400e+01 5.39929110e+01 1.07392978e+02\n",
      " 7.17050413e+01 1.04406688e+02 5.33982109e+01 7.23863439e+01\n",
      " 4.68110882e+01 8.95050851e+01 5.40265719e+01 9.03523010e+01\n",
      " 5.21756220e+01 6.63695724e+01 1.17722171e+01 7.03660361e+01\n",
      " 6.63244142e+01 1.48340118e+02 8.84860469e+01 8.60711550e+01\n",
      " 2.59027836e+01 6.36760728e+01 6.86990988e+01 9.95070078e+01\n",
      " 1.02370919e+02 1.88801292e+01 7.87269716e+01 6.61156722e+01\n",
      " 3.47286044e+00 6.03966863e+01 9.27428487e+01 1.06117857e+02\n",
      " 8.45753863e+01 1.28168628e+02 8.67146835e+01 1.31760415e+02\n",
      " 8.16914113e+01 0.00000000e+00 7.10542736e-15 3.51421912e+01\n",
      " 3.21653219e+01 1.22993882e+02 8.28421980e+01 1.42983059e+02\n",
      " 7.75722001e+01 1.31423702e+02 7.29593879e+01 1.38233741e+02\n",
      " 8.70554105e+01 1.22290035e+02 7.41706640e+01]\n",
      "95-th iteration, loss: 0.0057654405953333045, 44 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "95-th iteration, new layer inserted. now 61 layers\n",
      "[4.78924251e+01 8.16230709e+01 8.48547788e+01 7.15933815e+01\n",
      " 4.63167744e+01 8.08646157e+01 4.08386391e+01 7.27553289e+01\n",
      " 3.24147642e+01 1.98611323e+01 5.38885790e+01 1.06956849e+02\n",
      " 7.20331023e+01 1.03823554e+02 5.32781746e+01 7.32362534e+01\n",
      " 4.69518613e+01 8.91752229e+01 5.40358045e+01 9.06094090e+01\n",
      " 5.27007469e+01 6.66793737e+01 1.12312146e+01 7.05796418e+01\n",
      " 6.56347948e+01 4.98181474e+01 0.00000000e+00 9.96362948e+01\n",
      " 8.90392625e+01 8.38709199e+01 2.59063423e+01 6.51389233e+01\n",
      " 6.90915189e+01 9.87938614e+01 9.99425352e+01 2.04736286e+01\n",
      " 7.84215169e+01 6.88751120e+01 7.52241390e-01 6.33522372e+01\n",
      " 9.41572727e+01 1.05767929e+02 8.49628675e+01 1.27203658e+02\n",
      " 8.66626692e+01 1.31935595e+02 8.13308282e+01 2.12263935e+00\n",
      " 3.95283645e-04 3.72648416e+01 2.96773133e+01 1.21807877e+02\n",
      " 8.33226198e+01 1.43805540e+02 7.82084633e+01 1.32757860e+02\n",
      " 7.20066379e+01 1.37566889e+02 8.69079742e+01 1.21926442e+02\n",
      " 7.47669153e+01]\n",
      "96-th iteration, loss: 0.0057109837743805104, 42 gd steps\n",
      "insert gradient: -2.9305370507963812e-05\n",
      "96-th iteration, new layer inserted. now 61 layers\n",
      "[4.80155869e+01 8.15374535e+01 8.46145975e+01 7.19348559e+01\n",
      " 4.64443128e+01 8.08469106e+01 4.09485575e+01 7.28684170e+01\n",
      " 0.00000000e+00 7.10542736e-15 3.24660311e+01 1.96761176e+01\n",
      " 5.38787096e+01 1.06817171e+02 7.19939438e+01 1.03626504e+02\n",
      " 5.33779829e+01 7.35492797e+01 4.70161710e+01 8.90127205e+01\n",
      " 5.37813748e+01 9.05948219e+01 5.27777947e+01 6.66772448e+01\n",
      " 1.09702726e+01 7.06439332e+01 6.50680203e+01 4.99264653e+01\n",
      " 6.55082752e-01 9.97399559e+01 8.91348283e+01 8.30894082e+01\n",
      " 2.58345330e+01 6.55015939e+01 6.92878808e+01 9.84709178e+01\n",
      " 9.94000307e+01 2.13868754e+01 7.81538501e+01 1.34019793e+02\n",
      " 9.44865292e+01 1.05723855e+02 8.49190127e+01 1.27052343e+02\n",
      " 8.64888800e+01 1.32174564e+02 8.11880320e+01 2.97812135e+00\n",
      " 2.08063959e-07 3.81203240e+01 2.88205495e+01 1.21296823e+02\n",
      " 8.34181963e+01 1.43841149e+02 7.82730179e+01 1.33064954e+02\n",
      " 7.18496774e+01 1.37504095e+02 8.71778992e+01 1.21776612e+02\n",
      " 7.51789874e+01]\n",
      "97-th iteration, loss: 0.005698611587910263, 18 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.410240858537907e-06\n",
      "97-th iteration, new layer inserted. now 59 layers\n",
      "[4.80578532e+01 8.15111578e+01 8.45410483e+01 7.20380530e+01\n",
      " 4.64641796e+01 8.08256132e+01 4.09549235e+01 7.29035176e+01\n",
      " 1.83394603e-02 3.46377777e-02 3.24849999e+01 1.95832799e+01\n",
      " 5.38501681e+01 1.06764367e+02 7.20328210e+01 1.03592169e+02\n",
      " 5.34003201e+01 7.36728367e+01 4.70699794e+01 8.89841115e+01\n",
      " 5.36938326e+01 9.06041540e+01 5.28457230e+01 6.67068062e+01\n",
      " 1.09230282e+01 7.07004642e+01 6.48973239e+01 4.99497805e+01\n",
      " 7.32266855e-01 9.97594618e+01 8.91780269e+01 8.28480241e+01\n",
      " 2.58431257e+01 6.56279725e+01 6.93356671e+01 9.83554628e+01\n",
      " 9.91802411e+01 2.17041841e+01 7.80038653e+01 1.34252658e+02\n",
      " 9.45143033e+01 1.05695641e+02 8.48991609e+01 1.27007114e+02\n",
      " 8.64425850e+01 1.32269836e+02 8.11568506e+01 4.16946891e+01\n",
      " 2.85160215e+01 1.21122598e+02 8.34497237e+01 1.43835821e+02\n",
      " 7.82983669e+01 1.33169403e+02 7.18021470e+01 1.37472409e+02\n",
      " 8.72373876e+01 1.21708335e+02 7.53162821e+01]\n",
      "98-th iteration, loss: 0.005288622840075104, 85 gd steps\n",
      "insert gradient: -3.606411436045652e-05\n",
      "98-th iteration, new layer inserted. now 61 layers\n",
      "[4.76583571e+01 7.85342695e+01 8.38892473e+01 7.54200018e+01\n",
      " 4.67521605e+01 8.04276291e+01 4.18192243e+01 7.46391957e+01\n",
      " 7.55378584e-01 1.86811812e-01 3.51454352e+01 1.26805819e+01\n",
      " 5.35421927e+01 1.02408793e+02 7.48926892e+01 1.03096747e+02\n",
      " 5.31679555e+01 7.56141980e+01 4.68563767e+01 8.84635572e+01\n",
      " 5.23991055e+01 9.09016512e+01 5.52558684e+01 6.86130265e+01\n",
      " 8.50038619e+00 7.60240044e+01 5.91083708e+01 5.48853030e+01\n",
      " 1.52245347e+00 1.03140880e+02 8.90049574e+01 7.17454160e+01\n",
      " 2.57158377e+01 7.26801730e+01 0.00000000e+00 7.10542736e-15\n",
      " 7.00598555e+01 9.46214615e+01 9.42449717e+01 3.74649034e+01\n",
      " 7.20440425e+01 1.33741317e+02 9.60155813e+01 1.05329674e+02\n",
      " 8.45200646e+01 1.24129328e+02 8.71461852e+01 1.35560348e+02\n",
      " 7.89606044e+01 6.59794802e+01 1.74402919e+01 1.15748742e+02\n",
      " 8.52092754e+01 1.42352634e+02 7.90556469e+01 1.36547899e+02\n",
      " 7.06055190e+01 1.36619899e+02 8.91891758e+01 1.18337692e+02\n",
      " 8.05548281e+01]\n",
      "99-th iteration, loss: 0.005226470620141874, 30 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "99-th iteration, new layer inserted. now 61 layers\n",
      "[4.80127633e+01 7.80437551e+01 8.35122260e+01 7.62592863e+01\n",
      " 4.68844185e+01 8.04334567e+01 4.19717848e+01 7.54217156e+01\n",
      " 3.72446856e+01 1.00511994e+01 5.38028607e+01 1.01008915e+02\n",
      " 7.55364267e+01 1.03104549e+02 5.32298192e+01 7.58479204e+01\n",
      " 4.72996511e+01 5.88286173e+01 0.00000000e+00 2.94143087e+01\n",
      " 5.18268954e+01 9.09184608e+01 5.57171310e+01 6.88504177e+01\n",
      " 8.05918273e+00 7.69731780e+01 5.80117618e+01 5.64578336e+01\n",
      " 1.79102755e+00 1.03787004e+02 8.80381512e+01 6.92913214e+01\n",
      " 2.57798830e+01 7.39095327e+01 3.57675052e-02 1.20140350e+00\n",
      " 7.04063007e+01 9.39829490e+01 9.26117026e+01 4.17650282e+01\n",
      " 7.13329154e+01 1.30803890e+02 9.73372429e+01 1.04417826e+02\n",
      " 8.54671969e+01 1.22316612e+02 8.81550845e+01 1.35261136e+02\n",
      " 7.81430821e+01 7.26763153e+01 1.53130049e+01 1.13192135e+02\n",
      " 8.63787599e+01 1.40402897e+02 8.00975040e+01 1.35914805e+02\n",
      " 7.10085873e+01 1.36340147e+02 8.94349552e+01 1.17343528e+02\n",
      " 8.20363525e+01]\n",
      "0-th iteration, loss: 0.7697496074621527, 18 gd steps\n",
      "insert gradient: -0.4560707092078872\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  79.78156315    0.         3231.15330745]\n",
      "1-th iteration, loss: 0.5818364014088627, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.00053368e+01 1.26225708e+00 0.00000000e+00 1.02242823e+02\n",
      " 3.20710434e+03]\n",
      "2-th iteration, loss: 0.575792305388137, 37 gd steps\n",
      "insert gradient: -0.22476112214663832\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.98352807  105.14799684  156.40687874    0.         3049.93413541]\n",
      "3-th iteration, loss: 0.4607686838986418, 91 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  68.81254521  101.96001804  113.05419488   86.67499039  124.26727365\n",
      "    0.         2920.28093082]\n",
      "4-th iteration, loss: 0.3829454106388992, 22 gd steps\n",
      "insert gradient: -0.1953468721591299\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  61.47127497  105.76043984   63.03889249    0.           59.53673179\n",
      "   87.77109284   98.69224189   88.08469902 2897.7395601 ]\n",
      "5-th iteration, loss: 0.36812092498834303, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  66.25120898  103.92216483   58.46986756   20.34898932   48.90712997\n",
      "   95.01157713   93.17596298  101.4690827   214.74113919    0.\n",
      " 2684.26423982]\n",
      "6-th iteration, loss: 0.29053361352262813, 54 gd steps\n",
      "insert gradient: -0.1103851713896979\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  68.55651398  104.46134194   31.3032025    46.50292144   53.71912297\n",
      "  100.49396412   87.74116755  100.18154973  177.28405965  123.84143291\n",
      "  363.1053987     0.         2299.66752508]\n",
      "7-th iteration, loss: 0.25230104005142856, 72 gd steps\n",
      "insert gradient: -0.061213856348611116\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  63.34031283  109.10380525   43.12994267   28.81418574   52.66796463\n",
      "   99.28009064   93.46549197   97.758358    173.25535704   98.07759821\n",
      "  337.0821894    95.49787577  379.88402178    0.         1899.42010892]\n",
      "8-th iteration, loss: 0.22757405811525308, 63 gd steps\n",
      "insert gradient: -0.09061521768960132\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  62.58465378  109.86435785   54.19317467   16.83576683   53.72384419\n",
      "  100.38973016   93.66094631  100.57046572   82.72697631    0.\n",
      "   82.72697631   92.29238357  347.44669607   88.17211873  356.9442316\n",
      "   75.20647113 1863.13482683]\n",
      "9-th iteration, loss: 0.17526069150894577, 42 gd steps\n",
      "insert gradient: -0.0600351528102677\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  75.10005296  106.16901696   54.60066503    5.65287037   55.33303322\n",
      "  109.22822243   90.09186419  105.51064778   71.424689     72.91996799\n",
      "   43.05092862   85.35356767  126.6747588     0.          228.01456584\n",
      "   74.1301682   357.80285872   59.02717094 1864.58366692]\n",
      "10-th iteration, loss: 0.1402349906296489, 370 gd steps\n",
      "insert gradient: -0.019456811880919297\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  64.98199576  124.57040733   58.85778901    3.49796523   51.63577665\n",
      "  105.02502214   88.88215465  107.40208667   68.10777669   80.9168845\n",
      "   39.25074335   77.7791658    69.24694959    0.           49.46210685\n",
      "  100.10025398  181.6105247    77.94853953  354.28554662   51.50663892\n",
      " 1870.04662642]\n",
      "11-th iteration, loss: 0.13555063155067362, 207 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  65.2212872   125.78426268  113.99465753  108.98816729   87.3462107\n",
      "  109.86323887   66.83527837   83.75473793   41.08332502   80.73368408\n",
      "   52.37701507   27.74271903   31.20837139  124.4255163   183.87844483\n",
      "   74.12158403   59.82367364    0.          299.11836819   48.90364523\n",
      " 1879.95975849]\n",
      "12-th iteration, loss: 0.12662673441762795, 49 gd steps\n",
      "insert gradient: -0.013783303571633794\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[6.48514629e+01 1.29997612e+02 1.10807784e+02 1.14675973e+02\n",
      " 0.00000000e+00 1.06581410e-14 8.38815437e+01 1.11749866e+02\n",
      " 6.59054705e+01 8.66389761e+01 4.20239559e+01 8.39417017e+01\n",
      " 5.36549128e+01 3.97634136e+01 2.36273661e+01 1.04447713e+02\n",
      " 1.87753698e+02 8.24588177e+01 6.72996639e+01 3.10623336e+01\n",
      " 2.67469595e+02 3.21258642e+01 1.88824204e+03]\n",
      "13-th iteration, loss: 0.12361262989140462, 24 gd steps\n",
      "insert gradient: -0.00750700007102377\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  66.23906106  127.13163384  109.61531407  119.56101529   81.61563467\n",
      "  116.28620614   64.40534668   87.8399302    41.85354366   86.1377936\n",
      "   52.34511214   46.22714804   23.01405668   93.5991284   186.89464332\n",
      "   81.13202476   72.27634284   35.57861317  256.48186857   37.13839618\n",
      "  516.3667918     0.         1376.97811147]\n",
      "14-th iteration, loss: 0.12151035357560443, 25 gd steps\n",
      "insert gradient: -0.012666030097667761\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  66.96268153  126.08631716  109.72940051  118.66577034   79.29499413\n",
      "  122.25165629   62.77063423   88.734346     42.28596375   86.5765285\n",
      "   51.43572631   53.95956384   19.57226809   91.93242199  188.36113768\n",
      "   66.46283457   79.16802741   39.33905618  252.23754932   37.28136076\n",
      "  509.34311059   13.50670895 1377.60576533]\n",
      "15-th iteration, loss: 0.12120687505369532, 33 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  67.26103142  125.89774962  109.34593447  119.53531394   79.77359504\n",
      "  121.93674907   62.05311639   88.98231139   42.90644671   85.13911114\n",
      "   51.64189494   57.04331223   18.3612605    91.32076683  187.65567957\n",
      "   65.87996815   79.5519119    40.41733846  251.16735233   37.39495847\n",
      "  101.69069481    0.          406.76277923   14.31995238 1377.21289637]\n",
      "16-th iteration, loss: 0.1060582953757408, 127 gd steps\n",
      "insert gradient: -0.007105124004348596\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  65.92901571  120.66407556  112.41407925  107.55066564   91.96205798\n",
      "  111.26718472   68.7596866    96.4401335    44.0982971    80.00669057\n",
      "   49.83695813   87.00855445    6.16366253   81.7355284   129.68798841\n",
      "    0.           86.45865894   41.88722066   58.60447765   84.20697478\n",
      "  363.31637978   57.85631029  376.78732499   24.27535956 1345.72762164]\n",
      "17-th iteration, loss: 0.1045191704352058, 26 gd steps\n",
      "insert gradient: -0.00704817751564432\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  67.23038594  116.26531203  114.2835013   107.55146277   92.11030598\n",
      "  111.88132598   67.81929279   98.6140412    45.11592099   77.84645615\n",
      "   48.99416676   87.03881945    6.79843053   76.81175906  129.21818825\n",
      "   10.28316836   85.73813391   41.10068225   54.58585667   92.34193827\n",
      "  120.43859063    0.          240.87718125   61.20051949  376.97599693\n",
      "   25.12888276 1337.494409  ]\n",
      "18-th iteration, loss: 0.10313505850418307, 26 gd steps\n",
      "insert gradient: -0.01455487418014729\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[  66.80734491  116.27809584  114.4013986   108.46780102   91.6394289\n",
      "  112.21179564   67.9828078    99.48251628   45.70985857   76.7114192\n",
      "   50.30517713   83.18665597    8.44073922   73.73963927   79.88215395\n",
      "    0.           47.92929237   13.24878205   86.79550845   42.34984724\n",
      "   52.27594343   94.90092607  120.57695853   11.57804632  231.55625649\n",
      "   64.42811003  378.42563809   26.54701428 1331.55538537]\n",
      "19-th iteration, loss: 0.09838640906202151, 22 gd steps\n",
      "insert gradient: -0.014672411303017486\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[6.67177129e+01 1.20637585e+02 1.13180815e+02 1.11075084e+02\n",
      " 0.00000000e+00 1.06581410e-14 8.77725417e+01 1.16575719e+02\n",
      " 6.62436959e+01 1.00118268e+02 4.73116532e+01 7.76581836e+01\n",
      " 4.93251550e+01 9.08560646e+01 1.23187362e+01 6.32341465e+01\n",
      " 6.52502129e+01 2.46080675e+01 3.08532971e+01 3.16182716e+01\n",
      " 9.37921401e+01 3.74717196e+01 4.94275865e+01 9.39254320e+01\n",
      " 1.22190636e+02 1.41630059e+01 2.29566289e+02 6.55796693e+01\n",
      " 3.78343710e+02 2.85322079e+01 1.32763668e+03]\n",
      "20-th iteration, loss: 0.09287302680266557, 81 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  65.56181469  120.74663231  116.74278091  110.23633185   86.17285153\n",
      "  116.51700075   68.52794467  104.2966722    47.48039573   78.82615254\n",
      "   46.04889783   89.52041479   22.12276891   47.60585275   56.84857192\n",
      "   68.60788949    6.84363512   66.3070492    92.67794258   25.043077\n",
      "   50.66594873   91.33637338  129.63480699   21.92537294   27.49299108\n",
      "    0.          192.45093755   64.51043168  375.79127752   33.83603181\n",
      " 1330.5000651 ]\n",
      "21-th iteration, loss: 0.08959480899334327, 39 gd steps\n",
      "insert gradient: -0.007197135312915556\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[6.60368830e+01 1.20948724e+02 1.20347447e+02 1.08924630e+02\n",
      " 8.30208770e+01 1.17581107e+02 0.00000000e+00 2.13162821e-14\n",
      " 6.88889346e+01 1.05189729e+02 4.90183804e+01 7.79900963e+01\n",
      " 4.39246301e+01 8.28725297e+01 2.65842158e+01 4.99924724e+01\n",
      " 5.42357711e+01 1.50217320e+02 9.66852090e+01 2.58565811e+01\n",
      " 4.70726895e+01 8.88882585e+01 1.24987632e+02 4.37648769e+01\n",
      " 1.92334414e+01 2.36547814e+01 1.78004559e+02 6.81954292e+01\n",
      " 3.71672440e+02 3.21454722e+01 1.32693658e+03]\n",
      "22-th iteration, loss: 0.08879317837503797, 999 gd steps\n",
      "insert gradient: -0.003729196393774936\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[  65.9794831   120.95675558  121.84508036  109.03111546   82.57826009\n",
      "  117.08953671   69.68382365  107.08922207   50.07798358   77.19952234\n",
      "   43.30952561   80.49650195   28.34362274   50.51550097   55.65831527\n",
      "  143.20431614  100.15053101   31.00739226   42.86239177   87.18249023\n",
      "  122.73563851   60.2987701    11.76633702   33.27390967  154.68025389\n",
      "    0.           22.09717913   70.74569889  374.22217806   25.03710903\n",
      " 1315.75024762]\n",
      "23-th iteration, loss: 0.08805896823724667, 19 gd steps\n",
      "insert gradient: -0.0053949268294207426\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[6.66511796e+01 1.24032131e+02 1.20532418e+02 1.11217032e+02\n",
      " 8.35444021e+01 1.14568918e+02 0.00000000e+00 3.55271368e-14\n",
      " 6.81645160e+01 1.08541250e+02 5.09621137e+01 7.73261337e+01\n",
      " 4.25631340e+01 7.99588479e+01 3.00004752e+01 4.89115932e+01\n",
      " 5.70965005e+01 1.37836223e+02 1.04355105e+02 3.15840794e+01\n",
      " 4.01583444e+01 8.72553390e+01 1.23357530e+02 5.71000792e+01\n",
      " 7.90117366e+00 3.60588415e+01 1.48262668e+02 1.47791313e+01\n",
      " 2.16224754e+01 7.93442864e+01 3.72227961e+02 2.31366194e+01\n",
      " 1.31373570e+03]\n",
      "24-th iteration, loss: 0.08784395652659577, 25 gd steps\n",
      "insert gradient: -0.005168109075445793\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[ 66.07438464 123.45559445 121.37505801 110.41180961  82.40989673\n",
      " 114.82435735  70.19190639 108.54001574  50.60417555  77.252282\n",
      "  42.6990702   79.89278427  30.11580488  48.82011851  56.73227787\n",
      " 137.9688868  105.08066207  31.35674018  39.83763493  87.2106435\n",
      " 123.38130253  56.82646944   8.2179944   36.23068151 147.80751755\n",
      "  15.76180184  21.24410915  79.83476885 372.34751863  23.57600156\n",
      " 937.9001236    0.         375.16004944]\n",
      "25-th iteration, loss: 0.08642046278507824, 44 gd steps\n",
      "insert gradient: -0.0049073279993833725\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[ 65.88247953 123.04902743 121.85008121 109.20878686  81.90687073\n",
      " 115.21802137  71.0135031  108.53126814  51.08922738  77.38172839\n",
      "  42.67762141  80.21918346  30.27538908  49.34642745  55.81836307\n",
      " 136.21772472 109.39176137  27.96868586  39.3268522   86.82376201\n",
      " 123.43428751  47.06428246  10.00163389  36.06812364 145.31983952\n",
      "  19.96992596  21.40672879  85.31908052 371.48559546  23.92394314\n",
      " 395.01846133   0.         526.69128177  17.92886002 376.07385215]\n",
      "26-th iteration, loss: 0.08478041778833838, 73 gd steps\n",
      "insert gradient: -0.002504182584654197\n",
      "26-th iteration, new layer inserted. now 37 layers\n",
      "[ 66.08578581 122.41913144 121.47612152 109.01451134  81.67207212\n",
      " 114.77792127  71.26281459 109.5035882   50.43451668  77.63640734\n",
      "  42.61476597  80.51532845  31.28852439  47.26571791  54.26068626\n",
      " 135.48658531 114.07556503  27.6607754   36.07708223  85.89764071\n",
      " 120.98149496  45.03899684  10.68510834  39.85083291 144.41623228\n",
      "  20.14855671  20.21005209  91.10969628 366.42430506  34.08374951\n",
      " 386.95860814  22.17413803 516.42311639  20.06248198 185.93769285\n",
      "   0.         185.93769285]\n",
      "27-th iteration, loss: 0.08446871075770346, 24 gd steps\n",
      "insert gradient: -0.0053378010700051316\n",
      "27-th iteration, new layer inserted. now 39 layers\n",
      "[ 66.08114257 122.13595073 121.74594502 108.91012674  81.64731579\n",
      " 114.51606284  71.38999532 109.43310332  50.34047774  77.27177657\n",
      "  42.80785776  80.55773445  31.48492444  46.98435821  54.42045885\n",
      " 134.56040678 115.3119427   27.70917621  35.39974799  85.55374747\n",
      " 120.22460665  45.0097403   10.61181044  41.33170866 144.48497779\n",
      "  20.08890893  19.68714269  92.46949646 365.2165575   35.11539022\n",
      " 386.48558296  23.01959176 516.08690189  20.59897578 183.13928934\n",
      "   7.19007325 152.39354971   0.          30.47870994]\n",
      "28-th iteration, loss: 0.08321476203129098, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "28-th iteration, new layer inserted. now 41 layers\n",
      "[ 65.99856097 122.71414971 121.13880487 109.49036165  81.83861929\n",
      " 114.30818595  71.36201507 109.83997702  49.62741411  77.64470916\n",
      "  43.02113876  81.37744323  31.01867848  45.90942258  55.19329036\n",
      " 134.60591101 114.69615654  27.4289597   36.0674164   85.30570746\n",
      " 120.31737058  45.38760085   9.78250543  42.1476495  144.89698292\n",
      "  20.46967647  18.93195311  93.51983244 364.28868881  36.70457731\n",
      " 386.73521391  22.54087399 103.36137906   0.         413.44551624\n",
      "  17.30644807 179.26880095   5.43950166 148.62814543  16.80487861\n",
      "  26.16730469]\n",
      "29-th iteration, loss: 0.08256215558569963, 52 gd steps\n",
      "insert gradient: -0.0037381898088305232\n",
      "29-th iteration, new layer inserted. now 43 layers\n",
      "[ 65.29312802 122.25758532 121.7954778  110.12023076  80.61538006\n",
      " 114.49657046  72.42734305 110.82721216  49.24285583  77.34530996\n",
      "  42.77643936  81.16190598  31.35184561  45.79321056  53.75585472\n",
      " 137.5263838  112.66914603  28.753604    36.35135829  85.41600488\n",
      "  71.78386838   0.          47.85591225  44.66192214   9.16618213\n",
      "  46.76000813 143.37134436  19.89671182  18.84610262  95.87386413\n",
      " 362.92370026  37.87989704 388.94730421  20.62868368 103.71754908\n",
      "  12.48599513 402.80621335  18.65210054 180.40859835   5.27539121\n",
      " 148.411638    19.56364759  22.77309431]\n",
      "30-th iteration, loss: 0.0823715051799762, 14 gd steps\n",
      "insert gradient: -0.004936775168553887\n",
      "30-th iteration, new layer inserted. now 45 layers\n",
      "[6.64243261e+01 1.22210563e+02 0.00000000e+00 7.10542736e-15\n",
      " 1.20856756e+02 1.09732369e+02 8.16956945e+01 1.14581298e+02\n",
      " 7.09028219e+01 1.09853797e+02 5.01336656e+01 7.74911767e+01\n",
      " 4.26605798e+01 8.12381159e+01 3.14918199e+01 4.57678850e+01\n",
      " 5.38042055e+01 1.37606609e+02 1.12516520e+02 2.88274435e+01\n",
      " 3.65940695e+01 8.52949742e+01 7.04309843e+01 2.75267021e+00\n",
      " 4.64578466e+01 4.47408417e+01 9.46983178e+00 4.71536303e+01\n",
      " 1.43197699e+02 2.01343339e+01 1.89658854e+01 9.59590456e+01\n",
      " 3.63263973e+02 3.77536183e+01 3.89108525e+02 2.02663560e+01\n",
      " 1.03920146e+02 1.30037677e+01 4.02275997e+02 1.90235884e+01\n",
      " 1.80496435e+02 5.14456716e+00 1.48399020e+02 1.96735418e+01\n",
      " 2.27424357e+01]\n",
      "31-th iteration, loss: 0.08148090367767145, 82 gd steps\n",
      "insert gradient: -0.0028100614947114714\n",
      "31-th iteration, new layer inserted. now 45 layers\n",
      "[6.64092353e+01 1.23717962e+02 1.20952125e+02 1.09795347e+02\n",
      " 0.00000000e+00 7.10542736e-15 8.18153900e+01 1.14369018e+02\n",
      " 7.14398190e+01 1.11411884e+02 5.04691992e+01 7.80661952e+01\n",
      " 4.30099223e+01 8.23678257e+01 3.28155834e+01 4.28215517e+01\n",
      " 5.24941754e+01 1.39522395e+02 1.13657561e+02 2.69110147e+01\n",
      " 3.87863340e+01 8.64034579e+01 6.25616853e+01 1.79357532e+01\n",
      " 3.32178051e+01 5.52829303e+01 8.06453025e+00 5.60980573e+01\n",
      " 1.41693518e+02 2.42318649e+01 1.77120510e+01 9.46531345e+01\n",
      " 3.65560222e+02 3.46521717e+01 3.92321982e+02 1.68857155e+01\n",
      " 1.03611644e+02 1.58908839e+01 3.98936855e+02 2.22666660e+01\n",
      " 1.82482475e+02 3.86624380e+00 1.47318550e+02 2.11927253e+01\n",
      " 2.27700242e+01]\n",
      "32-th iteration, loss: 0.08145290849926931, 17 gd steps\n",
      "insert gradient: -0.0015900649909980165\n",
      "32-th iteration, new layer inserted. now 45 layers\n",
      "[ 66.08766333 123.53143283 120.97107093 109.80854487  82.10162663\n",
      " 114.51426184  71.37440085 111.32899197  50.52083334  78.14324324\n",
      "  43.06652606  82.36271788  32.80860561  42.7714119   52.52960469\n",
      " 139.52608777 113.75026904  26.88773725  38.78758222  86.3926705\n",
      "  62.46518476  18.20754461  32.98985783  55.69017884   7.95289445\n",
      "  56.30965022 141.71626455  24.38061239  17.60003795  94.60069807\n",
      " 365.52885611  34.5355405  235.42940405   0.         156.95293603\n",
      "  16.83412267 103.64070281  15.95750788 398.9279648   22.32314659\n",
      " 182.56560818   3.85106853 147.28477804  21.24582731  22.7859239 ]\n",
      "33-th iteration, loss: 0.08098332415463141, 44 gd steps\n",
      "insert gradient: -0.0032883022390450315\n",
      "33-th iteration, new layer inserted. now 45 layers\n",
      "[ 66.83629886 123.21354784 120.80975395 109.84657136  82.32517188\n",
      " 114.35728012  71.18158325 112.93748784  50.04997479  79.45658382\n",
      "  43.18704428  81.63114763  34.136443    41.24543685  51.80829332\n",
      " 139.32977187 115.75801816  25.28810065  40.10205042  84.52018156\n",
      "  56.82124905  27.40535472  24.14786056  78.31356103   1.83894163\n",
      "  65.49026683 141.23317521  29.36769521  14.58013084  91.72564325\n",
      " 364.66556946  33.82967849 234.83805633   5.31971416 156.40398731\n",
      "  15.84454125 104.53084924  17.44086994 399.60320604  23.09174623\n",
      " 186.57961948   3.34892698 143.52753581  22.38273379  22.05193204]\n",
      "34-th iteration, loss: 0.08088265811432341, 27 gd steps\n",
      "insert gradient: -0.0027064940168886635\n",
      "34-th iteration, new layer inserted. now 47 layers\n",
      "[ 66.23721611 124.29407378 120.68449587 110.17431431  82.91456852\n",
      " 114.03003455  70.50695774 112.51704754  52.01523055  77.10109129\n",
      "  42.98159241  82.82880452  33.9893918   41.26902515  52.00018555\n",
      " 139.29338985  69.55488285   0.          46.3699219   25.53686777\n",
      "  39.81408259  84.54469323  57.72674033  27.30369655  23.77732073\n",
      "  79.49735947   1.0932165   65.769614   142.63202076  31.42882989\n",
      "  12.99219227  91.64602481 363.13391728  34.00786156 235.56857107\n",
      "   5.54156508 155.99971636  16.41151048 105.72553904  17.73854744\n",
      " 400.03675115  23.52764601 187.22694804   3.44252756 142.80369388\n",
      "  22.59427038  22.21807885]\n",
      "35-th iteration, loss: 0.07999299926293599, 62 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "35-th iteration, new layer inserted. now 49 layers\n",
      "[6.74013959e+01 1.24758980e+02 1.20204218e+02 1.10379719e+02\n",
      " 8.23436465e+01 1.14050655e+02 7.09563104e+01 1.13932487e+02\n",
      " 5.13452118e+01 7.85879839e+01 4.37177493e+01 8.36706077e+01\n",
      " 3.50795715e+01 4.17054995e+01 5.01270634e+01 1.33384467e+02\n",
      " 7.04278617e+01 1.11525046e+01 4.25628626e+01 3.15634897e+01\n",
      " 3.41661786e+01 8.29724476e+01 6.02780048e+01 2.76758826e+01\n",
      " 2.05202910e+01 8.16292212e+01 2.61905553e-01 6.71879848e+01\n",
      " 1.45857318e+02 3.71873600e+01 8.44569382e+00 9.17595300e+01\n",
      " 3.61210215e+02 3.52335852e+01 1.19419227e+02 0.00000000e+00\n",
      " 1.19419227e+02 5.04651845e+00 1.54548581e+02 1.62644847e+01\n",
      " 1.09828328e+02 1.81952827e+01 4.00717148e+02 2.43470919e+01\n",
      " 1.89115876e+02 3.87898635e+00 1.42146549e+02 2.24272885e+01\n",
      " 2.17920227e+01]\n",
      "36-th iteration, loss: 0.07892411208004073, 20 gd steps\n",
      "insert gradient: -0.019025965717764513\n",
      "36-th iteration, new layer inserted. now 47 layers\n",
      "[ 67.74316463 124.74803664 119.46212704 108.78409386  82.63823806\n",
      " 113.60258093  71.73624823 114.36578509  51.2857641   81.00119789\n",
      "  43.58765297  84.05788259  37.18313379  39.72710174  49.81394503\n",
      " 125.74656384  74.55753423  20.04966664  37.88416459  41.5006631\n",
      "  27.37611866  81.10136679  64.04925084  26.35586538  16.8016588\n",
      " 155.22666631 146.76677639  43.60860415   5.22119198  89.99532578\n",
      " 362.94496073  30.43516742 112.729274    15.02892541 120.31919225\n",
      "   0.5593941  150.83020911  14.14897845 116.5576067   23.55050346\n",
      " 395.30789602  25.46946924 190.95221346   2.79751507 139.55157272\n",
      "  22.03397758  22.06864757]\n",
      "37-th iteration, loss: 0.07766103118234105, 93 gd steps\n",
      "insert gradient: -0.0028813331905744137\n",
      "37-th iteration, new layer inserted. now 47 layers\n",
      "[ 67.81484495 129.51996377 118.78765829 110.82129431  82.18264772\n",
      " 111.8913745   71.67180111 118.20252059  50.689295    80.52709657\n",
      "  44.04486246  86.74652058  39.11845361  37.91343691  50.32214991\n",
      " 113.8474332   77.20760799  30.59103367  35.279432    51.3340132\n",
      "  21.71056892  76.44992773  64.96588567  23.06532352  14.99436922\n",
      " 160.4792583   89.21857445   0.          59.47904963  47.92640425\n",
      "   3.96029523  83.9019209  367.51750938  28.46963691 105.55989431\n",
      "  22.29026567 273.91073551  11.65919173 119.2545802   25.69768713\n",
      " 396.56126807  24.70686316 194.9738692    3.84214541 137.13791302\n",
      "  22.07586284  21.40716383]\n",
      "38-th iteration, loss: 0.07657139588488215, 17 gd steps\n",
      "insert gradient: -0.012359671903672986\n",
      "38-th iteration, new layer inserted. now 47 layers\n",
      "[ 67.24674293 129.44980184 119.05298017 111.33838645  83.53457143\n",
      " 109.68095995  70.21102224 123.60099529  50.76498782  80.08145832\n",
      "  43.91040098  88.18381968  39.95328241  36.834857    50.65290705\n",
      " 109.41790465  78.58617299  35.59418311  32.35243157  56.66110215\n",
      "  21.3488628   72.0312428   61.451557    26.97836495  12.87058916\n",
      " 162.2233424   84.94158749  19.14701268  43.7163401   52.54367601\n",
      "   5.73705589  80.55814304 373.53528379  24.96330235 106.66117586\n",
      "  23.66788088 273.97686803   9.42930937 120.64805415  26.30725585\n",
      " 397.60182432  23.67588111 196.28741522   3.08055732 134.71552534\n",
      "  24.15829443  20.81590797]\n",
      "39-th iteration, loss: 0.07233302487428353, 496 gd steps\n",
      "insert gradient: -0.0013489278616517948\n",
      "39-th iteration, new layer inserted. now 47 layers\n",
      "[ 69.35802422 123.536079   121.64059562 115.50466829  81.44231413\n",
      " 116.7754723   68.91221615 126.14716543  55.46120042  82.15196917\n",
      "  43.75184454  87.66755323  45.58563154  40.0273485   43.6433947\n",
      "  95.25494856  92.18652654  47.88456614  24.00215803  65.41616309\n",
      "  21.52050873  60.68283997  59.88315124 121.37725472   0.\n",
      "  80.91816982  91.66306601  38.60744474  26.56538992  70.48181483\n",
      "   2.01975659  72.52790932 387.67529233  17.72079978  98.21775176\n",
      "  25.85025445 279.52240166   4.79358712 117.65455631  25.02471064\n",
      " 401.82093778  22.50840836 199.55055931   3.96372044 135.53754947\n",
      "  27.71863792  19.26109619]\n",
      "40-th iteration, loss: 0.06983433668894219, 371 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 43 layers\n",
      "[ 76.58488579 123.69030045 122.07235047 111.95359097  84.03713566\n",
      " 115.71976054  67.52729321 129.7575533   58.38848448  84.76188619\n",
      "  44.85988169  88.50684154  50.61931211  44.38913246  38.06938313\n",
      "  92.6020178   80.65161961  77.04415644  22.31838305  81.90172459\n",
      "  26.4174797   33.89027837  63.18680435 109.15534912  11.58040733\n",
      "  49.14555895  97.02068054  59.89998046  14.96206334 144.88854871\n",
      " 402.06380975   8.00863806  93.5447237   31.08772109 405.2581728\n",
      "  22.63077416 400.1600663   12.37195325 135.69318032   0.\n",
      " 203.53977049  35.75668404  18.33384208]\n",
      "41-th iteration, loss: 0.06871328357997525, 36 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 45 layers\n",
      "[ 74.81162922 131.89477654 120.33068189 113.13163668  82.83993905\n",
      " 117.75425446  65.04745502 135.00167356  58.37036566  87.40533965\n",
      "  44.4875678   88.84031773  51.18293628  50.22812627  33.23265765\n",
      "  95.18187067  75.16599384  95.0196181   22.40491726  78.61082978\n",
      "  37.51053468  16.30041319  62.09871163 104.4708912   11.85139862\n",
      "  40.5934977  104.6124118   64.78886954  12.17373895 141.51555759\n",
      "  80.40760221   0.         321.63040884   7.84761071  93.39237531\n",
      "  27.97073598 409.54464828  21.54360733 400.02732911   4.54818696\n",
      " 130.87277833  12.51121934 196.0564763   39.44010296  16.39307973]\n",
      "42-th iteration, loss: 0.06756938763779302, 19 gd steps\n",
      "insert gradient: -0.0057898294548166885\n",
      "42-th iteration, new layer inserted. now 45 layers\n",
      "[ 76.01940363 128.01624827 121.78022458 108.89971918  85.73982703\n",
      " 114.39995438  66.22388163 134.64655339  60.20401235  84.69691228\n",
      "  44.87998611  89.58624554  52.58448596  50.81543153  31.56060004\n",
      "  93.41424477  75.89429658  96.27972225  23.28083293  77.29817489\n",
      "  40.56910403  11.827334    60.60159116 104.06111498  11.74821272\n",
      "  37.81589358 108.88840638  66.39553141  12.11609294 139.14347513\n",
      "  77.97608005  14.06499512 317.37701848   5.17403589  90.81227874\n",
      "  30.23922954 409.77923015  19.71473465 399.6441157    2.05274438\n",
      " 130.81494349  14.68026091 194.11530834  39.07441976  15.46835771]\n",
      "43-th iteration, loss: 0.0665145225712784, 64 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 43 layers\n",
      "[ 77.62811833 125.48040486 120.66788332 109.92075213  85.61547675\n",
      " 115.10505076  64.6208486  137.41566444  58.75664369  86.88887422\n",
      "  45.22898003  89.69756171  52.67657146  58.38360255  26.47174452\n",
      "  92.44270064  76.82856736  96.46282814  26.05118965  71.91095675\n",
      "  47.73160129   5.68582212  58.0465579  102.25049976  13.74672986\n",
      "  28.4922083  117.06869714  71.33895811  10.31712326 131.24172681\n",
      "  78.17179675  24.80476417 406.90179693  31.87419661 412.50178407\n",
      "  16.53029843 105.79920486   0.         423.19681946  17.17880647\n",
      " 190.01703135  38.87128961  14.81040871]\n",
      "44-th iteration, loss: 0.06495646561856151, 87 gd steps\n",
      "insert gradient: -0.0014059786929244064\n",
      "44-th iteration, new layer inserted. now 45 layers\n",
      "[ 78.5194692  124.67423301 121.33531525 108.23607462  86.98955328\n",
      " 115.78570907  65.31119365 137.27449934  59.16324309  88.81437673\n",
      "  45.76355606  89.18396865  52.36895017  67.86204256  23.10136446\n",
      "  87.12539451  79.29388357  96.35212542  29.21535235  62.80278638\n",
      "  55.57991545   2.72350322  55.97126889 101.00853644  19.64040664\n",
      "  17.95400839  95.29500919   0.          23.8237523   82.18387441\n",
      "  12.95001554 110.53172241  77.46149516  31.58458685 405.23036748\n",
      "  36.81141872 411.84017093  15.83046157  92.87444529  20.87190625\n",
      " 415.49170232  22.41992225 180.00646554  36.80503843  14.73450066]\n",
      "45-th iteration, loss: 0.06471928222435833, 19 gd steps\n",
      "insert gradient: -0.0021532764037704498\n",
      "45-th iteration, new layer inserted. now 45 layers\n",
      "[ 78.3457415  124.02490845 121.15902621 109.97319292  86.59266625\n",
      " 116.24304085  64.0912712  137.6796341   60.59107328  87.78638375\n",
      "  45.31526394  89.66972516  52.20273699  67.60604335  22.9738733\n",
      "  88.18845071  78.85690995  96.94558098  29.40215945  62.82446281\n",
      "  56.14227521   2.84656514  55.38784725 100.58396644  19.70678242\n",
      "  16.32161592  93.82393533   6.12885583  22.65312865  83.83141176\n",
      "  13.34766325 106.82438511  77.36407536  32.33069958 404.91745217\n",
      "  37.09036508 411.91835977  15.8068787   93.35722245  20.92016943\n",
      " 415.3749081   22.24887503 179.59314369  36.82488713  14.87853033]\n",
      "46-th iteration, loss: 0.06401315955046387, 118 gd steps\n",
      "insert gradient: -0.001274934487436875\n",
      "46-th iteration, new layer inserted. now 45 layers\n",
      "[ 78.59592206 123.31692711 121.93503452 108.50740367  87.18997675\n",
      " 116.38277923  64.63119186 137.65823234  60.42721702  89.13957358\n",
      "  45.50212089  88.66988707  52.27663672  70.48338526  22.52061398\n",
      "  85.215034    77.66086435 105.84173096  28.69616434  60.927295\n",
      "  60.85031702   6.62274106  46.27750787 100.38643081  24.80591968\n",
      "   8.6140946   86.47082898  13.57942223  23.17920548  96.14511394\n",
      "  19.22046882  78.09211566  82.65919177  37.70435203 402.40673806\n",
      "  37.14323546 412.09949007  15.02593471  93.67953286  21.71862269\n",
      " 414.48898712  22.51333469 179.50176934  38.62725854  14.77309933]\n",
      "47-th iteration, loss: 0.06335899135809, 80 gd steps\n",
      "insert gradient: -0.0017555485230303623\n",
      "47-th iteration, new layer inserted. now 45 layers\n",
      "[ 78.74940474 124.44163295 122.55603307 108.73713865  87.02190926\n",
      " 117.08520135  64.69282932 138.82218068  61.2545658   90.10850168\n",
      "  45.5691558   88.46021686  51.77839228  73.70619587  22.45948438\n",
      "  83.41527248  75.36468969 113.17422134  31.17699593  58.67189011\n",
      "  63.29443333  17.96429098  32.07760481 100.19373489  28.09165421\n",
      "   2.17333862  84.7609574   21.73703706  20.99571147  99.5666309\n",
      "  25.60314413  60.4674783   84.88512387  44.36827048 401.88298186\n",
      "  35.97565038 414.21156663  13.21122741  94.17978555  22.64459245\n",
      " 414.08810907  23.13271196 180.16396811  40.99430411  14.88510309]\n",
      "48-th iteration, loss: 0.0631302073592734, 37 gd steps\n",
      "insert gradient: -0.0014099614353238147\n",
      "48-th iteration, new layer inserted. now 45 layers\n",
      "[ 78.92965583 124.64968361  73.54608315   0.          49.0307221\n",
      " 108.85595808  86.92995247 117.1663618   64.50987719 139.1145625\n",
      "  61.29440792  90.39169376  45.74774829  88.32202781  51.80963479\n",
      "  74.900933    22.45127075  82.61383577  74.47204073 115.62397753\n",
      "  32.40489524  57.36338297  62.79072343  21.98098613  29.08895581\n",
      "  99.77631038 114.62601814  24.4347746   19.50159729  99.99509978\n",
      "  27.44267895  56.42962809  84.78293701  46.45202047 402.29355782\n",
      "  35.42468567 414.94872272  12.41456484  94.38304215  22.74512342\n",
      " 413.8108009   23.246981   180.97663079  41.75485645  15.07872228]\n",
      "49-th iteration, loss: 0.06304676220534006, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "49-th iteration, new layer inserted. now 47 layers\n",
      "[ 79.1227284  124.6773173   73.00203842   1.52108473  48.5169734\n",
      " 108.79438313  86.56662484 117.34194161  65.0024579  139.19667126\n",
      "  61.66923577  90.46777955  45.54335732  88.33016008  51.9132062\n",
      "  75.05612034  22.39374298  82.55232357  74.33556818 116.07311544\n",
      "  32.54339076  57.16163707  62.75572982  22.78773696  28.60923303\n",
      "  99.69449114 114.65804528  24.94301446  19.15063843 100.05621315\n",
      "  27.771491    55.77913192  84.75819304  46.88601059 402.35988823\n",
      "  35.33074817  83.02147216   0.         332.08588863  12.2524217\n",
      "  94.56888311  22.79766773 413.75422947  23.23575052 181.15340611\n",
      "  41.95633938  15.14964973]\n",
      "50-th iteration, loss: 0.06257650855511669, 31 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "50-th iteration, new layer inserted. now 49 layers\n",
      "[ 79.47229339 124.68806153  70.15202324   6.56848446  46.51467516\n",
      " 110.1903468   85.97129337 119.19242302  65.33890607 138.68223045\n",
      "  62.43502118  90.94668277  45.26968011  88.21229482  52.33365795\n",
      "  75.90580711  21.90282483  82.4590512   73.53873047 117.92468578\n",
      "  33.13926391  56.46457274  62.73264568  26.12925465  26.19821061\n",
      "  99.46640614 113.82424823  27.65669117  18.03490668 100.28589982\n",
      "  28.98759024  52.9863476   84.63381943  49.13228854 201.68666328\n",
      "   0.         201.68666328  35.55560424  83.07235751   6.33637365\n",
      " 330.71711786   9.28515781  94.54524292  23.71629044 413.07367997\n",
      "  22.72619059 181.73618425  42.9444122   15.59326762]\n",
      "51-th iteration, loss: 0.062135014986117544, 14 gd steps\n",
      "insert gradient: -0.005436591870959293\n",
      "51-th iteration, new layer inserted. now 51 layers\n",
      "[7.93816335e+01 1.24623528e+02 6.96444899e+01 6.98850729e+00\n",
      " 0.00000000e+00 4.44089210e-16 4.59097877e+01 1.10332599e+02\n",
      " 8.57248641e+01 1.19372244e+02 6.52021197e+01 1.38623081e+02\n",
      " 6.26847581e+01 9.13113240e+01 4.54663315e+01 8.80185370e+01\n",
      " 5.23996098e+01 7.64500983e+01 2.18156099e+01 8.23190854e+01\n",
      " 7.33342501e+01 1.18417872e+02 3.36060954e+01 5.61532206e+01\n",
      " 6.25869768e+01 2.71866169e+01 2.54645641e+01 9.94222820e+01\n",
      " 1.14049390e+02 2.84422325e+01 1.75732030e+01 1.00562279e+02\n",
      " 2.96019193e+01 5.22621941e+01 8.40828187e+01 4.95434963e+01\n",
      " 2.00620058e+02 4.88702310e+00 2.00727589e+02 3.54670727e+01\n",
      " 8.36385952e+01 7.84384214e+00 3.30035173e+02 8.30131474e+00\n",
      " 9.43650155e+01 2.36919573e+01 4.12322904e+02 2.25006798e+01\n",
      " 1.81966188e+02 4.31631480e+01 1.58454131e+01]\n",
      "52-th iteration, loss: 0.06206405137530696, 9 gd steps\n",
      "insert gradient: -0.0015363346561375211\n",
      "52-th iteration, new layer inserted. now 51 layers\n",
      "[7.94604141e+01 1.24682041e+02 6.97803954e+01 7.11866496e+00\n",
      " 1.41050892e-01 1.30099179e-01 4.60508920e+01 1.10401540e+02\n",
      " 8.58246057e+01 1.19424754e+02 6.52677621e+01 1.38641771e+02\n",
      " 6.27044512e+01 9.13066952e+01 4.54425119e+01 8.80046080e+01\n",
      " 5.23816244e+01 7.64387267e+01 2.17671464e+01 8.22948476e+01\n",
      " 7.32982470e+01 1.18410840e+02 3.35797541e+01 5.61328078e+01\n",
      " 6.25695956e+01 2.71889817e+01 2.54313529e+01 9.94135488e+01\n",
      " 1.14038667e+02 2.84422139e+01 1.75448933e+01 1.00557384e+02\n",
      " 2.95843856e+01 5.22418370e+01 8.40918123e+01 4.95534442e+01\n",
      " 2.00590219e+02 4.93133939e+00 2.00710080e+02 3.54671945e+01\n",
      " 8.36533067e+01 7.87604024e+00 3.30031350e+02 8.29119735e+00\n",
      " 9.43698528e+01 2.36935634e+01 4.12319317e+02 2.25039073e+01\n",
      " 1.81976014e+02 4.31672442e+01 1.58459516e+01]\n",
      "53-th iteration, loss: 0.06103071884918987, 36 gd steps\n",
      "insert gradient: -0.004077481994771136\n",
      "53-th iteration, new layer inserted. now 51 layers\n",
      "[7.90052872e+01 1.25124417e+02 6.90742178e+01 1.18606897e+01\n",
      " 4.25265414e+01 1.11016690e+02 0.00000000e+00 7.10542736e-15\n",
      " 8.44699138e+01 1.20924061e+02 6.51223660e+01 1.37888983e+02\n",
      " 6.40660380e+01 9.23112815e+01 4.52338670e+01 8.68009561e+01\n",
      " 5.22029469e+01 7.94076808e+01 2.14228183e+01 8.10504694e+01\n",
      " 7.15875078e+01 1.20903514e+02 3.65187371e+01 5.31362424e+01\n",
      " 6.12261636e+01 3.58198158e+01 2.03798935e+01 9.86763128e+01\n",
      " 1.13231584e+02 3.44526401e+01 1.42693192e+01 1.02269249e+02\n",
      " 3.21479635e+01 4.50792953e+01 8.51121440e+01 5.56629288e+01\n",
      " 1.92258795e+02 1.36578791e+01 1.98383553e+02 3.65304506e+01\n",
      " 8.67415879e+01 1.28448938e+01 3.26797403e+02 4.76285629e+00\n",
      " 9.57463223e+01 2.43662168e+01 4.09738779e+02 2.20570037e+01\n",
      " 1.85385067e+02 4.42562128e+01 1.68213060e+01]\n",
      "54-th iteration, loss: 0.06090203724558858, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "54-th iteration, new layer inserted. now 51 layers\n",
      "[ 79.58916099 125.43250134  69.12609348  12.03469957  42.33077638\n",
      " 111.0031506   84.72701295 121.10327957  65.36851556 138.04284261\n",
      "  64.11220962  92.35885827  45.31997224  86.81864227  52.25766647\n",
      "  79.52924749  21.48335834  81.04406794  71.62219869 121.12613318\n",
      "  36.7998468   52.87196312  60.9906555   36.24472514  20.12788551\n",
      "  98.60988395 113.3083489   35.04221132  14.05480484 102.42537485\n",
      "  32.44453441  44.6013888   85.2580117   56.00045625 191.90357503\n",
      "  14.03121254 198.45567462  36.44771979  86.92357379  13.005908\n",
      " 326.60877063   4.53573741  95.73558347  24.29831603 204.73854218\n",
      "   0.         204.73854218  21.94773753 185.42207519  44.24792792\n",
      "  16.93058608]\n",
      "55-th iteration, loss: 0.05965866689528969, 17 gd steps\n",
      "insert gradient: -0.006556635471191229\n",
      "55-th iteration, new layer inserted. now 51 layers\n",
      "[7.88101690e+01 1.27361336e+02 6.98121468e+01 1.56030090e+01\n",
      " 0.00000000e+00 8.88178420e-16 3.80609021e+01 1.11048125e+02\n",
      " 8.27955562e+01 1.22014360e+02 6.51258648e+01 1.37118680e+02\n",
      " 6.57439344e+01 9.37665103e+01 4.47566534e+01 8.51710771e+01\n",
      " 5.16064715e+01 8.17248792e+01 2.17686435e+01 7.95037203e+01\n",
      " 6.93003854e+01 1.22656968e+02 4.18946270e+01 4.83110196e+01\n",
      " 5.66311146e+01 4.85267377e+01 1.57297824e+01 9.60420161e+01\n",
      " 1.08832655e+02 4.42469957e+01 1.07855795e+01 1.04603102e+02\n",
      " 3.86148242e+01 3.38219731e+01 8.52529908e+01 6.35853785e+01\n",
      " 1.84762382e+02 1.65301072e+01 2.00977569e+02 3.44855092e+01\n",
      " 8.94062922e+01 1.68296740e+01 4.22211014e+02 2.80850971e+01\n",
      " 1.99263705e+02 1.27153107e+01 1.96087381e+02 2.37044341e+01\n",
      " 1.85108289e+02 4.74346231e+01 1.49696142e+01]\n",
      "56-th iteration, loss: 0.059496572372995604, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "56-th iteration, new layer inserted. now 53 layers\n",
      "[7.85682174e+01 1.27303159e+02 6.98301590e+01 1.58463347e+01\n",
      " 7.98181762e-02 2.37203288e-01 3.81485566e+01 1.11100789e+02\n",
      " 8.31390773e+01 1.22286026e+02 6.51514933e+01 1.37203688e+02\n",
      " 6.56292453e+01 9.37019939e+01 4.48381121e+01 8.52413099e+01\n",
      " 5.16809465e+01 8.17399797e+01 2.17727915e+01 7.94768768e+01\n",
      " 6.92313370e+01 1.22643873e+02 4.19393462e+01 4.83335036e+01\n",
      " 5.66933511e+01 4.87462386e+01 1.59241988e+01 9.60722726e+01\n",
      " 1.08918775e+02 4.44275526e+01 1.05961820e+01 1.04642082e+02\n",
      " 3.86399188e+01 3.36227310e+01 8.49350873e+01 6.36047603e+01\n",
      " 1.84698558e+02 1.65874949e+01 2.00956105e+02 3.44638455e+01\n",
      " 8.95246171e+01 1.69159521e+01 2.11117813e+02 0.00000000e+00\n",
      " 2.11117813e+02 2.81421744e+01 1.99225655e+02 1.28014739e+01\n",
      " 1.95944251e+02 2.37191697e+01 1.85026042e+02 4.74420338e+01\n",
      " 1.49465801e+01]\n",
      "57-th iteration, loss: 0.0583219923479565, 46 gd steps\n",
      "insert gradient: -0.0025006399420581774\n",
      "57-th iteration, new layer inserted. now 51 layers\n",
      "[ 77.09488355 128.32175471  69.19239213  26.42639559  31.09846527\n",
      " 110.47532807  81.76897068 125.72577739  64.75794119 136.78186821\n",
      "  67.00664559  94.50385381  44.92396466  83.76276503  51.68036284\n",
      "  82.65249967  22.04375274  77.49288924  67.77816091 123.96069418\n",
      "  44.9467604   45.49325381  54.56838995  55.43661781  14.20793422\n",
      "  93.41091792 106.59218606  51.11672475   7.42297116 106.90638099\n",
      "  42.97110595  28.93617926  83.67103504  68.41128127 181.24930403\n",
      "  18.9013536  202.33148538  33.7197908   93.04270347  19.52849294\n",
      " 207.60148368   7.79918608 206.37025972  31.96397229 200.4961803\n",
      "  18.78222001 190.97276159  26.07478208 182.02894002  46.98099056\n",
      "  13.39613789]\n",
      "58-th iteration, loss: 0.05327874190843488, 30 gd steps\n",
      "insert gradient: -0.0038595500582014075\n",
      "58-th iteration, new layer inserted. now 53 layers\n",
      "[6.30550116e+01 1.35948143e+02 7.22427575e+01 7.16897171e+01\n",
      " 1.09620343e+01 1.09916502e+02 6.79464254e+01 1.43868716e+02\n",
      " 6.55509636e+01 1.24765763e+02 7.80606583e+01 9.79707235e+01\n",
      " 4.52416320e+01 7.81698997e+01 5.32279434e+01 8.52431404e+01\n",
      " 0.00000000e+00 7.10542736e-15 2.31871669e+01 7.09180403e+01\n",
      " 6.44828147e+01 1.20943907e+02 5.56675183e+01 4.29753796e+01\n",
      " 4.62023791e+01 7.15224853e+01 1.20754181e+01 8.18339208e+01\n",
      " 1.00941461e+02 6.00334117e+01 2.67355418e+00 1.12453503e+02\n",
      " 5.66912284e+01 1.85643108e+01 8.14645525e+01 7.96225249e+01\n",
      " 1.76062663e+02 1.76216120e+01 2.06113153e+02 3.07924798e+01\n",
      " 9.59592658e+01 2.48324966e+01 2.04565139e+02 1.18714221e+01\n",
      " 2.00571981e+02 3.66746826e+01 1.99899290e+02 2.13658602e+01\n",
      " 1.89555285e+02 2.95577411e+01 1.80549323e+02 4.70633572e+01\n",
      " 1.18331336e+01]\n",
      "59-th iteration, loss: 0.04977498232395374, 75 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "59-th iteration, new layer inserted. now 53 layers\n",
      "[6.58534000e+01 1.31111253e+02 6.97084532e+01 9.71584333e+01\n",
      " 1.47579993e-01 1.13042898e+02 6.48049515e+01 1.44721793e+02\n",
      " 6.82028870e+01 1.13980012e+02 8.02381636e+01 1.00643605e+02\n",
      " 4.60030397e+01 7.79552909e+01 4.98968609e+01 9.04681109e+01\n",
      " 2.66638326e+01 5.98739919e+01 6.38585579e+01 1.23397303e+02\n",
      " 5.54705212e+01 4.57539440e+01 4.51529354e+01 7.62421688e+01\n",
      " 1.24053526e+01 7.11329184e+01 9.63253190e+01 6.53817514e+01\n",
      " 3.09392262e-01 1.16635277e+02 6.28533635e+01 1.11365149e+01\n",
      " 4.06983944e+01 0.00000000e+00 4.06983944e+01 8.41478173e+01\n",
      " 1.76658534e+02 1.14428640e+01 2.07009457e+02 2.98485047e+01\n",
      " 9.79747626e+01 2.83569843e+01 2.04218064e+02 9.52359772e+00\n",
      " 1.97889438e+02 3.63977242e+01 2.01900521e+02 1.95836301e+01\n",
      " 1.89175449e+02 2.82227120e+01 1.81426528e+02 4.39875936e+01\n",
      " 1.20254932e+01]\n",
      "60-th iteration, loss: 0.04870920336503433, 420 gd steps\n",
      "insert gradient: -0.000857609570903244\n",
      "60-th iteration, new layer inserted. now 51 layers\n",
      "[6.52991525e+01 1.31735828e+02 6.83857821e+01 2.10401815e+02\n",
      " 6.61306195e+01 1.41439348e+02 6.93648434e+01 1.14060749e+02\n",
      " 8.05696946e+01 1.01166335e+02 4.57938614e+01 7.96648364e+01\n",
      " 4.94769044e+01 8.99165928e+01 2.92516636e+01 5.62139885e+01\n",
      " 5.96904339e+01 1.27104585e+02 5.67812982e+01 4.82597895e+01\n",
      " 4.43891548e+01 8.13433618e+01 1.33400453e+01 5.77956779e+01\n",
      " 8.91693091e+01 6.53732112e+01 2.47511059e-01 1.16623493e+02\n",
      " 7.02605286e+01 4.72645523e-02 4.15283886e+01 5.27816143e+00\n",
      " 4.38846137e+01 8.83393931e+01 1.76498541e+02 7.92374567e+00\n",
      " 2.06182045e+02 2.60649387e+01 1.02683069e+02 3.03926583e+01\n",
      " 2.05708272e+02 1.13742430e+01 1.93612906e+02 3.46859684e+01\n",
      " 2.01530099e+02 2.03553528e+01 1.86027428e+02 2.68774393e+01\n",
      " 1.82070450e+02 4.58924427e+01 1.32447841e+01]\n",
      "61-th iteration, loss: 0.04861282009922236, 10 gd steps\n",
      "insert gradient: -0.0006279270559348156\n",
      "61-th iteration, new layer inserted. now 53 layers\n",
      "[6.53254118e+01 1.31745423e+02 6.83844093e+01 2.10376855e+02\n",
      " 6.61163375e+01 1.41436323e+02 6.93793339e+01 1.14071471e+02\n",
      " 8.05815995e+01 1.01175817e+02 4.58042050e+01 7.96699543e+01\n",
      " 4.94873839e+01 8.99234623e+01 2.92574419e+01 5.62109200e+01\n",
      " 5.96937670e+01 1.27114301e+02 5.67894601e+01 4.82540741e+01\n",
      " 4.43632755e+01 8.13298994e+01 1.32928528e+01 5.77542265e+01\n",
      " 8.91076572e+01 6.53489108e+01 2.07014975e-01 1.16599605e+02\n",
      " 7.02353493e+01 1.33288178e-02 4.15031428e+01 5.26704147e+00\n",
      " 4.38650221e+01 8.83351222e+01 1.76485510e+02 7.91435667e+00\n",
      " 2.06165523e+02 2.60550190e+01 1.02678648e+02 3.03910489e+01\n",
      " 2.05697526e+02 1.13738566e+01 1.93600309e+02 3.46832271e+01\n",
      " 2.01524325e+02 2.03534896e+01 1.86022709e+02 2.68768051e+01\n",
      " 1.36555682e+02 0.00000000e+00 4.55185606e+01 4.58958178e+01\n",
      " 1.32482631e+01]\n",
      "62-th iteration, loss: 0.04837583820076824, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "62-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.50782296 131.84530851  68.34156523 209.90687768  66.21121785\n",
      " 141.39762066  70.03532255 114.25348776  80.43097879 101.48950588\n",
      "  46.06958905  79.81149779  49.61913259  90.38727978  29.85192837\n",
      "  55.12374008  58.81513909 127.81833211  57.97866887  48.33318165\n",
      "  43.50088288  81.70429584  13.38523007  55.64505949  88.34409808\n",
      "  65.21758142   0.92177765  29.1081466    0.          87.32443981\n",
      " 111.9655876    6.71513035  44.04017099  88.63580997 176.98024046\n",
      "   7.71488734 205.64831615  25.70003889 103.57819713  30.7332919\n",
      " 205.23184841  11.81073606 192.69230492  34.57487922 200.97068483\n",
      "  20.1207091  185.64346472  26.49919622 136.6410835    1.98176901\n",
      "  45.62695174  46.51203655  13.6104732 ]\n",
      "63-th iteration, loss: 0.045853149378389205, 999 gd steps\n",
      "insert gradient: -0.0012935761918620844\n",
      "63-th iteration, new layer inserted. now 53 layers\n",
      "[ 68.04301409 134.09557738  68.62071729 211.08989998  66.09940141\n",
      " 141.61118693  70.37940141 115.20621856  80.52452027 105.85739514\n",
      "  48.50487178  81.25978832  49.82122805  93.20138895  38.03689827\n",
      "  43.95717439  54.47786773 119.56829     72.28800869  48.84981435\n",
      "  41.87414236  90.38731388  21.06412039  24.77165217  86.08912705\n",
      "  84.97395834   3.57127977  82.64896512 119.28880178  21.50765929\n",
      "  32.54737947  95.78528849 181.66157476   3.46896012 205.17735345\n",
      "  21.54653631 107.0635971   35.84297284 199.43822022  13.44842508\n",
      " 192.63602466  37.38360964 196.29157507  18.43710568 146.63273202\n",
      "   0.          48.87757734  24.42151458 133.36368057  13.58027024\n",
      "  40.66709083  56.02978572  10.67902858]\n",
      "64-th iteration, loss: 0.043896742613145724, 90 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "64-th iteration, new layer inserted. now 53 layers\n",
      "[ 66.79828062 140.84347858  69.06207215 207.45735595  68.53428457\n",
      " 137.35023631  69.41993862 114.77326058  81.93773419 107.59349258\n",
      "  49.05259264  81.96235531  49.85594951  93.49937349  43.67910294\n",
      "  38.15335994  52.64993196 114.70490078  73.98387778  55.38143905\n",
      "  39.20404945  91.22792064  16.06408998   8.71432017 101.88929048\n",
      "  86.33916445   3.82173667  71.74971222 131.08076902  23.7552989\n",
      "  23.24748928  94.96006066 392.82015781  14.86209371 108.97612322\n",
      "  42.99176767 197.22822096  18.32566969 186.25173619  39.86070108\n",
      " 197.9245782   18.45904618  68.62130749   0.          68.62130749\n",
      "  27.12877752  37.07298032  40.89188521 130.84379297  16.80255842\n",
      "  36.69948128  74.1520228    6.81606595]\n",
      "65-th iteration, loss: 0.04298711797744187, 75 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "65-th iteration, new layer inserted. now 53 layers\n",
      "[ 67.72776517 139.23545874  70.7734994  207.49780337  68.01156728\n",
      " 140.56484976  68.77351983 113.97065531  82.91591065 108.39345982\n",
      "  50.12107516  81.65394412  50.38483111  94.64955629  46.12846398\n",
      "  37.27952083  51.18987042 110.01436893  76.97052614  57.72763497\n",
      "  38.52715538  94.17461307 121.14973484  89.17193671   2.33097666\n",
      "  67.24235084 140.06849462  30.21296261  17.42366068  93.48331404\n",
      "  98.33774134   0.         295.01322401  12.79984438 112.17218307\n",
      "  42.18430095 198.30202039  14.08838977 185.88706162  39.14203552\n",
      " 200.35527175  22.17269289  57.01875099  17.9508178   59.475617\n",
      "  31.90227747  36.18541915  37.43179516 134.75108183  23.0328927\n",
      "  33.60066531  79.71434798   5.22705152]\n",
      "66-th iteration, loss: 0.04269981254124901, 20 gd steps\n",
      "insert gradient: -0.0006394387879338\n",
      "66-th iteration, new layer inserted. now 55 layers\n",
      "[6.96282215e+01 1.37556319e+02 7.06608259e+01 2.07033841e+02\n",
      " 6.80477279e+01 1.43486109e+02 6.80196967e+01 1.12178098e+02\n",
      " 8.33389751e+01 1.08289550e+02 5.04917542e+01 8.17852721e+01\n",
      " 5.03753038e+01 9.46902413e+01 4.68053174e+01 3.76088222e+01\n",
      " 0.00000000e+00 3.55271368e-15 5.05934275e+01 1.07172864e+02\n",
      " 7.80831108e+01 5.97658326e+01 3.79280621e+01 9.17372209e+01\n",
      " 1.20960215e+02 9.04789749e+01 1.41654915e+00 6.62991512e+01\n",
      " 1.41309150e+02 3.44691617e+01 1.73391023e+01 9.14123066e+01\n",
      " 9.51986366e+01 7.66795560e+00 2.92360450e+02 1.06671497e+01\n",
      " 1.13659975e+02 4.30523280e+01 1.96770612e+02 1.34844808e+01\n",
      " 1.87465793e+02 3.73884917e+01 2.01835227e+02 2.28438978e+01\n",
      " 5.56339474e+01 2.06579114e+01 5.71257821e+01 3.24479226e+01\n",
      " 3.60556378e+01 3.55120504e+01 1.36717167e+02 2.62743715e+01\n",
      " 3.06126933e+01 8.01037396e+01 3.76373066e+00]\n",
      "67-th iteration, loss: 0.042478381907203475, 37 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "67-th iteration, new layer inserted. now 55 layers\n",
      "[ 69.14083804 138.38077738  68.32255129 208.93917505  67.41160044\n",
      " 145.62536966  68.61015058 112.16675439  82.64738563 110.32979413\n",
      "  50.72186398  81.37264664  50.23504733  95.48055688  48.11417654\n",
      "  36.61331615  50.02486237 105.75545695  78.7165966   61.37043232\n",
      "  37.6775712   89.81979843 119.70730977  92.34997261   0.68209484\n",
      "  16.5270782    0.          49.58123459 142.18785439  37.50554426\n",
      "  18.59745573  87.06852215  91.86408766  13.26090791 289.8621963\n",
      "   8.76011243 117.32928362  44.70917112 191.81879457  15.9970815\n",
      " 188.81608177  35.57809159 205.15879122  22.82175677  54.5699524\n",
      "  23.05232395  52.62642878  35.19443482  35.50678322  34.34270907\n",
      " 137.85363062  31.0249244   25.76981182  79.97496274   1.36135999]\n",
      "68-th iteration, loss: 0.042375976696697605, 999 gd steps\n",
      "insert gradient: -0.001093331559046903\n",
      "68-th iteration, new layer inserted. now 55 layers\n",
      "[ 68.92797662 138.75731618  69.39482788 208.34626364  67.11999989\n",
      " 145.07036058  68.38140865 112.29188261  83.07493041 109.90063012\n",
      "  50.65724514  81.72477092  50.33038083  95.21239145  48.58258377\n",
      "  36.99844182  49.4381359  105.12055843  78.99060345  62.11899196\n",
      "  37.37511851  89.92286026 119.12488589 108.95063033   1.11629043\n",
      "  49.3046669  141.88599637  39.14085523  19.33562673  85.45323461\n",
      "  91.69240442  15.22771256 288.84962109   7.35780891  88.59080027\n",
      "   0.          29.53026676  45.11499846 191.15019907  17.44963577\n",
      " 187.53498858  35.0534187  207.45363123  22.10726553  53.78491298\n",
      "  23.69284746  51.12673131  37.96847356  35.06072137  34.35983982\n",
      " 137.15030167  32.46840853  24.72707113  79.88080865   0.58289103]\n",
      "69-th iteration, loss: 0.04139306524971691, 43 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "69-th iteration, new layer inserted. now 56 layers\n",
      "[ 69.82573583 138.59104278  68.03415761 208.67473706  66.63461422\n",
      " 144.74570966  69.09807686 111.77963604  82.69377475 110.40332857\n",
      "  50.98010239  81.98547634  50.36301752  95.55490662  50.79217919\n",
      "  37.23112364  46.58865484 102.48656633  81.04412304  62.24544579\n",
      "  37.01368428  89.94932944 115.69697238 107.34916849   5.06047659\n",
      "  46.50188223 136.49526398  40.57904713  21.78636386  85.74277414\n",
      "  88.02330976  21.55582691 291.82148289   1.15793515  86.00433019\n",
      "  20.38495353  21.75397161  56.42577859  92.34682649   0.\n",
      "  92.34682649  20.09051989 184.21759858  34.9249448  211.55350154\n",
      "  22.3518838   53.00678326  22.82441581  48.18804215  44.43363454\n",
      "  33.97271938  35.59544391 133.93582269  32.86472211  24.52144489\n",
      "  79.91377084]\n",
      "70-th iteration, loss: 0.04040623365785811, 56 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "70-th iteration, new layer inserted. now 58 layers\n",
      "[6.90664785e+01 1.39493263e+02 6.77821728e+01 2.09063710e+02\n",
      " 6.51604107e+01 1.45392547e+02 7.01614696e+01 1.13216175e+02\n",
      " 8.18997234e+01 1.10404171e+02 5.11537236e+01 8.28983524e+01\n",
      " 5.00242808e+01 9.56289352e+01 5.27951370e+01 3.76849741e+01\n",
      " 4.41997930e+01 1.01436670e+02 8.16833314e+01 6.59860569e+01\n",
      " 3.57108216e+01 8.67258730e+01 1.13655800e+02 1.05295756e+02\n",
      " 1.02716302e+01 4.07983646e+01 1.29241618e+02 4.67796373e+01\n",
      " 2.46919209e+01 8.53142583e+01 8.69846639e+01 3.34688695e+01\n",
      " 1.90300366e+02 0.00000000e+00 9.51501828e+01 8.94371157e-03\n",
      " 7.97251912e+01 2.57181014e+01 2.33896952e+01 6.56496027e+01\n",
      " 8.73830472e+01 8.76069351e+00 9.15316470e+01 2.19483434e+01\n",
      " 1.75572235e+02 3.27024764e+01 2.20777551e+02 1.79990628e+01\n",
      " 5.02802710e+01 1.93075548e+01 4.81689586e+01 5.10756642e+01\n",
      " 3.34425185e+01 3.45876805e+01 1.30980219e+02 3.15813233e+01\n",
      " 2.52531810e+01 7.99137708e+01]\n",
      "71-th iteration, loss: 0.03991839695884391, 52 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "71-th iteration, new layer inserted. now 58 layers\n",
      "[ 68.83452222 137.75966295  68.09322615 209.11165818  64.62189643\n",
      " 146.0002313   70.50031367 112.58960121  82.04581721 110.60554208\n",
      "  51.40537244  82.75169356  49.77920644  95.36408738  52.96967415\n",
      "  38.89217342  42.81800313 100.81288036  82.03169844  67.96233142\n",
      "  34.53792807  85.19172968 113.48264225 104.14271864  11.95766567\n",
      "  37.44013189 127.92746223  51.05254941  25.56391725  82.18678881\n",
      "  87.52505335  39.49321265 186.82246746   7.96893244 169.72833716\n",
      "  30.41684049  20.12451367  69.34431996  86.89739867   6.84129146\n",
      "  92.91417457  23.2900105  173.44238014  32.21905314 148.07813315\n",
      "   0.          74.03906658  15.31696341  49.8428097   18.84504283\n",
      "  48.70597202  53.99735722  32.88899734  33.918985   129.20118254\n",
      "  32.67585091  25.63377778  79.91377084]\n",
      "72-th iteration, loss: 0.03901268401062574, 57 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "72-th iteration, new layer inserted. now 60 layers\n",
      "[ 68.20208212 137.47154704  69.19366057 209.19999343  62.64993911\n",
      " 148.01350076  71.78977959 111.73640378  81.22706885 111.22040986\n",
      "  52.34177665  83.87993492  49.19101449  94.94329945  54.05302487\n",
      "  43.1520234   38.12346151  99.28037554  82.64008329  75.97920983\n",
      "  31.766704    79.68929629 114.16924644 101.92738422  15.79462969\n",
      "  27.79145839  86.72064729   0.          43.36032365  57.07779561\n",
      "  31.00238526  63.57678998  88.05577782  55.45559761 179.98799878\n",
      "  13.8523427  163.25542404  39.72951543  15.61420107  83.9231973\n",
      "  87.29379896   3.21864782  96.78653691  22.38941464 170.51417036\n",
      "  20.29325906 144.61003261  16.90938363  69.15129214  11.78932866\n",
      "  52.09742369  13.99157894  51.48971306  63.11399571  30.27649517\n",
      "  36.48482601 125.2212993   37.51916656  22.95310168  79.91377084]\n",
      "73-th iteration, loss: 0.03859570811495543, 37 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "73-th iteration, new layer inserted. now 62 layers\n",
      "[ 67.83487447 137.05948873  68.18891507 209.78145005  64.45200643\n",
      " 147.40971557  70.3090457  112.33915113  82.50012496 112.28029557\n",
      "  52.36740315  83.50587403  49.14424128  94.91545566  53.78843522\n",
      "  43.27326189  38.33980383  99.19471948  82.5525759   76.74376021\n",
      "  31.81264026  79.50370501 115.08656456 101.42592747  16.61044082\n",
      "  25.6087456   83.43923119   8.68086396  39.00491555  59.82656612\n",
      "  32.90808777  58.51568142  88.74918812  57.71405964 179.61147549\n",
      "  14.72433992 162.10943855  40.70009204  14.94040925  85.64813995\n",
      "  87.79073458   3.16924862  97.63988474  22.34776712 112.84929594\n",
      "   0.          56.42464797  18.90531528 145.15514462  17.73376575\n",
      "  68.91769383  10.60080769  52.99040285  13.89019591  51.57625779\n",
      "  64.11617412  29.77868485  36.47481896 125.66186412  38.2563516\n",
      "  22.25283119  79.91377084]\n",
      "74-th iteration, loss: 0.038196120288684986, 40 gd steps\n",
      "insert gradient: -0.0005439611274246872\n",
      "74-th iteration, new layer inserted. now 64 layers\n",
      "[6.80555909e+01 1.36883745e+02 6.83639168e+01 2.10165795e+02\n",
      " 6.40032622e+01 1.47387932e+02 7.13571012e+01 1.12165684e+02\n",
      " 8.18040050e+01 1.12855337e+02 5.28510363e+01 8.33488200e+01\n",
      " 4.89346222e+01 9.48004257e+01 5.43945767e+01 4.39677719e+01\n",
      " 3.74592188e+01 9.85748666e+01 0.00000000e+00 1.42108547e-14\n",
      " 8.25681456e+01 7.94344504e+01 3.09843618e+01 7.81757871e+01\n",
      " 1.15965065e+02 1.01340125e+02 1.93163682e+01 2.14973973e+01\n",
      " 8.18123560e+01 1.35928858e+01 3.45885602e+01 6.46543316e+01\n",
      " 3.45995304e+01 5.27089622e+01 8.87144238e+01 6.21516122e+01\n",
      " 1.79174112e+02 1.62973318e+01 1.60395978e+02 4.22730692e+01\n",
      " 1.40574156e+01 8.73061701e+01 8.81924312e+01 4.04362550e+00\n",
      " 9.81246078e+01 2.14075734e+01 1.10297210e+02 6.34560508e+00\n",
      " 5.27262349e+01 1.93073436e+01 1.44817794e+02 1.95252863e+01\n",
      " 6.86686370e+01 1.00531038e+01 5.47766489e+01 1.31713688e+01\n",
      " 5.10839106e+01 6.58559837e+01 2.94416255e+01 3.53192521e+01\n",
      " 1.26573809e+02 3.91219538e+01 2.12505986e+01 7.99137708e+01]\n",
      "75-th iteration, loss: 0.03763291146963538, 19 gd steps\n",
      "insert gradient: -0.0021423565201589938\n",
      "75-th iteration, new layer inserted. now 64 layers\n",
      "[6.74595179e+01 1.33592026e+02 0.00000000e+00 1.42108547e-14\n",
      " 6.66230730e+01 2.12490047e+02 6.32291642e+01 1.47497933e+02\n",
      " 7.32176649e+01 1.09667587e+02 8.32363610e+01 1.14488517e+02\n",
      " 5.32186912e+01 8.44617283e+01 4.85420362e+01 9.39322131e+01\n",
      " 5.46153026e+01 5.17391496e+01 3.25264850e+01 9.34031227e+01\n",
      " 8.42158553e+01 9.33461061e+01 2.71870618e+01 6.98935215e+01\n",
      " 1.17146326e+02 1.00702076e+02 2.88817777e+01 6.90718000e+00\n",
      " 7.91970340e+01 2.39414238e+01 2.53343791e+01 8.20194437e+01\n",
      " 3.77804625e+01 3.61668547e+01 8.68105871e+01 8.52700255e+01\n",
      " 1.78810839e+02 1.71870503e+01 1.51790817e+02 5.48156410e+01\n",
      " 9.74617512e+00 8.99164366e+01 8.99654322e+01 6.46195284e+00\n",
      " 9.89115032e+01 1.99396595e+01 1.08524832e+02 2.32920227e+01\n",
      " 3.57917592e+01 2.57555293e+01 1.42242122e+02 2.30797234e+01\n",
      " 6.44024151e+01 1.04293563e+01 6.29856235e+01 1.08834183e+01\n",
      " 4.75095036e+01 7.20148382e+01 2.63723072e+01 3.33587555e+01\n",
      " 1.30129312e+02 3.95773680e+01 1.84765371e+01 7.99137708e+01]\n",
      "76-th iteration, loss: 0.036637253623768365, 760 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "76-th iteration, new layer inserted. now 62 layers\n",
      "[ 67.60286393 135.53893921  68.10203558 212.72651895  62.42296111\n",
      " 144.59736396  74.49527511 111.50993126  81.5088559  115.09599552\n",
      "  54.12294141  85.1094857   48.23387086  92.71068115  53.51923311\n",
      "  55.17118912  31.3329887   91.78546754  83.31742065  96.89794426\n",
      "  27.90094637  63.33009587  80.08018236   0.          40.04009118\n",
      "  98.8713813  116.85595803  28.5187631   19.9837775   88.17273862\n",
      "  40.2172423   30.50037703  81.28464181 102.83585605 179.72893963\n",
      "  14.21514067 146.57551363  67.81374188   7.62786215  87.77338783\n",
      "  92.19790967   8.20136504  98.35472013  16.68426136 102.62373706\n",
      "  40.36652953  30.45147662  32.13585001 141.92383707  28.13677665\n",
      "  51.95678126  18.7339669   67.99383933  12.86497756  42.76208449\n",
      "  77.32859927  22.38868758  34.36860752 136.11380656  39.20178839\n",
      "  14.67948136  79.91377084]\n",
      "77-th iteration, loss: 0.03628016100683945, 28 gd steps\n",
      "insert gradient: -0.0008858106261709086\n",
      "77-th iteration, new layer inserted. now 62 layers\n",
      "[ 67.86208607 136.68689008  68.31479345 211.70956557  63.81381533\n",
      " 144.9390961   73.48274524 111.50037238  81.18961946 116.55340997\n",
      "  54.37178534  84.91032925  48.30552899  92.81849346  53.22839474\n",
      "  54.83211643  32.19025022  91.55086815  81.92028215  98.66339772\n",
      "  30.87880796  60.01359027  76.75085598  11.54917487  31.15215709\n",
      " 107.11719538 115.92167972  34.34477344  17.60486959  87.44202185\n",
      "  40.98886596  29.14345268  78.84573787 106.17247737 177.74936602\n",
      "  15.41293647 148.18368145  70.94958627   6.8208824   85.73808256\n",
      "  93.84607404   7.98866833  96.7322421   15.83659473 103.82931891\n",
      "  46.30730619  28.65015169  35.71028619 139.11829503  29.90717098\n",
      "  49.5099623   22.87688851  68.2011534   13.45714361  41.03623588\n",
      "  78.67208302  21.630078    35.58220709 137.11087692  39.31622761\n",
      "  14.0100913   79.91377084]\n",
      "78-th iteration, loss: 0.03562320922994454, 70 gd steps\n",
      "insert gradient: -0.002420671028308882\n",
      "78-th iteration, new layer inserted. now 64 layers\n",
      "[6.82424842e+01 1.44314386e+02 6.38532205e+01 2.10010888e+02\n",
      " 6.40149874e+01 1.50728139e+02 7.25409532e+01 1.07849084e+02\n",
      " 0.00000000e+00 1.42108547e-14 7.90435135e+01 1.20372490e+02\n",
      " 5.47282434e+01 8.68758462e+01 4.76262238e+01 9.36578830e+01\n",
      " 5.54658286e+01 6.22240025e+01 2.62863041e+01 9.20677545e+01\n",
      " 8.17237691e+01 9.71422988e+01 3.57263113e+01 6.39395328e+01\n",
      " 7.04555223e+01 4.25042874e+01 1.34966488e+01 1.04027756e+02\n",
      " 1.10539706e+02 5.65271408e+01 1.39131411e+01 8.72545813e+01\n",
      " 4.29779390e+01 1.47427251e+01 8.07472754e+01 1.15795999e+02\n",
      " 1.81081707e+02 2.28111602e+01 1.35356511e+02 6.72421267e+01\n",
      " 8.88231838e+00 8.19739107e+01 1.01982577e+02 4.98157575e+00\n",
      " 8.76067030e+01 1.39792696e+01 1.10329463e+02 5.29791417e+01\n",
      " 2.30277656e+01 4.07657130e+01 1.44202878e+02 3.28959394e+01\n",
      " 4.28569535e+01 2.59576768e+01 6.79653283e+01 1.31325127e+01\n",
      " 4.06324429e+01 8.16574716e+01 2.17242001e+01 3.55395188e+01\n",
      " 1.35031615e+02 3.87837958e+01 1.32210645e+01 7.99137708e+01]\n",
      "79-th iteration, loss: 0.03519436083123243, 38 gd steps\n",
      "insert gradient: -0.0009489721244915329\n",
      "79-th iteration, new layer inserted. now 66 layers\n",
      "[6.77278852e+01 1.42834330e+02 6.71536098e+01 2.10058672e+02\n",
      " 6.29831234e+01 1.48176094e+02 7.21654623e+01 1.10394703e+02\n",
      " 2.26919484e-01 4.58058491e-02 8.01414418e+01 1.20629013e+02\n",
      " 5.45129669e+01 8.63041293e+01 4.85052389e+01 9.33943181e+01\n",
      " 5.46483834e+01 6.39118487e+01 2.61574704e+01 9.10596402e+01\n",
      " 0.00000000e+00 7.10542736e-15 8.12897623e+01 9.80794570e+01\n",
      " 3.67009763e+01 6.44990701e+01 6.65287733e+01 4.46663689e+01\n",
      " 1.23968180e+01 1.03764846e+02 1.09610927e+02 5.86612598e+01\n",
      " 1.36828264e+01 8.76634473e+01 4.43130842e+01 1.25715160e+01\n",
      " 8.10534796e+01 1.17038343e+02 1.80458561e+02 2.31419433e+01\n",
      " 1.33838618e+02 6.71123461e+01 9.48426847e+00 8.15629207e+01\n",
      " 1.01515032e+02 6.30388916e+00 8.64789256e+01 1.43110650e+01\n",
      " 1.10794460e+02 5.37479920e+01 2.23044032e+01 4.09119511e+01\n",
      " 1.45231100e+02 3.39930985e+01 4.21692192e+01 2.61934097e+01\n",
      " 6.80586071e+01 1.27406762e+01 4.03426061e+01 8.20672105e+01\n",
      " 2.18343054e+01 3.50150628e+01 1.34811533e+02 3.82609037e+01\n",
      " 1.24840265e+01 7.99137708e+01]\n",
      "80-th iteration, loss: 0.03503538526313251, 59 gd steps\n",
      "insert gradient: -0.0007499808644635153\n",
      "80-th iteration, new layer inserted. now 66 layers\n",
      "[6.79150117e+01 1.44524291e+02 6.70220369e+01 2.10115049e+02\n",
      " 6.25577587e+01 1.46896078e+02 7.28484815e+01 1.10149726e+02\n",
      " 1.83864828e-01 5.44913265e-02 7.98097083e+01 1.21581890e+02\n",
      " 5.46243631e+01 8.65893919e+01 4.84235445e+01 9.35939532e+01\n",
      " 5.47427593e+01 6.59527901e+01 2.53414446e+01 8.97034856e+01\n",
      " 0.00000000e+00 7.10542736e-15 8.14711566e+01 9.93522848e+01\n",
      " 3.68682088e+01 6.59873310e+01 6.26923339e+01 4.90009354e+01\n",
      " 1.10268935e+01 1.03598600e+02 1.08957861e+02 6.37522832e+01\n",
      " 1.24231271e+01 8.77431197e+01 4.60590787e+01 8.57190589e+00\n",
      " 8.15681449e+01 1.19694024e+02 1.81155174e+02 2.39972456e+01\n",
      " 1.30834118e+02 6.70098235e+01 1.04263935e+01 8.05751015e+01\n",
      " 1.01428326e+02 7.64128017e+00 8.42334568e+01 1.41716352e+01\n",
      " 1.11585048e+02 5.55762243e+01 2.10523315e+01 4.19178462e+01\n",
      " 1.46841730e+02 3.58602560e+01 4.02008152e+01 2.63367200e+01\n",
      " 6.84706939e+01 1.25209401e+01 4.01797178e+01 8.29039495e+01\n",
      " 2.17796705e+01 3.39727706e+01 1.33982994e+02 3.75043591e+01\n",
      " 1.19191059e+01 7.99137708e+01]\n",
      "81-th iteration, loss: 0.03500735916637211, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "81-th iteration, new layer inserted. now 66 layers\n",
      "[6.72848102e+01 1.44406926e+02 6.69621338e+01 2.10531084e+02\n",
      " 6.28318117e+01 1.46728684e+02 7.23198640e+01 1.09960511e+02\n",
      " 8.05588349e+01 1.21920930e+02 5.45700453e+01 8.64111241e+01\n",
      " 4.82043998e+01 9.35833143e+01 5.48778446e+01 6.62383017e+01\n",
      " 2.51931171e+01 8.95675133e+01 5.82071995e-02 8.58013935e-03\n",
      " 8.15297932e+01 9.94482292e+01 3.68403515e+01 6.61170194e+01\n",
      " 6.25163539e+01 4.95597436e+01 1.07979919e+01 1.03564221e+02\n",
      " 1.08900804e+02 6.43429411e+01 1.22642812e+01 8.77082300e+01\n",
      " 4.62596275e+01 8.17379525e+00 8.16500298e+01 1.19991098e+02\n",
      " 1.20841495e+02 0.00000000e+00 6.04207477e+01 2.41267877e+01\n",
      " 1.30491352e+02 6.70560552e+01 1.05383290e+01 8.04083214e+01\n",
      " 1.01457993e+02 7.79168635e+00 8.39850377e+01 1.41518058e+01\n",
      " 1.11668046e+02 5.57907998e+01 2.08849159e+01 4.20951491e+01\n",
      " 1.47050010e+02 3.60853466e+01 3.99820922e+01 2.63729724e+01\n",
      " 6.85293099e+01 1.25132495e+01 4.01478274e+01 8.30031598e+01\n",
      " 2.18069335e+01 3.38633249e+01 1.33894760e+02 3.74212456e+01\n",
      " 1.18550033e+01 7.99137708e+01]\n",
      "82-th iteration, loss: 0.034598496299081544, 38 gd steps\n",
      "insert gradient: -0.0006495679595409003\n",
      "82-th iteration, new layer inserted. now 66 layers\n",
      "[6.72193983e+01 1.45430105e+02 6.69941468e+01 2.11218056e+02\n",
      " 6.23924215e+01 1.45724322e+02 7.29527261e+01 1.10105007e+02\n",
      " 7.99520435e+01 1.23169878e+02 5.49555129e+01 8.66453932e+01\n",
      " 4.81887802e+01 9.31997222e+01 5.46766275e+01 6.80801551e+01\n",
      " 2.46875457e+01 8.84803642e+01 0.00000000e+00 7.10542736e-15\n",
      " 8.15109064e+01 1.00802977e+02 3.75588588e+01 6.55796982e+01\n",
      " 5.99197943e+01 5.22770571e+01 1.00985857e+01 1.02768975e+02\n",
      " 1.08657979e+02 7.08684890e+01 1.02836419e+01 8.61331612e+01\n",
      " 4.70691451e+01 5.53993817e+00 8.30391662e+01 1.20320495e+02\n",
      " 1.16208566e+02 1.15554897e+01 5.53177069e+01 2.70255167e+01\n",
      " 1.27341061e+02 6.75080769e+01 1.18794769e+01 8.10696264e+01\n",
      " 1.01675064e+02 1.07795797e+01 8.16345072e+01 1.18409281e+01\n",
      " 1.13491082e+02 5.76496114e+01 1.88021472e+01 4.37524744e+01\n",
      " 1.48367934e+02 3.81504940e+01 3.88932372e+01 2.59672760e+01\n",
      " 6.89650742e+01 1.26278741e+01 3.99007051e+01 8.36955843e+01\n",
      " 2.13369894e+01 3.32058823e+01 1.34071043e+02 3.65377334e+01\n",
      " 1.14782489e+01 7.99137708e+01]\n",
      "83-th iteration, loss: 0.03430254943574175, 21 gd steps\n",
      "insert gradient: -0.0015805462608926998\n",
      "83-th iteration, new layer inserted. now 64 layers\n",
      "[ 65.90372327 145.40719399  67.38966941 212.66619437  62.27136795\n",
      " 142.30947949  74.00961611 109.93078136  79.7061697  125.85521675\n",
      "  54.95724178  86.62698664  48.06221827  92.51236551  53.80903313\n",
      "  69.91161782  24.65186744  86.83409887  81.4004145  102.66986847\n",
      "  39.7229371   63.15620966  55.60900913  55.68928912  10.22830655\n",
      " 100.22396295 107.47081076  82.86448026   7.39198764  81.93960096\n",
      "  49.39786437   0.31577056  87.36279411 120.27343419 112.22869626\n",
      "  20.71777157  50.65051104  30.56197334 124.89489966  67.69019698\n",
      "  12.13483894  83.85389546 101.55563537  14.44652854  77.00782989\n",
      "   8.20609106 117.34061574  61.80718755  15.78020421  47.72254297\n",
      " 148.19551047  40.31003403  37.94666588  24.8154321   69.61136727\n",
      "  12.68177388  39.91348567  84.94231031  19.89669342  32.97908665\n",
      " 134.56612135  34.37771859  11.5052269   79.91377084]\n",
      "84-th iteration, loss: 0.033947129401117354, 146 gd steps\n",
      "insert gradient: -0.00030794845982461637\n",
      "84-th iteration, new layer inserted. now 64 layers\n",
      "[6.60574234e+01 1.45618173e+02 6.88056142e+01 2.15725151e+02\n",
      " 6.10214481e+01 1.34564450e+02 7.90958978e+01 1.10605331e+02\n",
      " 7.56567791e+01 1.31797107e+02 5.67029582e+01 8.65217768e+01\n",
      " 4.72009822e+01 9.17772171e+01 5.42944321e+01 6.98235421e+01\n",
      " 2.51525092e+01 8.64338875e+01 0.00000000e+00 7.10542736e-15\n",
      " 8.10659325e+01 1.04957202e+02 4.10731907e+01 5.94852031e+01\n",
      " 5.57741364e+01 5.78739734e+01 1.08861411e+01 9.62465276e+01\n",
      " 1.06680405e+02 9.09195668e+01 5.87576677e+00 7.53146073e+01\n",
      " 1.41631016e+02 1.15709636e+02 1.12229485e+02 2.82267571e+01\n",
      " 4.51125502e+01 3.17185107e+01 1.25255478e+02 6.91310668e+01\n",
      " 1.19409154e+01 8.69356456e+01 1.00260767e+02 1.90278191e+01\n",
      " 7.15315872e+01 5.66203476e+00 1.21183078e+02 6.54710531e+01\n",
      " 1.35466330e+01 5.24601891e+01 1.45814299e+02 4.13305375e+01\n",
      " 3.80316719e+01 2.24567044e+01 7.02277382e+01 1.20096614e+01\n",
      " 4.13588162e+01 8.64992612e+01 1.75402925e+01 3.55680078e+01\n",
      " 1.34442916e+02 3.14386134e+01 1.31705068e+01 7.99137708e+01]\n",
      "85-th iteration, loss: 0.033797797627421425, 19 gd steps\n",
      "insert gradient: -0.00034302184749598544\n",
      "85-th iteration, new layer inserted. now 64 layers\n",
      "[6.61540906e+01 1.42464672e+02 0.00000000e+00 1.42108547e-14\n",
      " 7.03692064e+01 2.16952739e+02 6.29334699e+01 1.21858655e+02\n",
      " 8.37269244e+01 1.13625032e+02 7.02597207e+01 1.38717196e+02\n",
      " 5.88352702e+01 8.48976244e+01 4.63332732e+01 9.15685599e+01\n",
      " 5.47718260e+01 6.98025602e+01 2.47640858e+01 8.71617483e+01\n",
      " 8.06944860e+01 1.06204239e+02 4.21181182e+01 5.68319815e+01\n",
      " 5.63758461e+01 5.95523298e+01 1.11272677e+01 9.29616408e+01\n",
      " 1.06340716e+02 9.55113555e+01 5.50905519e+00 7.05894098e+01\n",
      " 1.44260430e+02 1.10851451e+02 1.12455339e+02 3.23207172e+01\n",
      " 4.15958495e+01 3.24160640e+01 1.26250759e+02 6.96158714e+01\n",
      " 1.15594250e+01 8.89083776e+01 9.92960252e+01 2.34768625e+01\n",
      " 6.53806734e+01 6.17792510e+00 1.23673081e+02 6.77260240e+01\n",
      " 1.26719812e+01 5.55441163e+01 1.42971405e+02 4.12592697e+01\n",
      " 3.78908991e+01 2.06220697e+01 7.03121303e+01 1.13673947e+01\n",
      " 4.31935997e+01 8.78015205e+01 1.60265197e+01 3.75644318e+01\n",
      " 1.33112750e+02 3.11803443e+01 1.41807270e+01 7.99137708e+01]\n",
      "86-th iteration, loss: 0.03367616408469628, 54 gd steps\n",
      "insert gradient: -0.0005802462162250364\n",
      "86-th iteration, new layer inserted. now 64 layers\n",
      "[6.55239164e+01 1.42693468e+02 0.00000000e+00 1.42108547e-14\n",
      " 7.05077157e+01 2.17650390e+02 6.08698679e+01 1.26802006e+02\n",
      " 8.26611216e+01 1.11756742e+02 7.24011967e+01 1.37325634e+02\n",
      " 5.76317086e+01 8.63405642e+01 4.66296767e+01 9.10050124e+01\n",
      " 5.44559474e+01 7.17205613e+01 2.43639987e+01 8.60099550e+01\n",
      " 8.06414317e+01 1.06782827e+02 4.27080349e+01 5.58923934e+01\n",
      " 5.58895706e+01 6.08476857e+01 1.09696573e+01 9.18940049e+01\n",
      " 1.05951824e+02 9.75978738e+01 5.34410075e+00 6.92840490e+01\n",
      " 1.46676579e+02 1.05259213e+02 1.11087023e+02 3.52269126e+01\n",
      " 3.98606382e+01 3.33854879e+01 1.27216407e+02 6.99664855e+01\n",
      " 1.14131753e+01 8.97286385e+01 9.95443962e+01 2.68474762e+01\n",
      " 6.08098434e+01 6.18304134e+00 1.24952717e+02 6.83138549e+01\n",
      " 1.22777096e+01 5.79771653e+01 1.40869800e+02 4.09397915e+01\n",
      " 3.65054703e+01 1.95306822e+01 7.07467871e+01 1.19684764e+01\n",
      " 4.50520283e+01 8.86947594e+01 1.47416014e+01 3.90069109e+01\n",
      " 1.32513768e+02 3.27246063e+01 1.44839713e+01 7.99137708e+01]\n",
      "87-th iteration, loss: 0.03364691608768225, 21 gd steps\n",
      "insert gradient: -0.00020855279672996464\n",
      "87-th iteration, new layer inserted. now 64 layers\n",
      "[6.62251813e+01 1.42874247e+02 7.09265646e+01 2.17415033e+02\n",
      " 6.08447299e+01 1.26186985e+02 8.28078078e+01 1.12079585e+02\n",
      " 7.20485454e+01 1.37300860e+02 5.80418118e+01 8.62925133e+01\n",
      " 4.65580040e+01 9.10561541e+01 5.44930049e+01 7.15765008e+01\n",
      " 2.44416449e+01 8.60938065e+01 0.00000000e+00 7.10542736e-15\n",
      " 8.06633157e+01 1.06821873e+02 4.27426540e+01 5.58597833e+01\n",
      " 5.59008155e+01 6.08694241e+01 1.10311045e+01 9.18121509e+01\n",
      " 1.06045675e+02 9.77637546e+01 5.27925120e+00 6.91000542e+01\n",
      " 1.46943099e+02 1.04579504e+02 1.11029960e+02 3.56373276e+01\n",
      " 3.96618207e+01 3.35326052e+01 1.27304592e+02 7.00120886e+01\n",
      " 1.14672781e+01 8.97787515e+01 9.93753439e+01 2.71465669e+01\n",
      " 6.02995969e+01 6.34110453e+00 1.25096979e+02 6.83412964e+01\n",
      " 1.23723706e+01 5.82915981e+01 1.40625365e+02 4.08604146e+01\n",
      " 3.63862907e+01 1.95348837e+01 7.08265297e+01 1.20669177e+01\n",
      " 4.51282757e+01 8.87581428e+01 1.46700431e+01 3.92018668e+01\n",
      " 1.32452367e+02 3.28844443e+01 1.45788528e+01 7.99137708e+01]\n",
      "88-th iteration, loss: 0.033230238303261085, 122 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "88-th iteration, new layer inserted. now 64 layers\n",
      "[ 65.76799006 141.30957999  72.40734529 217.21838754  60.95195068\n",
      " 123.74165325  83.36214789 112.15797109  71.02230953 139.27292305\n",
      "  58.55367121  86.25635993  46.17928856  90.51790891  54.64226464\n",
      "  72.63926956  24.09543465  85.20705205  80.07405302 107.95719855\n",
      "  44.18277455  54.11539698  54.94789925  65.19878284  10.68863986\n",
      "  87.4822506  105.6062409  104.12865243   6.21906899  60.59376346\n",
      " 103.69965476   0.          51.84982738  73.08052607 108.15949248\n",
      "  55.08462196  29.75792751  40.87943022 126.90866246  73.94254049\n",
      "  10.35410101  91.02611702  98.83169055  41.64113271  40.15436968\n",
      "  12.60125945 132.34882166  67.74267306   9.92725631  69.00636042\n",
      " 134.97234355  42.10133012  29.42500543  22.200102    74.79715078\n",
      "  14.304415    45.149539    90.94399942  10.6469537   45.79444785\n",
      " 130.58621515  35.19145112  14.38053525  79.91377084]\n",
      "89-th iteration, loss: 0.03285162534968264, 19 gd steps\n",
      "insert gradient: -0.001123506243715374\n",
      "89-th iteration, new layer inserted. now 66 layers\n",
      "[6.45275531e+01 1.39390469e+02 7.22305452e+01 2.16930556e+02\n",
      " 6.25157873e+01 1.22269481e+02 8.52766155e+01 1.12158024e+02\n",
      " 0.00000000e+00 1.42108547e-14 6.89052613e+01 1.39567945e+02\n",
      " 5.87974399e+01 8.74914868e+01 4.62742051e+01 8.92863430e+01\n",
      " 5.32954778e+01 7.46810760e+01 2.42130956e+01 8.39244335e+01\n",
      " 7.97016835e+01 1.08328303e+02 4.71932309e+01 5.29524465e+01\n",
      " 4.96201177e+01 6.94668872e+01 1.15294408e+01 8.32578180e+01\n",
      " 1.03773125e+02 1.03868906e+02 6.96622085e+00 5.52056595e+01\n",
      " 9.97618026e+01 2.00931734e+01 4.25744277e+01 8.11220975e+01\n",
      " 1.03654261e+02 6.89643384e+01 2.32952271e+01 4.72950545e+01\n",
      " 1.24412387e+02 8.01954424e+01 9.01105999e+00 8.94389525e+01\n",
      " 9.85022605e+01 4.98690871e+01 3.42706155e+01 1.66786603e+01\n",
      " 1.32260768e+02 6.83419719e+01 8.59238609e+00 7.17671194e+01\n",
      " 1.33737769e+02 4.25409596e+01 2.65224261e+01 2.32281793e+01\n",
      " 7.69535125e+01 1.51153743e+01 4.40213805e+01 9.14175471e+01\n",
      " 8.88818755e+00 4.84740776e+01 1.29791000e+02 3.49305315e+01\n",
      " 1.40151472e+01 7.99137708e+01]\n",
      "90-th iteration, loss: 0.032421863163543355, 48 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "90-th iteration, new layer inserted. now 66 layers\n",
      "[ 64.36946105 136.46656087  74.09907199 144.48214561   0.\n",
      "  72.24107281  59.39015509 119.57987253  83.70819591 112.04152832\n",
      "  70.58423629 139.96159429  59.32728879  87.59977023  45.10563037\n",
      "  88.06639404  54.21877691  76.62366264  22.72453666  82.14442195\n",
      "  78.79936514 109.80904608  47.34730366  50.60957009  50.582513\n",
      "  70.84889946  10.72932484  80.83865371 101.89693523 104.92795994\n",
      "   6.87653124  53.38038617 101.12102851  26.11578333  36.89115855\n",
      "  84.54951741 101.43311139  75.50793715  20.49959459  49.71049112\n",
      " 121.98629208  83.15644472   7.44381221  88.7102004   99.00202075\n",
      "  54.79283673  30.851074    20.99450779 131.22578272  69.56547174\n",
      "   7.07755051  72.66354339 132.64314886  42.86380151  24.55303475\n",
      "  24.27139992  77.9775911   14.57418045  43.02786745  91.90479077\n",
      "   7.66830808  50.2129697  128.24096541  35.00032015  13.54902202\n",
      "  79.91377084]\n",
      "91-th iteration, loss: 0.031811743449406814, 61 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "91-th iteration, new layer inserted. now 68 layers\n",
      "[6.47425966e+01 1.35261753e+02 7.36936217e+01 1.43401189e+02\n",
      " 3.83621395e-02 7.13937881e+01 5.92645711e+01 1.18722636e+02\n",
      " 8.35659476e+01 1.10991709e+02 6.81841945e+01 1.39025718e+02\n",
      " 5.93771187e+01 8.75577354e+01 4.42825199e+01 8.68401143e+01\n",
      " 5.34808863e+01 7.73691044e+01 2.23443202e+01 8.05964735e+01\n",
      " 7.78408541e+01 1.09870179e+02 4.75720525e+01 4.92675273e+01\n",
      " 4.89690612e+01 7.06552845e+01 1.05693627e+01 7.91704257e+01\n",
      " 9.91397638e+01 6.99117365e+01 0.00000000e+00 3.49558683e+01\n",
      " 5.90084604e+00 5.45068188e+01 1.03848272e+02 2.93305514e+01\n",
      " 3.34656079e+01 8.46162047e+01 1.00282633e+02 7.79946352e+01\n",
      " 1.93466836e+01 4.90989800e+01 1.19668645e+02 8.39285894e+01\n",
      " 6.72731169e+00 8.80002702e+01 9.84739332e+01 5.66120518e+01\n",
      " 2.92731588e+01 2.29914346e+01 1.30244119e+02 6.99445471e+01\n",
      " 6.21807326e+00 7.26523478e+01 1.31502337e+02 4.27713920e+01\n",
      " 2.37192165e+01 2.42752231e+01 7.77977323e+01 1.37510865e+01\n",
      " 4.24998170e+01 9.20774079e+01 7.15152657e+00 5.05661156e+01\n",
      " 1.27635053e+02 3.45613803e+01 1.34326906e+01 7.99137708e+01]\n",
      "92-th iteration, loss: 0.03175678538271131, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "92-th iteration, new layer inserted. now 68 layers\n",
      "[6.46314741e+01 1.35011898e+02 7.36600582e+01 2.14620786e+02\n",
      " 5.91818079e+01 1.18575655e+02 8.34325790e+01 1.10910459e+02\n",
      " 6.81212226e+01 1.38898393e+02 5.93237825e+01 8.75168879e+01\n",
      " 4.41825278e+01 8.67001558e+01 5.33722520e+01 7.74416534e+01\n",
      " 2.23154966e+01 8.03924909e+01 7.77508573e+01 1.09835564e+02\n",
      " 4.75634377e+01 4.91400241e+01 4.87235049e+01 7.05568421e+01\n",
      " 1.05935576e+01 7.89751517e+01 9.88364133e+01 6.98518100e+01\n",
      " 5.48898803e-02 1.16319055e+01 0.00000000e+00 2.32638109e+01\n",
      " 5.84767327e+00 5.47054328e+01 1.04149081e+02 2.97100414e+01\n",
      " 3.30828665e+01 8.45360693e+01 1.00135795e+02 7.82416419e+01\n",
      " 1.92975355e+01 4.89400239e+01 1.19307073e+02 8.39582437e+01\n",
      " 6.70031510e+00 8.78914226e+01 9.84011649e+01 5.67999992e+01\n",
      " 2.91753824e+01 2.32877596e+01 1.30052367e+02 6.99896229e+01\n",
      " 6.17586234e+00 7.26087862e+01 1.31320399e+02 4.27717614e+01\n",
      " 2.37144249e+01 2.42424004e+01 7.77336143e+01 1.36249667e+01\n",
      " 4.24363599e+01 9.20983240e+01 7.16468968e+00 5.05820362e+01\n",
      " 1.27598111e+02 3.45273756e+01 1.34342044e+01 7.99137708e+01]\n",
      "93-th iteration, loss: 0.031007441240612665, 86 gd steps\n",
      "insert gradient: -0.004079649635607188\n",
      "93-th iteration, new layer inserted. now 68 layers\n",
      "[6.40016990e+01 1.29480215e+02 7.71325669e+01 2.10784384e+02\n",
      " 6.02893210e+01 1.17676727e+02 8.21023171e+01 1.11617960e+02\n",
      " 6.80537590e+01 1.37714635e+02 5.97976388e+01 8.80669733e+01\n",
      " 4.41279113e+01 8.50951744e+01 5.18468313e+01 7.86229138e+01\n",
      " 2.40702697e+01 7.52080164e+01 7.48852494e+01 1.15307424e+02\n",
      " 5.08143097e+01 4.45506005e+01 4.66647217e+01 7.17234772e+01\n",
      " 1.22372116e+01 7.24110811e+01 9.30035166e+01 7.80166377e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.53741311e+00 2.06695951e+01\n",
      " 7.72927777e-01 5.85890081e+01 1.06213546e+02 4.21716910e+01\n",
      " 2.77377322e+01 8.54743328e+01 9.80500124e+01 9.03499327e+01\n",
      " 1.81695697e+01 4.34572407e+01 1.12563480e+02 8.85095313e+01\n",
      " 6.55483899e+00 8.54178338e+01 9.75506462e+01 6.94939274e+01\n",
      " 2.40666951e+01 3.39194009e+01 1.26772042e+02 7.37405048e+01\n",
      " 4.61199208e+00 7.09321503e+01 1.29908390e+02 4.37234670e+01\n",
      " 2.32463890e+01 2.45677951e+01 7.75571590e+01 1.10647939e+01\n",
      " 4.32627558e+01 9.37524105e+01 6.65008272e+00 5.30918299e+01\n",
      " 1.27257653e+02 3.29006061e+01 1.32495957e+01 7.99137708e+01]\n",
      "94-th iteration, loss: 0.03094686729028271, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "94-th iteration, new layer inserted. now 66 layers\n",
      "[ 63.93096888 129.16879448  77.16955241 210.61789281  60.45927219\n",
      " 117.77625716  81.98301587 111.68590691  67.89034701 137.55425401\n",
      "  59.82188741  88.09698613  44.15864285  85.01288005  51.6948986\n",
      "  78.61683477  24.24154378  75.04563124  74.55482511 115.8843092\n",
      "  50.96717279  44.23156213  46.55742163  71.67110642  12.35207474\n",
      "  72.30164999  92.84799329  77.94893578   6.49766759  78.56344671\n",
      " 106.34534657  42.50760484  27.68366172  85.29277156  97.78493604\n",
      "  90.67558187  18.16899184  42.97750004 112.09653157  88.66009307\n",
      "   6.54709745  85.36408593  97.76796814  70.09375682  23.81084596\n",
      "  34.32157739 126.53063791  73.83413691   4.46109059  23.55129749\n",
      "   0.          47.10259497 129.69047623  43.65676179  23.2879838\n",
      "  24.80023085  77.44925563  10.90268926  43.40301723  93.84886991\n",
      "   6.70329066  53.18433162 127.36457196  32.96740783  13.18352579\n",
      "  79.91377084]\n",
      "95-th iteration, loss: 0.03041390133634712, 22 gd steps\n",
      "insert gradient: -0.0009069125593964907\n",
      "95-th iteration, new layer inserted. now 64 layers\n",
      "[ 64.16531048 124.02285376  75.46434784 211.47201704  59.4829995\n",
      " 124.77643134  80.00178537 114.1533595   67.47102827 137.50968204\n",
      "  58.75300841  88.43365178  45.24600189  85.16494941  52.11070571\n",
      "  79.63966896  26.49874038  70.52430581  63.67539837 135.43978371\n",
      "  51.88134677  38.11320929  48.794668    67.85614568  13.41535551\n",
      "  71.72754079  89.9038695   72.52551714  14.88057529  62.22007619\n",
      " 108.50338133  49.19389288  27.09072583  79.14395401  93.56772601\n",
      " 100.83117001  17.22937613  36.32059247 111.65854957  93.33271183\n",
      "   5.88987677  80.71149666 100.66863474  90.77093996  16.8597119\n",
      "  39.35896098 124.31157552  97.46531718   4.45530851  45.84691147\n",
      " 128.1016524   48.79115465  22.04635044  30.36343629  73.8811219\n",
      "   8.28358721  47.75160457  95.64787091   7.05566149  51.17563411\n",
      " 129.73463762  32.60528356  11.57301795  79.91377084]\n",
      "96-th iteration, loss: 0.030299301987687398, 14 gd steps\n",
      "insert gradient: -0.00028363818016565503\n",
      "96-th iteration, new layer inserted. now 66 layers\n",
      "[6.46264441e+01 1.24764337e+02 7.61213423e+01 2.11251030e+02\n",
      " 5.93910448e+01 1.24363666e+02 7.95794348e+01 1.13871951e+02\n",
      " 6.78347161e+01 1.37923148e+02 0.00000000e+00 1.42108547e-14\n",
      " 5.96926232e+01 8.85336460e+01 4.47983663e+01 8.50131444e+01\n",
      " 5.18107967e+01 7.96831073e+01 2.65329882e+01 7.06883547e+01\n",
      " 6.38011823e+01 1.35729706e+02 5.20138137e+01 3.81098095e+01\n",
      " 4.87112458e+01 6.78260344e+01 1.34735118e+01 7.17865619e+01\n",
      " 8.98491569e+01 7.25321144e+01 1.48012804e+01 6.22414265e+01\n",
      " 1.08648645e+02 4.93519708e+01 2.71809097e+01 7.90868546e+01\n",
      " 9.32630458e+01 1.00792576e+02 1.72527378e+01 3.64245508e+01\n",
      " 1.11760379e+02 9.32925879e+01 6.02840418e+00 8.06349500e+01\n",
      " 1.00816607e+02 9.08064560e+01 1.68938634e+01 3.92938931e+01\n",
      " 1.24200101e+02 9.73840264e+01 4.29185481e+00 4.57610606e+01\n",
      " 1.28017704e+02 4.89340372e+01 2.23149136e+01 3.04661889e+01\n",
      " 7.37890064e+01 8.13885441e+00 4.76758396e+01 9.56545245e+01\n",
      " 7.10545225e+00 5.11225099e+01 1.29785801e+02 3.26060732e+01\n",
      " 1.15460320e+01 7.99137708e+01]\n",
      "97-th iteration, loss: 0.030204792699194873, 25 gd steps\n",
      "insert gradient: -0.0013351766896561401\n",
      "97-th iteration, new layer inserted. now 66 layers\n",
      "[6.35316897e+01 1.27182596e+02 7.63523422e+01 2.09533076e+02\n",
      " 6.15825669e+01 1.20666699e+02 8.19022773e+01 1.12975495e+02\n",
      " 6.72284040e+01 1.35898760e+02 6.07862532e+01 8.87030030e+01\n",
      " 4.44923506e+01 8.59902989e+01 5.17699736e+01 8.12829804e+01\n",
      " 2.57399332e+01 7.20027592e+01 6.23507143e+01 1.37727711e+02\n",
      " 5.20721919e+01 3.83871652e+01 4.82777907e+01 6.82085249e+01\n",
      " 1.33856364e+01 7.19213249e+01 8.94795777e+01 7.34859717e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.50940347e+01 6.17940772e+01\n",
      " 1.07965148e+02 5.02211317e+01 2.73768578e+01 7.87362804e+01\n",
      " 9.17645259e+01 1.01017225e+02 1.61296305e+01 3.76332759e+01\n",
      " 1.14189728e+02 9.23464811e+01 6.42576520e+00 7.84802611e+01\n",
      " 1.02471138e+02 9.20889441e+01 1.74960281e+01 3.77635233e+01\n",
      " 1.23228228e+02 9.74311689e+01 4.10788371e+00 4.59902818e+01\n",
      " 1.28450692e+02 5.06239757e+01 2.21230544e+01 3.14533747e+01\n",
      " 7.31264991e+01 7.58369052e+00 4.80599457e+01 9.59468492e+01\n",
      " 7.48280947e+00 5.00545896e+01 1.29762607e+02 3.14673791e+01\n",
      " 1.18406926e+01 7.99137708e+01]\n",
      "98-th iteration, loss: 0.03016162450685321, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "98-th iteration, new layer inserted. now 68 layers\n",
      "[6.35219265e+01 1.27087791e+02 7.61212669e+01 1.39733745e+02\n",
      " 0.00000000e+00 6.98668725e+01 6.15923671e+01 1.20636392e+02\n",
      " 8.15653280e+01 1.13098023e+02 6.77881153e+01 1.36172131e+02\n",
      " 6.13248162e+01 8.87672679e+01 4.43523408e+01 8.59315946e+01\n",
      " 5.17900991e+01 8.13438518e+01 2.57068886e+01 7.19879535e+01\n",
      " 6.23209492e+01 1.37823469e+02 5.20586066e+01 3.84174753e+01\n",
      " 4.83143719e+01 6.82458443e+01 1.33932958e+01 7.19288634e+01\n",
      " 8.94316587e+01 7.35284914e+01 2.98813420e-03 4.22508777e-02\n",
      " 1.50975429e+01 6.17698014e+01 1.08009748e+02 5.02668757e+01\n",
      " 2.73574560e+01 7.87322300e+01 9.17557021e+01 1.01041490e+02\n",
      " 1.60475494e+01 3.77226206e+01 1.14298224e+02 9.23079817e+01\n",
      " 6.37613331e+00 7.83756345e+01 1.02537822e+02 9.21356789e+01\n",
      " 1.75066983e+01 3.77456505e+01 1.23214590e+02 9.74802524e+01\n",
      " 4.19535739e+00 4.60665912e+01 1.28484508e+02 5.07219431e+01\n",
      " 2.21495735e+01 3.14838602e+01 7.30807131e+01 7.51252778e+00\n",
      " 4.80400457e+01 9.59484312e+01 7.49379644e+00 4.99707141e+01\n",
      " 1.29704323e+02 3.13860201e+01 1.18244013e+01 7.99137708e+01]\n",
      "99-th iteration, loss: 0.03002995436440164, 78 gd steps\n",
      "insert gradient: -0.0010555634806651564\n",
      "99-th iteration, new layer inserted. now 64 layers\n",
      "[ 64.31215807 129.03490109  73.87488116 212.40191565  61.0967663\n",
      " 121.0140082   81.63496803 113.78765014  67.90460012 135.90678769\n",
      "  62.24863407  88.80415766  44.41041756  85.93910003  52.02326346\n",
      "  82.90084061  25.6562589   71.43248089  60.93690545 140.5739765\n",
      "  52.02046812  38.96844805  48.14848611  69.48394686  12.97402059\n",
      "  71.32194139  88.8462359   75.80330909  15.44157817  60.45810991\n",
      " 108.11154743  51.29521157  27.24110186  78.4554621   89.84932016\n",
      " 101.48075196  14.93598726  40.03396996 116.61155703  91.21101879\n",
      "   6.92377031  75.27320359 104.73733054  94.0622324   17.88518756\n",
      "  34.58142151 122.30179329  98.5747548    3.29146674  47.73850648\n",
      " 129.0928555   52.90133781  21.54277993  33.05808738  72.71045581\n",
      "   7.25258858  48.6336378   96.29278296   7.89437202  47.41067838\n",
      " 128.3012288   28.39107201  12.97899043  79.91377084]\n",
      "0-th iteration, loss: 0.7719359603994591, 18 gd steps\n",
      "insert gradient: -0.4492218382898956\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  80.39587824    0.         3256.03306861]\n",
      "1-th iteration, loss: 0.582839174856937, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.93826878e+01 1.27518258e+00 0.00000000e+00 1.03289789e+02\n",
      " 3.23196330e+03]\n",
      "2-th iteration, loss: 0.5762115340278193, 26 gd steps\n",
      "insert gradient: -0.24369408031610057\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  56.03587374  104.10934932  157.65392504    0.         3074.25153825]\n",
      "3-th iteration, loss: 0.460454882590416, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  67.22863405  105.32072421  113.84262341   89.44986083  124.91416112\n",
      "    0.         2935.48278643]\n",
      "4-th iteration, loss: 0.38091846586270534, 38 gd steps\n",
      "insert gradient: -0.1845007380232955\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  61.42432111  106.02987087   66.81550746    0.           56.26569049\n",
      "   89.91631299   98.99366445   89.36051634 2913.35758002]\n",
      "5-th iteration, loss: 0.3633801716962826, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  66.73321804  107.25538877   53.19786695   26.51401015   48.32376254\n",
      "   98.34310584   95.8249969   100.02824882  215.5136725     0.\n",
      " 2693.92090624]\n",
      "6-th iteration, loss: 0.3032112664866133, 21 gd steps\n",
      "insert gradient: -0.1855109059176296\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[6.73845994e+01 1.09978633e+02 4.33240591e+01 4.90879481e+01\n",
      " 0.00000000e+00 2.66453526e-15 4.22799493e+01 1.04460199e+02\n",
      " 9.28664824e+01 1.09437656e+02 1.92974361e+02 1.05740063e+02\n",
      " 2.66057552e+03]\n",
      "7-th iteration, loss: 0.29646555161492094, 54 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  71.2018483   108.88224072   37.43697376   47.20407812   51.94665646\n",
      "  102.18502235   90.51896975  107.87019128  193.98707346  107.18387843\n",
      "  120.90650243    0.         2539.03655106]\n",
      "8-th iteration, loss: 0.2575270082520881, 18 gd steps\n",
      "insert gradient: -0.02432633068585552\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[6.74921751e+01 0.00000000e+00 1.86517468e-14 1.01847791e+02\n",
      " 4.96080868e+01 3.91742000e+01 4.99549410e+01 9.88855977e+01\n",
      " 9.16212806e+01 1.11608804e+02 1.93310762e+02 1.02926336e+02\n",
      " 8.64719185e+01 7.86770867e+01 2.52798869e+03]\n",
      "9-th iteration, loss: 0.2503098958342382, 67 gd steps\n",
      "insert gradient: -0.04491269269958181\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  66.88588969  105.6737909    38.61311943   45.01823245   54.52278883\n",
      "  105.04423767   87.87084097  114.06230154  129.13484085    0.\n",
      "   64.56742042   75.33978697   96.73156305   86.61851821 2521.5609428 ]\n",
      "10-th iteration, loss: 0.1770825482127943, 248 gd steps\n",
      "insert gradient: -0.03421036497657606\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[  58.52719747  204.22558302   57.9150681   101.60691489  114.0546588\n",
      "  102.12473757   81.57961317   92.37938003   43.49747445   83.56322168\n",
      "   81.99780102   95.11419645  976.75383239    0.         1534.89887948]\n",
      "11-th iteration, loss: 0.1686070844753459, 66 gd steps\n",
      "insert gradient: -0.024340275284800553\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[  56.70972609  203.56016745   61.36721252  102.41026621  111.79446596\n",
      "   99.7082325    84.08475664   92.68164719   45.20076114   77.04251782\n",
      "   81.7137655    96.73421794  361.61751281    0.          602.69585468\n",
      "   45.61168233 1520.34414338]\n",
      "12-th iteration, loss: 0.1636966723151773, 31 gd steps\n",
      "insert gradient: -0.04225448204838241\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[  53.4379002   199.96507902   59.55531356  102.60702214  118.61158976\n",
      "   99.64327488   81.03359643   95.02325979   45.26788455   75.61819461\n",
      "   80.72396709   96.62117792   76.91007036    0.          282.00359131\n",
      "   24.55829169  590.28600981   41.89128072 1523.3435429 ]\n",
      "13-th iteration, loss: 0.15214936844845203, 66 gd steps\n",
      "insert gradient: -0.026973542059851904\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[  54.66886438  203.03857101   60.48148497  106.08104259  114.28350199\n",
      "   99.47903849   82.09783729   97.75754611   42.5720975    77.46457534\n",
      "   78.04047331  104.385551     68.85347257   34.58350193  228.76238897\n",
      "    0.           45.75247779   33.20385445  582.14669347   31.53372001\n",
      " 1521.61099308]\n",
      "14-th iteration, loss: 0.1389054059534019, 37 gd steps\n",
      "insert gradient: -0.025712485740936788\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  54.80997031  203.5326496    61.10216413  104.72456072  114.96703557\n",
      "   95.15550096   83.8954076   100.62417205   43.631754     69.9845565\n",
      "   77.71594774  110.81314869   64.40686264   53.66221907  204.4929014\n",
      "   60.23090945   15.58723234   52.2027631   368.89759943    0.\n",
      "  210.79862825   26.51229339 1522.27292377]\n",
      "15-th iteration, loss: 0.13239939917550259, 36 gd steps\n",
      "insert gradient: -0.04006917911082531\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  51.74807168  102.35035704    0.          102.35035704   60.78280958\n",
      "  106.42884859  115.76616487   96.64923659   82.75030868  101.77035659\n",
      "   42.76432156   67.75796067   78.823114    110.1292158    66.00456133\n",
      "   57.41318714  195.14410403   71.79045728   11.2019648    51.23661343\n",
      "  363.00000199   24.24647417  212.7672323    23.18364746 1518.36642745]\n",
      "16-th iteration, loss: 0.12936024415217418, 17 gd steps\n",
      "insert gradient: -0.01592993480628937\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[5.37220501e+01 9.43061274e+01 0.00000000e+00 1.06581410e-14\n",
      " 5.68935610e+00 9.39233095e+01 6.12132500e+01 1.08557576e+02\n",
      " 1.13963001e+02 9.89922271e+01 8.16364567e+01 1.03123128e+02\n",
      " 4.38339130e+01 6.64843209e+01 7.88132665e+01 1.10334136e+02\n",
      " 6.57726880e+01 5.93318239e+01 1.93995819e+02 7.44957984e+01\n",
      " 1.07047275e+01 5.12971185e+01 3.61703794e+02 2.77299466e+01\n",
      " 2.12645617e+02 2.27955333e+01 1.51743496e+03]\n",
      "17-th iteration, loss: 0.12845076982091844, 23 gd steps\n",
      "insert gradient: -0.008301855222143594\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  54.44183075   91.81886367    8.11456286   90.08542572   61.87665896\n",
      "  110.01301608  111.78086887  100.52788754   81.43395893  104.7903941\n",
      "   44.19733536   64.99937757   78.56204131  111.24250469   65.63012711\n",
      "   60.89580593  192.79439477   76.74096637   10.10786666   51.37449792\n",
      "  160.3634728     0.          200.454341     30.40901861  212.24431375\n",
      "   22.85226263 1516.84026599]\n",
      "18-th iteration, loss: 0.10741180428337807, 115 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[  59.82920348   98.60898117   14.04359548   73.65339483   57.98333699\n",
      "  106.44892834  109.49901101  111.10369535   70.34264116  119.07561235\n",
      "   64.52407932   53.53269566   52.48941933  138.32339389   64.54097551\n",
      "   82.35793928   44.09125802    0.          132.27377405   87.47011999\n",
      "   19.83254462   38.02006468  119.92430731   52.9194564   186.22118379\n",
      "   65.35172809  197.19086678   25.55331127 1522.76518119]\n",
      "19-th iteration, loss: 0.10615279842565101, 23 gd steps\n",
      "insert gradient: -0.006443437035769141\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[  58.52754378   99.18492766   13.65009269   73.82843746   59.36653198\n",
      "  106.01716621  107.34145646  110.54956777   70.62492877  119.44952173\n",
      "   64.48444262   54.56227522   51.32763042  139.5982663    64.86388413\n",
      "   83.09708896   42.31557148    5.0446853   130.5680732    87.14306795\n",
      "   19.92446053   37.72794947  119.23478374   53.11736355  186.23194215\n",
      "   66.42477757  196.6789303    25.29722473 1521.55857321]\n",
      "20-th iteration, loss: 0.10503739933520866, 16 gd steps\n",
      "insert gradient: -0.011193426158809179\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[5.86812962e+01 1.01564477e+02 1.38872597e+01 7.34220133e+01\n",
      " 5.94585713e+01 1.05171454e+02 0.00000000e+00 1.77635684e-14\n",
      " 1.05288168e+02 1.10871248e+02 6.88868418e+01 1.22156698e+02\n",
      " 6.52450784e+01 5.99305701e+01 4.38845725e+01 1.42403183e+02\n",
      " 6.66256935e+01 9.26770203e+01 3.64470916e+01 1.58507804e+01\n",
      " 1.24551213e+02 8.54942637e+01 1.99281899e+01 4.03061880e+01\n",
      " 1.16840572e+02 5.41940657e+01 1.85298012e+02 7.15830889e+01\n",
      " 1.96143366e+02 2.57345925e+01 1.51910871e+03]\n",
      "21-th iteration, loss: 0.10463343050692943, 41 gd steps\n",
      "insert gradient: -0.006665614023348643\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[  58.69229735  100.78047294   14.41864678   71.86184039   58.85118098\n",
      "  106.10456658  106.65616143  111.04511349   68.86176696  121.30226651\n",
      "   64.21105211   61.88273587   43.01987838  142.95196075   66.10730191\n",
      "   94.48909978   35.01905245   17.00565718  124.25464737   85.28553664\n",
      "   19.15095507   40.67858123  116.77709178   54.75347674  184.79821984\n",
      "   74.1455324   195.65272345   25.67190432 1327.81095262    0.\n",
      "  189.68727895]\n",
      "22-th iteration, loss: 0.1028513619375092, 24 gd steps\n",
      "insert gradient: -0.01315465716035002\n",
      "22-th iteration, new layer inserted. now 33 layers\n",
      "[5.92011652e+01 1.01077428e+02 0.00000000e+00 3.55271368e-15\n",
      " 1.58326008e+01 6.76924912e+01 5.89973526e+01 1.06979030e+02\n",
      " 1.06122805e+02 1.10899801e+02 6.85648668e+01 1.21108062e+02\n",
      " 6.37721291e+01 6.66964818e+01 3.95636204e+01 1.45924900e+02\n",
      " 6.46606030e+01 9.88445456e+01 3.26036418e+01 1.78260877e+01\n",
      " 1.24616018e+02 8.41393080e+01 1.97252054e+01 4.11342197e+01\n",
      " 1.16180468e+02 5.80135600e+01 1.83086984e+02 7.91277125e+01\n",
      " 1.95324389e+02 2.31020413e+01 1.31510864e+03 1.56487226e+01\n",
      " 1.92770746e+02]\n",
      "23-th iteration, loss: 0.10244788165582763, 34 gd steps\n",
      "insert gradient: -0.0072540960485711166\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  58.85410912  101.40227798   15.93699266   67.66676093   59.64131403\n",
      "  105.41783939  105.86691467  112.04403528   68.61700296  120.87558992\n",
      "   63.72677479   66.28552761   39.53278435  145.75877462   64.67298645\n",
      "   98.35148163   32.88061316   17.37250645  124.3798224    83.9820953\n",
      "   19.86153706   40.79531009  115.26667938   59.69988297  182.35035545\n",
      "   81.03715548  195.49546793   23.07903375 1309.96986656   18.67879854\n",
      "  165.92731255    0.           27.65455209]\n",
      "24-th iteration, loss: 0.10112325681940622, 16 gd steps\n",
      "insert gradient: -0.006588741627053203\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[6.02334055e+01 1.01908212e+02 1.65756372e+01 6.61462822e+01\n",
      " 5.87343742e+01 1.06156861e+02 0.00000000e+00 1.06581410e-14\n",
      " 1.06158381e+02 1.09761144e+02 6.98300817e+01 1.21158972e+02\n",
      " 6.35106457e+01 6.67928488e+01 3.93267846e+01 1.46607312e+02\n",
      " 6.43532188e+01 9.76818243e+01 3.35387213e+01 1.69369135e+01\n",
      " 1.24499526e+02 8.33776744e+01 2.01567555e+01 4.15014947e+01\n",
      " 1.14015262e+02 6.01860217e+01 1.81462363e+02 8.58841387e+01\n",
      " 1.95784531e+02 2.43811170e+01 1.30424065e+03 1.47470776e+01\n",
      " 1.61332241e+02 1.60268417e+01 2.34972572e+01]\n",
      "25-th iteration, loss: 0.10094003688709478, 16 gd steps\n",
      "insert gradient: -0.007092926360643822\n",
      "25-th iteration, new layer inserted. now 37 layers\n",
      "[5.88811695e+01 1.01716898e+02 1.66423506e+01 6.61068407e+01\n",
      " 5.93054423e+01 1.06364832e+02 3.63391540e-01 4.59966871e-02\n",
      " 1.06584377e+02 1.10166900e+02 6.94981832e+01 1.21023179e+02\n",
      " 6.34580765e+01 6.67399829e+01 3.91711112e+01 1.46480871e+02\n",
      " 6.41659452e+01 9.75686294e+01 3.33088930e+01 1.68157308e+01\n",
      " 1.24253541e+02 8.33463382e+01 2.02341187e+01 4.15417139e+01\n",
      " 7.60511763e+01 0.00000000e+00 3.80255882e+01 6.03359634e+01\n",
      " 1.81485241e+02 8.60477276e+01 1.95721293e+02 2.44133627e+01\n",
      " 1.30402310e+03 1.46022032e+01 1.61138551e+02 1.62202756e+01\n",
      " 2.32621845e+01]\n",
      "26-th iteration, loss: 0.09934783169897905, 22 gd steps\n",
      "insert gradient: -0.01464917421581871\n",
      "26-th iteration, new layer inserted. now 35 layers\n",
      "[  59.45914369  102.37406742   16.58296842   64.22828608   59.21481885\n",
      "  105.5587205   105.69752639  113.29163976   68.26264888  121.2152398\n",
      "   65.04706186   67.35727958   38.68363672  143.45534625   65.4569107\n",
      "  100.04050718   37.03736126   14.45306764  123.40329272   85.60084347\n",
      "   23.14955322   36.17010738   68.37650565   16.73997764   29.91192882\n",
      "   66.51077686  177.97680224   89.1015256   196.23371158   26.19128181\n",
      " 1302.02042894   12.86733459  158.53557911   20.09874397   21.85251311]\n",
      "27-th iteration, loss: 0.09907871883163574, 12 gd steps\n",
      "insert gradient: -0.0074299687410174006\n",
      "27-th iteration, new layer inserted. now 35 layers\n",
      "[  58.84615112  102.2345775    16.63967575   64.22668285   59.35589187\n",
      "  105.81716057  106.51423821  113.39431975   68.39900201  121.22672272\n",
      "   64.77485917   67.1633161    38.48273097  143.46092457   65.62525241\n",
      "  100.04897508   36.96764987   14.38630439  123.3504777    85.65981295\n",
      "   23.33489439   36.15118971   68.32970988   17.02450934   29.77108569\n",
      "   66.63431961  177.77569913   89.15481772  196.17294721   26.252805\n",
      " 1301.984467     12.84203279  158.46260133   20.09695085   21.87740902]\n",
      "28-th iteration, loss: 0.09878967891015024, 18 gd steps\n",
      "insert gradient: -0.006435389863360869\n",
      "28-th iteration, new layer inserted. now 37 layers\n",
      "[  59.03478581  102.01243564   16.41562706   64.20020241   59.86396005\n",
      "  105.76266907  107.38546348  111.61427939   68.42658986  121.56015344\n",
      "   64.5195661    67.68432236   38.4329196   142.94272885   66.11375678\n",
      "  100.25758034   37.46139959   13.78043077  123.50846425   86.20642775\n",
      "   24.17276249   35.15619504   67.42483945   20.03530795   28.06042054\n",
      "   68.34890926  147.08568176    0.           29.41713635   90.2285181\n",
      "  196.10492735   27.06312158 1301.73737856   12.62977148  157.61614564\n",
      "   20.49554042   22.13368693]\n",
      "29-th iteration, loss: 0.09455776479143614, 67 gd steps\n",
      "insert gradient: -0.00312518343082571\n",
      "29-th iteration, new layer inserted. now 39 layers\n",
      "[  58.44337059  100.49055424   13.13408068   70.92840555   59.23738875\n",
      "  105.09539779  110.47154105  110.7975787    66.60450112  121.40358021\n",
      "   66.53329414   68.81201248   36.20397319  138.10001533   69.58877345\n",
      "  100.91089633   45.54506269    9.9545437   118.76162994   91.85878236\n",
      "   31.3533965    18.0218728    65.08471764   45.47977955   18.5286074\n",
      "   75.79677901  135.85780243   27.03465894   14.47082392   99.95464556\n",
      "  129.88921777    0.           64.94460889   23.54466517 1305.07972983\n",
      "   12.07896432  151.41384276   23.69835238   22.40610998]\n",
      "30-th iteration, loss: 0.09333941875133096, 47 gd steps\n",
      "insert gradient: -0.006984513427164912\n",
      "30-th iteration, new layer inserted. now 41 layers\n",
      "[5.76423685e+01 9.92613148e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.14639221e+01 7.55868819e+01 5.93283657e+01 1.03443840e+02\n",
      " 1.10841253e+02 1.10123158e+02 6.67280275e+01 1.22369978e+02\n",
      " 6.59530794e+01 7.04916706e+01 3.50378931e+01 1.37833410e+02\n",
      " 6.96897885e+01 1.01025264e+02 5.13164755e+01 6.09548578e+00\n",
      " 1.15744749e+02 9.33019671e+01 3.72217139e+01 7.65288743e+00\n",
      " 6.55371729e+01 5.74442531e+01 1.54358972e+01 7.86281130e+01\n",
      " 1.27548976e+02 3.67105837e+01 1.24456080e+01 1.01718677e+02\n",
      " 1.28773301e+02 8.23570426e+00 5.86179244e+01 2.39759995e+01\n",
      " 1.30901165e+03 1.22505734e+01 1.48762271e+02 2.45472125e+01\n",
      " 2.26432683e+01]\n",
      "31-th iteration, loss: 0.09290644493352994, 35 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "31-th iteration, new layer inserted. now 43 layers\n",
      "[5.84579294e+01 9.79463807e+01 1.75647651e-01 7.23066858e-02\n",
      " 1.16746226e+01 7.59998758e+01 5.84908092e+01 1.04460584e+02\n",
      " 1.10602913e+02 1.10140276e+02 6.67137352e+01 1.21302208e+02\n",
      " 6.66070247e+01 7.16025103e+01 3.39534026e+01 1.37014391e+02\n",
      " 7.06175661e+01 1.00326987e+02 5.25724127e+01 5.30764214e+00\n",
      " 1.15039991e+02 9.32224574e+01 3.96875867e+01 3.44325488e+00\n",
      " 6.63488314e+01 6.15315672e+01 1.44841264e+01 7.96562467e+01\n",
      " 1.24402548e+02 4.16996536e+01 1.07823185e+01 1.02340147e+02\n",
      " 1.31190790e+02 1.09348156e+01 5.32135787e+01 2.56741895e+01\n",
      " 5.24134757e+02 0.00000000e+00 7.86202135e+02 1.22164321e+01\n",
      " 1.47656862e+02 2.50700042e+01 2.26011568e+01]\n",
      "32-th iteration, loss: 0.09235957320333077, 17 gd steps\n",
      "insert gradient: -0.003154522859266716\n",
      "32-th iteration, new layer inserted. now 45 layers\n",
      "[5.87347280e+01 9.77830314e+01 1.03945982e-01 9.55335691e-02\n",
      " 1.16290110e+01 7.63226621e+01 5.83979373e+01 1.03166612e+02\n",
      " 1.11085416e+02 1.09844451e+02 6.61611421e+01 1.21720359e+02\n",
      " 6.68249083e+01 7.22663717e+01 3.33900047e+01 1.36306465e+02\n",
      " 7.09229142e+01 1.00337980e+02 5.32362457e+01 4.89760938e+00\n",
      " 1.14516071e+02 9.30514725e+01 4.10178334e+01 1.38200410e+00\n",
      " 6.70938371e+01 6.29884497e+01 1.42380886e+01 8.03823963e+01\n",
      " 1.23021200e+02 4.38521965e+01 9.74415387e+00 1.02619990e+02\n",
      " 1.32568226e+02 1.30454227e+01 4.96841533e+01 2.72645439e+01\n",
      " 3.14080128e+02 0.00000000e+00 2.09386752e+02 6.59134381e+00\n",
      " 7.85267314e+02 1.23146320e+01 1.46801282e+02 2.54922688e+01\n",
      " 2.22791311e+01]\n",
      "33-th iteration, loss: 0.09190243994486905, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "33-th iteration, new layer inserted. now 45 layers\n",
      "[5.90699162e+01 9.76483971e+01 1.92519290e-01 2.40460364e-01\n",
      " 1.18178249e+01 7.66859137e+01 5.81558962e+01 1.04144137e+02\n",
      " 1.10390293e+02 1.09843209e+02 6.62204542e+01 1.21201747e+02\n",
      " 6.72029823e+01 7.30770162e+01 3.27121851e+01 1.35533558e+02\n",
      " 7.14171563e+01 1.00338857e+02 5.40219041e+01 4.49731553e+00\n",
      " 1.14108986e+02 9.25934970e+01 1.09630348e+02 6.38883010e+01\n",
      " 1.39921480e+01 8.10731784e+01 1.22255027e+02 4.66056173e+01\n",
      " 8.66548690e+00 1.03120424e+02 1.34039185e+02 1.46950356e+01\n",
      " 4.59405682e+01 2.89222749e+01 1.25407684e+02 0.00000000e+00\n",
      " 1.88111527e+02 6.00498592e+00 2.08776738e+02 9.01476520e+00\n",
      " 7.83009262e+02 1.18027342e+01 1.46112181e+02 2.58397690e+01\n",
      " 2.19226801e+01]\n",
      "34-th iteration, loss: 0.09059221955897484, 106 gd steps\n",
      "insert gradient: -0.0032040183964557284\n",
      "34-th iteration, new layer inserted. now 43 layers\n",
      "[ 57.89345707  95.59374842  11.7549773   77.95695759  58.78464289\n",
      " 103.20490385 110.35481898 109.75959827  65.73864716 120.65316471\n",
      "  67.02008512  76.40457283  30.28900826 135.82635276  71.77133687\n",
      "  99.68056605  56.4460206    1.19030831 114.95392507  91.33001255\n",
      " 106.56391256  69.10618671  13.70443923  80.39194513 118.43897469\n",
      "  55.53661768   6.71977167 104.34681597 132.48879556  18.33715751\n",
      "  41.98702888  33.07946455 118.59865354  13.88543184 193.35053054\n",
      "   9.05979594 208.34039537  12.53944815 773.51621828  11.14281725\n",
      " 145.99399894  26.78090803  20.68549556]\n",
      "35-th iteration, loss: 0.09057783075736778, 18 gd steps\n",
      "insert gradient: -0.002188541238359171\n",
      "35-th iteration, new layer inserted. now 45 layers\n",
      "[ 58.01042098  95.57889448  11.79204702  77.96792392  58.67821548\n",
      " 103.18228285 110.36599106 109.77219253  65.72676026 120.64228728\n",
      "  67.00746235  76.45496904  30.24125723 135.82582104  71.77718924\n",
      "  99.66240497  56.45735177   1.09891891 114.9494131   91.32851012\n",
      " 106.49913547  69.24349617  13.67002171  80.36789349 118.36913331\n",
      "  55.68252021   6.65744574 104.36461744  79.46840498   0.\n",
      "  52.97893665  18.36402426  41.91142629  33.05541339 118.42718186\n",
      "  13.96778741 193.56650216   9.21067547 208.28134044  12.57537596\n",
      " 773.29646143  11.13119675 145.98427114  26.7544052   20.65982636]\n",
      "36-th iteration, loss: 0.08995770872511655, 34 gd steps\n",
      "insert gradient: -0.0025602598278619527\n",
      "36-th iteration, new layer inserted. now 45 layers\n",
      "[5.77069512e+01 9.47982131e+01 1.19302951e+01 7.77371688e+01\n",
      " 5.87229094e+01 1.03145689e+02 1.10196400e+02 1.09723715e+02\n",
      " 6.54070722e+01 1.20403304e+02 6.71258851e+01 7.71523069e+01\n",
      " 2.93761499e+01 1.35890955e+02 7.17001181e+01 9.90213356e+01\n",
      " 5.69315870e+01 6.15513282e-02 1.15360040e+02 9.12532385e+01\n",
      " 1.05816851e+02 7.31132905e+01 1.19123457e+01 8.03144236e+01\n",
      " 1.19632467e+02 5.67631315e+01 4.84549812e+00 1.04742601e+02\n",
      " 7.53009716e+01 1.00737385e+01 4.74046821e+01 2.29608711e+01\n",
      " 3.91176502e+01 3.42049095e+01 1.13556333e+02 1.70241061e+01\n",
      " 1.98312339e+02 1.31112713e+01 2.08305166e+02 1.38264435e+01\n",
      " 7.69108484e+02 1.11742975e+01 1.46035517e+02 2.67801961e+01\n",
      " 2.04526763e+01]\n",
      "37-th iteration, loss: 0.0898180038105337, 17 gd steps\n",
      "insert gradient: -0.004433009508586435\n",
      "37-th iteration, new layer inserted. now 45 layers\n",
      "[5.74517274e+01 9.47020086e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.19573499e+01 7.74542284e+01 5.86941994e+01 1.03062468e+02\n",
      " 1.10126945e+02 1.09790963e+02 6.52973170e+01 1.20126632e+02\n",
      " 6.72299047e+01 7.73447413e+01 2.91633066e+01 1.35621967e+02\n",
      " 7.17857827e+01 9.89765816e+01 1.72118008e+02 9.14167442e+01\n",
      " 1.05590342e+02 7.46157425e+01 1.12544275e+01 8.04386035e+01\n",
      " 1.19830251e+02 5.70974039e+01 4.08475065e+00 1.05164233e+02\n",
      " 7.53052189e+01 1.22928727e+01 4.57667751e+01 2.40180604e+01\n",
      " 3.78630795e+01 3.45638737e+01 1.11852007e+02 1.83169948e+01\n",
      " 1.99493725e+02 1.45519702e+01 2.08138521e+02 1.41451376e+01\n",
      " 7.67832412e+02 1.11485235e+01 1.46142855e+02 2.67405248e+01\n",
      " 2.03929476e+01]\n",
      "38-th iteration, loss: 0.08962365580294031, 36 gd steps\n",
      "insert gradient: -0.0038328487373837663\n",
      "38-th iteration, new layer inserted. now 47 layers\n",
      "[5.78304082e+01 9.45705253e+01 1.64783767e-01 1.69691237e-02\n",
      " 1.21356552e+01 7.71483361e+01 5.86910923e+01 1.02806867e+02\n",
      " 1.09325006e+02 1.09579623e+02 6.53009327e+01 1.19990624e+02\n",
      " 6.72979763e+01 7.73103508e+01 2.88527949e+01 1.35077344e+02\n",
      " 7.18952424e+01 9.88474913e+01 1.37331761e+02 0.00000000e+00\n",
      " 3.43329404e+01 9.16460016e+01 1.05170811e+02 7.65517997e+01\n",
      " 1.03838216e+01 8.07016075e+01 1.19668737e+02 5.73920503e+01\n",
      " 3.19840505e+00 1.05709466e+02 7.56693508e+01 1.44488655e+01\n",
      " 4.39718864e+01 2.52916191e+01 3.60966403e+01 3.51713295e+01\n",
      " 1.09881952e+02 2.00384091e+01 2.00665938e+02 1.65481490e+01\n",
      " 2.07791006e+02 1.45630995e+01 7.66310090e+02 1.11447688e+01\n",
      " 1.46248056e+02 2.66039964e+01 2.02947732e+01]\n",
      "39-th iteration, loss: 0.08900480607424603, 18 gd steps\n",
      "insert gradient: -0.0021001141424775195\n",
      "39-th iteration, new layer inserted. now 49 layers\n",
      "[5.77716268e+01 9.43060589e+01 2.20947090e-01 8.37427140e-02\n",
      " 1.21585206e+01 7.66656076e+01 5.84007199e+01 1.02952742e+02\n",
      " 1.09445490e+02 1.09794112e+02 6.44321539e+01 1.19277648e+02\n",
      " 6.78138836e+01 7.77021916e+01 2.84753220e+01 1.34361109e+02\n",
      " 7.19543227e+01 9.83926281e+01 1.34656281e+02 6.83055944e+00\n",
      " 3.15739719e+01 9.21550866e+01 1.05778652e+02 7.80861487e+01\n",
      " 8.88756893e+00 8.07672589e+01 1.20228991e+02 5.76846048e+01\n",
      " 2.36597888e+00 1.06190817e+02 7.62614485e+01 1.62087871e+01\n",
      " 4.28647896e+01 2.58448625e+01 3.43990179e+01 3.52872571e+01\n",
      " 1.09271883e+02 2.13595819e+01 2.01110728e+02 1.78469759e+01\n",
      " 1.55632844e+02 0.00000000e+00 5.18776145e+01 1.46078905e+01\n",
      " 7.65422250e+02 1.14536156e+01 1.46433830e+02 2.61781758e+01\n",
      " 2.04747694e+01]\n",
      "40-th iteration, loss: 0.08769711988777075, 46 gd steps\n",
      "insert gradient: -0.005766683038656394\n",
      "40-th iteration, new layer inserted. now 45 layers\n",
      "[ 57.14801117  93.43026812  12.72235286  75.05683694  58.22898428\n",
      " 102.29555937 108.04536678 109.48974613  64.16150124 117.01205895\n",
      "  68.58250298  80.35183525  26.63523582 130.18271869  73.40045669\n",
      "  98.72393802 132.02823771  13.7528657   25.97129862  96.52209389\n",
      " 106.05437815  83.61660783   5.38467132  81.44764519 120.9467873\n",
      " 164.9386568   78.35973413  20.70034903  39.16901255  27.78623329\n",
      "  29.81757917  36.77112789 108.41905714  25.77405819 201.54154267\n",
      "  20.18281105 150.79103945   8.84336829  46.72362635  18.86837323\n",
      " 763.89877416  12.32321383 146.59902715  25.31915865  21.17308178]\n",
      "41-th iteration, loss: 0.08619425516448821, 60 gd steps\n",
      "insert gradient: -0.0013560079917514605\n",
      "41-th iteration, new layer inserted. now 45 layers\n",
      "[ 56.65352323  91.76323876  12.64419944  74.92087254  57.47900085\n",
      " 100.89061603 106.92735187 108.04180043  62.77837924 115.6862469\n",
      "  67.98965492  82.21576338  25.43204587 124.08686942  74.07165264\n",
      "  97.88064991 130.54505197  15.59811582  22.96857349  97.9282796\n",
      " 104.5553574   85.9692071    3.81047231  81.8620807  118.90907186\n",
      " 161.88064057  79.29867969  22.36754727  35.42480775  29.26380003\n",
      "  27.73890689  38.24823292 107.11899493  28.51806067 199.12066472\n",
      "  22.03964398 146.95842498  12.74039806  42.92379447  21.63252118\n",
      " 760.12393583  11.68543836 145.70254041  24.7781005   20.60174967]\n",
      "42-th iteration, loss: 0.08542582032492754, 27 gd steps\n",
      "insert gradient: -0.0009942277348577013\n",
      "42-th iteration, new layer inserted. now 47 layers\n",
      "[ 56.43515051  90.90665324  12.45551351  74.82481059  56.94707604\n",
      " 100.36437402 106.60908918 107.38700698  62.2661542  114.47547774\n",
      "  67.94139095  83.11992415  25.27303937 120.44418693  74.72741556\n",
      "  97.05931883 129.61169003  16.16807182  22.14064835  98.34729157\n",
      " 103.76892521  87.16062606   3.07833613  81.92995126 118.1325776\n",
      " 159.9192407   80.33070889  23.71971833  33.53623955  30.15793648\n",
      "  26.48699663  39.12511113 106.61535317  30.02514139 197.8463855\n",
      "  22.80702114 145.07481035  14.55095975  41.492767    23.16030124\n",
      " 757.05534368  11.32633348 116.10303136   0.          29.02575784\n",
      "  24.78922756  20.57238019]\n",
      "43-th iteration, loss: 0.07777510294081791, 252 gd steps\n",
      "insert gradient: -0.0027573721729905174\n",
      "43-th iteration, new layer inserted. now 49 layers\n",
      "[ 53.74575655  85.719299    10.22899295  74.09150399  54.15098584\n",
      "  95.29675069 101.02750765 100.5903693   57.35253794 106.43169946\n",
      "  66.73314117  84.06754534  26.51928546  91.02234904  77.12523532\n",
      "  91.07873957 119.23251095  17.74727355  20.90925572  93.24813262\n",
      "  96.16696643  89.93171346   1.92480384  74.87253188  82.20343646\n",
      "   0.          27.40114549 150.54456873  74.54170089  27.95765385\n",
      "  18.64044168  46.13330862  20.25890785  51.20648405 103.27589489\n",
      "  34.26595563 181.95000637  23.37953309 133.36902156  21.02108071\n",
      "  34.74593857  27.1924512  711.18070788   6.14140767 110.12243136\n",
      "  10.45903704  23.98391784  29.77492136  14.12992225]\n",
      "44-th iteration, loss: 0.07611586485764454, 72 gd steps\n",
      "insert gradient: -0.002326887818558975\n",
      "44-th iteration, new layer inserted. now 51 layers\n",
      "[ 53.11956244  84.84514717  10.46422509  73.56851646  53.64170104\n",
      "  94.96093011  99.69893944 100.61462525  56.71081302 105.3076999\n",
      "  66.40864769  84.89845413  26.2857176   88.15145727  77.01648863\n",
      "  91.18589305 115.83152833  18.62006026  21.55747698  91.54855843\n",
      "  94.98473727  89.15047459   2.08357832  72.65452614  78.7062025\n",
      "  10.08669869  22.35811936 152.1144427   74.4137939   27.396161\n",
      "  15.56896845  51.70858377  20.45608928  51.02862558 103.20128795\n",
      "  35.58658952 135.41637867   0.          45.13879289  24.05549091\n",
      " 132.21311588  20.8445959   34.49852398  27.56590888 705.67327555\n",
      "   5.65532116 109.37213067  11.02264397  24.26230758  30.75242675\n",
      "  12.88140995]\n",
      "45-th iteration, loss: 0.07464235869490213, 31 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 53 layers\n",
      "[ 53.45769727  83.70475304  10.58797998  74.0701147   53.48635741\n",
      "  94.39827873  99.41331646  99.90039593  56.64144349 104.38386508\n",
      "  66.00326722  87.7661219   26.30908096  84.09174682  77.40442776\n",
      "  90.93984788 112.33356792  21.20472701  22.37750604  89.4310806\n",
      "  94.22085491  89.03049387   2.13292589  70.3281953   80.01420695\n",
      "  14.63209205  16.79029306  77.9792582    0.          77.9792582\n",
      "  71.95026409  24.78523746  13.86755972  61.49768884  20.79193799\n",
      "  47.89023386 106.01144708  36.90519893 130.2754175   11.38690317\n",
      "  38.11820643  29.17550262 130.81509234  23.26769782  32.38371274\n",
      "  29.70133921 699.69227884   4.14878761 109.19510104  12.92742367\n",
      "  25.6420081   33.86195994  11.49576153]\n",
      "46-th iteration, loss: 0.0743113407286576, 23 gd steps\n",
      "insert gradient: -0.010296752379433633\n",
      "46-th iteration, new layer inserted. now 55 layers\n",
      "[5.28046571e+01 8.36674952e+01 1.07748540e+01 7.42247313e+01\n",
      " 5.37617745e+01 9.45211460e+01 9.91914729e+01 9.97425276e+01\n",
      " 5.64523511e+01 1.04673675e+02 6.58547914e+01 8.82660134e+01\n",
      " 2.62790999e+01 8.32013859e+01 7.74096044e+01 9.06476198e+01\n",
      " 1.11217972e+02 2.24027039e+01 2.25496484e+01 8.87700575e+01\n",
      " 9.39373291e+01 8.89234975e+01 2.03051474e+00 6.98982557e+01\n",
      " 8.03365410e+01 1.52473616e+01 1.54257560e+01 5.86098791e+01\n",
      " 0.00000000e+00 1.95366264e+01 6.12557466e-01 7.82333601e+01\n",
      " 7.15597267e+01 2.41272629e+01 1.34493468e+01 6.33537747e+01\n",
      " 2.09964137e+01 4.65801687e+01 1.06410101e+02 3.74185495e+01\n",
      " 1.29841877e+02 1.32366666e+01 3.66390854e+01 2.99112179e+01\n",
      " 1.30236973e+02 2.38342165e+01 3.18949262e+01 3.01300216e+01\n",
      " 6.98546625e+02 4.16568031e+00 1.08938836e+02 1.33894130e+01\n",
      " 2.56347185e+01 3.42849013e+01 1.11129113e+01]\n",
      "47-th iteration, loss: 0.07353395892464153, 56 gd steps\n",
      "insert gradient: -0.026913562786382097\n",
      "47-th iteration, new layer inserted. now 55 layers\n",
      "[ 53.04265928  83.43648458  10.84671759  73.95585351  53.40542785\n",
      "  94.42404567  98.97891695  99.66108007  56.35709363 104.38062908\n",
      "  65.92511166  89.37252393  26.23705825  81.06885666  77.43163584\n",
      "  90.49238665 108.50623419  24.87621062  22.4936958   87.46042056\n",
      "  93.1888623   66.55880902   0.          22.18626967   1.08495929\n",
      "  69.48003233  80.78424795  13.78073296  13.33188228  57.87702508\n",
      "   3.92933152  97.26249544  72.49988079  22.56301776  12.17955177\n",
      "  66.9637424   21.5892247   43.63065902 107.61683384  38.54372474\n",
      " 129.20175469  16.68303263  34.38713622  31.04272305 129.26084776\n",
      "  24.50219216  30.84277995  31.07662998 696.62357792   4.24220472\n",
      " 108.41442646  13.82646717  25.37047887  35.07383014  10.82252307]\n",
      "48-th iteration, loss: 0.07233878344627992, 27 gd steps\n",
      "insert gradient: -0.0017934414818550082\n",
      "48-th iteration, new layer inserted. now 55 layers\n",
      "[5.35406744e+01 8.33719957e+01 1.15416994e+01 7.32228297e+01\n",
      " 5.30511632e+01 9.42603516e+01 9.89181365e+01 9.96240796e+01\n",
      " 5.66384826e+01 1.03999448e+02 6.56910899e+01 9.21416822e+01\n",
      " 2.70428501e+01 7.60223104e+01 7.83552149e+01 9.06557436e+01\n",
      " 7.77388168e+01 0.00000000e+00 2.59129389e+01 2.97189793e+01\n",
      " 2.27788781e+01 8.57482126e+01 9.22706067e+01 6.71091083e+01\n",
      " 3.12857579e-01 9.29901084e+01 8.28458227e+01 7.77092445e+00\n",
      " 1.12790881e+01 5.66954698e+01 7.43683754e+00 9.69569118e+01\n",
      " 7.53388149e+01 2.02880953e+01 9.66183086e+00 7.31095860e+01\n",
      " 2.19851713e+01 3.79499546e+01 1.10556163e+02 4.02944364e+01\n",
      " 1.27552597e+02 2.20014914e+01 3.11052142e+01 3.38937775e+01\n",
      " 1.28122050e+02 2.57205687e+01 2.92923349e+01 3.35373797e+01\n",
      " 6.93593512e+02 3.75970515e+00 1.07435816e+02 1.47989375e+01\n",
      " 2.59243600e+01 3.63346390e+01 1.03862074e+01]\n",
      "49-th iteration, loss: 0.07227097508828614, 24 gd steps\n",
      "insert gradient: -0.0017539411280231892\n",
      "49-th iteration, new layer inserted. now 57 layers\n",
      "[5.34266138e+01 8.33187702e+01 1.14254635e+01 7.31638090e+01\n",
      " 5.30210984e+01 9.42351550e+01 9.88635229e+01 9.95903081e+01\n",
      " 5.65970680e+01 1.03997268e+02 6.56574512e+01 9.21460714e+01\n",
      " 2.70114600e+01 7.59321254e+01 7.82807654e+01 9.06190573e+01\n",
      " 7.76572804e+01 0.00000000e+00 7.10542736e-15 1.62481806e-01\n",
      " 2.58317300e+01 2.97680676e+01 2.27657562e+01 8.57250147e+01\n",
      " 9.22530520e+01 6.71008966e+01 3.01478442e-01 9.29822588e+01\n",
      " 8.28731019e+01 7.70241978e+00 1.12737580e+01 5.66978708e+01\n",
      " 7.45916576e+00 9.69470994e+01 7.53315612e+01 2.02474569e+01\n",
      " 9.57670190e+00 7.31318438e+01 2.19390680e+01 3.78809123e+01\n",
      " 1.10547553e+02 4.02806448e+01 1.27510408e+02 2.20351182e+01\n",
      " 3.10380062e+01 3.39008688e+01 1.28088704e+02 2.57054424e+01\n",
      " 2.92359812e+01 3.35415426e+01 6.93563402e+02 3.76037687e+00\n",
      " 1.07421188e+02 1.48109479e+01 2.59304303e+01 3.63590472e+01\n",
      " 1.04034124e+01]\n",
      "50-th iteration, loss: 0.0710988951427865, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "50-th iteration, new layer inserted. now 55 layers\n",
      "[ 54.46182022  83.55995499  11.08650931  71.83293432  51.64467437\n",
      "  94.2631164  100.53894591  99.95496573  55.57376072 104.83432632\n",
      "  64.97437345  94.05470878  28.32456924  71.24700385  78.86931337\n",
      "  90.32964646  73.393415    10.15694664  23.14180256  37.67229287\n",
      "  18.89438618  82.76361502  89.19495708  66.21566417   1.24997488\n",
      "  23.0287185    0.          69.0861555   97.78187504  59.03929068\n",
      "   9.45066503  95.32928142  78.1060806   18.60895465   8.25660145\n",
      "  76.97370787  22.57114702  34.44640148 111.47467044  40.95022216\n",
      " 125.24779429  26.05680039  28.78106924  37.07331299 127.06896058\n",
      "  27.02585657  27.55588658  35.09612374 690.46082719   3.67485749\n",
      " 105.64826523  15.43236466  25.97051802  38.50776569  10.72951018]\n",
      "51-th iteration, loss: 0.07060635981775029, 44 gd steps\n",
      "insert gradient: -0.0013893110733972699\n",
      "51-th iteration, new layer inserted. now 57 layers\n",
      "[5.29968097e+01 8.38712219e+01 1.14966477e+01 7.21449793e+01\n",
      " 5.33852099e+01 9.41698741e+01 9.90180733e+01 9.93793603e+01\n",
      " 5.63116505e+01 1.05105174e+02 6.46459559e+01 9.42678973e+01\n",
      " 2.83278616e+01 7.05892905e+01 7.90436155e+01 9.05177837e+01\n",
      " 7.24406607e+01 1.09957865e+01 2.27892130e+01 3.88909103e+01\n",
      " 1.84659131e+01 8.24960320e+01 8.92582700e+01 6.60803840e+01\n",
      " 1.64697273e+00 2.28408426e+01 6.99958912e-03 6.88997728e+01\n",
      " 9.77756810e+01 5.96783670e+01 9.49093120e+00 9.47509619e+01\n",
      " 7.83724856e+01 1.84527427e+01 8.01290530e+00 7.75183064e+01\n",
      " 2.26423486e+01 3.39610196e+01 8.37443147e+01 0.00000000e+00\n",
      " 2.79147716e+01 4.11488193e+01 1.24993363e+02 2.66753288e+01\n",
      " 2.83970941e+01 3.76130432e+01 1.26903214e+02 2.73290025e+01\n",
      " 2.72253522e+01 3.54144093e+01 6.89854116e+02 3.49994129e+00\n",
      " 1.05222459e+02 1.56479222e+01 2.60171249e+01 3.88917809e+01\n",
      " 1.06405789e+01]\n",
      "52-th iteration, loss: 0.06978855311004814, 39 gd steps\n",
      "insert gradient: -0.0027538124678035242\n",
      "52-th iteration, new layer inserted. now 57 layers\n",
      "[5.32857877e+01 8.43868307e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.14710775e+01 7.18186270e+01 5.33600762e+01 9.41508589e+01\n",
      " 9.88679275e+01 9.92620232e+01 5.62807559e+01 1.05327395e+02\n",
      " 6.41885325e+01 9.58523034e+01 2.91381415e+01 6.75744656e+01\n",
      " 7.95654583e+01 9.11604720e+01 6.90600449e+01 1.37428250e+01\n",
      " 2.20442206e+01 4.45601106e+01 1.68897293e+01 8.13049634e+01\n",
      " 8.94610957e+01 6.54063684e+01 2.86931702e+00 8.91026089e+01\n",
      " 9.73710799e+01 6.30626932e+01 9.61495471e+00 9.16891388e+01\n",
      " 7.92097923e+01 1.82292150e+01 7.49445925e+00 7.98606888e+01\n",
      " 2.33997074e+01 3.10641001e+01 8.25228078e+01 5.98383881e+00\n",
      " 2.68037552e+01 4.30991108e+01 1.23499716e+02 2.99283617e+01\n",
      " 2.60516174e+01 3.99836671e+01 1.26661065e+02 2.92394844e+01\n",
      " 2.60461183e+01 3.66200931e+01 6.87073758e+02 2.38218612e+00\n",
      " 1.03079619e+02 1.65995628e+01 2.60218583e+01 4.08106511e+01\n",
      " 1.04491229e+01]\n",
      "53-th iteration, loss: 0.06963819580341453, 15 gd steps\n",
      "insert gradient: -0.002960302859959036\n",
      "53-th iteration, new layer inserted. now 59 layers\n",
      "[5.30892719e+01 8.44063882e+01 5.21264027e-03 4.54188272e-02\n",
      " 0.00000000e+00 3.46944695e-18 1.14520705e+01 7.17321887e+01\n",
      " 5.33725893e+01 9.42115322e+01 9.89429855e+01 9.93306507e+01\n",
      " 5.62520780e+01 1.05300356e+02 6.41092401e+01 9.60723038e+01\n",
      " 2.92836069e+01 6.69209443e+01 7.96734375e+01 9.11390639e+01\n",
      " 6.83864884e+01 1.39700946e+01 2.18290859e+01 4.56019287e+01\n",
      " 1.67976042e+01 8.10347132e+01 8.96943684e+01 6.52476915e+01\n",
      " 2.94029314e+00 8.87029230e+01 9.73020707e+01 6.37139401e+01\n",
      " 9.62866807e+00 9.09127228e+01 7.92225339e+01 1.82293628e+01\n",
      " 7.47146911e+00 8.02635914e+01 2.36709511e+01 3.05323213e+01\n",
      " 8.21945757e+01 6.85186249e+00 2.65377516e+01 4.36507132e+01\n",
      " 1.23093118e+02 3.06313144e+01 2.56660665e+01 4.05157409e+01\n",
      " 1.26617259e+02 2.96530150e+01 2.58663379e+01 3.68136067e+01\n",
      " 6.86473536e+02 2.16363124e+00 1.02596378e+02 1.67873704e+01\n",
      " 2.58585361e+01 4.11344004e+01 1.03331083e+01]\n",
      "54-th iteration, loss: 0.06741553152887829, 31 gd steps\n",
      "insert gradient: -0.0025521548116753025\n",
      "54-th iteration, new layer inserted. now 53 layers\n",
      "[ 51.30673294  86.87714601  13.29245917  67.74687269  53.92797109\n",
      "  94.98169545  98.69763094  99.62767466  56.32677311 107.26686329\n",
      "  61.31909482  98.41714249  36.56167812  48.1542534   82.6198433\n",
      "  93.89518139  56.63336725  17.55262515  17.67089881  68.84401797\n",
      "  13.77614434  74.1457301   90.91635063  65.46346547   5.89844343\n",
      "  76.51762455  96.81335731  85.27331266  10.19143667  70.63394634\n",
      "  78.9108311   20.14507301   6.69277199  90.58097958  29.31432306\n",
      "  14.41624572  76.2462104   22.74321145  21.07321858  62.40304625\n",
      " 113.42141003  47.37982134  16.43552279  55.72195714 123.50093383\n",
      "  35.37879339  21.77023145  41.63183408 767.22332575  21.65402831\n",
      "  22.76319659  49.82385971   7.98028072]\n",
      "55-th iteration, loss: 0.06694029959391862, 9 gd steps\n",
      "insert gradient: -0.01660826729345757\n",
      "55-th iteration, new layer inserted. now 55 layers\n",
      "[5.12234199e+01 8.66906736e+01 1.28121992e+01 6.75446335e+01\n",
      " 5.35794714e+01 9.47964245e+01 9.84184158e+01 9.95237279e+01\n",
      " 5.62803470e+01 1.07265403e+02 6.13466802e+01 9.84514927e+01\n",
      " 3.66736168e+01 4.82055808e+01 8.26665121e+01 9.39167666e+01\n",
      " 5.66558230e+01 1.75374294e+01 1.76329696e+01 6.88265407e+01\n",
      " 1.35431932e+01 7.40427459e+01 9.06963348e+01 6.53651378e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.65235833e+00 7.64389909e+01\n",
      " 9.66805767e+01 8.52668007e+01 1.02011406e+01 7.06044253e+01\n",
      " 7.89079970e+01 2.01207730e+01 6.66751001e+00 9.05734316e+01\n",
      " 2.93300651e+01 1.43996756e+01 7.62237227e+01 2.27284459e+01\n",
      " 2.10010266e+01 6.23911482e+01 1.13408599e+02 4.73919977e+01\n",
      " 1.64234775e+01 5.57221830e+01 1.23500734e+02 3.53700922e+01\n",
      " 2.17679303e+01 4.16394639e+01 7.67229473e+02 2.16602481e+01\n",
      " 2.27479702e+01 4.98235210e+01 7.98221287e+00]\n",
      "56-th iteration, loss: 0.06671725688302234, 16 gd steps\n",
      "insert gradient: -0.004978440605762576\n",
      "56-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.15921862  86.60085293  12.79122271  67.59173247  53.32270397\n",
      "  94.55348563  98.24538865  99.48019884  56.13647365 106.9423047\n",
      "  61.54985397  98.37176662  36.66294475  48.10140356  82.55459742\n",
      "  93.89588727  57.25823096  17.39659893  17.90002597  69.07566724\n",
      "  13.26765267  73.7079509   90.55711834  65.35443673   6.05367091\n",
      "  76.30291653  96.02584891  85.27502127  10.38805518  70.25113447\n",
      "  78.83026853  19.94573262   6.46876166  90.53473776  29.73094994\n",
      "  14.45489692  76.15129471  22.7616935   20.58530967  62.37267533\n",
      " 113.28455297  47.55269121  16.33707948  55.72497357 123.49517402\n",
      "  35.30269616  21.72332397  41.68859974 767.23444187  21.7141094\n",
      "  22.60403354  49.82617288   8.03710521]\n",
      "57-th iteration, loss: 0.0661287315562337, 16 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "57-th iteration, new layer inserted. now 55 layers\n",
      "[ 53.34986619  84.54308329  13.63681319  68.22283817  52.84171753\n",
      "  92.60186031  97.59204069  99.27356759  57.67346003 104.33493983\n",
      "  62.17627249  96.58381437  37.72395084  45.78076088  83.80088029\n",
      "  90.36116523  62.09306015  15.23787769  15.92888614  71.36431783\n",
      "  12.68441917  69.76063771  92.38226416  64.94337367   6.78231157\n",
      "  74.70597757  93.83557544  87.41627072  12.46246476  63.90369285\n",
      "  80.01352066  18.23792935   6.37779875  90.82289658  32.6570443\n",
      "  12.16116822  74.79981207  25.44961015  19.21816155  63.88121556\n",
      " 111.57958797  50.25973606  15.19653087  57.40576245 124.26766603\n",
      "  35.80918039  21.35562977  41.93308485 191.65388567   0.\n",
      " 574.96165702  22.34992075  21.87485745  50.18739601   7.58921044]\n",
      "58-th iteration, loss: 0.06232963812889851, 81 gd steps\n",
      "insert gradient: -0.004354825085305459\n",
      "58-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.80429197  84.24917008  16.85758953  62.35048423  52.33428744\n",
      "  93.99472292  94.60324768 101.91339749  56.6062735  105.74333407\n",
      "  60.26468177  98.19893616  47.10173688  29.31673265  84.59675928\n",
      "  87.4460583   67.47461684   8.95874867   5.84436033  82.25248951\n",
      "   9.53711011  56.04070892  96.8628372   63.53122222  10.19054091\n",
      "  72.91820978  91.21654641  96.13713812  21.03282889  35.17112177\n",
      "  83.19788797  13.11180149   9.2543416   93.11754678 111.20983791\n",
      "  33.04135162  16.74451277  73.37077605 106.34948665  59.03029964\n",
      "   9.52978881  68.67242803 122.0391203   34.88121752  19.83147305\n",
      "  48.68263328 186.93408264   8.68130539 569.04366217  22.34053382\n",
      "  22.38936556  54.15864092   7.40578971]\n",
      "59-th iteration, loss: 0.06131079379749079, 27 gd steps\n",
      "insert gradient: -0.005627246916428855\n",
      "59-th iteration, new layer inserted. now 53 layers\n",
      "[ 51.98446699  84.91528975  17.2695066   60.51958494  52.05377679\n",
      "  93.9196204   94.86584726 102.57745288  57.02269411 103.57065702\n",
      "  59.99504944  99.67033105  48.49423406  26.98437876  83.3911665\n",
      "  90.09615147  66.87450548   9.02252353   1.59667237  83.70385857\n",
      "   7.13761353  55.62380107 100.11676412  64.54204113  10.55172871\n",
      "  73.76417059  89.42666258  97.15669414  22.60741495  29.91591609\n",
      "  83.42283012  12.47502727  10.8351927   93.21665198 110.35833445\n",
      "  35.81041262  15.82979323  73.88331155 105.63708554  59.58446034\n",
      "   8.55815957  69.8143696  122.1944175   35.38215032  19.51740129\n",
      "  48.82641087 186.42393638   7.62285052 568.34068798  22.57572243\n",
      "  22.03568028  54.87281237   7.08325628]\n",
      "60-th iteration, loss: 0.06048285919813914, 27 gd steps\n",
      "insert gradient: -0.004751603260220329\n",
      "60-th iteration, new layer inserted. now 53 layers\n",
      "[5.21461592e+01 8.64716450e+01 1.95316453e+01 5.56259289e+01\n",
      " 5.16659660e+01 9.37721769e+01 9.53223642e+01 1.03423912e+02\n",
      " 5.73751319e+01 1.02747434e+02 0.00000000e+00 7.10542736e-15\n",
      " 5.91365735e+01 1.00538531e+02 5.19536109e+01 2.34665764e+01\n",
      " 8.07473425e+01 9.60746827e+01 6.58762372e+01 9.48795804e+01\n",
      " 5.08609362e+00 5.61633009e+01 1.01218477e+02 6.72644843e+01\n",
      " 1.03022005e+01 7.31406666e+01 8.83212073e+01 9.85162593e+01\n",
      " 2.61366549e+01 2.08026717e+01 8.20950053e+01 1.23809726e+01\n",
      " 1.44161074e+01 9.36134311e+01 1.06775522e+02 4.11520109e+01\n",
      " 1.48757379e+01 7.54757949e+01 1.05529968e+02 6.12066035e+01\n",
      " 7.29212780e+00 7.26629886e+01 1.21441820e+02 3.64504837e+01\n",
      " 1.95342446e+01 4.90409885e+01 1.86054073e+02 7.16651202e+00\n",
      " 5.66988988e+02 2.34013515e+01 2.11296642e+01 5.63377557e+01\n",
      " 6.30711626e+00]\n",
      "61-th iteration, loss: 0.06000387289221979, 32 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "61-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.98219817  85.77018152  21.09689468  52.10218214  51.89026857\n",
      "  92.82585134  95.71223239 103.89138498  57.03361869 103.07921923\n",
      "  59.78009989  99.45581747  54.78885836  21.2745918   78.89474787\n",
      " 101.28705337  61.2640705   94.88856186   4.80239485  55.88132289\n",
      " 102.34582214  68.96583014  10.71432806  71.56203963  87.70081712\n",
      "  98.8569643   29.36123772  14.66700977  78.22437031  14.24304084\n",
      "  17.19966642  94.44580437 104.04823773  45.7099475   13.29856335\n",
      "  77.71587213 106.02855022  62.35492625   5.89023123  76.76874689\n",
      " 120.89796902  35.59892137  19.73835006  48.71061856 186.06488547\n",
      "   7.63159019 282.80082501   0.         282.80082501  24.05390634\n",
      "  20.31612786  57.52700559   5.99115342]\n",
      "62-th iteration, loss: 0.05960248631697549, 21 gd steps\n",
      "insert gradient: -0.001828196004116003\n",
      "62-th iteration, new layer inserted. now 55 layers\n",
      "[5.15898881e+01 8.66912142e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.12373937e+01 5.12504734e+01 5.27841287e+01 9.36470721e+01\n",
      " 9.46835268e+01 1.03433506e+02 5.74088115e+01 1.02718694e+02\n",
      " 5.94417988e+01 1.00029220e+02 5.55557652e+01 2.07983831e+01\n",
      " 7.79051521e+01 1.02817359e+02 6.10478684e+01 9.48876520e+01\n",
      " 4.63952268e+00 5.57162140e+01 1.02316304e+02 6.96994474e+01\n",
      " 1.08124093e+01 7.09398052e+01 8.78326296e+01 9.89975069e+01\n",
      " 2.97434895e+01 1.33652214e+01 7.70173828e+01 1.55230426e+01\n",
      " 1.77674311e+01 9.48395030e+01 1.03346714e+02 4.69317895e+01\n",
      " 1.26223004e+01 7.84789732e+01 1.06087099e+02 6.27810571e+01\n",
      " 5.61536425e+00 7.80614971e+01 1.20948410e+02 3.47639938e+01\n",
      " 2.00208350e+01 4.87983497e+01 1.85292328e+02 7.07129372e+00\n",
      " 2.81999182e+02 6.15385110e+00 2.82319534e+02 2.39280948e+01\n",
      " 1.97032262e+01 5.77573562e+01 5.78440212e+00]\n",
      "63-th iteration, loss: 0.059460652016943005, 31 gd steps\n",
      "insert gradient: -0.0011712923321059554\n",
      "63-th iteration, new layer inserted. now 53 layers\n",
      "[ 51.63827545  86.7066729   21.57952561  50.46697533  52.80613509\n",
      "  93.4327212   94.78720323 103.71679391  57.16803902 102.52249137\n",
      "  59.62125007  99.95513412  55.98936482  20.69073819  77.00543296\n",
      " 104.3876159   60.38658459  94.71887087   4.57252224  55.69711215\n",
      " 102.48693183  70.16251595  10.9381651   70.07655189  87.88507522\n",
      "  99.04416053  30.43028283  12.30994253  75.76135719  16.56505822\n",
      "  18.15592218  94.99688395 102.53276926  48.29009692  12.10978451\n",
      "  79.10114926 106.24356846  62.99982694   5.31200064  79.16634839\n",
      " 120.94233919  33.87972116  20.44878399  48.73460943 183.907936\n",
      "   6.52596358 280.41364425   8.27794272 282.04298472  23.8894632\n",
      "  19.20732193  57.70475058   5.30936507]\n",
      "64-th iteration, loss: 0.05936265696613508, 19 gd steps\n",
      "insert gradient: -0.0021474488894526566\n",
      "64-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.04716544  86.86331159  21.98179226  49.6121705   52.52760931\n",
      "  93.16918972  94.89328574 103.64040736  57.02259346 102.60903883\n",
      "  59.73235407  99.83684345  56.59141831  20.59569063  75.98781862\n",
      " 106.74040916  59.25067288  94.39662485   4.53788234  55.90004307\n",
      " 102.70866355  70.66625449  11.23386397  68.84822053  88.0574384\n",
      "  99.1039345   31.49910366  11.13709405  74.03652302  17.50831273\n",
      "  18.7505047   95.10175014 101.56535739  50.19484541  11.45785878\n",
      "  79.96568341 106.2913222   63.03762358   4.97967894  80.69460898\n",
      " 121.05960677  33.04289278  20.7880567   48.60977798 183.24071078\n",
      "   6.47566326 279.00676968   9.58975046 281.95869886  24.04049192\n",
      "  18.50252298  57.51372416   4.83638825]\n",
      "65-th iteration, loss: 0.05914625925494514, 109 gd steps\n",
      "insert gradient: -0.000878218212960834\n",
      "65-th iteration, new layer inserted. now 55 layers\n",
      "[ 51.64198735  87.83390699  23.55225605  46.03431563  52.20073927\n",
      "  93.31405508  95.2813352  102.64310444  57.06125318 103.06798084\n",
      "  58.92155196  99.95757033  59.45201308  20.30698558  69.51197957\n",
      " 118.12569766  55.15354908  89.61667077   4.88496706  60.2257237\n",
      " 102.7668493   70.00400282  12.65662719  65.22740258  88.41142573\n",
      "  99.329534    35.47331373   9.76466831  65.90259207  20.47859169\n",
      "  20.60897218  94.13569109  98.55353952  57.63720752   8.54947179\n",
      "  85.0781333  105.16627657  61.88203385   3.49856961  88.41018352\n",
      "  91.11129108   0.          30.37043036  30.73624408  22.4069817\n",
      "  45.80097503 185.10654842   5.46916335 275.25265918  10.64232035\n",
      " 282.29656266  25.25413574  16.5222      56.44291654   3.82630404]\n",
      "66-th iteration, loss: 0.05891565370044063, 17 gd steps\n",
      "insert gradient: -0.002365161594238975\n",
      "66-th iteration, new layer inserted. now 57 layers\n",
      "[5.17062047e+01 8.65243683e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.39001736e+01 4.54000394e+01 5.27779017e+01 9.23241087e+01\n",
      " 9.54644836e+01 1.03532126e+02 5.66801578e+01 1.03063086e+02\n",
      " 5.89432072e+01 9.97307895e+01 5.96256009e+01 2.01844064e+01\n",
      " 6.90912291e+01 1.18542453e+02 5.53277919e+01 8.80625696e+01\n",
      " 4.84615300e+00 6.17450889e+01 1.02469046e+02 6.97811977e+01\n",
      " 1.28110521e+01 6.52350004e+01 8.84867254e+01 9.86239325e+01\n",
      " 3.58382797e+01 9.71111197e+00 6.50839630e+01 2.04008614e+01\n",
      " 2.15493798e+01 9.33727445e+01 9.73560654e+01 5.92783448e+01\n",
      " 8.39804628e+00 8.41716068e+01 1.04876776e+02 5.98570756e+01\n",
      " 4.17935173e+00 8.77993783e+01 8.83738773e+01 9.46432730e+00\n",
      " 2.96649842e+01 3.77635167e+01 1.99032840e+01 4.56936650e+01\n",
      " 1.86320115e+02 4.38420273e+00 2.74184399e+02 1.02172963e+01\n",
      " 2.81911195e+02 2.58677031e+01 1.60133009e+01 5.64187172e+01\n",
      " 3.56630377e+00]\n",
      "67-th iteration, loss: 0.05877439489893395, 66 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "67-th iteration, new layer inserted. now 57 layers\n",
      "[ 51.90836157  87.41796956  23.96778026  45.51489167  52.31974354\n",
      "  92.73422813  95.305455   102.89684304  56.96579687 102.66243841\n",
      "  59.08669206  99.89620022  59.82677087  19.94215552  68.69489924\n",
      " 119.19178868  55.1991857   87.30641514   4.95733207  61.94908072\n",
      " 102.54994673  69.88627828  12.89187737  64.92915684  88.47259503\n",
      "  98.59100678  36.21755177   9.52433104  64.93319719  20.44282049\n",
      "  21.65566292  93.30728287  97.20296346  59.9845069    8.08686529\n",
      "  84.04345958 105.23435798  59.32909213   4.16608614  87.665312\n",
      "  83.95376826  12.31420524  29.89542175  41.40816425  19.68961419\n",
      "  46.62346133 186.81851559   4.37692138  68.50746681   0.\n",
      " 205.52240043   9.73508888 281.89136937  25.9786074   15.97888795\n",
      "  56.45361432   3.71803873]\n",
      "68-th iteration, loss: 0.058091486059286226, 93 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "68-th iteration, new layer inserted. now 59 layers\n",
      "[ 51.90925293  87.86322184  24.92707811  43.96971806  52.24933927\n",
      "  92.72975359  94.81541445 102.98344151  56.85976258 102.46985752\n",
      "  58.85557178  99.80292598  61.42532615  19.84464955  44.02585468\n",
      "   0.          22.01292734 122.21897801  54.6697839   84.00388257\n",
      "   5.33490273  64.03138518 102.27314508  68.88913191  13.49004433\n",
      "  64.96212209  87.7013116   99.97533962  37.29037265   8.44913116\n",
      "  65.44392455  21.3518592   20.09116862  94.11146482  97.86035566\n",
      "  63.36065208   5.74486498  84.68550863 108.58068357  58.61856003\n",
      "   2.971175    88.03105168  71.39553203  20.47312023  29.09268934\n",
      "  52.73933503  16.91122814  49.96725583 193.67270232   0.99656157\n",
      "  70.6541998   10.90225996 192.83573251  11.75624871 277.01665969\n",
      "  26.34785794  18.75289609  56.20412916   5.87079499]\n",
      "69-th iteration, loss: 0.05777868837544448, 38 gd steps\n",
      "insert gradient: -0.002279316942132696\n",
      "69-th iteration, new layer inserted. now 59 layers\n",
      "[5.12389049e+01 8.83254672e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.61265448e+01 4.16061845e+01 5.26746789e+01 9.29404037e+01\n",
      " 9.42693135e+01 1.03401455e+02 5.67660770e+01 1.02407207e+02\n",
      " 5.92335865e+01 9.97331714e+01 6.02606783e+01 2.59436806e+01\n",
      " 4.04341032e+01 8.97293246e+00 1.72358741e+01 1.19133842e+02\n",
      " 5.51560651e+01 8.26409311e+01 4.35689510e+00 6.70735552e+01\n",
      " 1.01729161e+02 6.96867853e+01 1.38492540e+01 6.44332055e+01\n",
      " 8.77486096e+01 1.00326423e+02 3.78391795e+01 6.71436050e+00\n",
      " 6.57488074e+01 2.25673962e+01 1.95743728e+01 9.44159568e+01\n",
      " 9.75775456e+01 6.55970952e+01 4.70561861e+00 8.48584015e+01\n",
      " 1.09383796e+02 5.93942933e+01 2.64717014e+00 8.83186958e+01\n",
      " 6.80842408e+01 2.36622445e+01 2.71001709e+01 5.75806195e+01\n",
      " 1.57620365e+01 5.08405672e+01 2.67094520e+02 1.30180160e+01\n",
      " 1.89441847e+02 1.26106860e+01 2.75458169e+02 2.66436621e+01\n",
      " 1.84593955e+01 5.47635126e+01 6.00441029e+00]\n",
      "70-th iteration, loss: 0.05775230253212072, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "70-th iteration, new layer inserted. now 61 layers\n",
      "[5.16425879e+01 8.83815644e+01 2.73639669e-02 4.07674338e-02\n",
      " 2.61683368e+01 4.15579060e+01 5.25660889e+01 9.29407347e+01\n",
      " 9.44311246e+01 1.03469277e+02 5.67210610e+01 1.02340933e+02\n",
      " 5.91558517e+01 9.97533796e+01 6.02504791e+01 2.60463572e+01\n",
      " 4.04162749e+01 9.16730263e+00 1.71567966e+01 1.19010878e+02\n",
      " 5.51124751e+01 8.26321212e+01 4.35875065e+00 6.71391008e+01\n",
      " 1.01695537e+02 6.97147557e+01 1.38479904e+01 6.44021761e+01\n",
      " 8.77361760e+01 1.00306721e+02 3.78490300e+01 6.67304342e+00\n",
      " 6.57487464e+01 2.25970554e+01 1.95584180e+01 9.44093913e+01\n",
      " 9.75625049e+01 6.56313344e+01 4.70547324e+00 8.48566955e+01\n",
      " 7.28968262e+01 0.00000000e+00 3.64484131e+01 5.93982513e+01\n",
      " 2.64057139e+00 8.83105231e+01 6.80338058e+01 2.36954976e+01\n",
      " 2.70419488e+01 5.76404353e+01 1.57384911e+01 5.08624715e+01\n",
      " 2.67033103e+02 1.30324352e+01 1.89423889e+02 1.25965715e+01\n",
      " 2.75445922e+02 2.66632743e+01 1.84576691e+01 5.47421752e+01\n",
      " 6.01651052e+00]\n",
      "71-th iteration, loss: 0.0571863327684328, 29 gd steps\n",
      "insert gradient: -0.003710785934733629\n",
      "71-th iteration, new layer inserted. now 61 layers\n",
      "[5.19709164e+01 8.85587043e+01 5.45050133e-01 1.60909206e-01\n",
      " 2.69609486e+01 3.97437714e+01 5.18293006e+01 9.24510610e+01\n",
      " 9.47486908e+01 1.02726528e+02 5.57579508e+01 1.02875285e+02\n",
      " 5.85958776e+01 9.97109613e+01 5.94109767e+01 2.84135430e+01\n",
      " 3.90035297e+01 1.35975999e+01 1.52693369e+01 1.14541303e+02\n",
      " 5.48022129e+01 8.26892243e+01 2.71457277e+00 6.95474133e+01\n",
      " 1.00789100e+02 7.10564467e+01 1.33904038e+01 6.35304238e+01\n",
      " 8.79186497e+01 9.95939744e+01 3.83422071e+01 4.41917299e+00\n",
      " 6.55640517e+01 2.37521708e+01 1.89031569e+01 9.41149696e+01\n",
      " 9.71448869e+01 6.64548029e+01 3.98769984e+00 8.43070336e+01\n",
      " 6.99273217e+01 5.40841686e+00 3.38604330e+01 6.11106796e+01\n",
      " 2.49536405e+00 8.87627100e+01 6.73167391e+01 2.61363724e+01\n",
      " 2.47831563e+01 5.98872917e+01 1.56063425e+01 5.06079870e+01\n",
      " 2.66496222e+02 1.37995312e+01 1.89362365e+02 1.25581893e+01\n",
      " 2.74634694e+02 2.67170731e+01 1.82264135e+01 5.43815841e+01\n",
      " 5.95626160e+00]\n",
      "72-th iteration, loss: 0.056826010965468854, 16 gd steps\n",
      "insert gradient: -0.0012948116404286783\n",
      "72-th iteration, new layer inserted. now 61 layers\n",
      "[5.15799592e+01 8.83628912e+01 0.00000000e+00 7.10542736e-15\n",
      " 2.69488312e+01 3.95051724e+01 5.16001921e+01 9.22889894e+01\n",
      " 9.44739699e+01 1.02534943e+02 5.56590281e+01 1.02802240e+02\n",
      " 5.83800517e+01 9.96552721e+01 5.95016719e+01 2.84208344e+01\n",
      " 3.89166619e+01 1.36941125e+01 1.53471673e+01 1.14410962e+02\n",
      " 5.48477480e+01 8.27532787e+01 2.73213335e+00 6.96304533e+01\n",
      " 1.00671888e+02 7.11239431e+01 1.33768340e+01 6.34968455e+01\n",
      " 8.77453810e+01 9.96155314e+01 3.83510585e+01 4.31134219e+00\n",
      " 6.55758803e+01 2.37363366e+01 1.87856049e+01 9.41248316e+01\n",
      " 9.69809947e+01 6.64363622e+01 3.84156935e+00 8.42772415e+01\n",
      " 6.97765692e+01 5.39507139e+00 3.36874465e+01 6.10688186e+01\n",
      " 2.24092728e+00 8.87090436e+01 6.72048945e+01 2.60540142e+01\n",
      " 2.44762207e+01 5.98202213e+01 1.52931231e+01 5.04713985e+01\n",
      " 2.66308364e+02 1.36234533e+01 1.89178739e+02 1.24136712e+01\n",
      " 2.74528644e+02 2.66867400e+01 1.82239090e+01 5.43926888e+01\n",
      " 6.01611114e+00]\n",
      "73-th iteration, loss: 0.05612370307251622, 16 gd steps\n",
      "insert gradient: -0.0014882214559435334\n",
      "73-th iteration, new layer inserted. now 59 layers\n",
      "[ 51.13031901  88.03768257  28.03234856  37.65196718  51.17225634\n",
      "  91.35423994  94.41528985 100.1577773   55.53958656 104.49872454\n",
      "  56.89820497  98.36152699  60.98134143  28.20733057  35.60765561\n",
      "  15.08271428  16.73602011 110.53920216  56.36006255  84.44240819\n",
      "   0.91610953  71.8241195   98.58902082  73.33383777  12.5658868\n",
      "  63.39906932  86.70027482 100.82911808  38.91881625   1.86208433\n",
      "  66.47028068  24.30376778  17.68880643  95.00045051  95.41534309\n",
      "  67.25800025   3.39578985  84.71944275  68.94088541   6.76669863\n",
      "  32.96109273  62.37804794   1.84470168  89.5051733   67.64148815\n",
      "  27.12894294  23.48262125  61.56082962  14.8489601   50.15969772\n",
      " 265.60265922  12.66513339 187.88353572  11.04592556 272.56575514\n",
      "  25.95235111  16.92994116  54.04741374   5.80526107]\n",
      "74-th iteration, loss: 0.05497863891778751, 970 gd steps\n",
      "insert gradient: -0.0005580547304736617\n",
      "74-th iteration, new layer inserted. now 57 layers\n",
      "[ 50.45612244  86.42448357  29.41891992  36.11417868  50.84760043\n",
      "  90.96672484  92.50912055 103.17749228  55.77455193 100.39644465\n",
      "  57.98106201  98.54324697  58.07062842  35.48378079  31.32498231\n",
      "  26.52857453  13.92905264 100.23225367  55.42068776 158.69675827\n",
      "  96.14929673  79.33618256  12.54091417  59.57685879  86.68997235\n",
      "  99.00846798 105.1803847   26.20300385  16.71287172  95.23679664\n",
      "  94.45096654  70.59721674   1.84750248  85.97100269  65.15795139\n",
      "  11.13492554  29.35161738  66.05371301   0.43752849  92.22769973\n",
      "  65.83564399  30.661297    20.79131684  66.30440302  14.17110776\n",
      "  48.20142666 260.32154879  13.67369708 139.08769712   0.\n",
      "  46.36256571  10.96389538 266.57245922  26.8924972   16.37104583\n",
      "  54.19884959   5.10447707]\n",
      "75-th iteration, loss: 0.05462090184578778, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "75-th iteration, new layer inserted. now 57 layers\n",
      "[ 50.53588606  86.80881421  30.70472023  35.07508635  50.10263205\n",
      "  90.53442924  93.09426291 102.79433871  56.05951917 100.4764226\n",
      "  57.899138    98.93895062  56.55257865  42.52587735  26.74468807\n",
      "  34.806368    14.41532817  87.23212444  58.41111816 155.27363928\n",
      "  95.19229574  86.13850338  12.76260692  54.16736164  88.54708116\n",
      "  97.93869472 101.35270337  30.26410552  16.54270417  95.01573089\n",
      "  93.74818258  74.59362638   0.46407376  87.73081342  60.77561695\n",
      "  16.07744338  25.10800871 168.84086241  63.9593515   34.80289508\n",
      "  17.09215151  74.48530542  14.14818871  44.85538476 260.46251005\n",
      "  14.75543697 135.99368727   5.676409    43.22277059  14.13141426\n",
      " 130.35348424   0.         130.35348424  25.55054311  14.45793624\n",
      "  55.34991519   4.00129107]\n",
      "76-th iteration, loss: 0.041542402474403835, 467 gd steps\n",
      "insert gradient: -0.0009308092802350308\n",
      "76-th iteration, new layer inserted. now 52 layers\n",
      "[ 47.85121791  79.08996968  18.45429024  48.4520249   48.49750397\n",
      "  84.93745839  84.95452142  96.6580322   49.93996679  92.57873089\n",
      "  51.67341376  95.18464982  56.06020963  75.69060125  12.93280927\n",
      " 108.01841663  62.57749917 107.20344434  93.03454027 142.72578708\n",
      "  91.68271567 115.64854308  78.72684507  36.51951138   8.18697027\n",
      " 114.08436604  73.46007874 160.9222549   69.77055819   3.33559886\n",
      "   7.84087593 160.75721868  81.84938861  38.06755196   0.79151218\n",
      "  88.507285     7.14384863  40.1073341  179.73407559   0.\n",
      "  59.91135853   5.01947437 106.15364497  18.27652368  42.44044186\n",
      "  16.10913604 113.31686752  16.87053616 127.95247216  18.86745676\n",
      "  14.89178472  52.71347568]\n",
      "77-th iteration, loss: 0.0390439997079704, 491 gd steps\n",
      "insert gradient: -0.0007199764558175055\n",
      "77-th iteration, new layer inserted. now 48 layers\n",
      "[4.78980857e+01 7.84637490e+01 2.05158238e+01 4.81203080e+01\n",
      " 4.76504600e+01 8.53621604e+01 8.22257944e+01 1.00719982e+02\n",
      " 5.01321094e+01 9.19378024e+01 5.11228953e+01 9.46170584e+01\n",
      " 5.70241639e+01 9.08743278e+01 1.26897568e+01 8.33264355e+01\n",
      " 6.73458777e+01 1.09843924e+02 8.67465152e+01 1.33156146e+02\n",
      " 9.94920572e+01 1.19924449e+02 6.79304154e+01 1.76510227e+02\n",
      " 7.60034118e+01 1.48233371e+02 8.11318904e+01 1.28930902e+02\n",
      " 0.00000000e+00 4.29769672e+01 7.95766256e+01 4.34535596e+01\n",
      " 1.08312534e-01 9.40649256e+01 1.09798994e+01 3.92688701e+01\n",
      " 1.75769340e+02 9.16169153e+00 1.49205481e+02 4.06144065e+01\n",
      " 2.64003940e+01 2.06069487e+01 1.24224035e+02 2.61760527e+01\n",
      " 1.16368594e+02 2.35363803e+01 1.51247771e+01 5.27134757e+01]\n",
      "78-th iteration, loss: 0.038393538282561625, 27 gd steps\n",
      "insert gradient: -0.0011361620198623548\n",
      "78-th iteration, new layer inserted. now 48 layers\n",
      "[ 47.78636536  81.6256452   23.83873467  39.42010596  47.99370243\n",
      "  84.92935786  83.62036778 101.00325462  50.05461833  91.64984281\n",
      "  50.92373574  93.42945278  55.97865203 100.63465248  16.23739481\n",
      "  61.2487376   71.06085681 112.85905325  84.36045379 129.89957845\n",
      " 101.81094198 123.3603946   65.40114422 176.57373744  76.30112401\n",
      " 146.8234859   80.62289278 122.7681712    5.23133848  39.23082295\n",
      "  78.28378472  44.4032365    0.37280234  95.16826007  16.18514339\n",
      "  33.23017482 177.2301129   10.2589703  141.50612723  51.1184458\n",
      "  19.20352632  26.98679352 126.16796375  27.91942573 112.3394929\n",
      "  26.05758945  15.16127173  52.71347568]\n",
      "79-th iteration, loss: 0.038363375405400045, 15 gd steps\n",
      "insert gradient: -0.0005348001463763864\n",
      "79-th iteration, new layer inserted. now 48 layers\n",
      "[ 47.84623912  81.59793961  23.83330864  39.36691224  47.99050154\n",
      "  84.85316226  83.32603696 100.82433257  49.92331969  91.72938193\n",
      "  51.15185383  93.60902345  56.05099751 100.58561958  16.27993702\n",
      "  61.13072933  71.04480403 112.93188713  84.35636939 129.91015744\n",
      " 101.86320447 123.32819247  65.41493251 176.54629479  76.31605157\n",
      " 146.85900107  80.60679087 122.6905819    5.28841797  39.18787479\n",
      "  78.27922548  44.41825203   0.34785774  95.1846077   16.263786\n",
      "  33.20322228 177.19529784  10.25648947 141.39996364  51.16436453\n",
      "  19.14004651  27.02500072 126.19345807  27.90742194 112.34041115\n",
      "  26.08047486  15.14938747  52.71347568]\n",
      "80-th iteration, loss: 0.038298885217606174, 30 gd steps\n",
      "insert gradient: -0.0005868547175900882\n",
      "80-th iteration, new layer inserted. now 50 layers\n",
      "[ 48.06345588  81.05708375  24.0812541   39.10626288  48.26684642\n",
      "  84.79121187  62.50570941   0.          20.83523647 100.49654989\n",
      "  49.96240555  91.95810294  50.83892471  94.43284066  55.93537543\n",
      "  99.79134179  17.12454058  58.85825804  71.36559619 114.27234692\n",
      "  83.82119001 129.55698899 102.31239102 122.99088138  65.54275686\n",
      " 175.94352743  76.48450279 147.34870754  80.37525763 121.24379986\n",
      "   6.1320475   38.57587275  78.26872979  44.77244271   0.30756983\n",
      "  95.56232452  17.06702511  32.60319393 177.17543858  10.52309101\n",
      " 139.90009975  52.34837615  18.26374735  27.95634349 126.74004761\n",
      "  27.79583752 112.13514043  26.37959235  15.03936512  52.71347568]\n",
      "81-th iteration, loss: 0.03829028776732381, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "81-th iteration, new layer inserted. now 52 layers\n",
      "[ 48.01442494  81.09147985  24.11475023  39.07570521  48.24360772\n",
      "  84.80425921  62.49187767   0.22507713  20.82257082 100.49837833\n",
      "  49.96912762  91.92819449  50.81971446  94.38271956  55.94193324\n",
      "  99.75651523  17.16817039  58.65663454  71.36520645 114.34176902\n",
      "  83.77365277 129.50098252 102.32373393 122.95774639  65.53124766\n",
      " 175.91299491  76.46745382 147.40228665  80.37061585 121.11903819\n",
      "   6.20060389  38.51924981  78.28420948  44.8088731    0.30360256\n",
      "  95.60056701  17.1319842   32.55458806 177.19221498  10.57386274\n",
      " 139.77714194  52.44675804  18.18990954  28.05257612 126.78238499\n",
      "  27.77905415  28.0283413    0.          84.08502389  26.40581653\n",
      "  15.03314958  52.71347568]\n",
      "82-th iteration, loss: 0.033000977497612466, 88 gd steps\n",
      "insert gradient: -0.00037923817292634756\n",
      "82-th iteration, new layer inserted. now 50 layers\n",
      "[ 48.41532398  85.75722383  40.21767123  24.7939176   42.59965053\n",
      "  83.67111614  52.92161677 141.46049971  52.94797007  90.6514614\n",
      "  50.92723492  94.50029766  59.09714789  95.1489556   22.89108681\n",
      "  48.35948587  69.65493975 115.99496388  83.76858805 128.61939584\n",
      " 105.19512922 120.10004662  63.70773346 133.56776118   0.\n",
      "  44.52258706  76.11222953 149.3975389   79.81350011 113.84712042\n",
      "   9.07307962  39.30239185  78.68235404 140.42351729  19.49912126\n",
      "  31.10283807 175.04517997  11.30079018 134.4600798   54.75986661\n",
      "  11.63394394  37.84892166 121.34353383  39.76206125  19.90806113\n",
      "  18.69775466  81.30473419  19.20476776  18.20869597  52.71347568]\n",
      "83-th iteration, loss: 0.032379099171312306, 55 gd steps\n",
      "insert gradient: -0.0003407552105473583\n",
      "83-th iteration, new layer inserted. now 50 layers\n",
      "[ 46.70494828  85.86070243  43.79400708  19.65927221  42.2552298\n",
      "  85.80680622  50.87016381 141.63949823  53.28206738  90.28480337\n",
      "  49.83749697  92.14953669  60.31762722  94.37041064  23.23125606\n",
      "  48.18606478  67.80577464 114.44541291  82.62332619 135.63076555\n",
      " 103.07253578 112.39941028  67.82049974 182.5629209   70.25091315\n",
      " 146.38552958  84.04831144 111.33969398   7.54681843  43.82077232\n",
      "  76.66787227 141.97934553  16.49756864  39.91017046 126.98049978\n",
      "   0.          42.32683326  13.50579744 131.7711535   53.16698642\n",
      "  11.73240479  36.25237672 120.53698889  43.63314245  16.82876278\n",
      "  22.56473565  84.7071304   17.39755433  17.78193251  52.71347568]\n",
      "84-th iteration, loss: 0.032206616472775354, 35 gd steps\n",
      "insert gradient: -0.0003479487067522537\n",
      "84-th iteration, new layer inserted. now 52 layers\n",
      "[4.74567511e+01 8.58406575e+01 4.34449967e+01 1.97874381e+01\n",
      " 4.19125705e+01 8.55581311e+01 4.94845459e+01 1.42480547e+02\n",
      " 5.35210868e+01 8.99544421e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.93758399e+01 9.25018736e+01 6.01156161e+01 9.43925269e+01\n",
      " 2.29174470e+01 4.86995574e+01 6.74047542e+01 1.14232854e+02\n",
      " 8.19580086e+01 1.36143528e+02 1.02797450e+02 1.12317018e+02\n",
      " 6.76562188e+01 1.82428702e+02 6.99184942e+01 1.45857319e+02\n",
      " 8.41347832e+01 1.10848770e+02 7.19776095e+00 4.45209802e+01\n",
      " 7.67467101e+01 1.42454520e+02 1.57316718e+01 4.03931433e+01\n",
      " 1.25571344e+02 2.75540854e+00 4.09451787e+01 1.45626969e+01\n",
      " 1.30479428e+02 5.29933088e+01 1.18945453e+01 3.63227950e+01\n",
      " 1.19931701e+02 4.43300386e+01 1.64307639e+01 2.34978697e+01\n",
      " 8.54035683e+01 1.72767741e+01 1.76204769e+01 5.27134757e+01]\n",
      "85-th iteration, loss: 0.03211428151736423, 22 gd steps\n",
      "insert gradient: -0.0017102227885868204\n",
      "85-th iteration, new layer inserted. now 54 layers\n",
      "[4.70286288e+01 8.59289309e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.34427990e+01 1.93062452e+01 4.17042662e+01 8.57817906e+01\n",
      " 4.95222333e+01 1.41760972e+02 5.33666802e+01 9.02443181e+01\n",
      " 1.36395988e-01 1.67671299e-02 4.96313261e+01 9.24239732e+01\n",
      " 5.98938497e+01 9.49399537e+01 2.25987462e+01 4.95159944e+01\n",
      " 6.71974606e+01 1.13394787e+02 8.21243876e+01 1.37089133e+02\n",
      " 1.02413813e+02 1.12014357e+02 6.76944056e+01 1.82697951e+02\n",
      " 6.96864687e+01 1.45474469e+02 8.45573940e+01 1.09961207e+02\n",
      " 6.95339684e+00 4.57014084e+01 7.70260826e+01 1.43519217e+02\n",
      " 1.45873493e+01 4.23637964e+01 1.23196824e+02 5.74072922e+00\n",
      " 3.86016385e+01 1.79876117e+01 1.28707289e+02 5.31780250e+01\n",
      " 1.14220324e+01 3.78104820e+01 1.19012900e+02 4.58264294e+01\n",
      " 1.53585046e+01 2.55955498e+01 8.69823075e+01 1.65835124e+01\n",
      " 1.74207994e+01 5.27134757e+01]\n",
      "86-th iteration, loss: 0.03193792428800455, 43 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "86-th iteration, new layer inserted. now 54 layers\n",
      "[4.72338158e+01 8.56549907e+01 8.75796552e-02 6.84944493e-02\n",
      " 4.33285726e+01 1.93250404e+01 4.19894756e+01 8.53898676e+01\n",
      " 4.95082285e+01 1.42066089e+02 5.32158470e+01 8.96647849e+01\n",
      " 4.94681775e+01 9.21780087e+01 5.97942374e+01 9.51850194e+01\n",
      " 2.21761066e+01 5.00696045e+01 6.67297905e+01 1.13131037e+02\n",
      " 8.17202465e+01 1.37619112e+02 1.01969439e+02 1.11746275e+02\n",
      " 6.78203355e+01 1.82337873e+02 6.95140126e+01 1.45015435e+02\n",
      " 8.47189833e+01 1.08934236e+02 6.67895204e+00 4.66944707e+01\n",
      " 7.73434776e+01 3.61378687e+01 0.00000000e+00 1.08413606e+02\n",
      " 1.32285808e+01 4.44960531e+01 1.21048504e+02 8.14093693e+00\n",
      " 3.64270789e+01 2.10246365e+01 1.26397853e+02 5.37098132e+01\n",
      " 1.05714269e+01 4.02942204e+01 1.17728618e+02 4.74451761e+01\n",
      " 1.41080374e+01 2.81559077e+01 8.86434333e+01 1.60153408e+01\n",
      " 1.70345474e+01 5.27134757e+01]\n",
      "87-th iteration, loss: 0.03156964892196577, 67 gd steps\n",
      "insert gradient: -0.00030201380078940143\n",
      "87-th iteration, new layer inserted. now 56 layers\n",
      "[4.70217166e+01 8.54982600e+01 8.69119703e-02 2.33439325e-02\n",
      " 0.00000000e+00 1.73472348e-18 4.30765336e+01 1.80654159e+01\n",
      " 4.24174104e+01 8.52019379e+01 4.94545456e+01 1.41537432e+02\n",
      " 5.30545617e+01 8.92661828e+01 4.91176309e+01 9.11608966e+01\n",
      " 5.99504301e+01 9.59463855e+01 2.13338347e+01 5.14554957e+01\n",
      " 6.57895730e+01 1.11805577e+02 8.14747416e+01 1.39569329e+02\n",
      " 1.00977557e+02 1.09946338e+02 6.84477263e+01 1.82565281e+02\n",
      " 6.78279635e+01 1.45008289e+02 8.59377666e+01 1.07683248e+02\n",
      " 6.03569263e+00 4.83494372e+01 7.62904532e+01 3.64011162e+01\n",
      " 1.52143464e+00 1.08676504e+02 9.63369311e+00 5.15160600e+01\n",
      " 1.17801833e+02 1.22438837e+01 3.12075873e+01 2.81442704e+01\n",
      " 1.18741889e+02 5.57105237e+01 7.79756739e+00 5.07289662e+01\n",
      " 1.13174737e+02 5.26626887e+01 1.08515331e+01 3.67357857e+01\n",
      " 9.29750955e+01 1.52767453e+01 1.54560948e+01 5.27134757e+01]\n",
      "88-th iteration, loss: 0.0314984304006605, 16 gd steps\n",
      "insert gradient: -0.000226972336990246\n",
      "88-th iteration, new layer inserted. now 52 layers\n",
      "[ 47.19794851  85.65130467  43.49629323  17.83142251  42.15151769\n",
      "  85.00332912  48.53186484 141.94010153  53.36557973  89.72941096\n",
      "  48.83862341  91.12432254  59.93620424  96.14331337  21.12010102\n",
      "  51.58891611  65.53678614 111.72464012  81.38717708 139.91582474\n",
      " 100.74365099 109.55626015  68.55517686 182.59624256  67.40253083\n",
      " 145.53638242  85.690027   107.99592756   5.82996061  48.63954238\n",
      "  75.91662149  37.02916069   1.5370212  109.22284728   8.65940038\n",
      "  52.12431233 117.13606019  12.26737132  30.70195458  29.07080319\n",
      " 117.82778562  55.90397833   7.53619754  52.72211477 112.43956484\n",
      "  53.9020604   10.52725977  37.55575629  93.31670931  15.52668368\n",
      "  15.26889715  52.71347568]\n",
      "89-th iteration, loss: 0.02493126112536348, 855 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "89-th iteration, new layer inserted. now 44 layers\n",
      "[ 45.97936243  83.87995635  89.49157736  86.85477148  51.25240521\n",
      " 130.84348615  51.49273924  84.72000173  44.55659947  85.75233865\n",
      "  43.43525962  86.16574502  20.28775494  60.69252925  74.13697832\n",
      "  79.98760218  80.89703638  25.53557841   0.         102.14231364\n",
      "  94.46777161 104.3611892   68.52599489 171.24714008  63.66932047\n",
      " 130.54781522  84.76585157 155.25054531  85.28365016 151.83314337\n",
      "   0.32130173  53.21410103  99.20235709  27.77763106  19.97919755\n",
      "  71.77685346  95.86159232 148.01770474  87.92278317 133.05992086\n",
      "  97.21467999  16.57806463   5.51132247  52.71347568]\n",
      "90-th iteration, loss: 0.022352458514289096, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "90-th iteration, new layer inserted. now 46 layers\n",
      "[4.82416636e+01 8.60236826e+01 8.75938386e+01 8.83097384e+01\n",
      " 5.04431444e+01 1.26788232e+02 5.40528493e+01 8.46912708e+01\n",
      " 4.43346256e+01 8.65749009e+01 4.56846545e+01 7.37120820e+01\n",
      " 2.50722587e+01 6.21860738e+01 7.35686953e+01 7.66298893e+01\n",
      " 6.90106753e+01 2.46435727e+01 1.24940094e+01 1.98429922e+01\n",
      " 0.00000000e+00 7.93719687e+01 9.26948745e+01 1.01778115e+02\n",
      " 7.24020566e+01 1.72207419e+02 5.85934887e+01 1.37396471e+02\n",
      " 8.38051168e+01 1.50629861e+02 8.35735384e+01 1.49037141e+02\n",
      " 3.38526802e-03 5.04123468e+01 1.03203900e+02 2.76929424e+01\n",
      " 1.66829310e+01 7.66862531e+01 9.52274835e+01 1.45268215e+02\n",
      " 8.43642007e+01 1.36972127e+02 9.62282417e+01 1.68342777e+01\n",
      " 5.67724458e+00 5.27134757e+01]\n",
      "91-th iteration, loss: 0.020722566080268012, 36 gd steps\n",
      "insert gradient: -0.0008807288742058324\n",
      "91-th iteration, new layer inserted. now 44 layers\n",
      "[4.92382594e+01 8.82218676e+01 8.50937057e+01 8.88690259e+01\n",
      " 4.82835734e+01 1.17788740e+02 0.00000000e+00 7.10542736e-15\n",
      " 5.94334019e+01 8.43730011e+01 4.51823135e+01 8.54481973e+01\n",
      " 4.45563340e+01 6.75304454e+01 2.75228127e+01 6.75470074e+01\n",
      " 6.73705510e+01 8.79117424e+01 6.14306773e+01 3.65270905e+01\n",
      " 1.70903526e+01 8.98156413e+01 8.54207734e+01 1.07838287e+02\n",
      " 7.61170457e+01 1.68966587e+02 5.57960621e+01 1.46566071e+02\n",
      " 8.38996915e+01 1.40911292e+02 8.40723019e+01 1.83165510e+02\n",
      " 1.05466111e+02 2.61812750e+01 1.58889791e+01 8.60692320e+01\n",
      " 1.00201363e+02 1.41937919e+02 8.26369626e+01 1.42390093e+02\n",
      " 9.30593414e+01 1.78077595e+01 2.92022547e+00 5.27134757e+01]\n",
      "92-th iteration, loss: 0.020240304934417096, 17 gd steps\n",
      "insert gradient: -0.00135213878729131\n",
      "92-th iteration, new layer inserted. now 44 layers\n",
      "[4.92991056e+01 8.83573900e+01 0.00000000e+00 7.10542736e-15\n",
      " 8.41757631e+01 8.91482007e+01 4.71386014e+01 1.10837727e+02\n",
      " 6.35202035e+01 8.44329195e+01 4.51176180e+01 8.54568436e+01\n",
      " 4.49884869e+01 6.64769676e+01 2.76971326e+01 6.90628076e+01\n",
      " 6.52159548e+01 9.20390536e+01 5.94820723e+01 3.60350927e+01\n",
      " 1.88093845e+01 8.72093542e+01 8.45875413e+01 1.10678988e+02\n",
      " 7.65807463e+01 1.66866143e+02 5.66384483e+01 1.48383180e+02\n",
      " 8.24923352e+01 1.38526776e+02 8.45613565e+01 1.78963360e+02\n",
      " 1.05169679e+02 2.70347613e+01 1.52641772e+01 8.86469480e+01\n",
      " 1.02440503e+02 1.39165824e+02 8.34943834e+01 1.43366258e+02\n",
      " 9.29650296e+01 1.78430172e+01 1.81449150e+00 5.27134757e+01]\n",
      "93-th iteration, loss: 0.018212720959754677, 62 gd steps\n",
      "insert gradient: -0.00043903413418030845\n",
      "93-th iteration, new layer inserted. now 44 layers\n",
      "[ 47.88758946  87.90671503   0.1929322    0.18111691  81.08552833\n",
      "  90.10327354  47.96420064  92.92261775  68.89269363  87.45460294\n",
      "  45.62357646  86.02727592  45.02840291  60.1329023   26.90982871\n",
      "  76.46184034  56.85190827 106.97091346  52.02336164  28.95451249\n",
      "  30.06225524  79.4371345   72.40112697 133.45905232  81.15819526\n",
      " 147.25195095  61.56020836 160.72387292  73.21366108 126.8601496\n",
      "  92.72552724 159.04163999 102.98094586  37.32656708   8.33578753\n",
      " 103.30864449 105.66022141 130.76206479  92.70045001 135.62251025\n",
      "  94.04628244  18.21684669   3.56644931  52.71347568]\n",
      "94-th iteration, loss: 0.017806313148064128, 34 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "94-th iteration, new layer inserted. now 44 layers\n",
      "[ 48.34232851  88.37650539  80.02110277  89.84765423  47.17186542\n",
      "  91.79291137  69.70939295  87.52301724  45.37163958  85.48085608\n",
      "  44.60397258  58.55102856  26.89070981  77.98495921  56.00950627\n",
      " 107.28388881  50.5362723   26.83424405  32.5233473   79.36475295\n",
      "  68.80911826 137.09711152  81.64303331 142.31403704  63.66058053\n",
      " 161.72751555  71.34210322 125.53334542  93.08899738 153.39038943\n",
      " 101.87913943  42.54345662   5.47427294 110.13909352 103.44154288\n",
      "  26.63247264   0.         106.52989056  92.92533877 133.15531087\n",
      "  94.60585542  17.95564152   3.84493658  52.71347568]\n",
      "95-th iteration, loss: 0.016574396455137922, 20 gd steps\n",
      "insert gradient: -0.0002389551973055954\n",
      "95-th iteration, new layer inserted. now 42 layers\n",
      "[5.08410901e+01 9.07403398e+01 7.74007410e+01 9.02162094e+01\n",
      " 4.68050773e+01 9.29484495e+01 6.86490985e+01 8.91616485e+01\n",
      " 4.67435901e+01 8.60551498e+01 4.63923300e+01 6.02689073e+01\n",
      " 0.00000000e+00 3.55271368e-15 2.67318204e+01 7.73287361e+01\n",
      " 4.48523605e+01 1.13746130e+02 6.49912351e+01 2.02916489e+01\n",
      " 2.91551020e+01 8.05671978e+01 5.60805342e+01 1.51303729e+02\n",
      " 7.79504905e+01 1.33766755e+02 7.37278578e+01 1.51444061e+02\n",
      " 7.18756326e+01 1.23261240e+02 9.25632408e+01 1.46146199e+02\n",
      " 1.04213818e+02 1.75314691e+02 9.87606054e+01 1.38433470e+02\n",
      " 9.25265731e+01 1.31525063e+02 9.86278988e+01 1.64608680e+01\n",
      " 1.66821219e+00 5.27134757e+01]\n",
      "96-th iteration, loss: 0.015794321824125223, 205 gd steps\n",
      "insert gradient: -0.00016029560077928192\n",
      "96-th iteration, new layer inserted. now 42 layers\n",
      "[ 48.9532717   91.52593541  76.2907509   92.04160596  48.31407635\n",
      "  91.91791469  70.17901191  87.91152487  45.65570046  86.38214004\n",
      "  46.17736777  66.54134334  24.17566564  77.95625578  48.29845297\n",
      "  94.59376382  80.66068017  22.02931662  21.88153519  81.79373167\n",
      "  52.36366031 122.24862873   0.          30.56215718  75.66890712\n",
      " 133.06395056  73.08051192 144.20730282  79.44186882 120.85460256\n",
      "  93.69880916 141.58835114 105.79284098 179.24444629  93.24450978\n",
      " 142.67217165  90.87326507 133.97455219  99.17749401  15.91907129\n",
      "   3.12558147  52.71347568]\n",
      "97-th iteration, loss: 0.015624215622328437, 39 gd steps\n",
      "insert gradient: -0.00010380127095399232\n",
      "97-th iteration, new layer inserted. now 44 layers\n",
      "[ 49.17947216  92.54178613  75.37759325  92.59907597  48.08159087\n",
      "  91.07426356  70.3727577   88.38866561  45.6381478   86.98088092\n",
      "  46.39437246  67.70820082  22.60775159  79.76930833  49.3393578\n",
      "  89.74920157  84.62435316  21.41754517  19.39336532  85.33269513\n",
      "  51.71308505 117.69661002   5.85902175  28.59440258  69.98289223\n",
      " 134.09859151  73.25054567 143.1213408   81.06843248 117.73448012\n",
      "  94.21131375 143.07017914 106.26192079 143.05092164   0.\n",
      "  35.76273041  90.53097541 146.82293571  88.31970266 139.52912912\n",
      "  96.48924515  14.52629521   5.16578135  52.71347568]\n",
      "98-th iteration, loss: 0.015338504153525478, 35 gd steps\n",
      "insert gradient: -0.00021603253435326165\n",
      "98-th iteration, new layer inserted. now 44 layers\n",
      "[ 48.10240301  91.8733538   75.77297076  91.91004587  47.39878147\n",
      "  88.67986842  71.25821299  90.49991262  45.15341369  87.09064103\n",
      "  46.66410343  68.65966665  20.66564967  81.01008977  49.56869577\n",
      "  93.2425117   80.63661607  22.2922182   20.72352562  86.45583065\n",
      "  50.5465445  107.12634331  10.21529048  26.74706558  71.18072559\n",
      " 128.79604282  77.30864924 138.48681518  81.98473281 121.02288291\n",
      "  92.39483868 138.93086927 105.05323529 134.34373051   8.07958782\n",
      "  31.98068711  86.21350431 153.67235838  85.82481353 146.88553458\n",
      "  87.3439967   12.07519711  10.23477305  52.71347568]\n",
      "99-th iteration, loss: 0.015164549384805477, 59 gd steps\n",
      "insert gradient: -4.590839440473885e-05\n",
      "99-th iteration, new layer inserted. now 44 layers\n",
      "[ 48.14959255  91.57807641  75.73237016  89.82290693  46.61925018\n",
      "  87.55383845  71.50701801  92.01483551  45.60732249  86.7314006\n",
      "  46.13689861  71.49739937  19.29531982  79.82433354  50.2152245\n",
      "  95.57146491  75.77902938  27.02517108  21.45225582  82.71005879\n",
      "  50.44369804 105.59759428  11.0929458   18.47759857  75.26330486\n",
      " 126.42429714  77.33581425 137.60350898  82.92935227 121.52056733\n",
      "  92.41133793 136.28732013 104.46532981 131.32498137  11.57557421\n",
      "  26.61898571  85.75597106 155.28441456  85.2748498  148.28782935\n",
      "  79.70090458  16.32116219  12.60025986  52.71347568]\n",
      "0-th iteration, loss: 0.7729762834912453, 18 gd steps\n",
      "insert gradient: -0.46860116979651567\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  81.10159192    0.         3284.61447259]\n",
      "1-th iteration, loss: 0.5776774045903978, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.06308803e+01 1.29135332e+00 0.00000000e+00 1.04599619e+02\n",
      " 3.26301894e+03]\n",
      "2-th iteration, loss: 0.5714134554137762, 31 gd steps\n",
      "insert gradient: -0.2592658171411371\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.34187038  110.59771075  159.77055126    0.         3115.52574963]\n",
      "3-th iteration, loss: 0.47821699138666895, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  52.49208783  120.12305969  114.27375123  113.72555649  126.4729363\n",
      "    0.         2972.114003  ]\n",
      "4-th iteration, loss: 0.38272832226047754, 33 gd steps\n",
      "insert gradient: -0.17941891730972662\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  61.61914379  105.61236306   62.5092429     0.           59.03650719\n",
      "   90.08249843   97.98558995   88.32564021 2956.36284584]\n",
      "5-th iteration, loss: 0.3712659355803325, 16 gd steps\n",
      "insert gradient: -0.18113524610822798\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[6.63748330e+01 1.03573010e+02 5.91408797e+01 1.74968161e+01\n",
      " 5.23117647e+01 9.04757658e+01 0.00000000e+00 4.35207426e-14\n",
      " 9.34435901e+01 9.90880485e+01 2.95099675e+03]\n",
      "6-th iteration, loss: 0.3623542366358394, 16 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  65.91958582  110.08471996   51.41287882   32.33664822   46.70867806\n",
      "  100.12067303   96.09954857  101.65324476  218.42973419    0.\n",
      " 2730.37167735]\n",
      "7-th iteration, loss: 0.3025774979286484, 23 gd steps\n",
      "insert gradient: -0.1895712918937142\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[7.07981735e+01 1.07138339e+02 3.52708153e+01 5.45209133e+01\n",
      " 0.00000000e+00 7.99360578e-15 4.86330402e+01 1.03060440e+02\n",
      " 9.01246107e+01 1.02016941e+02 1.97837482e+02 1.05108028e+02\n",
      " 2.69379111e+03]\n",
      "8-th iteration, loss: 0.2973478762499129, 237 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  74.15124204  105.32625358   33.22199976   55.22402416   52.25382771\n",
      "  103.4228965    90.78384229  108.6595984   195.44490901  106.99946442\n",
      "  122.12396711    0.         2564.60330931]\n",
      "9-th iteration, loss: 0.25610982293522155, 31 gd steps\n",
      "insert gradient: -0.022094727726032708\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  68.48696867  108.9382395    42.99338287   44.79497024   51.50398094\n",
      "   98.27165467   91.33859226  116.49789887  120.60665162    0.\n",
      "   76.74968739   87.38807119   88.70022525   97.91440802 2543.3591707 ]\n",
      "10-th iteration, loss: 0.21045244871910007, 150 gd steps\n",
      "insert gradient: -0.04314735260401709\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[  56.52172355  106.24594331   42.1608176    44.57097304   56.15376698\n",
      "  109.06439658   88.41131419  108.39443448   89.74688099   95.99075052\n",
      "  104.98745789   22.90434355   52.63175949  158.9339705   940.90952387\n",
      "    0.         1568.18253979]\n",
      "11-th iteration, loss: 0.20081916845682538, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  56.01800153  109.54026053   38.13774797   57.78883035   54.0205695\n",
      "  117.52143127   85.18128221  113.0943631    90.02939329   98.42499124\n",
      "  105.21842771   31.4739109    43.62349426  170.51163382  132.50208042\n",
      "    0.          795.0124825    26.17167927 1557.14840377]\n",
      "12-th iteration, loss: 0.17691241639733432, 23 gd steps\n",
      "insert gradient: -0.05170309607787725\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  59.61131267   94.72942939   36.85598972   67.24862641   58.19652186\n",
      "  116.29385568   83.93057662  115.5831947    89.72804312  110.95280499\n",
      "  102.17934571   43.09476734   30.10337728  107.58090436    0.\n",
      "   76.84350311  112.15838447   48.35590508  794.35853854   13.3851049\n",
      " 1547.27121157]\n",
      "13-th iteration, loss: 0.1615510514592586, 23 gd steps\n",
      "insert gradient: -0.05162065075566896\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[5.56965539e+01 9.43273330e+01 4.11999844e+01 7.26207350e+01\n",
      " 5.84765674e+01 1.15904451e+02 8.58296816e+01 1.15888939e+02\n",
      " 9.05079605e+01 1.13522079e+02 9.97777812e+01 7.60673681e+01\n",
      " 1.90554343e+01 7.29383494e+01 2.55097838e+01 4.46428081e+01\n",
      " 1.04003135e+02 8.88859053e+01 7.95105423e+02 7.70523377e-01\n",
      " 1.53429153e+03]\n",
      "14-th iteration, loss: 0.15675888331718305, 28 gd steps\n",
      "insert gradient: -0.026174622537790695\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  59.12916294   96.44016974   43.42023963   73.17847799   53.42870865\n",
      "  115.3771469    88.56399361  116.32598898   91.44200401  113.97103731\n",
      "  100.28372774   77.84047741   17.63134241   71.44764781   28.73081727\n",
      "   43.82544596   77.77475619    0.           25.92491873   90.59435469\n",
      " 2324.39449264]\n",
      "15-th iteration, loss: 0.14490564622869428, 34 gd steps\n",
      "insert gradient: -0.009326008789343965\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[  59.49876726   94.82165029   42.32311319   77.41799362   55.6054594\n",
      "  115.84411405   88.9419785   114.84244194   91.40580282  115.51577358\n",
      "   95.54015768  116.80050853   16.90994605   50.98777974   40.75329867\n",
      "   30.68329571   62.56064798   33.50603714   10.77587164  114.18333908\n",
      " 2309.2956639 ]\n",
      "16-th iteration, loss: 0.13146482811550284, 74 gd steps\n",
      "insert gradient: -0.03483796639162749\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[5.88531567e+01 9.76687120e+01 4.63228755e+01 8.52219334e+01\n",
      " 5.95190103e+01 1.09751592e+02 9.17260160e+01 1.23285576e+02\n",
      " 9.55991963e+01 1.15607543e+02 0.00000000e+00 7.10542736e-15\n",
      " 9.91659526e+01 1.03997643e+02 3.40919419e+01 1.53248429e+01\n",
      " 5.91972009e+01 7.24460796e+01 1.86579212e+01 8.63139643e+01\n",
      " 1.88970748e+01 7.18245969e+01 2.28649276e+03]\n",
      "17-th iteration, loss: 0.11793204439347761, 29 gd steps\n",
      "insert gradient: -0.0178637584885329\n",
      "17-th iteration, new layer inserted. now 23 layers\n",
      "[6.05084645e+01 9.68404414e+01 5.12295037e+01 9.24602022e+01\n",
      " 5.68720313e+01 1.08719279e+02 0.00000000e+00 2.84217094e-14\n",
      " 1.00023746e+02 1.14707152e+02 9.79088621e+01 1.22326588e+02\n",
      " 1.05133676e+02 1.01865057e+02 4.67149281e+01 4.30673618e+00\n",
      " 5.91321417e+01 9.35302769e+01 1.86753671e+01 6.28924639e+01\n",
      " 4.08311997e+01 4.95091500e+01 2.27209622e+03]\n",
      "18-th iteration, loss: 0.11491039047017626, 27 gd steps\n",
      "insert gradient: -0.0046551309012272165\n",
      "18-th iteration, new layer inserted. now 23 layers\n",
      "[5.73933075e+01 9.74135597e+01 5.38964823e+01 9.46488213e+01\n",
      " 5.67625668e+01 1.08450793e+02 9.55253870e-01 7.38753695e-02\n",
      " 1.01346445e+02 1.13209966e+02 9.91140428e+01 1.24459055e+02\n",
      " 1.05683434e+02 1.02903785e+02 4.90089805e+01 3.83128502e+00\n",
      " 5.79298140e+01 9.60996302e+01 2.11263756e+01 5.50747033e+01\n",
      " 4.52489165e+01 4.99231348e+01 2.26435171e+03]\n",
      "19-th iteration, loss: 0.11382098823008847, 18 gd steps\n",
      "insert gradient: -0.0076530759739419744\n",
      "19-th iteration, new layer inserted. now 21 layers\n",
      "[  58.83289134   96.90361628   53.42950458   96.03911779   57.12551519\n",
      "  106.56484428  104.10325509  109.02443218  100.9323394   124.00319769\n",
      "  106.61896662  102.88161204   50.70521677    6.00523156   53.41545606\n",
      "   97.93164191   24.61224318   45.31025802   48.96996892   51.51785313\n",
      " 2259.75325727]\n",
      "20-th iteration, loss: 0.11305989541187493, 27 gd steps\n",
      "insert gradient: -0.005095193921437675\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[  58.34132523   97.53022091   53.36741256   94.38129312   58.88137374\n",
      "  107.52736507  103.64457342  109.27091767  101.42046178  122.72334194\n",
      "  106.87711829  103.41070949   52.2986459     9.06021529   49.83042814\n",
      "   98.19950559   27.77326214   40.20660274   50.05935638   53.11107027\n",
      "  820.13646902    0.         1435.23882079]\n",
      "21-th iteration, loss: 0.11102145404160518, 22 gd steps\n",
      "insert gradient: -0.01490712223198563\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[  58.87210125  100.62124073   54.49490773   93.49966139   56.52779088\n",
      "  108.11818975  102.54258616  113.42735446  101.6919528   123.6351047\n",
      "   74.77309373    0.           32.0456116   104.5915154    55.23297801\n",
      "   16.20972305   40.82218686   98.78075087   33.16880315   32.59971409\n",
      "   51.34086279   57.03234899  811.95548425   13.44572251 1426.48143042]\n",
      "22-th iteration, loss: 0.10614476142329032, 123 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[  57.51176796   98.54660559   54.79916172   95.7831784    57.03575933\n",
      "  103.32132363  105.25066176  105.79498709  100.81865023  126.66064725\n",
      "   71.35887209   21.66939177   17.24555839  114.63061508   58.21904665\n",
      "   29.12071413   27.09660889  102.65385564   48.82382947   17.34984573\n",
      "   46.80031807   76.51694333  801.02747002   25.23188058  157.79230922\n",
      "    0.         1262.33847378]\n",
      "23-th iteration, loss: 0.10421879994842094, 32 gd steps\n",
      "insert gradient: -0.0083263761785331\n",
      "23-th iteration, new layer inserted. now 29 layers\n",
      "[  57.35125584   99.03070227   55.06975842   96.02153503   56.82277749\n",
      "  103.04433981  106.04528116  103.47015891  101.4980896   125.31687739\n",
      "   72.84158055   22.42223108   15.57551963  115.93009183   59.30134979\n",
      "   30.40746867   25.38464119  102.73288304   52.79410996   19.88041937\n",
      "   42.66622682   78.10427034  599.58447264    0.          199.86149088\n",
      "   25.79185751  153.01554464   18.48742838 1258.30790192]\n",
      "24-th iteration, loss: 0.10228493234755952, 59 gd steps\n",
      "insert gradient: -0.009658861464100497\n",
      "24-th iteration, new layer inserted. now 31 layers\n",
      "[  56.85799727   98.13705239   55.33108954   96.73341896   56.58618556\n",
      "  100.99653098  106.67883675  102.6631187   100.73265552  127.11984734\n",
      "   73.59158035   22.75865264   14.29578194  117.87879738   59.55530073\n",
      "   33.32731211   22.86279252  103.6397598    54.42382301   24.8520097\n",
      "   36.66118202   84.35725456  589.29599207   17.66608573  148.44567145\n",
      "    0.           49.48189048   23.3794128   154.52864889   19.11091278\n",
      " 1265.19445993]\n",
      "25-th iteration, loss: 0.09398210169413285, 50 gd steps\n",
      "insert gradient: -0.02050674318225707\n",
      "25-th iteration, new layer inserted. now 33 layers\n",
      "[5.53080598e+01 9.84499401e+01 5.77143204e+01 9.33049537e+01\n",
      " 5.56964810e+01 9.81990071e+01 1.07282000e+02 1.01415421e+02\n",
      " 9.61169938e+01 1.35974805e+02 6.36645913e+01 4.17059981e+01\n",
      " 8.41224462e+00 1.23701461e+02 5.75310096e+01 5.01121566e+01\n",
      " 1.31891806e+01 1.06730166e+02 5.78185103e+01 3.54813766e+01\n",
      " 2.19602353e+01 1.01649988e+02 0.00000000e+00 1.77635684e-14\n",
      " 5.83466393e+02 2.25566529e+01 1.30787218e+02 5.66277097e+01\n",
      " 1.91300009e+01 6.25456358e+01 1.46186540e+02 1.71079501e+01\n",
      " 1.27399241e+03]\n",
      "26-th iteration, loss: 0.09322523962834985, 42 gd steps\n",
      "insert gradient: -0.006755280043146658\n",
      "26-th iteration, new layer inserted. now 33 layers\n",
      "[  55.30421205   97.62127731   56.7283804    95.52562872   56.61924042\n",
      "   98.13347022  107.00608844  100.61757466   80.83888906    0.\n",
      "   13.47314818  137.1696451    64.32917766   44.21480286    6.98385907\n",
      "  124.26835647   58.04595808   52.35890017   12.26852043  106.46311603\n",
      "   56.92547545   37.21012108   21.70885407  101.30892026  583.90115796\n",
      "   21.88008924  130.7729258    59.19390265   18.3851122    62.84382064\n",
      "  145.53388897   16.76945656 1275.89623063]\n",
      "27-th iteration, loss: 0.08697386929754002, 188 gd steps\n",
      "insert gradient: -0.004990531097104889\n",
      "27-th iteration, new layer inserted. now 33 layers\n",
      "[  55.19868889   98.3320039    57.14891821   97.65148528   54.69596338\n",
      "   99.11561696  105.04763612  108.34105798   68.92777639   25.4900219\n",
      "    5.71935629  135.38211864   64.4636305   189.21076938   56.46668773\n",
      "   70.38781147    6.92421434  102.79827083   55.13180877   46.23113898\n",
      "   17.28091394   98.1980266   250.95248377    0.          334.60331169\n",
      "   22.39320406  129.10076116   72.76269761   14.61856281   62.91425869\n",
      "  144.04428618   14.73900274 1279.90907997]\n",
      "28-th iteration, loss: 0.08485264130446443, 88 gd steps\n",
      "insert gradient: -0.006855222607266746\n",
      "28-th iteration, new layer inserted. now 33 layers\n",
      "[  56.25015808   98.21896446   56.93040368   95.90541772   55.50813547\n",
      "   99.28390394  103.76993492  110.32325897   68.24965059  178.39350019\n",
      "   59.72082859  190.33389013   55.07904901   75.31189191    5.83019699\n",
      "  101.15871344   55.45960225   51.79953886   15.20467394   96.10871505\n",
      "  142.28881089    0.          106.71660817    2.8621528   332.75776148\n",
      "   27.05926369  127.73396342   76.70858734   13.8235777    61.03809799\n",
      "  146.08438836   12.99346083 1286.06229644]\n",
      "29-th iteration, loss: 0.08196385424905507, 35 gd steps\n",
      "insert gradient: -0.0039071514145154395\n",
      "29-th iteration, new layer inserted. now 35 layers\n",
      "[  56.19678541   98.44944014   56.24289213   95.639558     55.39233139\n",
      "   98.3759976   100.87169949  115.29019677   67.78140961  176.42378447\n",
      "   60.38718762  188.73852205   55.09362463   82.28726802    4.21130087\n",
      "   98.31649613   54.63597697   53.92190273   14.29742644   94.267226\n",
      "  130.97861692   15.91024298  108.70370192    4.20791143  285.12828569\n",
      "    0.           47.52138095   22.3779838   126.2962954    83.16534863\n",
      "   13.7023273    56.26927317  147.37153286   12.1059401  1297.5804004 ]\n",
      "30-th iteration, loss: 0.08061610807754446, 14 gd steps\n",
      "insert gradient: -0.011168005172428257\n",
      "30-th iteration, new layer inserted. now 37 layers\n",
      "[5.63632154e+01 9.76013870e+01 0.00000000e+00 2.13162821e-14\n",
      " 5.53277525e+01 9.57084640e+01 5.42722159e+01 9.73648641e+01\n",
      " 1.01123164e+02 1.15010799e+02 6.81460720e+01 1.75330135e+02\n",
      " 5.96171622e+01 1.87905687e+02 5.52390775e+01 8.32620509e+01\n",
      " 3.79224662e+00 9.78186973e+01 5.54407028e+01 5.46197618e+01\n",
      " 1.36822992e+01 9.41867837e+01 1.29453843e+02 2.17672402e+01\n",
      " 1.07309585e+02 1.37343688e-01 2.82328027e+02 8.12673273e+00\n",
      " 4.43168954e+01 2.43164243e+01 1.26249967e+02 8.46981835e+01\n",
      " 1.33296160e+01 5.55349317e+01 1.47446015e+02 1.25277044e+01\n",
      " 1.30037338e+03]\n",
      "31-th iteration, loss: 0.08035416046040429, 14 gd steps\n",
      "insert gradient: -0.0037463885220144363\n",
      "31-th iteration, new layer inserted. now 37 layers\n",
      "[5.55203366e+01 9.75397776e+01 3.60552467e-01 5.03760795e-02\n",
      " 5.57557017e+01 9.60518791e+01 5.50350768e+01 9.74208019e+01\n",
      " 1.00911659e+02 1.14863208e+02 6.81636645e+01 1.75239477e+02\n",
      " 5.96916471e+01 1.87729241e+02 5.50981307e+01 8.33052136e+01\n",
      " 0.00000000e+00 7.10542736e-15 3.62157161e+00 9.76680205e+01\n",
      " 5.52731606e+01 5.46661804e+01 1.35733243e+01 9.41563252e+01\n",
      " 1.29331845e+02 2.24972845e+01 3.89443898e+02 8.97405366e+00\n",
      " 4.36019995e+01 2.46804438e+01 1.26104011e+02 8.47406713e+01\n",
      " 1.29254684e+01 5.54950888e+01 1.47603925e+02 1.26088149e+01\n",
      " 1.30101082e+03]\n",
      "32-th iteration, loss: 0.07926728933624533, 30 gd steps\n",
      "insert gradient: -0.004535848003484095\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[5.60528336e+01 9.68164303e+01 0.00000000e+00 1.06581410e-14\n",
      " 5.43996538e+01 9.56022668e+01 5.58235462e+01 9.67798240e+01\n",
      " 1.00594914e+02 1.13892476e+02 6.84745970e+01 1.73010897e+02\n",
      " 6.06765261e+01 1.85558616e+02 5.56168814e+01 8.67128289e+01\n",
      " 2.93666646e+00 9.61466865e+01 5.48488695e+01 5.60125773e+01\n",
      " 1.28468105e+01 9.43972605e+01 1.28816511e+02 2.75040137e+01\n",
      " 3.87605444e+02 1.93509934e+01 3.31649907e+01 3.22490605e+01\n",
      " 1.22675700e+02 8.60933386e+01 1.19264336e+01 5.63153644e+01\n",
      " 1.47261950e+02 1.18347406e+01 1.30750443e+03]\n",
      "33-th iteration, loss: 0.07891847368552256, 46 gd steps\n",
      "insert gradient: -0.0031535173768953\n",
      "33-th iteration, new layer inserted. now 35 layers\n",
      "[  55.25876234   97.11090359   55.48254032   95.24757963   54.57347185\n",
      "   96.34031244  100.77038296  113.77416434   68.36319975  172.8147312\n",
      "   60.7978865   184.92181753   55.96183206   87.29123063    2.78339126\n",
      "   95.42891017   55.14212827   56.49218192   12.53507034   94.48064819\n",
      "   73.54623228    0.           55.15967421   27.50019506  386.24385986\n",
      "   23.20003882   30.57006698   35.43435039  121.37057861   86.35346954\n",
      "   11.39084913   57.08428385  146.48096767   11.26968671 1309.3881177 ]\n",
      "34-th iteration, loss: 0.07760465964555835, 59 gd steps\n",
      "insert gradient: -0.003672919599931525\n",
      "34-th iteration, new layer inserted. now 37 layers\n",
      "[5.54305643e+01 9.69727747e+01 5.54324925e+01 9.50189964e+01\n",
      " 5.44050731e+01 9.56001393e+01 1.00365200e+02 1.14557366e+02\n",
      " 6.83142882e+01 1.72005343e+02 6.10824922e+01 1.83244301e+02\n",
      " 5.76463294e+01 8.99775580e+01 1.23475250e+00 9.48814461e+01\n",
      " 5.72277806e+01 6.02346799e+01 1.08134066e+01 9.44875141e+01\n",
      " 6.48313091e+01 1.47272051e+01 4.75211367e+01 3.36652500e+01\n",
      " 2.57938662e+02 0.00000000e+00 1.28969331e+02 3.25452454e+01\n",
      " 2.51515423e+01 4.29214190e+01 1.17405196e+02 8.61783644e+01\n",
      " 1.09197055e+01 5.80438003e+01 1.44866599e+02 1.07458872e+01\n",
      " 1.31235564e+03]\n",
      "35-th iteration, loss: 0.07672721235043078, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "35-th iteration, new layer inserted. now 37 layers\n",
      "[  55.72392676   96.61041749   55.96749054   94.8272335    54.10760563\n",
      "   95.79953864   99.60943735  115.64487618   68.06213474  171.61992449\n",
      "   61.07419288  182.51422867   57.91921189  188.65930368   57.71392458\n",
      "   65.20702453    9.27009263   95.97594966   59.50891122   23.34739965\n",
      "   41.96674554   39.59214575   41.99445046    0.          209.97225229\n",
      "    8.07512527  126.65220768   36.19252821   22.20907876   46.60012823\n",
      "  117.17009443   86.67703577   10.80759993   58.79782661  145.77726208\n",
      "    8.92461341 1313.85969037]\n",
      "36-th iteration, loss: 0.07654578539170737, 19 gd steps\n",
      "insert gradient: -0.002211861751911072\n",
      "36-th iteration, new layer inserted. now 39 layers\n",
      "[5.55909522e+01 9.65727258e+01 5.52987645e+01 9.48099316e+01\n",
      " 5.46721501e+01 9.57806422e+01 9.93037140e+01 1.15623148e+02\n",
      " 6.78998626e+01 1.71485747e+02 6.10735550e+01 1.21580773e+02\n",
      " 0.00000000e+00 6.07903866e+01 5.77463766e+01 1.88183824e+02\n",
      " 5.74811143e+01 6.55472329e+01 8.94345356e+00 9.61308925e+01\n",
      " 5.96925461e+01 2.39802041e+01 4.13681992e+01 3.99868720e+01\n",
      " 4.11338665e+01 1.15696694e+00 2.09093988e+02 8.62394872e+00\n",
      " 1.26614205e+02 3.64208481e+01 2.19638749e+01 4.69640410e+01\n",
      " 1.17302732e+02 8.68210579e+01 1.10737559e+01 5.89517862e+01\n",
      " 1.45875355e+02 8.92924235e+00 1.31424407e+03]\n",
      "37-th iteration, loss: 0.07636275984316256, 33 gd steps\n",
      "insert gradient: -0.00349499273785841\n",
      "37-th iteration, new layer inserted. now 39 layers\n",
      "[5.52154398e+01 9.64339824e+01 0.00000000e+00 1.42108547e-14\n",
      " 5.50398382e+01 9.45688857e+01 5.43113369e+01 9.55907441e+01\n",
      " 9.92437291e+01 1.15734402e+02 6.77978367e+01 1.71497043e+02\n",
      " 6.12145672e+01 1.82566008e+02 5.79503114e+01 1.87747658e+02\n",
      " 5.74913388e+01 6.62063737e+01 8.70639023e+00 9.64417596e+01\n",
      " 5.97908725e+01 2.45926861e+01 4.05591373e+01 4.11129157e+01\n",
      " 3.99435775e+01 2.42910106e+00 2.07734887e+02 9.38763986e+00\n",
      " 1.26584179e+02 3.68647081e+01 2.17125809e+01 4.75166424e+01\n",
      " 1.17294885e+02 8.69072987e+01 1.11211991e+01 5.90309939e+01\n",
      " 1.45818322e+02 8.84522050e+00 1.31483415e+03]\n",
      "38-th iteration, loss: 0.07550618354282294, 28 gd steps\n",
      "insert gradient: -0.0009104388002992245\n",
      "38-th iteration, new layer inserted. now 37 layers\n",
      "[  54.22978113   96.10816827   56.43982143   94.57443052   54.00348053\n",
      "   94.08968117   99.22399091  115.77850999   66.55477155  171.31850049\n",
      "   60.91129449  181.31108275   59.27768327  185.26988863   57.40067914\n",
      "   72.03099325    5.54555138   99.66034714   60.92897787   31.15403789\n",
      "   30.5636261    59.68833858   34.42993597   11.07243023  196.74700745\n",
      "   10.95351097  127.91501581   42.73098286   18.88405631   52.50980541\n",
      "  115.4548935    87.30750652   10.89301207   58.9130884   144.22530272\n",
      "    8.15635822 1320.42444903]\n",
      "39-th iteration, loss: 0.07535274530153314, 23 gd steps\n",
      "insert gradient: -0.005456120605614065\n",
      "39-th iteration, new layer inserted. now 39 layers\n",
      "[  54.95355759   96.12805353   55.44095665   94.35220162   54.07537512\n",
      "   94.20843584   99.12447162  115.67450508   66.63721325  171.4124197\n",
      "   60.84379401  181.163144     58.92277725  185.19397454   57.217691\n",
      "   72.0538602     5.42921555   99.61744399   60.7738954    31.29303222\n",
      "   30.71853491   60.10417839   34.41213719   11.21666844  130.95573972\n",
      "    0.           65.47786986   11.0393885   127.8062902    42.82154368\n",
      "   18.82912857   52.5363317   115.28791305   87.27544256   10.94101338\n",
      "   58.91190483  144.18822345    8.42127119 1320.65229595]\n",
      "40-th iteration, loss: 0.0753488082310865, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 41 layers\n",
      "[5.49534002e+01 9.61274178e+01 5.54370221e+01 9.43510260e+01\n",
      " 5.40745080e+01 9.42087176e+01 9.91237772e+01 1.15673834e+02\n",
      " 6.66377692e+01 1.71413218e+02 6.08439253e+01 1.81162925e+02\n",
      " 5.89219177e+01 1.85194113e+02 5.72180397e+01 7.20545840e+01\n",
      " 5.42964794e+00 9.96178824e+01 6.07747478e+01 3.12939680e+01\n",
      " 3.07212853e+01 6.01099297e+01 3.44232159e+01 1.12292784e+01\n",
      " 1.30965178e+02 1.54648368e-02 6.54873084e+01 1.10503355e+01\n",
      " 1.27815217e+02 4.28290196e+01 1.88397333e+01 5.25393630e+01\n",
      " 1.15289154e+02 8.72756056e+01 1.09419766e+01 5.89117199e+01\n",
      " 1.44187402e+02 8.42300090e+00 2.64130680e+02 0.00000000e+00\n",
      " 1.05652272e+03]\n",
      "41-th iteration, loss: 0.07484362084662721, 55 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 43 layers\n",
      "[  56.10868328   95.89975518   54.72631172   94.0546066    53.69766232\n",
      "   94.04607716   99.13435825  114.67478029   66.71417427  170.68662386\n",
      "   60.78081651  180.55268559   58.91623464  184.49436713   57.67720468\n",
      "   74.24281725    4.41376398  100.30848839   59.66102723   33.77907037\n",
      "   27.95468694   67.66878685   32.70429065   13.54649279  124.66303247\n",
      "    5.98914169   58.62372173   15.3828918   130.14988146   46.11708219\n",
      "   17.64385691   54.37775805  114.44127702   86.53998859   11.28454847\n",
      "   58.78420853  141.01353467    9.57232454   53.15756864    0.\n",
      "  212.63027457    2.35666922 1058.89858086]\n",
      "42-th iteration, loss: 0.07462401020964976, 38 gd steps\n",
      "insert gradient: -0.001175940302646931\n",
      "42-th iteration, new layer inserted. now 45 layers\n",
      "[5.50553295e+01 9.59232389e+01 5.49727537e+01 9.42488751e+01\n",
      " 5.39481628e+01 9.41688001e+01 9.88854058e+01 1.14560737e+02\n",
      " 6.67964748e+01 1.70660376e+02 6.08653050e+01 1.80418170e+02\n",
      " 5.89515277e+01 1.84358885e+02 5.77267733e+01 7.43777868e+01\n",
      " 4.32149555e+00 1.00354820e+02 5.95647134e+01 3.37516708e+01\n",
      " 2.76596331e+01 6.81043237e+01 3.23769617e+01 1.36931688e+01\n",
      " 1.24253135e+02 6.72075092e+00 5.80462014e+01 1.58805609e+01\n",
      " 1.30131337e+02 4.64064497e+01 1.75861427e+01 5.44906519e+01\n",
      " 1.14225062e+02 8.64125454e+01 1.12683510e+01 5.86343758e+01\n",
      " 1.40745031e+02 9.93735432e+00 5.32368544e+01 0.00000000e+00\n",
      " 3.55271368e-15 1.44586441e+00 2.12697802e+02 2.40922857e+00\n",
      " 1.05907545e+03]\n",
      "43-th iteration, loss: 0.07416395371979674, 22 gd steps\n",
      "insert gradient: -0.004376987488999199\n",
      "43-th iteration, new layer inserted. now 43 layers\n",
      "[5.38948225e+01 9.44949800e+01 5.38952692e+01 9.47579322e+01\n",
      " 5.57836554e+01 9.44639214e+01 9.82594855e+01 1.13176719e+02\n",
      " 6.59109965e+01 1.69393353e+02 6.11490632e+01 1.78593033e+02\n",
      " 6.06213775e+01 1.81516524e+02 6.00698485e+01 7.58636154e+01\n",
      " 2.67620959e+00 1.01239518e+02 6.09668404e+01 3.42496594e+01\n",
      " 2.52285849e+01 7.28915016e+01 2.92802930e+01 1.38745661e+01\n",
      " 1.21878577e+02 1.37896199e+01 5.10853805e+01 2.12220672e+01\n",
      " 1.31822088e+02 5.13521570e+01 1.62488923e+01 5.56709079e+01\n",
      " 1.13040787e+02 8.51689740e+01 1.12204160e+01 5.88513254e+01\n",
      " 1.39940985e+02 1.70416348e+01 5.09359672e+01 1.01691854e+01\n",
      " 2.11321610e+02 7.80828440e-01 1.05892882e+03]\n",
      "44-th iteration, loss: 0.07342602180541354, 48 gd steps\n",
      "insert gradient: -0.0031430993410587623\n",
      "44-th iteration, new layer inserted. now 45 layers\n",
      "[5.45282170e+01 9.51576176e+01 0.00000000e+00 7.10542736e-15\n",
      " 5.44467896e+01 9.40868602e+01 5.35553154e+01 9.37176529e+01\n",
      " 9.85176083e+01 1.12851401e+02 6.69733825e+01 1.68707461e+02\n",
      " 6.10944448e+01 1.77958429e+02 6.01447103e+01 1.80403992e+02\n",
      " 5.98960088e+01 7.63193530e+01 2.20185316e+00 1.01588202e+02\n",
      " 6.14088605e+01 3.50316002e+01 2.38435968e+01 7.51142445e+01\n",
      " 2.82396943e+01 1.32103474e+01 1.21495497e+02 1.87282000e+01\n",
      " 4.67271778e+01 2.57091766e+01 1.30583951e+02 5.38153099e+01\n",
      " 1.56360715e+01 5.63223851e+01 1.12577711e+02 8.43778528e+01\n",
      " 1.12961240e+01 5.93995057e+01 1.39477045e+02 2.03230962e+01\n",
      " 4.85634618e+01 1.34970198e+01 2.11530474e+02 3.23946768e-01\n",
      " 1.05934077e+03]\n",
      "45-th iteration, loss: 0.07339567283733221, 17 gd steps\n",
      "insert gradient: -0.0020750946164760165\n",
      "45-th iteration, new layer inserted. now 47 layers\n",
      "[5.43758022e+01 9.51451749e+01 3.13310394e-02 7.22057336e-04\n",
      " 5.44899488e+01 9.41548428e+01 5.37272187e+01 9.37553494e+01\n",
      " 9.85052191e+01 1.12814180e+02 6.69078334e+01 1.68658809e+02\n",
      " 6.10323678e+01 1.77903794e+02 6.01054117e+01 1.80344380e+02\n",
      " 5.98899609e+01 7.63236543e+01 2.15395265e+00 1.01590555e+02\n",
      " 6.14204385e+01 3.50458417e+01 2.37826897e+01 7.51644850e+01\n",
      " 2.82147265e+01 1.32056843e+01 0.00000000e+00 8.88178420e-16\n",
      " 1.21508472e+02 1.88576055e+01 4.66040465e+01 2.58344820e+01\n",
      " 1.30539280e+02 5.38755145e+01 1.56064940e+01 5.63365340e+01\n",
      " 1.12553662e+02 8.43494558e+01 1.12738708e+01 5.94005639e+01\n",
      " 1.39458727e+02 2.03904108e+01 4.84831842e+01 1.35827554e+01\n",
      " 2.11526230e+02 3.08531946e-01 1.05933878e+03]\n",
      "46-th iteration, loss: 0.07331000065919997, 27 gd steps\n",
      "insert gradient: -0.0012937186226548612\n",
      "46-th iteration, new layer inserted. now 45 layers\n",
      "[5.41807538e+01 9.52166741e+01 5.45555932e+01 9.41292869e+01\n",
      " 5.38033998e+01 9.37264028e+01 9.84234941e+01 1.12669948e+02\n",
      " 6.69090541e+01 1.68554605e+02 6.10402561e+01 1.77744799e+02\n",
      " 6.01755968e+01 1.80071681e+02 6.00708649e+01 7.64608689e+01\n",
      " 1.98875012e+00 1.01712985e+02 6.16100849e+01 3.51958242e+01\n",
      " 2.34655924e+01 7.55917509e+01 2.79013011e+01 1.31306069e+01\n",
      " 1.21618934e+02 0.00000000e+00 7.10542736e-15 1.98237912e+01\n",
      " 4.55943322e+01 2.68007727e+01 1.30215613e+02 5.43726625e+01\n",
      " 1.54597025e+01 5.65024670e+01 1.12460654e+02 8.41874227e+01\n",
      " 1.12902425e+01 5.94841108e+01 1.39337670e+02 2.09832575e+01\n",
      " 4.78619162e+01 1.42109489e+01 2.11488483e+02 1.92106240e-01\n",
      " 1.05931607e+03]\n",
      "47-th iteration, loss: 0.07330835501553587, 10 gd steps\n",
      "insert gradient: -0.0008692845090075565\n",
      "47-th iteration, new layer inserted. now 47 layers\n",
      "[5.41822705e+01 9.52173308e+01 5.45564895e+01 9.41304473e+01\n",
      " 5.38057739e+01 9.37255536e+01 9.84191676e+01 1.12665974e+02\n",
      " 6.69040711e+01 1.68549726e+02 6.10349995e+01 1.77739208e+02\n",
      " 6.01722133e+01 1.80065315e+02 6.00707859e+01 7.64615829e+01\n",
      " 1.98363340e+00 1.01713537e+02 6.16117526e+01 3.51974931e+01\n",
      " 2.34602716e+01 7.55974350e+01 2.78970804e+01 1.31299753e+01\n",
      " 1.21620189e+02 1.40165102e-02 2.42073463e-03 1.98378100e+01\n",
      " 4.55801253e+01 2.68145912e+01 1.30210357e+02 5.43794328e+01\n",
      " 1.54572799e+01 5.65044589e+01 1.12458668e+02 8.41843810e+01\n",
      " 1.12885733e+01 5.94843015e+01 1.39334967e+02 2.09906686e+01\n",
      " 4.78520565e+01 1.42195546e+01 1.69189511e+02 0.00000000e+00\n",
      " 4.22973777e+01 1.90016326e-01 1.05931462e+03]\n",
      "48-th iteration, loss: 0.07257727599331043, 21 gd steps\n",
      "insert gradient: -0.0024628588466877933\n",
      "48-th iteration, new layer inserted. now 49 layers\n",
      "[5.47628146e+01 9.44524525e+01 0.00000000e+00 7.10542736e-15\n",
      " 5.39787093e+01 9.41993789e+01 5.38955372e+01 9.32656926e+01\n",
      " 9.86183270e+01 1.11303174e+02 6.68979323e+01 1.67872313e+02\n",
      " 6.06886352e+01 1.76887284e+02 6.05620876e+01 1.78188946e+02\n",
      " 6.17163076e+01 7.79617544e+01 3.94940105e-01 1.02979435e+02\n",
      " 6.36972710e+01 3.69714842e+01 2.05450034e+01 7.91949708e+01\n",
      " 2.35635937e+01 1.21025327e+01 1.22714817e+02 5.27170908e+00\n",
      " 1.00741649e-01 2.51239031e+01 3.70397637e+01 3.62808740e+01\n",
      " 1.28516546e+02 5.96055296e+01 1.40002957e+01 5.87126359e+01\n",
      " 1.11693495e+02 8.26287559e+01 1.15453156e+01 6.04422261e+01\n",
      " 1.38018887e+02 2.64463480e+01 4.28845393e+01 1.95131452e+01\n",
      " 1.66378469e+02 4.13121856e+00 4.11035990e+01 2.30180054e+00\n",
      " 1.05805527e+03]\n",
      "49-th iteration, loss: 0.07252757505105002, 15 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "49-th iteration, new layer inserted. now 49 layers\n",
      "[5.40460364e+01 9.44028844e+01 5.44065319e+01 9.42522598e+01\n",
      " 5.39813562e+01 9.33130295e+01 9.85991628e+01 1.11244997e+02\n",
      " 6.68073474e+01 1.67920355e+02 6.06713687e+01 1.76826950e+02\n",
      " 6.05831709e+01 1.78104220e+02 6.18490998e+01 7.80097963e+01\n",
      " 2.61928833e-01 1.03026115e+02 6.38539474e+01 3.70561242e+01\n",
      " 2.04925098e+01 7.92669090e+01 2.34414220e+01 1.21463456e+01\n",
      " 1.22758108e+02 5.37591097e+00 2.61839768e-02 2.52298878e+01\n",
      " 3.69952729e+01 3.64895117e+01 1.28353008e+02 5.96677046e+01\n",
      " 1.39163206e+01 5.87097636e+01 1.11628074e+02 8.25708225e+01\n",
      " 1.14923096e+01 6.04340262e+01 6.89892805e+01 0.00000000e+00\n",
      " 6.89892805e+01 2.65081161e+01 4.28021883e+01 1.96531759e+01\n",
      " 1.66276719e+02 4.22408784e+00 4.11373385e+01 2.35692846e+00\n",
      " 1.05807321e+03]\n",
      "50-th iteration, loss: 0.07216153317915261, 183 gd steps\n",
      "insert gradient: -0.0008996321233991973\n",
      "50-th iteration, new layer inserted. now 47 layers\n",
      "[  54.02109997   94.72734966   54.08465028   94.00913347   54.400439\n",
      "   93.60730708   98.42389484  110.90250745   65.93643542  168.4289836\n",
      "   60.54330157  176.20303465   61.11565397  176.35880815   63.67417905\n",
      "  182.04640296   66.07354471   37.75802556   19.0833692    80.30335013\n",
      "   21.45323695   11.69927313   73.97271247    0.           49.31514165\n",
      "   32.97752319   35.42680213   39.46365223  126.92933117   60.91836635\n",
      "   13.28542023   59.02979139  111.68684257   82.27883435   12.1093132\n",
      "   60.11961218   66.98099605    3.60801954   66.97761367   27.96877904\n",
      "   42.42659523   21.13858429  164.47248071    5.35759819   41.51398268\n",
      "    2.78951069 1058.13438284]\n",
      "51-th iteration, loss: 0.07135456080369851, 25 gd steps\n",
      "insert gradient: -0.0002794244820849669\n",
      "51-th iteration, new layer inserted. now 49 layers\n",
      "[5.36073907e+01 9.43727737e+01 5.42896760e+01 9.40505800e+01\n",
      " 5.40243750e+01 9.35930091e+01 7.38804922e+01 0.00000000e+00\n",
      " 2.46268307e+01 1.10965592e+02 6.36921962e+01 1.69768158e+02\n",
      " 5.95273656e+01 1.75073880e+02 6.28547842e+01 1.73173945e+02\n",
      " 6.66951457e+01 1.77094176e+02 6.99877953e+01 3.95885840e+01\n",
      " 1.66720663e+01 8.18253863e+01 1.65542113e+01 9.31351634e+00\n",
      " 7.35181493e+01 7.58809163e-01 4.90801482e+01 3.77565122e+01\n",
      " 3.27965654e+01 4.36139146e+01 1.25956402e+02 6.34591716e+01\n",
      " 1.21434227e+01 6.00362767e+01 1.11888245e+02 8.21106707e+01\n",
      " 1.25362674e+01 5.95780384e+01 6.44035811e+01 7.45651440e+00\n",
      " 6.40485876e+01 3.02609011e+01 4.15276712e+01 2.26228518e+01\n",
      " 1.61526259e+02 7.34471051e+00 4.34723742e+01 4.04094437e+00\n",
      " 1.05907531e+03]\n",
      "52-th iteration, loss: 0.06978251158117489, 78 gd steps\n",
      "insert gradient: -0.0015668463846756494\n",
      "52-th iteration, new layer inserted. now 47 layers\n",
      "[5.36858227e+01 9.36243901e+01 5.28475202e+01 9.38001660e+01\n",
      " 5.39583945e+01 9.30125847e+01 6.86162185e+01 7.98697040e+00\n",
      " 2.37205982e+01 1.05576031e+02 6.11538232e+01 1.70188568e+02\n",
      " 5.93445971e+01 1.71735403e+02 6.35224849e+01 1.69384358e+02\n",
      " 6.80272152e+01 1.73437809e+02 7.16802158e+01 4.72242911e+01\n",
      " 1.35361508e+01 8.38536594e+01 1.19560179e+01 3.24132459e-01\n",
      " 1.21257510e+02 4.29773938e+01 2.92765204e+01 4.67039385e+01\n",
      " 1.23482241e+02 6.54895445e+01 1.00305572e+01 6.48754012e+01\n",
      " 1.12008815e+02 8.41880100e+01 1.21856869e+01 5.91714529e+01\n",
      " 6.43546863e+01 8.91797436e+00 6.09766739e+01 3.18240715e+01\n",
      " 4.01221379e+01 2.44702386e+01 1.58333241e+02 1.09433956e+01\n",
      " 4.63544405e+01 5.83434021e+00 1.06180349e+03]\n",
      "53-th iteration, loss: 0.06915891289237037, 51 gd steps\n",
      "insert gradient: -0.0016864636246289896\n",
      "53-th iteration, new layer inserted. now 45 layers\n",
      "[  52.03709555   92.9983413    53.28395324   93.5646499    53.76354448\n",
      "   92.61901977   64.95238194   10.90415244   24.83424378  102.49687989\n",
      "   60.34301019  169.80985618   59.13037773  169.85132196   63.89409984\n",
      "  166.94569952   68.46598082  169.89477205   73.0386914    51.1629848\n",
      "   10.56934427   86.33968295  128.63681884   46.55407066   26.6163511\n",
      "   48.50287796  122.55670474   67.02452289    8.8367183    70.77655422\n",
      "  111.15591473   86.04929734   11.19436853   58.81813173   66.04318529\n",
      "    7.94357359   58.6769404    33.49006466   39.00531783   25.9381226\n",
      "  156.00450723   13.77202826   47.86657875    6.61260487 1065.00951734]\n",
      "54-th iteration, loss: 0.06840132656081062, 161 gd steps\n",
      "insert gradient: -0.0012350312841583452\n",
      "54-th iteration, new layer inserted. now 47 layers\n",
      "[  52.52885176   93.42329503   52.86471522   93.1006249    52.83159909\n",
      "   93.02147096   63.16559723   12.68231656   25.27288715   99.82603894\n",
      "   59.64083981  170.04046827   57.70937456  101.74542298    0.\n",
      "   67.83028199   62.80947413  166.50275435   67.5345403   168.55416711\n",
      "   71.52145662   51.49082876    8.03312773   95.4363178   124.32570974\n",
      "   52.65618948   22.45178068   54.60681156  119.34774205   71.34463396\n",
      "    5.41258678   82.99635488  109.48640637   90.18979267   10.86988799\n",
      "   56.23520522   70.41508586   10.07082635   48.81305563   44.02054016\n",
      "   32.37004816   34.22515946  145.50145217   20.07042779   39.44185698\n",
      "   11.44837935 1067.40503051]\n",
      "55-th iteration, loss: 0.06806126650842141, 27 gd steps\n",
      "insert gradient: -0.0021713389140499703\n",
      "55-th iteration, new layer inserted. now 49 layers\n",
      "[5.39830439e+01 9.32364872e+01 5.24152210e+01 9.26586430e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.26061151e+01 9.25340759e+01\n",
      " 6.32551480e+01 1.27275136e+01 2.53792599e+01 1.00021500e+02\n",
      " 5.98049479e+01 1.70083704e+02 5.81905927e+01 1.00702235e+02\n",
      " 8.11813884e-01 6.66719059e+01 6.25197113e+01 1.66636501e+02\n",
      " 6.77919162e+01 1.68465169e+02 7.16215121e+01 5.18127210e+01\n",
      " 7.92582341e+00 9.58840821e+01 1.24336201e+02 5.28014086e+01\n",
      " 2.22291027e+01 5.46631325e+01 1.19127050e+02 7.13521935e+01\n",
      " 4.58092587e+00 8.30657888e+01 1.08648779e+02 9.03061162e+01\n",
      " 1.04266732e+01 5.60137781e+01 7.06791932e+01 1.00225348e+01\n",
      " 4.79108871e+01 4.45803935e+01 3.18321107e+01 3.45164671e+01\n",
      " 1.44702293e+02 1.99253546e+01 3.84872510e+01 1.21120396e+01\n",
      " 1.06744903e+03]\n",
      "56-th iteration, loss: 0.06776686156779235, 25 gd steps\n",
      "insert gradient: -0.0034144210332787807\n",
      "56-th iteration, new layer inserted. now 47 layers\n",
      "[5.26291367e+01 9.28164906e+01 5.25131422e+01 9.27362100e+01\n",
      " 5.26522492e+01 9.25078986e+01 6.30380657e+01 1.24893036e+01\n",
      " 2.49992267e+01 9.99177630e+01 5.94314813e+01 1.69509674e+02\n",
      " 5.78182942e+01 1.00400214e+02 6.72246505e-01 6.63176903e+01\n",
      " 6.22808140e+01 1.66287792e+02 6.75337524e+01 1.68122533e+02\n",
      " 7.15143692e+01 5.18503884e+01 7.62239387e+00 9.58985382e+01\n",
      " 1.24247466e+02 5.27379199e+01 2.21072337e+01 5.45137983e+01\n",
      " 1.18474562e+02 7.12761697e+01 4.54730832e+00 8.30557682e+01\n",
      " 1.08350810e+02 9.03656204e+01 1.05759168e+01 5.60407423e+01\n",
      " 7.08711845e+01 1.01778001e+01 4.80572799e+01 4.49175531e+01\n",
      " 3.20579578e+01 3.47514230e+01 1.44802066e+02 2.02891827e+01\n",
      " 3.87141700e+01 1.23699606e+01 1.06766286e+03]\n",
      "57-th iteration, loss: 0.06761301747618824, 35 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "57-th iteration, new layer inserted. now 49 layers\n",
      "[ 52.48718302  92.37979207  52.38252649  92.1179542   52.47158312\n",
      "  91.56601014  63.06144393  12.50348904  24.39519234 100.41998385\n",
      "  59.58337238 167.45143304  58.68941875  99.38331114   0.99751483\n",
      "  64.76510749  62.76509107 165.02473175  67.47459577 166.8634359\n",
      "  71.78540893  51.90427147   7.34706577  96.60890231 123.77859318\n",
      "  52.48840305  21.7639452   55.0924032  116.87927759  70.85757785\n",
      "   4.7621514   83.81035035 108.49825154  90.60060682  10.85907746\n",
      "  56.0394933   70.95097005  10.87603439  47.30485468  47.52580011\n",
      "  30.84338657  36.29972628 143.7959263   22.49046775  38.70461614\n",
      "  13.63824573 533.98822538   0.         533.98822538]\n",
      "58-th iteration, loss: 0.06747810253198387, 51 gd steps\n",
      "insert gradient: -0.003307199287565639\n",
      "58-th iteration, new layer inserted. now 51 layers\n",
      "[ 52.88926782  92.50899652  52.34145919  91.86117413  51.92888052\n",
      "  91.64201487  62.84053363  13.04083742  24.2406896  100.22119523\n",
      "  59.18743439 167.91574437  58.64482096  99.45175894   1.09883622\n",
      "  64.07352     62.56077642 164.91641544  67.32964428 166.9938647\n",
      "  71.30176631  51.45036464   7.437589    96.9865841  123.6766329\n",
      "  52.33745862  21.42599432  56.18149957 116.64859064  70.40914681\n",
      "   4.8598294   84.24865992 108.22534338  90.34146072  11.15416682\n",
      "  56.23719549  70.15996149  11.73340503  46.23304567  50.71752984\n",
      "  29.2593954   38.32488864 141.51593169  24.05658768  37.89863352\n",
      "  14.5081184  533.12576068   3.11335139 400.10748883   0.\n",
      " 133.36916294]\n",
      "59-th iteration, loss: 0.06696729105365523, 28 gd steps\n",
      "insert gradient: -0.008899699707362976\n",
      "59-th iteration, new layer inserted. now 53 layers\n",
      "[5.23389293e+01 9.25035483e+01 5.22308811e+01 9.20492669e+01\n",
      " 5.21193039e+01 9.16870974e+01 6.27861057e+01 1.28692249e+01\n",
      " 2.41226386e+01 1.00362873e+02 5.93142161e+01 1.67598875e+02\n",
      " 5.85049824e+01 9.96207432e+01 8.89880403e-01 6.40460188e+01\n",
      " 6.25840774e+01 1.64754650e+02 6.72248050e+01 1.66966144e+02\n",
      " 7.10638766e+01 5.13728804e+01 7.46254103e+00 9.70848545e+01\n",
      " 1.23404941e+02 5.22651385e+01 2.12328055e+01 5.64083862e+01\n",
      " 1.16349380e+02 7.03621913e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.87582992e+00 8.43586470e+01 1.08343651e+02 9.02822856e+01\n",
      " 1.13577681e+01 5.63666983e+01 6.98526879e+01 1.20609527e+01\n",
      " 4.59599397e+01 5.18040673e+01 2.87667396e+01 3.88004674e+01\n",
      " 1.40449957e+02 2.42745614e+01 3.72497769e+01 1.44226897e+01\n",
      " 5.32365748e+02 2.96779957e+00 3.99228850e+02 6.69821668e+00\n",
      " 1.32159118e+02]\n",
      "60-th iteration, loss: 0.06675227787282301, 27 gd steps\n",
      "insert gradient: -0.008230064687840762\n",
      "60-th iteration, new layer inserted. now 51 layers\n",
      "[ 53.14136679  92.52312031  52.36776942  91.73366048  51.50796485\n",
      "  91.89436189  63.36624351  10.84864644  25.07710149 100.94212322\n",
      "  59.97966171 166.98484223  58.31249465  99.98368396   0.88359905\n",
      "  63.89462973  62.68824776 164.52427877  67.43944019 166.63785558\n",
      "  71.37319322  51.83691913   7.09053957  97.71931319 123.71489824\n",
      "  52.13433425  20.97234523  57.05061939 116.02196127  70.24680438\n",
      "   4.75577074  84.73755387 108.32037676  89.99013366  11.51188249\n",
      "  56.67929465  69.05202933  13.19831057  45.24474418  54.45319329\n",
      "  27.51725219  41.27443907 139.34263355  26.53528329  37.58858925\n",
      "  15.59481622 531.57169767   2.68277755 397.9140673   10.67020164\n",
      " 127.54001391]\n",
      "61-th iteration, loss: 0.06667258639705054, 25 gd steps\n",
      "insert gradient: -0.004843589931737165\n",
      "61-th iteration, new layer inserted. now 53 layers\n",
      "[ 52.18334039  92.17541321  52.16378803  91.9322946   52.21712175\n",
      "  91.97778831  63.290944    10.61188347  25.03236398 100.87406473\n",
      "  59.88108051 166.9739301   58.33049367  99.98911908   0.90148479\n",
      "  63.87775626  62.66279384 164.50401362  67.39632794 166.59101543\n",
      "  71.36622377  51.87098896   7.12512502  97.74785536 123.736583\n",
      "  52.11313373  20.94808307  57.04959462 115.97625465  70.24053852\n",
      "   4.79530292  84.75174443 108.35771504  89.9627161   11.53683275\n",
      "  56.68569244  68.98218815  13.24068852  45.20522148  54.57024594\n",
      "  27.47075117  41.37422835 139.26218534  26.63474273  37.53740642\n",
      "  15.63591899 531.50962239   2.65830968 397.83421319  10.87427487\n",
      "  95.52002184   0.          31.84000728]\n",
      "62-th iteration, loss: 0.06602420091117359, 18 gd steps\n",
      "insert gradient: -0.01652882058341568\n",
      "62-th iteration, new layer inserted. now 55 layers\n",
      "[5.23714235e+01 9.22928723e+01 5.21600865e+01 9.17766714e+01\n",
      " 5.18568714e+01 9.17238821e+01 6.33084806e+01 1.04240039e+01\n",
      " 2.51388876e+01 1.00852208e+02 5.97299747e+01 1.66866881e+02\n",
      " 5.83283132e+01 1.00013056e+02 9.06066161e-01 6.38483756e+01\n",
      " 6.26080444e+01 1.64469121e+02 6.72790120e+01 1.66498066e+02\n",
      " 7.12498600e+01 5.18365003e+01 7.13869565e+00 9.77423827e+01\n",
      " 1.23599942e+02 5.20740992e+01 2.08914183e+01 5.70541049e+01\n",
      " 1.15853096e+02 7.02204445e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.84409195e+00 8.47587684e+01 1.08482752e+02 8.98666939e+01\n",
      " 1.15474078e+01 5.66718285e+01 6.87360865e+01 1.33264531e+01\n",
      " 4.50709977e+01 5.48556767e+01 2.72163331e+01 4.14760140e+01\n",
      " 1.38914576e+02 2.67667205e+01 3.73072513e+01 1.56255115e+01\n",
      " 5.31181183e+02 2.44952061e+00 3.97427019e+02 1.05374319e+01\n",
      " 9.45925843e+01 4.87880443e+00 3.08378065e+01]\n",
      "63-th iteration, loss: 0.06601665015022119, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "63-th iteration, new layer inserted. now 57 layers\n",
      "[5.23712628e+01 9.22929624e+01 5.21606653e+01 9.17771243e+01\n",
      " 5.18577912e+01 9.17240444e+01 6.33088186e+01 1.04242175e+01\n",
      " 2.51393851e+01 1.00852251e+02 5.97298784e+01 1.66866831e+02\n",
      " 5.83282553e+01 1.00013040e+02 9.05985255e-01 6.38483218e+01\n",
      " 6.26079768e+01 1.64469130e+02 6.72790791e+01 1.66498173e+02\n",
      " 7.12503136e+01 5.18368039e+01 7.13983100e+00 9.77428113e+01\n",
      " 1.23601188e+02 5.20753943e+01 2.08955532e+01 5.70565046e+01\n",
      " 1.15857662e+02 7.02241855e+01 1.18705625e-02 3.74101034e-03\n",
      " 4.85596251e+00 8.47622884e+01 1.08487415e+02 8.98685182e+01\n",
      " 1.15529286e+01 5.66735964e+01 6.87376812e+01 1.33282205e+01\n",
      " 4.50726892e+01 5.48567715e+01 2.72174759e+01 4.14765911e+01\n",
      " 1.38914747e+02 2.67670819e+01 3.73073542e+01 1.56255742e+01\n",
      " 5.31181147e+02 2.44939578e+00 3.97426905e+02 1.05369549e+01\n",
      " 9.45918520e+01 4.88196297e+00 7.70910008e+00 0.00000000e+00\n",
      " 2.31273002e+01]\n",
      "64-th iteration, loss: 0.06441567803746047, 19 gd steps\n",
      "insert gradient: -0.003583079506652635\n",
      "64-th iteration, new layer inserted. now 59 layers\n",
      "[5.41926928e+01 9.24311189e+01 5.22730477e+01 9.16523630e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.12572041e+01 9.02956731e+01\n",
      " 6.28773426e+01 1.22510381e+01 2.51012347e+01 9.98596717e+01\n",
      " 5.86636343e+01 1.67957871e+02 5.81025455e+01 9.98071239e+01\n",
      " 1.36972042e+00 6.31510896e+01 6.22955084e+01 1.64818076e+02\n",
      " 6.73110741e+01 1.66287819e+02 7.17109771e+01 5.21389373e+01\n",
      " 6.94184034e+00 9.82772671e+01 1.23369975e+02 5.28693858e+01\n",
      " 2.07054504e+01 5.71354656e+01 1.16053933e+02 7.04341180e+01\n",
      " 4.59530783e-02 2.16366297e-01 4.77501114e+00 8.50127546e+01\n",
      " 1.08064547e+02 8.96483883e+01 1.15645432e+01 5.63737412e+01\n",
      " 6.78282646e+01 1.42511933e+01 4.42512658e+01 5.66593364e+01\n",
      " 2.57889020e+01 4.34548646e+01 1.38174500e+02 2.82568768e+01\n",
      " 3.75050728e+01 1.70617604e+01 5.31135737e+02 9.40508832e-01\n",
      " 3.96689269e+02 5.01500478e-01 9.19095177e+01 1.51795934e+01\n",
      " 2.52609337e+00 1.17610323e+01 1.79097135e+01]\n",
      "65-th iteration, loss: 0.06414695425500438, 20 gd steps\n",
      "insert gradient: -0.008043713049681902\n",
      "65-th iteration, new layer inserted. now 61 layers\n",
      "[5.29206703e+01 9.20117892e+01 5.21108263e+01 9.18700457e+01\n",
      " 6.99375824e-01 1.71866614e-01 5.19699648e+01 9.05484612e+01\n",
      " 6.28886556e+01 1.23281164e+01 2.51100481e+01 9.98192078e+01\n",
      " 5.86550210e+01 1.67992057e+02 5.80756604e+01 9.97886877e+01\n",
      " 1.40981101e+00 6.31162982e+01 6.22354399e+01 1.64757403e+02\n",
      " 6.72034998e+01 1.66212477e+02 7.16295107e+01 5.21239587e+01\n",
      " 6.92866957e+00 9.82595995e+01 1.23285019e+02 5.28493007e+01\n",
      " 2.07004059e+01 5.71343247e+01 1.16055717e+02 7.04284763e+01\n",
      " 2.68480289e-02 2.10507821e-01 0.00000000e+00 2.77555756e-17\n",
      " 4.75788872e+00 8.50049921e+01 1.08076438e+02 8.96470191e+01\n",
      " 1.15924499e+01 5.63334457e+01 6.77858575e+01 1.42589370e+01\n",
      " 4.41991639e+01 5.66806106e+01 2.57069912e+01 4.34572594e+01\n",
      " 1.38074059e+02 2.82189462e+01 3.73902682e+01 1.70190778e+01\n",
      " 5.31082588e+02 8.79650506e-01 3.96629300e+02 2.17936210e-01\n",
      " 9.18478016e+01 1.51767991e+01 2.29640104e+00 1.17848713e+01\n",
      " 1.75764816e+01]\n",
      "66-th iteration, loss: 0.06414422634125397, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "66-th iteration, new layer inserted. now 63 layers\n",
      "[5.29196094e+01 9.20117326e+01 5.21111581e+01 9.18700273e+01\n",
      " 6.98435645e-01 1.70377243e-01 5.19693854e+01 9.05483095e+01\n",
      " 6.28874590e+01 1.23274585e+01 2.51086224e+01 9.98185954e+01\n",
      " 5.86546847e+01 1.67992122e+02 5.80756513e+01 9.97888577e+01\n",
      " 1.41063949e+00 6.31164174e+01 6.22356141e+01 1.09838422e+02\n",
      " 0.00000000e+00 5.49192112e+01 6.72036472e+01 1.66212640e+02\n",
      " 7.16297184e+01 5.21241447e+01 6.92947278e+00 9.82598227e+01\n",
      " 1.23285251e+02 5.28498258e+01 2.07020329e+01 5.71353324e+01\n",
      " 1.16057911e+02 7.04303060e+01 3.26695303e-02 2.12335069e-01\n",
      " 5.84023863e-03 1.82724803e-03 4.76372896e+00 8.50067085e+01\n",
      " 1.08078823e+02 8.96478578e+01 1.15948325e+01 5.63340539e+01\n",
      " 6.77864134e+01 1.42595992e+01 4.41995996e+01 5.66810072e+01\n",
      " 2.57071063e+01 4.34573407e+01 1.38073841e+02 2.82188160e+01\n",
      " 3.73898968e+01 1.70189412e+01 5.31082417e+02 8.79345321e-01\n",
      " 3.96629109e+02 2.16948769e-01 9.18476080e+01 1.51769050e+01\n",
      " 2.29562107e+00 1.17850553e+01 1.75753994e+01]\n",
      "67-th iteration, loss: 0.06406295734638173, 15 gd steps\n",
      "insert gradient: -0.015574978957228562\n",
      "67-th iteration, new layer inserted. now 59 layers\n",
      "[5.21150025e+01 9.20124873e+01 5.24480378e+01 9.20452282e+01\n",
      " 5.25010842e+01 9.07880598e+01 6.26805514e+01 1.24994555e+01\n",
      " 2.48275804e+01 9.96402044e+01 5.86121855e+01 1.68018351e+02\n",
      " 5.79985957e+01 9.97422525e+01 1.53691917e+00 6.30019407e+01\n",
      " 6.21157812e+01 1.09673840e+02 2.92825753e-01 5.47522400e+01\n",
      " 6.69759249e+01 1.66195696e+02 7.14843214e+01 5.20677366e+01\n",
      " 7.01384775e+00 9.82327511e+01 1.23026688e+02 5.28986478e+01\n",
      " 2.06567832e+01 5.70996669e+01 1.16085182e+02 7.04435781e+01\n",
      " 3.08620752e-02 2.24308419e-01 1.19520790e-02 1.34740930e-02\n",
      " 4.76996289e+00 8.50006917e+01 1.08189512e+02 8.96328455e+01\n",
      " 1.15832872e+01 5.61992750e+01 6.76709191e+01 1.43057851e+01\n",
      " 4.40470851e+01 5.68554085e+01 2.55785459e+01 4.35040646e+01\n",
      " 1.37838507e+02 2.81269065e+01 3.71183613e+01 1.69403714e+01\n",
      " 5.30959002e+02 6.86285109e-01 4.88190119e+02 1.54510406e+01\n",
      " 1.76653485e+00 1.21005965e+01 1.69229522e+01]\n",
      "68-th iteration, loss: 0.06405333374449315, 6 gd steps\n",
      "insert gradient: -0.0015445859638748296\n",
      "68-th iteration, new layer inserted. now 61 layers\n",
      "[5.21166780e+01 9.20136216e+01 0.00000000e+00 7.10542736e-15\n",
      " 5.24497270e+01 9.20461036e+01 5.25020854e+01 9.07884357e+01\n",
      " 6.26800218e+01 1.24991744e+01 2.48267087e+01 9.96398983e+01\n",
      " 5.86118078e+01 1.68017942e+02 5.79980458e+01 9.97417811e+01\n",
      " 1.53570187e+00 6.30013832e+01 6.21152978e+01 1.09673439e+02\n",
      " 2.92256514e-01 5.47518306e+01 6.69756039e+01 1.66195684e+02\n",
      " 7.14844652e+01 5.20677630e+01 7.01438761e+00 9.82329435e+01\n",
      " 1.23027457e+02 5.28997948e+01 2.06603696e+01 5.71017786e+01\n",
      " 1.16089461e+02 7.04471160e+01 4.20456412e-02 2.27841761e-01\n",
      " 2.31681412e-02 1.70057199e-02 4.78118088e+00 8.50039967e+01\n",
      " 1.08194009e+02 8.96345590e+01 1.15881919e+01 5.62006862e+01\n",
      " 6.76722983e+01 1.43072066e+01 4.40483153e+01 5.68561562e+01\n",
      " 2.55791547e+01 4.35042967e+01 1.37838485e+02 2.81268770e+01\n",
      " 3.71181547e+01 1.69402594e+01 5.30958952e+02 6.86046886e-01\n",
      " 4.88190059e+02 1.54513746e+01 1.76612122e+00 1.21009569e+01\n",
      " 1.69224674e+01]\n",
      "69-th iteration, loss: 0.06404580880202608, 11 gd steps\n",
      "insert gradient: -0.003481518364831383\n",
      "69-th iteration, new layer inserted. now 61 layers\n",
      "[5.21670644e+01 9.20438610e+01 3.37385265e-02 2.97189965e-02\n",
      " 5.24838349e+01 9.20645931e+01 5.25221629e+01 9.08001991e+01\n",
      " 6.26641629e+01 1.24960514e+01 2.48006636e+01 9.96300350e+01\n",
      " 5.86024182e+01 1.68008007e+02 5.79837654e+01 9.97297739e+01\n",
      " 1.51011424e+00 6.29857815e+01 6.21005241e+01 1.09658495e+02\n",
      " 2.80846938e-01 5.47364976e+01 6.69581504e+01 1.66190636e+02\n",
      " 7.14719818e+01 5.20597450e+01 7.00622518e+00 9.82274426e+01\n",
      " 1.23011000e+02 5.29003157e+01 2.06558583e+01 5.70993144e+01\n",
      " 1.16089233e+02 7.04468958e+01 4.07895652e-02 2.27552350e-01\n",
      " 2.23050114e-02 1.66811737e-02 4.78034528e+00 8.50025140e+01\n",
      " 1.08195378e+02 8.96324301e+01 1.15850774e+01 5.61931324e+01\n",
      " 6.76642278e+01 1.43063672e+01 4.40372699e+01 5.68613308e+01\n",
      " 2.55685586e+01 4.35033662e+01 1.37825678e+02 2.81217537e+01\n",
      " 3.71070844e+01 1.69367179e+01 5.30952239e+02 6.76547602e-01\n",
      " 4.88182744e+02 1.54612344e+01 1.74701728e+00 1.21118066e+01\n",
      " 1.68997023e+01]\n",
      "70-th iteration, loss: 0.0639285085577126, 50 gd steps\n",
      "insert gradient: -0.007217201274698465\n",
      "70-th iteration, new layer inserted. now 61 layers\n",
      "[5.22349813e+01 9.20720806e+01 5.22315938e+01 9.21746323e+01\n",
      " 5.24165953e+01 9.15106729e+01 6.21779930e+01 1.37817482e+01\n",
      " 2.41915184e+01 9.89836624e+01 5.84121406e+01 1.68056190e+02\n",
      " 5.79277980e+01 9.95030858e+01 1.78515170e+00 6.22249332e+01\n",
      " 6.21537580e+01 1.08829009e+02 9.68148470e-01 5.38293255e+01\n",
      " 6.61658366e+01 1.66619591e+02 7.08871645e+01 5.15331544e+01\n",
      " 7.17865258e+00 9.80601532e+01 1.22054196e+02 5.35844612e+01\n",
      " 2.05573337e+01 5.69482087e+01 1.16398769e+02 7.05497480e+01\n",
      " 3.24012358e-02 3.17809809e-01 7.93968776e-02 9.52216346e-02\n",
      " 0.00000000e+00 1.38777878e-17 4.84776339e+00 8.49262733e+01\n",
      " 1.08372073e+02 8.95773340e+01 1.16214394e+01 5.54492772e+01\n",
      " 6.72352354e+01 1.48204550e+01 4.30503761e+01 5.80019978e+01\n",
      " 2.48882803e+01 4.38423720e+01 1.36698933e+02 2.79350830e+01\n",
      " 3.63354541e+01 1.68888661e+01 5.30698076e+02 2.86097855e-01\n",
      " 4.87855721e+02 1.63595167e+01 9.32105438e-01 1.30480972e+01\n",
      " 1.59297441e+01]\n",
      "71-th iteration, loss: 0.06392550588473703, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "71-th iteration, new layer inserted. now 63 layers\n",
      "[5.22352089e+01 9.20721532e+01 5.22313994e+01 9.21743483e+01\n",
      " 5.24160219e+01 9.15104677e+01 6.21778350e+01 1.37818678e+01\n",
      " 2.41914181e+01 9.89835602e+01 5.84121623e+01 1.68056314e+02\n",
      " 5.79279300e+01 9.95031683e+01 1.78549369e+00 6.22249258e+01\n",
      " 6.21539240e+01 1.08828993e+02 9.68395390e-01 5.38292900e+01\n",
      " 6.61658956e+01 1.66619697e+02 7.08872739e+01 5.15331534e+01\n",
      " 7.17902895e+00 9.80602591e+01 1.22054454e+02 5.35849910e+01\n",
      " 2.05588328e+01 5.69491607e+01 1.16400845e+02 7.05514399e+01\n",
      " 3.77627747e-02 3.19498666e-01 8.47881597e-02 9.69032276e-02\n",
      " 5.39986489e-03 1.68159296e-03 4.85316325e+00 8.49278371e+01\n",
      " 7.22494369e+01 0.00000000e+00 3.61247185e+01 8.95781541e+01\n",
      " 1.16238320e+01 5.54499083e+01 6.72358896e+01 1.48211812e+01\n",
      " 4.30508375e+01 5.80024090e+01 2.48884445e+01 4.38424337e+01\n",
      " 1.36698786e+02 2.79350133e+01 3.63353304e+01 1.68888591e+01\n",
      " 5.30698072e+02 2.86018548e-01 4.87855713e+02 1.63595483e+01\n",
      " 9.32027640e-01 1.30481351e+01 1.59295780e+01]\n",
      "72-th iteration, loss: 0.06378395059661476, 29 gd steps\n",
      "insert gradient: -0.0035548978273388506\n",
      "72-th iteration, new layer inserted. now 61 layers\n",
      "[5.21481008e+01 9.21069016e+01 5.21374546e+01 9.20521320e+01\n",
      " 5.23801781e+01 9.14275183e+01 6.17862039e+01 1.45347652e+01\n",
      " 2.40055510e+01 9.85430060e+01 5.82252937e+01 1.68025953e+02\n",
      " 5.79737440e+01 9.94304405e+01 1.94293676e+00 6.15276322e+01\n",
      " 6.24789230e+01 1.08309456e+02 1.33158044e+00 5.31593372e+01\n",
      " 6.57952539e+01 1.66745602e+02 7.03784737e+01 5.10445685e+01\n",
      " 7.27821046e+00 9.79436559e+01 1.21369947e+02 5.41810318e+01\n",
      " 2.04445601e+01 5.69635796e+01 1.16886926e+02 7.14990816e+01\n",
      " 1.22753573e-02 6.64087901e-01 0.00000000e+00 5.55111512e-17\n",
      " 4.85580867e+00 8.46466591e+01 7.09668901e+01 2.85400759e+00\n",
      " 3.47851913e+01 8.96196386e+01 1.20956657e+01 5.40874476e+01\n",
      " 6.68499484e+01 1.52031818e+01 4.18876616e+01 5.90673444e+01\n",
      " 2.40953568e+01 4.38746302e+01 1.35336106e+02 2.76091344e+01\n",
      " 3.56634177e+01 1.70303991e+01 5.30860494e+02 3.91645214e-01\n",
      " 4.87989214e+02 1.64139581e+01 9.87139186e-01 1.31312805e+01\n",
      " 1.54797014e+01]\n",
      "73-th iteration, loss: 0.06316907856759363, 28 gd steps\n",
      "insert gradient: -0.0064447721645482705\n",
      "73-th iteration, new layer inserted. now 59 layers\n",
      "[5.00463117e+01 9.09672033e+01 5.07535514e+01 9.22576538e+01\n",
      " 5.42658703e+01 9.21386811e+01 6.13771300e+01 1.38434399e+01\n",
      " 2.29725383e+01 9.83400344e+01 5.79779544e+01 1.66375352e+02\n",
      " 5.74098886e+01 9.92705369e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.42285349e+00 6.06115991e+01 6.26538443e+01 1.07719865e+02\n",
      " 1.25626522e+00 5.23361327e+01 6.56059808e+01 1.66391621e+02\n",
      " 7.00886214e+01 5.06516731e+01 6.90371078e+00 9.78612063e+01\n",
      " 1.20810206e+02 5.45051167e+01 1.98275472e+01 5.67256399e+01\n",
      " 1.15500887e+02 7.36019414e+01 3.90276078e+00 8.37914473e+01\n",
      " 6.91190744e+01 4.91007989e+00 3.24223299e+01 8.98315272e+01\n",
      " 1.20317589e+01 5.15433953e+01 6.67070481e+01 1.53055090e+01\n",
      " 4.01791251e+01 6.01157531e+01 2.33717796e+01 4.39408788e+01\n",
      " 1.33695229e+02 2.67810680e+01 3.45662527e+01 1.79601042e+01\n",
      " 5.32297482e+02 1.46865008e+00 4.89392173e+02 1.62490046e+01\n",
      " 1.85955464e+00 1.30741478e+01 1.49230327e+01]\n",
      "74-th iteration, loss: 0.060674654787767276, 28 gd steps\n",
      "insert gradient: -0.0022689670742508474\n",
      "74-th iteration, new layer inserted. now 59 layers\n",
      "[5.06935925e+01 9.04645130e+01 5.04179620e+01 9.04758524e+01\n",
      " 5.09139198e+01 9.06482926e+01 6.08980267e+01 1.33510303e+01\n",
      " 2.22366607e+01 9.80147307e+01 5.69268498e+01 1.64281698e+02\n",
      " 5.61629093e+01 9.86612218e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.23281518e+00 5.97897867e+01 6.16273350e+01 1.06735026e+02\n",
      " 1.06147504e+00 5.12298122e+01 6.42904638e+01 1.64404306e+02\n",
      " 6.93301017e+01 5.05513078e+01 6.19774108e+00 9.76813476e+01\n",
      " 1.19905247e+02 5.39042983e+01 1.93101709e+01 5.63632490e+01\n",
      " 1.12517104e+02 7.32038596e+01 2.58435278e+00 8.31616129e+01\n",
      " 6.76163288e+01 4.30861043e+00 3.04456315e+01 8.96340487e+01\n",
      " 1.11265293e+01 5.10237376e+01 6.64403978e+01 1.46418628e+01\n",
      " 3.90299034e+01 6.00222719e+01 2.21796446e+01 4.36201362e+01\n",
      " 1.32935933e+02 2.67120141e+01 3.43092346e+01 1.84429813e+01\n",
      " 5.32908672e+02 1.07237624e+00 4.89932831e+02 1.61763898e+01\n",
      " 2.03194276e+00 1.30850457e+01 1.44928943e+01]\n",
      "75-th iteration, loss: 0.05905585423753153, 90 gd steps\n",
      "insert gradient: -0.00132920479474303\n",
      "75-th iteration, new layer inserted. now 57 layers\n",
      "[ 49.80200939  89.023489    49.49223551  88.57952111  49.64997536\n",
      "  88.46705688  59.14340316  14.71063631  21.44164216  95.71106747\n",
      "  55.70110454 160.57987962  55.59769919  97.47814545   1.23546472\n",
      "  57.29394559  61.20447359 104.85676179   1.14204411  48.61093014\n",
      "  64.27411645 159.61434441  69.12716852  49.54691718   5.52792998\n",
      "  97.59464375 116.26106001  53.16264975  18.38733037  57.85336404\n",
      " 110.09508939  74.5838971    2.84702798  82.39381603  66.46039068\n",
      "   7.17740214  28.45536763  90.57698342  11.83748118  47.10909422\n",
      "  66.13018446  13.75586656  36.86023444  62.52630154  19.47298574\n",
      "  45.81692783 127.12399478  28.69198101  31.55035262  19.50092515\n",
      " 534.42489612   0.73183522 491.64123518  15.9568174    3.99967989\n",
      "  13.39128679  12.88768269]\n",
      "76-th iteration, loss: 0.05793475833870107, 31 gd steps\n",
      "insert gradient: -0.0027506042291950057\n",
      "76-th iteration, new layer inserted. now 59 layers\n",
      "[4.97422854e+01 8.85091523e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.86896027e+01 8.76306398e+01 4.87471791e+01 8.76769127e+01\n",
      " 5.86541522e+01 1.45463718e+01 2.12593713e+01 9.50593635e+01\n",
      " 5.51417385e+01 1.59338165e+02 5.51659565e+01 9.72392753e+01\n",
      " 9.84358802e-01 5.66290174e+01 6.09944892e+01 1.04518216e+02\n",
      " 8.28959134e-01 4.80217199e+01 6.42250215e+01 1.58402189e+02\n",
      " 6.90732291e+01 4.94107535e+01 5.15828485e+00 9.76370861e+01\n",
      " 1.15208751e+02 5.30611799e+01 1.77715131e+01 5.82881351e+01\n",
      " 1.08694474e+02 7.51854222e+01 1.98970172e+00 8.16608954e+01\n",
      " 6.55660128e+01 7.75605941e+00 2.66291392e+01 9.08498451e+01\n",
      " 1.13101906e+01 4.52839865e+01 6.62254121e+01 1.31622507e+01\n",
      " 3.58264454e+01 6.33787527e+01 1.83961909e+01 4.66088930e+01\n",
      " 1.24628530e+02 2.89798114e+01 3.07365217e+01 2.04898192e+01\n",
      " 5.35083613e+02 1.09772083e+00 4.92412742e+02 1.59063743e+01\n",
      " 4.96222961e+00 1.37551354e+01 1.22660676e+01]\n",
      "77-th iteration, loss: 0.056385859478282264, 31 gd steps\n",
      "insert gradient: -0.0010308868987333362\n",
      "77-th iteration, new layer inserted. now 59 layers\n",
      "[4.86667689e+01 8.74675811e+01 4.81279625e+01 8.66139886e+01\n",
      " 4.83990625e+01 8.68112744e+01 5.78571417e+01 1.40505745e+01\n",
      " 0.00000000e+00 1.77635684e-15 2.07720188e+01 9.35853134e+01\n",
      " 5.34841268e+01 1.57482334e+02 5.39753382e+01 9.65161271e+01\n",
      " 8.81336007e-01 5.55720815e+01 6.05188308e+01 1.03919482e+02\n",
      " 4.89507326e-01 4.71890925e+01 6.41485380e+01 1.55403088e+02\n",
      " 6.96009161e+01 5.00326985e+01 4.17865652e+00 9.82432815e+01\n",
      " 1.13585583e+02 5.27107782e+01 1.67625232e+01 5.87449961e+01\n",
      " 1.05144490e+02 7.58520281e+01 8.78944231e-01 8.14143779e+01\n",
      " 6.49865558e+01 6.97386373e+00 2.44555789e+01 9.16069670e+01\n",
      " 9.94645315e+00 4.46422603e+01 6.72558861e+01 1.23077185e+01\n",
      " 3.46292150e+01 6.42978876e+01 1.71164096e+01 4.77502566e+01\n",
      " 1.22721715e+02 3.01209420e+01 2.99113065e+01 2.22709925e+01\n",
      " 5.37922352e+02 1.78513839e+00 4.95551636e+02 1.58590668e+01\n",
      " 6.38435885e+00 1.46904263e+01 1.15733560e+01]\n",
      "78-th iteration, loss: 0.05566914398786666, 32 gd steps\n",
      "insert gradient: -0.0048847293359660475\n",
      "78-th iteration, new layer inserted. now 59 layers\n",
      "[4.82482092e+01 8.62457782e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.77951631e+01 8.56625402e+01 4.80287027e+01 8.57772765e+01\n",
      " 5.68419825e+01 1.45095090e+01 2.20201975e-01 4.29397270e-01\n",
      " 2.10223675e+01 9.12982568e+01 5.17227365e+01 1.57355167e+02\n",
      " 5.29694474e+01 9.56375787e+01 1.03706655e+00 5.43484376e+01\n",
      " 5.99961725e+01 1.50912164e+02 6.51172363e+01 1.51634500e+02\n",
      " 7.05470797e+01 5.13395582e+01 2.90597631e+00 9.96594652e+01\n",
      " 1.11843888e+02 5.31259943e+01 1.55730126e+01 6.01937269e+01\n",
      " 1.01534179e+02 7.66371187e+01 3.20987241e-01 8.19920641e+01\n",
      " 6.61066826e+01 5.39656005e+00 2.50876660e+01 9.29816370e+01\n",
      " 9.99604169e+00 4.61796643e+01 6.82935667e+01 1.26241956e+01\n",
      " 3.46866249e+01 6.62156343e+01 1.67580426e+01 4.98437526e+01\n",
      " 1.21764497e+02 3.44007842e+01 2.97165659e+01 2.36161228e+01\n",
      " 5.41670893e+02 2.10670286e+00 4.99750835e+02 1.52402418e+01\n",
      " 8.46412430e+00 1.54847122e+01 1.14549975e+01]\n",
      "79-th iteration, loss: 0.05562273578349501, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "79-th iteration, new layer inserted. now 61 layers\n",
      "[4.82724614e+01 8.62963785e+01 8.86300983e-02 3.79482403e-02\n",
      " 4.78898016e+01 8.57326192e+01 4.81209540e+01 8.58143897e+01\n",
      " 5.68474313e+01 1.45060304e+01 2.06273366e-01 4.23746297e-01\n",
      " 2.10129978e+01 9.12877635e+01 5.17202881e+01 1.57325286e+02\n",
      " 5.29349119e+01 9.56049165e+01 9.86553212e-01 5.43102222e+01\n",
      " 5.99460698e+01 1.50831312e+02 6.50627388e+01 1.51575150e+02\n",
      " 7.05388553e+01 5.13511186e+01 2.86599118e+00 9.96708859e+01\n",
      " 1.11849151e+02 5.31398147e+01 1.55777403e+01 6.02227811e+01\n",
      " 1.01549814e+02 7.66408376e+01 3.00680398e-01 8.19953719e+01\n",
      " 6.61164302e+01 5.37769779e+00 2.51001329e+01 9.29977767e+01\n",
      " 1.00502657e+01 4.61965294e+01 6.82978493e+01 1.26340428e+01\n",
      " 3.46967433e+01 6.62371157e+01 1.67740498e+01 4.98750594e+01\n",
      " 1.21769125e+02 3.44644102e+01 2.97583098e+01 2.36348283e+01\n",
      " 3.61149150e+02 0.00000000e+00 1.80574575e+02 2.10502499e+00\n",
      " 4.99803470e+02 1.52464003e+01 8.49236206e+00 1.55009078e+01\n",
      " 1.14831398e+01]\n",
      "80-th iteration, loss: 0.055342428812030976, 47 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "80-th iteration, new layer inserted. now 59 layers\n",
      "[4.78752079e+01 8.66533409e+01 4.76487185e+01 8.58696392e+01\n",
      " 4.77068343e+01 8.60006755e+01 5.67417652e+01 1.52084772e+01\n",
      " 2.08939947e+01 9.06903740e+01 5.20140776e+01 1.56710909e+02\n",
      " 5.27123618e+01 9.51684716e+01 1.34082316e+00 5.35849283e+01\n",
      " 5.97751121e+01 1.49468448e+02 6.51443885e+01 1.50068228e+02\n",
      " 7.11441146e+01 5.25030827e+01 2.21885143e+00 1.00748676e+02\n",
      " 1.11799741e+02 5.36320807e+01 1.48524498e+01 6.12835118e+01\n",
      " 1.00759889e+02 7.67720408e+01 1.87223068e-01 8.21176493e+01\n",
      " 6.62764678e+01 4.89655498e+00 2.55343359e+01 9.31840684e+01\n",
      " 1.03417383e+01 4.62642353e+01 6.76924450e+01 1.28042061e+01\n",
      " 3.46981581e+01 6.70636806e+01 1.63471671e+01 5.10656560e+01\n",
      " 1.21150532e+02 3.65600554e+01 2.94570753e+01 2.37299406e+01\n",
      " 3.63041852e+02 2.26569240e+00 1.21685817e+02 0.00000000e+00\n",
      " 6.08429083e+01 1.21895604e+00 5.01665300e+02 1.46009808e+01\n",
      " 9.37792642e+00 1.55468034e+01 1.14832803e+01]\n",
      "81-th iteration, loss: 0.05516313213076419, 46 gd steps\n",
      "insert gradient: -0.000719963400929456\n",
      "81-th iteration, new layer inserted. now 61 layers\n",
      "[4.82051978e+01 8.66395738e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.76480641e+01 8.55209590e+01 4.76163820e+01 8.57488522e+01\n",
      " 5.65880476e+01 1.59029463e+01 2.05827029e+01 9.02329553e+01\n",
      " 5.22125557e+01 1.56649407e+02 5.28184854e+01 9.44654214e+01\n",
      " 2.04449962e+00 5.22155355e+01 5.91367700e+01 1.48900398e+02\n",
      " 6.55495480e+01 1.48901512e+02 7.16431279e+01 5.42153058e+01\n",
      " 1.36660159e+00 1.02288682e+02 1.11702103e+02 5.43425625e+01\n",
      " 1.39805781e+01 6.30420432e+01 9.95717125e+01 7.66450191e+01\n",
      " 3.24184414e-01 8.19810972e+01 6.61066711e+01 5.12779102e+00\n",
      " 2.60790263e+01 9.29972774e+01 1.07321831e+01 4.60995609e+01\n",
      " 6.60666866e+01 1.33637446e+01 3.49645308e+01 6.84233396e+01\n",
      " 1.56668502e+01 5.26465122e+01 1.19324571e+02 3.82470670e+01\n",
      " 2.82706565e+01 2.40536300e+01 3.63794672e+02 3.09950217e+00\n",
      " 1.23475855e+02 2.86696060e+00 6.17836143e+01 1.21232925e+00\n",
      " 5.02465989e+02 1.40792343e+01 9.82174816e+00 1.53234245e+01\n",
      " 1.12073452e+01]\n",
      "82-th iteration, loss: 0.055126751476007055, 26 gd steps\n",
      "insert gradient: -0.0051374407817951565\n",
      "82-th iteration, new layer inserted. now 61 layers\n",
      "[4.80822131e+01 8.64602631e+01 4.77514267e+01 8.55850351e+01\n",
      " 4.77256725e+01 8.56946012e+01 5.65276266e+01 1.61950186e+01\n",
      " 2.03835133e+01 9.02362020e+01 5.22414977e+01 1.56637179e+02\n",
      " 5.29150666e+01 9.43675330e+01 2.23293801e+00 5.18454421e+01\n",
      " 5.91278738e+01 1.48756653e+02 6.56115998e+01 1.48632840e+02\n",
      " 7.17811923e+01 5.46589338e+01 1.15750640e+00 1.02698322e+02\n",
      " 1.11651788e+02 5.45604899e+01 1.36992118e+01 6.35673764e+01\n",
      " 9.93618310e+01 7.65660252e+01 0.00000000e+00 7.10542736e-15\n",
      " 3.67579857e-01 8.18934960e+01 6.59541972e+01 5.25195691e+00\n",
      " 2.61949294e+01 9.28508765e+01 1.08752474e+01 4.60648062e+01\n",
      " 6.54803179e+01 1.35951989e+01 3.50378830e+01 6.88421238e+01\n",
      " 1.54996834e+01 5.30734208e+01 1.18968914e+02 3.87336023e+01\n",
      " 2.81102762e+01 2.43414127e+01 3.63782686e+02 3.31593353e+00\n",
      " 1.23994742e+02 3.25246623e+00 6.16356551e+01 1.33946137e+00\n",
      " 5.02285035e+02 1.40495115e+01 9.86893493e+00 1.51812517e+01\n",
      " 1.11308406e+01]\n",
      "83-th iteration, loss: 0.05512611052042354, 5 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "83-th iteration, new layer inserted. now 63 layers\n",
      "[4.80823420e+01 8.64604270e+01 4.77517984e+01 8.55852436e+01\n",
      " 4.77259797e+01 8.56946994e+01 5.65276865e+01 1.61951255e+01\n",
      " 2.03835327e+01 9.02362186e+01 5.22414358e+01 1.56637075e+02\n",
      " 5.29149552e+01 9.43674054e+01 2.23265680e+00 5.18452360e+01\n",
      " 5.91277667e+01 1.48756561e+02 6.56116277e+01 1.48632786e+02\n",
      " 7.17814164e+01 5.46590936e+01 1.15751526e+00 1.02698481e+02\n",
      " 1.11652101e+02 5.45608227e+01 1.37001780e+01 6.35680383e+01\n",
      " 9.93630794e+01 7.65670044e+01 3.60643849e-03 9.79128708e-04\n",
      " 3.71186295e-01 8.18944688e+01 6.59554373e+01 5.25312747e+00\n",
      " 2.61964494e+01 9.28513185e+01 1.08767878e+01 4.60652674e+01\n",
      " 6.54806364e+01 1.35957130e+01 3.50383980e+01 6.88424070e+01\n",
      " 1.55000047e+01 5.30736366e+01 7.93126160e+01 0.00000000e+00\n",
      " 3.96563080e+01 3.87337711e+01 2.81102719e+01 2.43414899e+01\n",
      " 3.63782690e+02 3.31601047e+00 1.23994900e+02 3.25259750e+00\n",
      " 6.16356110e+01 1.33949055e+00 5.02284980e+02 1.40495491e+01\n",
      " 9.86896334e+00 1.51812307e+01 1.11308801e+01]\n",
      "84-th iteration, loss: 0.05512594386407397, 5 gd steps\n",
      "insert gradient: -0.0008804441326861611\n",
      "84-th iteration, new layer inserted. now 65 layers\n",
      "[4.80824507e+01 8.64605765e+01 4.77521441e+01 8.55854455e+01\n",
      " 4.77262872e+01 8.56947996e+01 5.65277433e+01 1.61952295e+01\n",
      " 2.03835380e+01 9.02362316e+01 5.22413523e+01 1.56636953e+02\n",
      " 5.29147852e+01 9.43672251e+01 2.23228038e+00 5.18449693e+01\n",
      " 5.91275266e+01 1.48756360e+02 6.56114154e+01 1.48632593e+02\n",
      " 7.17812490e+01 5.46591313e+01 1.15727971e+00 1.02698509e+02\n",
      " 1.11651960e+02 5.45607749e+01 1.36997714e+01 6.35679761e+01\n",
      " 9.93624612e+01 7.65665515e+01 2.08649268e-03 5.26271593e-04\n",
      " 3.69666356e-01 8.18940145e+01 6.59548651e+01 5.25270948e+00\n",
      " 2.61958668e+01 9.28510915e+01 1.08763007e+01 4.60650960e+01\n",
      " 6.54803177e+01 1.35956280e+01 3.50382741e+01 6.88424677e+01\n",
      " 1.54998978e+01 5.30737335e+01 7.93125085e+01 0.00000000e+00\n",
      " 7.10542736e-15 8.24369943e-04 3.96562005e+01 3.87338975e+01\n",
      " 2.81102398e+01 2.43415705e+01 3.63782667e+02 3.31605708e+00\n",
      " 1.23995034e+02 3.25272732e+00 6.16355503e+01 1.33952713e+00\n",
      " 5.02284911e+02 1.40495668e+01 9.86898708e+00 1.51811952e+01\n",
      " 1.11308923e+01]\n",
      "85-th iteration, loss: 0.05468973786409176, 26 gd steps\n",
      "insert gradient: -0.006420869132594738\n",
      "85-th iteration, new layer inserted. now 65 layers\n",
      "[4.77473757e+01 8.57489649e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.67383189e+01 8.49071612e+01 4.85257739e+01 8.61152881e+01\n",
      " 5.66001740e+01 1.70971450e+01 1.91191746e+01 9.06252564e+01\n",
      " 5.20334981e+01 1.55542654e+02 5.34991288e+01 9.36992570e+01\n",
      " 3.13473937e+00 4.87653464e+01 5.83917512e+01 1.48072808e+02\n",
      " 6.60292051e+01 1.46662754e+02 7.31018173e+01 1.64967439e+02\n",
      " 1.11952702e+02 5.57510060e+01 1.16960614e+01 6.79088333e+01\n",
      " 9.56226170e+01 7.60600951e+01 8.81690616e-02 7.03956784e-03\n",
      " 4.51325264e-01 8.12521978e+01 6.48503231e+01 7.65169422e+00\n",
      " 2.72914820e+01 9.21205160e+01 1.27963786e+01 4.55728718e+01\n",
      " 6.14079740e+01 1.57939399e+01 3.53112683e+01 7.38031391e+01\n",
      " 1.49078230e+01 5.06697188e+01 7.25876974e+01 5.98088252e+00\n",
      " 5.03917982e-04 5.98170689e+00 3.37015969e+01 5.00646420e+01\n",
      " 2.73122890e+01 2.39064454e+01 3.63037766e+02 3.91925341e+00\n",
      " 1.28050324e+02 4.29997867e+00 5.94377638e+01 2.23744935e+00\n",
      " 5.00095044e+02 1.50807183e+01 9.71983681e+00 1.46100663e+01\n",
      " 1.11772489e+01]\n",
      "86-th iteration, loss: 0.054619889341434906, 12 gd steps\n",
      "insert gradient: -0.0074955026821087985\n",
      "86-th iteration, new layer inserted. now 67 layers\n",
      "[4.78624186e+01 8.58429589e+01 1.87966928e-01 9.24602179e-02\n",
      " 4.69270406e+01 8.49613552e+01 4.85241004e+01 8.61078952e+01\n",
      " 5.65882649e+01 1.70857831e+01 1.90819032e+01 9.06258448e+01\n",
      " 5.20445570e+01 1.55539796e+02 5.35012571e+01 9.36982874e+01\n",
      " 3.11937780e+00 4.87576372e+01 5.83976285e+01 1.48054895e+02\n",
      " 6.60268314e+01 1.46647930e+02 7.30932181e+01 1.64935540e+02\n",
      " 1.11928142e+02 5.57426222e+01 1.16668149e+01 6.79086022e+01\n",
      " 9.56288849e+01 7.60586948e+01 8.05996946e-02 5.50883482e-03\n",
      " 0.00000000e+00 4.33680869e-19 4.43764606e-01 8.12500416e+01\n",
      " 6.48473749e+01 7.65733271e+00 2.72913362e+01 9.21192688e+01\n",
      " 1.28075635e+01 4.55820468e+01 6.14121624e+01 1.58094273e+01\n",
      " 3.53225541e+01 7.38162357e+01 1.49335851e+01 5.06697568e+01\n",
      " 7.25852107e+01 6.00759536e+00 1.34554707e-03 6.00841903e+00\n",
      " 3.37095759e+01 5.00789201e+01 2.73141798e+01 2.39023391e+01\n",
      " 3.63031890e+02 3.91755688e+00 1.28052393e+02 4.30395861e+00\n",
      " 5.94323540e+01 2.24540701e+00 5.00088895e+02 1.50801363e+01\n",
      " 9.71776374e+00 1.46062992e+01 1.11752019e+01]\n",
      "87-th iteration, loss: 0.05461855141394605, 5 gd steps\n",
      "insert gradient: -0.0013434586662178464\n",
      "87-th iteration, new layer inserted. now 67 layers\n",
      "[4.78626090e+01 8.58430755e+01 1.87772801e-01 9.22169955e-02\n",
      " 4.69270235e+01 8.49605655e+01 4.85214949e+01 8.61071457e+01\n",
      " 5.65884384e+01 1.70862116e+01 1.90821992e+01 9.06261512e+01\n",
      " 5.20454370e+01 1.55540453e+02 5.35022102e+01 9.36989772e+01\n",
      " 3.12108452e+00 4.87582326e+01 5.83987375e+01 1.48055064e+02\n",
      " 6.60277135e+01 1.46648017e+02 7.30939736e+01 1.64935132e+02\n",
      " 1.11928323e+02 5.57429354e+01 1.16677171e+01 6.79093915e+01\n",
      " 9.56311576e+01 7.60600871e+01 8.53619561e-02 6.89573426e-03\n",
      " 4.76262549e-03 1.38689944e-03 4.48527231e-01 8.12514021e+01\n",
      " 6.48487957e+01 7.65889239e+00 2.72930564e+01 9.21197363e+01\n",
      " 1.28090428e+01 4.55826092e+01 6.14126575e+01 1.58101059e+01\n",
      " 3.53230580e+01 7.38165736e+01 1.49340937e+01 5.06697056e+01\n",
      " 7.25851280e+01 6.00815067e+00 1.28812574e-03 6.00897433e+00\n",
      " 3.37096297e+01 5.00791673e+01 2.73140699e+01 2.39022429e+01\n",
      " 3.63031815e+02 3.91750089e+00 1.28052484e+02 4.30399888e+00\n",
      " 5.94322662e+01 2.24554206e+00 5.00088805e+02 1.50800467e+01\n",
      " 9.71764238e+00 1.46062364e+01 1.11749888e+01]\n",
      "88-th iteration, loss: 0.05461758289185661, 5 gd steps\n",
      "insert gradient: -0.003918095493755761\n",
      "88-th iteration, new layer inserted. now 69 layers\n",
      "[4.78627809e+01 8.58431900e+01 1.87601368e-01 9.19717829e-02\n",
      " 4.69270287e+01 8.49598084e+01 4.85189665e+01 8.61064213e+01\n",
      " 5.65886027e+01 1.70866270e+01 1.90824425e+01 9.06264443e+01\n",
      " 5.20462459e+01 1.55541043e+02 5.35029909e+01 9.36995130e+01\n",
      " 3.12248206e+00 4.87586440e+01 5.83994941e+01 1.48054935e+02\n",
      " 6.60280045e+01 1.46647746e+02 7.30938095e+01 1.64934436e+02\n",
      " 1.11927574e+02 5.57425213e+01 1.16660162e+01 6.79088902e+01\n",
      " 9.56299218e+01 7.60591111e+01 8.18332876e-02 5.91868803e-03\n",
      " 1.23403559e-03 4.09798982e-04 0.00000000e+00 5.42101086e-20\n",
      " 4.44998657e-01 8.12504204e+01 6.48475930e+01 7.65790293e+00\n",
      " 2.72916870e+01 9.21192735e+01 1.28079480e+01 4.55823583e+01\n",
      " 6.14123365e+01 1.58100847e+01 3.53228391e+01 7.38166791e+01\n",
      " 1.49341461e+01 5.06695284e+01 7.25849124e+01 6.00857210e+00\n",
      " 1.10444002e-03 6.00939574e+00 3.37095488e+01 5.00793773e+01\n",
      " 2.73139381e+01 2.39021511e+01 3.63031717e+02 3.91742011e+00\n",
      " 1.28052559e+02 4.30403348e+00 5.94321684e+01 2.24568216e+00\n",
      " 5.00088707e+02 1.50799463e+01 9.71751851e+00 1.46061669e+01\n",
      " 1.11747607e+01]\n",
      "89-th iteration, loss: 0.05461588436773504, 7 gd steps\n",
      "insert gradient: -0.0036697355642016515\n",
      "89-th iteration, new layer inserted. now 67 layers\n",
      "[4.78634612e+01 8.58436522e+01 1.87299144e-01 9.13501642e-02\n",
      " 4.69272576e+01 8.49576126e+01 4.85114246e+01 8.61042479e+01\n",
      " 5.65890799e+01 1.70878474e+01 1.90831615e+01 9.06273232e+01\n",
      " 5.20487179e+01 1.55542836e+02 5.35054732e+01 9.37012405e+01\n",
      " 3.12682472e+00 4.87600259e+01 5.84021349e+01 1.48054881e+02\n",
      " 6.60295862e+01 1.46647365e+02 7.30944689e+01 1.64932695e+02\n",
      " 1.11926506e+02 5.57422103e+01 1.16642555e+01 6.79090425e+01\n",
      " 9.56307027e+01 7.60592017e+01 8.18070509e-02 6.00050953e-03\n",
      " 1.20843028e-03 5.73064725e-04 4.44973118e-01 8.12504599e+01\n",
      " 6.48473372e+01 7.65819708e+00 2.72915297e+01 9.21190760e+01\n",
      " 1.28079749e+01 4.55826543e+01 6.14124175e+01 1.58109217e+01\n",
      " 3.53231101e+01 7.38172948e+01 1.49348837e+01 5.06691520e+01\n",
      " 7.25844272e+01 6.01001119e+00 7.07148119e-04 6.01083480e+00\n",
      " 3.37094725e+01 5.00800596e+01 2.73135748e+01 2.39018705e+01\n",
      " 3.63031449e+02 3.91720734e+00 1.28052805e+02 4.30414419e+00\n",
      " 5.94318838e+01 2.24610001e+00 5.00088420e+02 1.50796613e+01\n",
      " 9.71715154e+00 1.46059652e+01 1.11740996e+01]\n",
      "90-th iteration, loss: 0.054607300787010045, 14 gd steps\n",
      "insert gradient: -0.0008785208707625909\n",
      "90-th iteration, new layer inserted. now 67 layers\n",
      "[4.78703948e+01 8.58487891e+01 1.90377752e-01 8.98037375e-02\n",
      " 4.69335710e+01 8.49467830e+01 4.84688804e+01 8.60918169e+01\n",
      " 5.65914916e+01 1.70944273e+01 1.90860436e+01 9.06323076e+01\n",
      " 5.20627288e+01 1.55552369e+02 5.35188181e+01 9.37100801e+01\n",
      " 0.00000000e+00 7.10542736e-15 3.14794925e+00 4.87665559e+01\n",
      " 5.84164617e+01 1.48053607e+02 6.60382467e+01 1.46644585e+02\n",
      " 7.30980921e+01 1.64921823e+02 1.11920063e+02 5.57405420e+01\n",
      " 1.16544189e+01 6.79102753e+01 9.56357403e+01 7.60598803e+01\n",
      " 8.20568785e-02 6.62524681e-03 2.63774316e-03 1.19673553e-03\n",
      " 4.45227407e-01 8.12508242e+01 6.48460015e+01 7.66027128e+00\n",
      " 2.72908738e+01 9.21180132e+01 1.28088351e+01 4.55847132e+01\n",
      " 6.14130245e+01 1.58161934e+01 3.53249159e+01 7.38210947e+01\n",
      " 1.49393686e+01 5.06667932e+01 7.25813627e+01 1.20384373e+01\n",
      " 3.37089516e+01 5.00843251e+01 2.73114857e+01 2.39001707e+01\n",
      " 3.63029734e+02 3.91588715e+00 1.28054276e+02 4.30483037e+00\n",
      " 5.94300858e+01 2.24869680e+00 5.00086598e+02 1.50780059e+01\n",
      " 9.71497861e+00 1.46047162e+01 1.11702197e+01]\n",
      "91-th iteration, loss: 0.05450031255866427, 18 gd steps\n",
      "insert gradient: -0.0006038189390276493\n",
      "91-th iteration, new layer inserted. now 65 layers\n",
      "[4.81260032e+01 8.61691551e+01 4.77195226e+01 8.48790760e+01\n",
      " 4.72153900e+01 8.56536469e+01 5.66839965e+01 1.73116674e+01\n",
      " 1.90667298e+01 9.08098614e+01 5.23464669e+01 1.55433845e+02\n",
      " 5.35415823e+01 9.37400170e+01 2.51503835e-04 2.69764927e-02\n",
      " 3.05044443e+00 4.85406238e+01 5.87731000e+01 1.47535269e+02\n",
      " 6.63583776e+01 1.46314487e+02 7.34033579e+01 1.64243677e+02\n",
      " 1.11917440e+02 5.58786978e+01 1.15753167e+01 6.82263293e+01\n",
      " 9.58368952e+01 7.60896854e+01 1.63469196e-02 3.27892967e-02\n",
      " 5.57229808e-03 2.73471256e-02 3.80490083e-01 8.12568175e+01\n",
      " 6.46651220e+01 7.84729846e+00 2.71510696e+01 9.20104486e+01\n",
      " 1.28643179e+01 4.56643686e+01 6.13041615e+01 1.60713883e+01\n",
      " 3.52692571e+01 7.40461807e+01 1.50016885e+01 5.04005554e+01\n",
      " 7.21907000e+01 1.26197457e+01 3.34811711e+01 5.05200323e+01\n",
      " 2.72083243e+01 2.37999573e+01 3.62926154e+02 3.83156005e+00\n",
      " 1.28244166e+02 4.39927975e+00 5.93008977e+01 2.40285309e+00\n",
      " 4.99971108e+02 1.50981883e+01 9.63510036e+00 1.45800477e+01\n",
      " 1.10803294e+01]\n",
      "92-th iteration, loss: 0.05449960792360092, 5 gd steps\n",
      "insert gradient: -0.005724227553401777\n",
      "92-th iteration, new layer inserted. now 67 layers\n",
      "[4.81259134e+01 8.61692093e+01 4.77195798e+01 8.48793362e+01\n",
      " 4.72159728e+01 8.56536737e+01 5.66838120e+01 1.73115479e+01\n",
      " 1.90664218e+01 9.08097918e+01 5.23463601e+01 1.55433757e+02\n",
      " 5.35414799e+01 9.37400533e+01 5.09673599e-04 2.70127332e-02\n",
      " 3.05070484e+00 4.85405338e+01 5.87730716e+01 1.47534808e+02\n",
      " 6.63580946e+01 1.46314068e+02 7.34028621e+01 1.64243181e+02\n",
      " 1.11916828e+02 5.58782580e+01 1.15735269e+01 6.82256814e+01\n",
      " 9.58348567e+01 7.60883511e+01 1.16766345e-02 3.14550525e-02\n",
      " 9.02016957e-04 2.60128807e-02 0.00000000e+00 3.46944695e-18\n",
      " 3.75819805e-01 8.12554833e+01 6.46635192e+01 7.84597815e+00\n",
      " 2.71492349e+01 9.20098374e+01 1.28628585e+01 4.56638938e+01\n",
      " 6.13035352e+01 1.60710290e+01 3.52687169e+01 7.40461277e+01\n",
      " 1.50013756e+01 5.04003191e+01 7.21903753e+01 1.26199739e+01\n",
      " 3.34809613e+01 5.05202913e+01 2.72083558e+01 2.37999220e+01\n",
      " 3.62926056e+02 3.83153751e+00 1.28244240e+02 4.39934698e+00\n",
      " 5.93007889e+01 2.40293505e+00 4.99971003e+02 1.50982033e+01\n",
      " 9.63508504e+00 1.45799991e+01 1.10803225e+01]\n",
      "93-th iteration, loss: 0.05449958643566907, 4 gd steps\n",
      "insert gradient: -0.0005881691386587962\n",
      "93-th iteration, new layer inserted. now 69 layers\n",
      "[4.81258549e+01 8.61692739e+01 4.77196280e+01 8.48795738e+01\n",
      " 0.00000000e+00 7.10542736e-15 4.72164997e+01 8.56536821e+01\n",
      " 5.66836335e+01 1.73114319e+01 1.90661522e+01 9.08097342e+01\n",
      " 5.23463215e+01 1.55433724e+02 5.35415402e+01 9.37402289e+01\n",
      " 1.02226302e-03 2.71883643e-02 3.05121944e+00 4.85406102e+01\n",
      " 5.87733830e+01 1.47534632e+02 6.63583902e+01 1.46313992e+02\n",
      " 7.34032616e+01 1.64242960e+02 1.11917096e+02 5.58785017e+01\n",
      " 1.15741816e+01 6.82262461e+01 9.58361381e+01 7.60892925e+01\n",
      " 1.49982649e-02 3.23958782e-02 4.22524832e-03 2.69536606e-02\n",
      " 3.32455009e-03 9.40779865e-04 3.79144355e-01 8.12564068e+01\n",
      " 6.46644536e+01 7.84712660e+00 2.71504003e+01 9.20101275e+01\n",
      " 1.28638762e+01 4.56641993e+01 6.13036926e+01 1.60713368e+01\n",
      " 3.52688661e+01 7.40462955e+01 1.50014962e+01 5.04002026e+01\n",
      " 7.21901772e+01 1.26203283e+01 3.34808793e+01 5.05205856e+01\n",
      " 2.72084089e+01 2.37998826e+01 3.62925979e+02 3.83153810e+00\n",
      " 1.28244329e+02 4.39941941e+00 5.93006898e+01 2.40301250e+00\n",
      " 4.99970906e+02 1.50982290e+01 9.63507217e+00 1.45799573e+01\n",
      " 1.10803302e+01]\n",
      "94-th iteration, loss: 0.05449956584404634, 4 gd steps\n",
      "insert gradient: -0.006032004944225398\n",
      "94-th iteration, new layer inserted. now 67 layers\n",
      "[4.81257669e+01 8.61693269e+01 4.77196778e+01 8.48798272e+01\n",
      " 5.69909087e-04 2.53416382e-04 4.72170696e+01 8.56537061e+01\n",
      " 5.66834540e+01 1.73113168e+01 1.90658563e+01 9.08096686e+01\n",
      " 5.23462278e+01 1.55433645e+02 5.35414646e+01 9.37402862e+01\n",
      " 1.31446872e-03 2.72455441e-02 3.05151387e+00 4.85405454e+01\n",
      " 5.87734099e+01 1.47534219e+02 6.63582051e+01 1.46313631e+02\n",
      " 7.34029177e+01 1.64242511e+02 1.11916634e+02 5.58781792e+01\n",
      " 1.15728100e+01 6.82258044e+01 9.58346608e+01 7.60883426e+01\n",
      " 1.16766185e-02 3.14457674e-02 9.03882603e-04 2.60035132e-02\n",
      " 3.75826647e-01 8.12554537e+01 6.46632795e+01 7.84622282e+00\n",
      " 2.71490725e+01 9.20096689e+01 1.28628364e+01 4.56638570e+01\n",
      " 6.13031997e+01 1.60710911e+01 3.52684434e+01 7.40462803e+01\n",
      " 1.50012571e+01 5.03999865e+01 7.21898739e+01 1.26205773e+01\n",
      " 3.34806906e+01 5.05208501e+01 2.72084430e+01 2.37998465e+01\n",
      " 3.62925885e+02 3.83151951e+00 1.28244405e+02 4.39948754e+00\n",
      " 5.93005828e+01 2.40309341e+00 4.99970802e+02 1.50982454e+01\n",
      " 9.63505700e+00 1.45799100e+01 1.10803251e+01]\n",
      "95-th iteration, loss: 0.05449908022448389, 5 gd steps\n",
      "insert gradient: -0.0005686789089381326\n",
      "95-th iteration, new layer inserted. now 69 layers\n",
      "[4.81257059e+01 8.61693885e+01 4.77197178e+01 8.48800590e+01\n",
      " 1.08693319e-03 4.84237788e-04 0.00000000e+00 5.42101086e-20\n",
      " 4.72175871e+01 8.56537131e+01 5.66832808e+01 1.73112064e+01\n",
      " 1.90655971e+01 9.08096141e+01 5.23461960e+01 1.55433617e+02\n",
      " 5.35415357e+01 9.37404691e+01 1.83719162e-03 2.74283635e-02\n",
      " 3.05203861e+00 4.85406306e+01 5.87737417e+01 1.47534062e+02\n",
      " 6.63585382e+01 1.46313577e+02 7.34033752e+01 1.64242308e+02\n",
      " 1.11916959e+02 5.58784681e+01 1.15736248e+01 6.82264474e+01\n",
      " 9.58361540e+01 7.60894298e+01 1.55106098e-02 3.25323473e-02\n",
      " 4.73957492e-03 2.70900443e-02 3.79663740e-01 8.12565219e+01\n",
      " 6.46643757e+01 7.84752815e+00 2.71504289e+01 9.20100166e+01\n",
      " 1.28640126e+01 4.56642126e+01 6.13034078e+01 1.60714422e+01\n",
      " 3.52686376e+01 7.40464627e+01 1.50014069e+01 5.03998780e+01\n",
      " 7.21896841e+01 1.26209396e+01 3.34806168e+01 5.05211466e+01\n",
      " 2.72084968e+01 2.37998068e+01 3.62925810e+02 3.83152166e+00\n",
      " 1.28244496e+02 4.39956041e+00 5.93004842e+01 2.40317037e+00\n",
      " 4.99970705e+02 1.50982716e+01 9.63504413e+00 1.45798686e+01\n",
      " 1.10803333e+01]\n",
      "96-th iteration, loss: 0.05449875697047382, 5 gd steps\n",
      "insert gradient: -0.0034842147828717603\n",
      "96-th iteration, new layer inserted. now 71 layers\n",
      "[4.81256135e+01 8.61694364e+01 4.77197547e+01 8.48803039e+01\n",
      " 1.64228842e-03 7.27244181e-04 5.56209185e-04 2.43006393e-04\n",
      " 4.72181433e+01 8.56537346e+01 5.66831063e+01 1.73110968e+01\n",
      " 1.90653114e+01 9.08095517e+01 5.23461094e+01 1.55433544e+02\n",
      " 5.35414715e+01 9.37405338e+01 2.13980694e-03 2.74928826e-02\n",
      " 3.05234345e+00 4.85405746e+01 5.87737895e+01 1.47533668e+02\n",
      " 6.63583910e+01 1.46313239e+02 7.34030893e+01 1.64241877e+02\n",
      " 1.11916554e+02 5.58781898e+01 1.15724105e+01 6.82260833e+01\n",
      " 9.58348884e+01 7.60886243e+01 1.26956207e-02 3.17266559e-02\n",
      " 1.92497197e-03 2.62842969e-02 0.00000000e+00 3.46944695e-18\n",
      " 3.76849457e-01 8.12557121e+01 6.46633626e+01 7.84678090e+00\n",
      " 2.71492914e+01 9.20096152e+01 1.28631294e+01 4.56639196e+01\n",
      " 6.13029649e+01 1.60712391e+01 3.52682587e+01 7.40464615e+01\n",
      " 1.50011956e+01 5.03996695e+01 7.21893886e+01 1.26211961e+01\n",
      " 3.34804355e+01 5.05214127e+01 2.72085316e+01 2.37997706e+01\n",
      " 3.62925717e+02 3.83150442e+00 1.28244573e+02 4.39962855e+00\n",
      " 5.93003779e+01 2.40325091e+00 4.99970602e+02 1.50982883e+01\n",
      " 9.63502875e+00 1.45798217e+01 1.10803283e+01]\n",
      "97-th iteration, loss: 0.054498688002896256, 4 gd steps\n",
      "insert gradient: -0.0005444992975622377\n",
      "97-th iteration, new layer inserted. now 73 layers\n",
      "[4.81255352e+01 8.61694866e+01 4.77197771e+01 8.48805300e+01\n",
      " 2.15519329e-03 9.50434502e-04 1.07039748e-03 4.65215130e-04\n",
      " 0.00000000e+00 5.42101086e-20 4.72186579e+01 8.56537433e+01\n",
      " 5.66829367e+01 1.73109914e+01 1.90650522e+01 9.08094973e+01\n",
      " 5.23460659e+01 1.55433506e+02 5.35415080e+01 9.37406844e+01\n",
      " 2.60055699e-03 2.76432872e-02 3.05280629e+00 4.85406210e+01\n",
      " 5.87740452e+01 1.47533449e+02 6.63585967e+01 1.46313109e+02\n",
      " 7.34033483e+01 1.64241613e+02 1.11916684e+02 5.58783271e+01\n",
      " 1.15726813e+01 6.82264557e+01 9.58356388e+01 7.60892006e+01\n",
      " 1.47334434e-02 3.23024727e-02 3.96414893e-03 2.68600317e-02\n",
      " 2.04029392e-03 5.75734877e-04 3.78889751e-01 8.12562733e+01\n",
      " 6.46638901e+01 7.84753250e+00 2.71499755e+01 9.20097611e+01\n",
      " 1.28637498e+01 4.56641002e+01 6.13029977e+01 1.60714409e+01\n",
      " 3.52682982e+01 7.40465944e+01 1.50012473e+01 5.03995335e+01\n",
      " 7.21891698e+01 1.26215289e+01 3.34803313e+01 5.05217001e+01\n",
      " 2.72085790e+01 2.37997320e+01 3.62925637e+02 3.83150118e+00\n",
      " 1.28244660e+02 4.39969973e+00 5.93002774e+01 2.40332861e+00\n",
      " 4.99970503e+02 1.50983114e+01 9.63501467e+00 1.45797789e+01\n",
      " 1.10803318e+01]\n",
      "98-th iteration, loss: 0.054498612328452385, 4 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "98-th iteration, new layer inserted. now 73 layers\n",
      "[4.81254343e+01 8.61695256e+01 4.77197916e+01 8.48807606e+01\n",
      " 2.68587518e-03 1.17726482e-03 1.60275579e-03 6.90157441e-04\n",
      " 5.33178868e-04 2.24942311e-04 4.72191911e+01 8.56537598e+01\n",
      " 5.66827677e+01 1.73108884e+01 1.90647798e+01 9.08094387e+01\n",
      " 5.23459904e+01 1.55433443e+02 5.35414639e+01 9.37407641e+01\n",
      " 2.93053069e-03 2.77227985e-02 3.05313848e+00 4.85405827e+01\n",
      " 5.87741296e+01 1.47533086e+02 6.63585118e+01 1.46312807e+02\n",
      " 7.34031563e+01 1.64241211e+02 1.11916371e+02 5.58781204e+01\n",
      " 1.15717206e+01 6.82262163e+01 9.58347123e+01 7.60886272e+01\n",
      " 1.27327070e-02 3.17288215e-02 1.96396158e-03 2.62863134e-02\n",
      " 4.05605996e-05 1.98207436e-06 3.76890027e-01 8.12556938e+01\n",
      " 6.46631357e+01 7.84703632e+00 2.71491438e+01 9.20094517e+01\n",
      " 1.28631188e+01 4.56638867e+01 6.13026352e+01 1.60713062e+01\n",
      " 3.52679898e+01 7.40466157e+01 1.50010797e+01 5.03993369e+01\n",
      " 7.21888869e+01 1.26217973e+01 3.34801620e+01 5.05219691e+01\n",
      " 2.72086145e+01 2.37996954e+01 3.62925547e+02 3.83148623e+00\n",
      " 1.28244739e+02 4.39976815e+00 5.93001722e+01 2.40340830e+00\n",
      " 4.99970401e+02 1.50983286e+01 9.63499901e+00 1.45797328e+01\n",
      " 1.10803270e+01]\n",
      "99-th iteration, loss: 0.05449853664608986, 4 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "99-th iteration, new layer inserted. now 75 layers\n",
      "[4.81253433e+01 8.61695646e+01 4.77197895e+01 8.48809727e+01\n",
      " 3.17585944e-03 1.38453388e-03 2.09481790e-03 8.94597686e-04\n",
      " 1.02645908e-03 4.28441518e-04 4.72196847e+01 8.56537645e+01\n",
      " 5.66826032e+01 1.73107898e+01 1.90645310e+01 9.08093874e+01\n",
      " 5.23459524e+01 1.55433410e+02 5.35415066e+01 9.37409176e+01\n",
      " 3.39674043e-03 2.78760866e-02 3.05360679e+00 4.85406324e+01\n",
      " 5.87743923e+01 1.47532873e+02 6.63587289e+01 1.46312683e+02\n",
      " 7.34034299e+01 1.64240953e+02 1.11916514e+02 5.58782681e+01\n",
      " 1.15720263e+01 6.82266053e+01 9.58355065e+01 7.60892330e+01\n",
      " 1.48730167e-02 3.23340725e-02 4.10564484e-03 2.68914796e-02\n",
      " 2.18337669e-03 6.07146543e-04 3.79032843e-01 8.12562843e+01\n",
      " 6.46636960e+01 7.84781923e+00 2.71498665e+01 9.20096091e+01\n",
      " 1.28637701e+01 4.56640769e+01 6.13026784e+01 1.60715167e+01\n",
      " 3.52680379e+01 7.40467512e+01 1.50011362e+01 5.03992021e+01\n",
      " 7.21886693e+01 1.26221307e+01 3.34800582e+01 5.05222559e+01\n",
      " 2.72086607e+01 2.37996570e+01 3.62925468e+02 3.83148315e+00\n",
      " 6.41224134e+01 0.00000000e+00 6.41224134e+01 4.39983898e+00\n",
      " 5.93000720e+01 2.40348559e+00 4.99970303e+02 1.50983511e+01\n",
      " 9.63498436e+00 1.45796902e+01 1.10803293e+01]\n",
      "0-th iteration, loss: 0.7726973386932242, 18 gd steps\n",
      "insert gradient: -0.48309390362400745\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  81.76523215    0.         3311.49190203]\n",
      "1-th iteration, loss: 0.5761261481875187, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.93070885e+01 1.32546302e+00 0.00000000e+00 1.07362505e+02\n",
      " 3.29121904e+03]\n",
      "2-th iteration, loss: 0.5702315813133456, 24 gd steps\n",
      "insert gradient: -0.24753528326440982\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.776444    106.40584265  160.55594623    0.         3130.8409514 ]\n",
      "3-th iteration, loss: 0.4777851033173392, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  53.29324966  117.51571207  119.87223537  107.39934044  126.91967284\n",
      "    0.         2982.61231167]\n",
      "4-th iteration, loss: 0.3827939607954975, 36 gd steps\n",
      "insert gradient: -0.16577879118472447\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  61.20357917  106.977978     66.66585633    0.           56.13966849\n",
      "   89.93678482   99.26515173   88.58735193 2964.87832345]\n",
      "5-th iteration, loss: 0.36564410180709755, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  68.04805196  108.56419203   50.6661369    28.84217837   46.90524736\n",
      "  105.62587414   94.36794832   99.24988369  219.00463213    0.\n",
      " 2737.55790163]\n",
      "6-th iteration, loss: 0.2917559608520766, 167 gd steps\n",
      "insert gradient: -0.09626785658035815\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  70.56178249  103.48016055   29.71047497   51.56635524   52.84279292\n",
      "  101.91768113   87.74515079  102.22284903  180.06350741  122.00632071\n",
      "  367.35064376    0.         2326.55407714]\n",
      "7-th iteration, loss: 0.26190910001261825, 37 gd steps\n",
      "insert gradient: -0.054497877817218716\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  65.67833212  106.1961307    41.27249631   35.87746837   50.75651446\n",
      "  100.44231477   93.19991989  101.52182531  174.51279902  107.84450706\n",
      "  342.26570576   77.42059315  385.72719951    0.         1928.63599757]\n",
      "8-th iteration, loss: 0.2283480656699234, 133 gd steps\n",
      "insert gradient: -0.09736467850693033\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  64.83889252  111.60835873   53.05501319   19.64257241   52.53388582\n",
      "  101.31441017   93.61257424  102.46185528   92.34495021    0.\n",
      "   71.82385017   92.19685898  352.21449259   90.49019096  358.70322363\n",
      "   84.84787898 1867.61797787]\n",
      "9-th iteration, loss: 0.17742439274591879, 33 gd steps\n",
      "insert gradient: -0.0712123165945622\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  74.4771814   108.3189188    54.23596788    5.95872876   57.93568258\n",
      "   99.76979552   90.55176835  114.9646982    70.24360434   73.33188744\n",
      "   44.17184685   86.49945709  126.90298899    0.          228.42538019\n",
      "   75.39456969  361.62884511   57.42470851 1875.65539434]\n",
      "10-th iteration, loss: 0.14359552708380272, 39 gd steps\n",
      "insert gradient: -0.028974381521524158\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  62.14027499  129.89872248  112.65602473  107.27544686   89.05280201\n",
      "  105.97410614   68.58087242   80.78983069   39.07286198   80.31341886\n",
      "   62.87130863    0.           62.87130863   59.84983666  195.14379451\n",
      "   73.25132959  357.2750582    55.91581485 1878.51565896]\n",
      "11-th iteration, loss: 0.14034140070076204, 24 gd steps\n",
      "insert gradient: -0.00649590672196981\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[6.29514263e+01 1.29253149e+02 1.14055260e+02 1.05296972e+02\n",
      " 0.00000000e+00 1.77635684e-14 8.75508158e+01 1.10323580e+02\n",
      " 6.78833146e+01 8.00923068e+01 4.01550313e+01 7.94649574e+01\n",
      " 6.20497873e+01 4.27650781e+00 5.87031738e+01 7.86197111e+01\n",
      " 1.90130494e+02 7.22562836e+01 3.57716086e+02 5.33603215e+01\n",
      " 1.87623995e+03]\n",
      "12-th iteration, loss: 0.1371106515911838, 43 gd steps\n",
      "insert gradient: -0.021824360680695494\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[  66.99806714  124.29827573  114.23037177  107.77422009   88.33195976\n",
      "  108.03920223   65.45937073   84.50954804   42.63875419   78.616318\n",
      "   52.70345918   25.05819688   32.55008064  127.45135828  181.34956321\n",
      "   73.78685468  359.90728157   48.6777971  1880.02451323]\n",
      "13-th iteration, loss: 0.13553153426199308, 107 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[  65.0348583   126.68599871  113.80414442  109.19993466   87.29068642\n",
      "  110.07272838   66.69785669   83.96738351   41.11141095   80.98823102\n",
      "   52.38426902   28.20619499   31.00975204  123.48604283  184.56223549\n",
      "   73.77007475   59.85506342    0.          299.27531711   48.50095467\n",
      " 1881.41834095]\n",
      "14-th iteration, loss: 0.12420771965035267, 52 gd steps\n",
      "insert gradient: -0.008505873421528514\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  64.78722798  128.82800981  110.88464132  117.58125298   81.4718395\n",
      "  115.99303713   65.88292935   87.34082332   41.28728216   85.96373887\n",
      "   53.02812305   44.79098089   22.46039969   96.67345389  186.82586899\n",
      "   83.20635255   69.47007244   38.13000978  256.45429821   36.97258853\n",
      "  516.38864306    0.         1377.0363815 ]\n",
      "15-th iteration, loss: 0.12237165555912263, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  67.2625822   126.18699301  109.68989446  119.72008683   81.42765354\n",
      "  117.14618971   63.64382935   88.26355522   42.09709604   85.31137654\n",
      "   53.33114897   48.91901272   20.99262853   93.69029957  186.47618902\n",
      "   78.8585105    73.80139742   38.33839842  254.8446621    36.99090404\n",
      "  102.93047646    0.          411.72190583    9.04781851 1376.48090461]\n",
      "16-th iteration, loss: 0.11086250869321859, 72 gd steps\n",
      "insert gradient: -0.021227683441281068\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  65.90163897  115.95735276  113.76938553  112.04109973   90.77322372\n",
      "  111.9296308    68.91410808   95.53704482   44.8899834    81.2728399\n",
      "   49.0544285    77.90357367    9.6304217    84.75998467  139.22299237\n",
      "    0.           69.61149618   41.98070635   69.01852963   73.20345385\n",
      "  257.482914      8.91970531  105.46478972   49.32605756  378.09220615\n",
      "   27.06740635 1360.32305204]\n",
      "17-th iteration, loss: 0.10470843127294749, 999 gd steps\n",
      "insert gradient: -0.008171027720293073\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  66.80491491  116.88510918  114.82748977  107.63637041   92.83162267\n",
      "  112.15580623   67.85755552   98.66782588   45.69432789   78.17593736\n",
      "   48.83854879   85.29488871    7.42970992   75.65848638   84.91822586\n",
      "    0.           42.45911293   14.77102216   85.78466978   43.83665084\n",
      "   54.36146261   89.3226813   363.27036312   60.49959694  380.03115707\n",
      "   23.14374647 1345.52540621]\n",
      "18-th iteration, loss: 0.09939715035105312, 26 gd steps\n",
      "insert gradient: -0.011226211476172808\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[  68.72523316  115.48344278  116.13582537  109.77578321   90.05384598\n",
      "  116.30020057   66.84012011  101.04704416   46.45554282   77.29952223\n",
      "   50.5988945    89.11668441   13.51118414   58.47684475   65.35196609\n",
      "   28.87697067   29.12684743   37.62662935   92.51779342   36.61219774\n",
      "   49.90632091   96.90758152  362.49203651   62.47163407  381.35904318\n",
      "   24.36311272 1336.89548543]\n",
      "19-th iteration, loss: 0.09650413463051633, 31 gd steps\n",
      "insert gradient: -0.010163261744463534\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[  67.30970934  119.81763313  115.79446162  110.41493676   86.86926854\n",
      "  118.84919696   67.74653297  102.37778973   47.96015069   79.31872965\n",
      "   47.41096311   91.63551361   18.84897333   51.37573081   59.02873145\n",
      "   49.82980225   17.95007971   51.17887372   93.47490241   28.22507191\n",
      "   51.09466764   95.35185645  136.67563143    0.          227.79271905\n",
      "   61.86745699  381.74887543   27.61659812 1332.96092447]\n",
      "20-th iteration, loss: 0.09323290809797134, 38 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  66.79661677  119.92209918  116.59371947  110.64147361   86.80328917\n",
      "  117.24766811   68.18354985  104.20978442   47.88153826   78.93085988\n",
      "   46.8507575    88.88808807   22.10361331   46.61433866   57.49809919\n",
      "   67.54856989    7.73650057   68.47346804   91.42184562   22.63391311\n",
      "   53.21717248   91.93398799  130.17322498   18.1301647    27.98962793\n",
      "    0.          195.92739553   61.77679056  380.08844109   31.1849075\n",
      " 1334.89328742]\n",
      "21-th iteration, loss: 0.08987302335396422, 39 gd steps\n",
      "insert gradient: -0.006879042046219527\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[6.56977192e+01 1.18985606e+02 1.19544982e+02 1.08858956e+02\n",
      " 8.42651874e+01 1.19226322e+02 6.82228254e+01 1.04655717e+02\n",
      " 4.92820532e+01 7.88366601e+01 4.49509609e+01 8.21506351e+01\n",
      " 2.64485542e+01 4.86243327e+01 5.39160145e+01 7.94902320e+01\n",
      " 8.04138710e-01 7.35410529e+01 9.43968640e+01 2.46262822e+01\n",
      " 4.88802293e+01 8.85893272e+01 1.23677671e+02 4.58131740e+01\n",
      " 2.06208993e+01 2.60523034e+01 1.76190903e+02 6.86652951e+01\n",
      " 3.74345378e+02 3.21804190e+01 1.33271602e+03]\n",
      "22-th iteration, loss: 0.08950704830881895, 20 gd steps\n",
      "insert gradient: -0.0026749849765173883\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[  66.88573055  120.33072933  119.9299757   108.90678347   83.6449901\n",
      "  118.65148808   68.97338695  105.37547376   49.78423954   78.10096369\n",
      "   44.11265594   82.23986405   27.14909065   48.77781214   54.12953634\n",
      "  153.4122042    95.49994035   25.91844612   47.81178404   88.16066202\n",
      "  123.92745961   48.69472285   19.53906136   26.54299452  175.06612203\n",
      "   69.44760003  373.44825927   30.35249461 1164.73594204    0.\n",
      "  166.39084886]\n",
      "23-th iteration, loss: 0.08877207177774162, 50 gd steps\n",
      "insert gradient: -0.007487680170568964\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  65.55804156  120.3197284   121.15832804  108.54885109   83.40746553\n",
      "  117.81210258   69.48760631  106.47344139   50.03727674   77.61709603\n",
      "   43.44336914   81.50479931   28.25445019   48.31062897   54.81568275\n",
      "  148.31816911   97.5559752    30.52606801   43.96529465   87.58772167\n",
      "  123.70448678   55.83244981   14.73808101   30.96754976  175.25948668\n",
      "   70.04788443  373.50254917   27.307019   1156.58267507    8.55034105\n",
      "  141.56245947    0.           23.59374324]\n",
      "24-th iteration, loss: 0.08662707156871637, 16 gd steps\n",
      "insert gradient: -0.0031892858273844375\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[6.65880393e+01 1.23357101e+02 1.20397907e+02 1.09242172e+02\n",
      " 8.35257458e+01 1.16412900e+02 6.91778255e+01 1.06411484e+02\n",
      " 4.90446217e+01 7.92133552e+01 4.39392864e+01 8.12841248e+01\n",
      " 0.00000000e+00 3.55271368e-15 2.82126446e+01 4.93083283e+01\n",
      " 5.45905009e+01 1.45201881e+02 9.93955463e+01 2.97095951e+01\n",
      " 4.40962457e+01 8.93742307e+01 1.23133063e+02 5.87194106e+01\n",
      " 1.41360745e+01 2.97394504e+01 1.74815707e+02 7.10931044e+01\n",
      " 3.73995479e+02 2.77690774e+01 1.15036693e+03 3.53098830e+00\n",
      " 1.38077572e+02 2.18979361e+01 2.01807000e+01]\n",
      "25-th iteration, loss: 0.086438557811432, 22 gd steps\n",
      "insert gradient: -0.0032608016851268704\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[  66.23221063  122.0112226   120.46401341  109.52469059   83.13630592\n",
      "  116.88328497   69.25852899  106.13807022   49.78841523   77.54030605\n",
      "   43.7037208    81.46045456   28.11064477   49.76360127   55.56153915\n",
      "  144.03812266   99.88173957   28.97318148   43.92495709   89.18200513\n",
      "  123.30441013   59.98554265   14.19819592   29.96421403  148.73631223\n",
      "    0.           24.78938537   71.22931405  374.00440066   28.49177716\n",
      " 1147.92241692    2.84658612  136.95209522   23.95272312   18.97725435]\n",
      "26-th iteration, loss: 0.08532744608746223, 122 gd steps\n",
      "insert gradient: -0.0030777118066244117\n",
      "26-th iteration, new layer inserted. now 37 layers\n",
      "[  67.15679778  125.4194638   120.28949911  110.02902964   82.84404349\n",
      "  114.38081449   70.83770853  107.55888166   51.0286134    77.4228927\n",
      "   43.44172322   81.04692461   31.32884775   46.42338577   55.68140625\n",
      "  136.61686785  111.08625583   24.84204855   41.28146298   85.90182924\n",
      "   80.61942035    0.           40.30971017   42.92985363   10.05261928\n",
      "   47.622853    146.78781027   17.58265471   19.06055305   89.61597236\n",
      "  370.26907542   30.40189736 1128.15781725    5.93307334  136.04481074\n",
      "   24.85983325   21.19675224]\n",
      "27-th iteration, loss: 0.08409005511313246, 52 gd steps\n",
      "insert gradient: -0.004109517595545524\n",
      "27-th iteration, new layer inserted. now 39 layers\n",
      "[  66.4199637   128.43821545  118.80919475  109.97551215   83.13359154\n",
      "  114.02310275   71.86331161  109.29030569   49.98063597   79.84336375\n",
      "   44.46658786   83.25558012   33.47304447   40.74655477   53.25201471\n",
      "  137.4433167   116.71052398   18.6959532    44.1732723    84.45884721\n",
      "   59.17823463   21.11056343   27.01644449   56.99321474    7.93993416\n",
      "   71.56067146  144.29185786   25.10312377   15.22785874   88.20643358\n",
      "  185.75718548    0.          185.75718548   29.61054343 1127.92558777\n",
      "    4.69458557  133.61274089   28.83808013   21.21036056]\n",
      "28-th iteration, loss: 0.08318677293750884, 51 gd steps\n",
      "insert gradient: -0.005022744885959946\n",
      "28-th iteration, new layer inserted. now 39 layers\n",
      "[  65.37299791  126.54760114  117.65055112  108.9642029    84.18573405\n",
      "  112.9880244    72.10495178  111.26773653   48.31173083   83.33438425\n",
      "   43.03869708   85.85648488   36.01783652   33.43620494   52.68654423\n",
      "  137.56169128  122.80226864   14.11093977   42.53576308   84.57915927\n",
      "   58.03847629   29.19278226   18.38909747   78.16435821    5.20036418\n",
      "   69.87626717  139.23250613   33.85265104    7.90320982   97.04807145\n",
      "  184.285755     16.73625743  172.53728231   26.26514487 1136.5053005\n",
      "    4.26002083  124.27078925   29.58425666   19.06822396]\n",
      "29-th iteration, loss: 0.08206862902777905, 142 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "29-th iteration, new layer inserted. now 41 layers\n",
      "[ 67.87569068 124.31579294 115.0667314  108.58642323  87.66926767\n",
      " 108.29711182  72.81510689 111.66831101  48.7339559   81.90820195\n",
      "  45.24376993  88.30454179  41.8234025   26.12455052  48.58497327\n",
      " 134.89595745 137.30574794  13.46053198  29.5894167   94.46876623\n",
      "  57.81826755  54.88331441  13.32350116  83.87372184  12.80712839\n",
      "  41.38422008 126.45190842  63.1144445    2.32190426  92.20976222\n",
      " 179.64839432  32.22301559 168.96867539  13.40029528 229.0894621\n",
      "   0.         916.3578484    9.52016225 121.19827083  27.15990448\n",
      "  23.2058108 ]\n",
      "30-th iteration, loss: 0.0756648747706631, 303 gd steps\n",
      "insert gradient: -0.01000133733793092\n",
      "30-th iteration, new layer inserted. now 39 layers\n",
      "[ 72.14428003 120.43940199 112.21454755 114.43716534  81.23957441\n",
      " 112.97829222  69.16754575 116.77392968  51.10886556  78.49124647\n",
      "  46.76097658  91.5197711   48.48962527  30.81762082  44.2174574\n",
      "  92.70961754 161.29645776 155.12553507  60.15265212  53.54999471\n",
      "  26.79687062  75.68336949  25.39362631  20.50316523 113.98743509\n",
      "  76.27964098  11.79921378  30.71486068 173.4540131   34.11738236\n",
      " 169.95644921  18.89031675 203.85388278  22.78091909 930.42912974\n",
      "   7.68859847 165.18978677  33.44994236  15.4702926 ]\n",
      "31-th iteration, loss: 0.07417636985743484, 94 gd steps\n",
      "insert gradient: -0.0029577889778102805\n",
      "31-th iteration, new layer inserted. now 41 layers\n",
      "[ 74.14779625 118.09171255 112.42682349 114.16590063  82.6019796\n",
      " 114.04576045  69.15630255 117.16047429  51.24023805  78.61084604\n",
      "  47.10798415  92.45605614  50.15951641  31.40057368  41.98051593\n",
      "  94.00262764 160.48583605 150.59397869  64.31639905  59.92880655\n",
      "  25.906917    80.09610518  29.82613246  13.66925383 111.07727326\n",
      "  75.24937434  10.8094288   17.13940206 177.05362937  35.10595773\n",
      " 139.54876456   0.          34.88719114  14.70036051 202.25862046\n",
      "  21.35420147 925.29875954   4.51265077 162.91536969  31.25752617\n",
      "  19.46883527]\n",
      "32-th iteration, loss: 0.0730172257287702, 32 gd steps\n",
      "insert gradient: -0.0033980862394137686\n",
      "32-th iteration, new layer inserted. now 43 layers\n",
      "[ 74.39376527 118.15794285 112.40241999 114.22067405  83.21866859\n",
      " 112.88349182  70.07112119 115.79245723  51.63821556  78.02422577\n",
      "  47.42164566  92.65678597  51.34532922  30.41330876  41.55082998\n",
      "  94.02207722 128.00306384   0.          32.00076596 150.42335202\n",
      "  65.16083871  62.83594983  23.84718268  79.90077451  30.31853195\n",
      "  11.73612384 111.83354795  74.84483301   6.89940753  18.75259761\n",
      " 182.05871937  28.2763576  128.61303525  19.36113643  35.51158143\n",
      "  26.25332955 194.91555236  25.52034546 922.95109842   4.21621715\n",
      " 164.52251684  32.43782726  17.91284618]\n",
      "33-th iteration, loss: 0.07182483847702785, 76 gd steps\n",
      "insert gradient: -0.0034274767306617417\n",
      "33-th iteration, new layer inserted. now 45 layers\n",
      "[ 71.20808802 118.12751949 112.95473297 116.15156534  82.57881627\n",
      " 115.49275405  68.26798832 115.36352979  52.49573833  78.80478432\n",
      "  46.10807567  91.81939101  50.49172208  27.95950345  43.25993291\n",
      "  92.38428624 121.05390794  22.86968395  27.62918199 141.33970597\n",
      "  67.75245944  68.84958663  20.72260101  77.19324197  34.09176533\n",
      "   5.77814822  90.2943068    0.          22.5735767   71.76566233\n",
      "   3.63059784  29.34694397 186.37271016  23.45185672 121.10845707\n",
      "  32.93649906  29.43987196  42.89982639 187.15472495  26.57642708\n",
      " 915.14008474   4.21573425 165.941819    31.01727256  18.35470761]\n",
      "34-th iteration, loss: 0.06987656477634883, 24 gd steps\n",
      "insert gradient: -0.0029940124383855773\n",
      "34-th iteration, new layer inserted. now 43 layers\n",
      "[ 70.07680599 116.1595505  112.27803457 117.53146033  82.4178481\n",
      " 117.17373019  70.2489367  112.05757295  53.63160557  79.45634923\n",
      "  45.99180691  91.21669463  54.5884559   29.37909126  37.87590139\n",
      "  90.52890459 121.82438433  30.58657276  24.51054136 132.58772875\n",
      "  72.84755683  76.55690021  20.00777028  72.88494895  31.96965061\n",
      "   1.37149544  85.39169426  21.75161512  17.28157482 105.68274687\n",
      " 187.22287662  23.55384763 118.11233464  38.01323893  30.22734849\n",
      "  43.4103093  185.60197625  27.469258   912.46051209   4.38045358\n",
      " 165.97735647  29.60375747  19.19877582]\n",
      "35-th iteration, loss: 0.0684181868285556, 331 gd steps\n",
      "insert gradient: -0.0014142289921061561\n",
      "35-th iteration, new layer inserted. now 43 layers\n",
      "[ 66.09091219 116.10471764 111.06100582 124.30269337  80.8601127\n",
      " 119.54935199  71.43218943 113.63343566  53.29961329  80.20119254\n",
      "  46.34844268  92.03224796  55.07538896  32.23385375  33.4186449\n",
      "  91.26880261 122.30974475  41.65145391  25.60997818 105.53508443\n",
      "  81.87032117  87.63546865  20.98747861  63.83061617 110.46831835\n",
      "  31.05276035  16.99327725 102.27313418 115.41614286   0.\n",
      "  76.94409524  24.13612545 108.43301498  49.67960413  30.43433598\n",
      "  43.2461454  187.1632729   26.91065927 906.92706905   5.34397027\n",
      " 168.78607129  31.58136753  19.00614647]\n",
      "36-th iteration, loss: 0.06752370110421954, 64 gd steps\n",
      "insert gradient: -0.0015475149161835408\n",
      "36-th iteration, new layer inserted. now 45 layers\n",
      "[ 64.31273506 117.27325411 110.53890373 124.99663621  79.96221065\n",
      " 121.94303984  71.7797728  113.14381794  53.75163604  80.91013282\n",
      "  45.97622866  92.67778711  56.03762515  36.86838944  28.22652242\n",
      "  91.66398097 123.38726329  45.27867886  27.09191512  92.50269538\n",
      "  84.20617658  90.06912059  25.13062858  52.16939424 107.97473135\n",
      "  40.5569455   15.34835367  94.89103127 122.68922366  18.10832284\n",
      "  58.19507807  26.17686197 103.43887115  68.56976788  22.66922124\n",
      "  51.81028978 185.96773816  23.51235487 537.99576922   0.\n",
      " 358.66384615   6.45699569 177.5497453   33.7739403   18.63801589]\n",
      "37-th iteration, loss: 0.06646596113320322, 59 gd steps\n",
      "insert gradient: -0.0030215266705075935\n",
      "37-th iteration, new layer inserted. now 45 layers\n",
      "[ 64.68378595 115.56839765 111.15510897 124.95260675  80.47465437\n",
      " 121.68795105  70.80792309 113.58174004  54.56767755  81.18355945\n",
      "  45.92420071  91.56594459  55.75508882  38.38191797  27.69695346\n",
      "  91.35987421 123.33021151  47.32491219  26.49986126  91.47213216\n",
      "  85.07539514  90.72739345  25.24925856  48.97369177 108.44316692\n",
      "  42.8506931   14.42137336  94.60414595 121.39152308  21.59793837\n",
      "  57.17252183  24.67439621 106.48901317  73.13354027  20.15832975\n",
      "  54.57228734 184.54533278  24.52177381 518.07345693  15.91384034\n",
      " 355.32215879   4.55584003 177.44390047  34.58056614  20.81127568]\n",
      "38-th iteration, loss: 0.06536133671599101, 48 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 47 layers\n",
      "[ 64.26098103 112.2043928  112.37986065 128.12977265  78.78696375\n",
      " 123.44190487  71.91563418 112.78766988  56.56242191  81.21491223\n",
      "  45.56012871  91.04393775  58.14165974  39.19866527  25.0099265\n",
      "  93.46908212 124.68593063  47.25387317  25.91116516  90.48946077\n",
      "  84.24083394  92.20553245  26.60016427  41.19873805 106.33761624\n",
      "  45.94926132  17.3451552   90.62193428 117.14766783  32.24106945\n",
      "  52.28995938  27.0532446  112.05208537  74.46909179  20.57696245\n",
      "  56.49542811 186.27070391  25.63372198 513.06684579  21.97010863\n",
      "  70.67141683   0.         282.68566734   2.21236305 169.3784633\n",
      "  36.66064036  21.7105237 ]\n",
      "39-th iteration, loss: 0.06454912218292684, 32 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "39-th iteration, new layer inserted. now 49 layers\n",
      "[ 63.63145121 113.34162178 112.73135088 127.53907777  79.31249865\n",
      " 124.55282594  71.85910208 111.85356195  57.07065955  81.65185527\n",
      "  45.42649912  91.68741459  59.12584459  41.04214187  22.82846761\n",
      "  94.35912349 125.52510075  49.52453488  26.10511583  87.67757599\n",
      "  82.41089301  92.93107079  28.27727051  35.15098578 107.77381702\n",
      "  49.99080205  17.54139583  86.137312   117.61089174  39.31888835\n",
      "  48.99010438  28.79734143 111.32286576  75.36913748  20.06375598\n",
      "  56.70851673 185.98086646  26.12897493 128.59572907   0.\n",
      " 385.7871872   26.06173033  68.2220172   12.0384617  279.73329167\n",
      "   2.91896441 162.31244881  35.58149799  23.2483522 ]\n",
      "40-th iteration, loss: 0.06273139713222003, 49 gd steps\n",
      "insert gradient: -0.0038867150098801763\n",
      "40-th iteration, new layer inserted. now 49 layers\n",
      "[ 66.6115512  111.92700231 114.54315885 123.59465345  78.80741429\n",
      " 129.86307733  71.95923644 111.15598613  56.58798713  83.31367656\n",
      "  46.2392056   90.7826158   59.81339496  44.41628828  18.78876961\n",
      "  96.4658399  129.38104269  53.25666141  27.82870571  80.72650693\n",
      "  73.84082137  95.218927    27.07022499  24.42524294 118.49313166\n",
      "  59.69102427  14.95659379  79.27048936 117.63806957  53.24162238\n",
      "  47.25789806  29.01380537 106.89501868  80.8590911   20.97560555\n",
      "  50.81294005 195.28898883  18.29214202 119.92917373  18.16925614\n",
      " 384.29678281  29.51873978  63.6944395   10.99456672 283.54575263\n",
      "   4.11613246 154.69999539  38.88950484  22.30182462]\n",
      "41-th iteration, loss: 0.062458382135035996, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 51 layers\n",
      "[ 65.47957904 111.93627851 114.82755604 123.82932511  78.48927804\n",
      " 128.80195271  71.40378285 111.43977619  57.94766804  83.33920408\n",
      "  45.45052789  90.56929258  60.08329393  45.04702392  18.54613408\n",
      "  96.41598256 129.37635892  53.17348871  28.0064494   80.61116183\n",
      "  73.50462403  95.38211975  27.16426129  24.49743413 119.2590553\n",
      "  60.20676353  14.63365807  78.89600833 117.34930197  53.40422817\n",
      "  47.42857391  29.20534807 106.67279676  81.08739529  20.8793024\n",
      "  50.29266811 194.88131125  17.18867492 119.57742028  19.02431416\n",
      "  96.09881528   0.         288.29644585  29.37731071  63.61254485\n",
      "  10.44958354 283.63765183   4.32352209 154.59247726  39.17342183\n",
      "  22.26539632]\n",
      "42-th iteration, loss: 0.06171377530745031, 35 gd steps\n",
      "insert gradient: -0.0011068585460271053\n",
      "42-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.76345188 111.41753919 115.54895418 120.37896721  78.26312664\n",
      " 131.31986536  71.75811843 111.69385916  57.41393321  83.68196848\n",
      "  45.73876151  91.20326413  59.67479166  49.92223155  14.69614121\n",
      "  97.32003627 130.89229752  55.98040758  27.67577756  77.13084709\n",
      "  72.44061551  96.83597113  28.29885678  20.65276065 121.54037787\n",
      "  67.24289677  12.71937512  72.55296279 118.62005109  56.17634975\n",
      "  47.39653556  30.17084588 103.23294184  84.62958941  23.52395367\n",
      "  43.12005009 150.24379234   0.          50.08126411  12.23145438\n",
      " 117.80162618  22.79303299  92.1677975   10.24071274 282.93340655\n",
      "  23.46956652  68.24933605   4.65000594 284.33718254   4.7950858\n",
      " 152.67063334  40.16791239  21.02766918]\n",
      "43-th iteration, loss: 0.05988322665613241, 70 gd steps\n",
      "insert gradient: -0.0017780650108602239\n",
      "43-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.91498083 112.01513286 114.81892318 117.93965886  76.95137987\n",
      " 137.48197941  70.74577282 112.03377382  58.30822449  83.47657316\n",
      "  46.50706132  91.37080546  58.10381233  67.70357739   6.02231523\n",
      "  99.37243229 137.95233351  59.71323651  25.32122911  72.68703691\n",
      "  62.96984169  98.58818352  27.24579732  16.74405709 128.93778473\n",
      "  80.34009695   9.98080597  58.75784278  94.0747842    0.\n",
      "  31.3582614   53.82998655  45.63847699  28.22375555 103.74911853\n",
      "  86.20027776  26.94567438  30.51153606 145.51436624  23.82746243\n",
      "  43.77210557  17.46196601 122.6343768   17.92036389  86.69913853\n",
      "  13.64589106 274.55572425   7.59109154 360.72817248   9.14203603\n",
      " 156.37053567  34.050473    20.55026562]\n",
      "44-th iteration, loss: 0.05824123212908725, 54 gd steps\n",
      "insert gradient: -0.008198083639464405\n",
      "44-th iteration, new layer inserted. now 53 layers\n",
      "[6.53587779e+01 1.13881101e+02 1.14507934e+02 1.18688564e+02\n",
      " 7.52412097e+01 1.38067009e+02 7.32826961e+01 1.13782304e+02\n",
      " 5.82697283e+01 8.59790254e+01 4.40001686e+01 9.10608048e+01\n",
      " 5.93687785e+01 1.86248143e+02 1.36735864e+02 7.10133198e+01\n",
      " 2.36920023e+01 6.51844679e+01 5.88821656e+01 1.02871989e+02\n",
      " 2.85674779e+01 1.22694968e+01 1.33931160e+02 8.82678052e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.60884668e+01 3.51888259e+01\n",
      " 6.56081714e+01 2.72415693e+01 3.53702663e+01 7.30425516e+01\n",
      " 3.78267080e+01 2.48310254e+01 1.05990736e+02 8.93053180e+01\n",
      " 3.19335156e+01 2.28380699e+01 1.44766932e+02 3.01469591e+01\n",
      " 3.69431949e+01 1.18042380e+01 1.35719803e+02 1.58518410e+01\n",
      " 7.89776130e+01 1.52051368e+01 2.64354326e+02 6.45784068e+00\n",
      " 3.68073948e+02 9.44755944e+00 1.59782161e+02 3.14007938e+01\n",
      " 2.14623364e+01]\n",
      "45-th iteration, loss: 0.056612865857031056, 37 gd steps\n",
      "insert gradient: -0.002161029029665841\n",
      "45-th iteration, new layer inserted. now 51 layers\n",
      "[ 63.26891177 113.88574807 112.52914474 118.57864333  74.83311753\n",
      " 141.3818372   72.60061207 113.36212255  59.35743315  84.27018374\n",
      "  45.79318633  90.59255706  57.28000428 180.29455782 140.01438455\n",
      "  77.40696156  24.48021585  60.37597477  57.36535792 102.74763728\n",
      "  30.47941165  10.85906876 134.82066865  89.27629031  21.02981047\n",
      "  29.79262198  59.2963429   41.82566886  28.31410017  79.48569165\n",
      "  34.91064642  19.37790002 109.45657303  90.97955618  33.21315378\n",
      "  18.85751902 149.31474031  32.25173041  35.49660539   2.30898766\n",
      " 142.77154402  13.41839995  75.00236789  13.78179209 248.37555896\n",
      "  15.09374465 374.57402341   8.98772084 160.33365495  33.12791178\n",
      "  21.42101768]\n",
      "46-th iteration, loss: 0.05612242774982279, 36 gd steps\n",
      "insert gradient: -0.007479650155026942\n",
      "46-th iteration, new layer inserted. now 51 layers\n",
      "[ 63.03889475 115.72144294 112.86972306 118.94247182  71.47930789\n",
      " 142.18098169  72.57523288 116.3446243   59.58787901  84.2965979\n",
      "  44.40442459  91.14124034  57.50014582 178.86238084 140.6660235\n",
      "  79.18031519  25.80516735  55.01162191  58.45653488 102.96986984\n",
      "  32.71298001  11.44512138 133.10027879  87.23817179  22.64347081\n",
      "  26.21211335  58.14912565  48.42686681  25.32897456  81.10932557\n",
      "  33.38846152  17.22933027  84.13892699   0.          28.046309\n",
      "  90.92126229  34.38267185  16.25011055 150.408298    34.20157238\n",
      " 184.87747567  10.03603386  76.21745636  14.42107335 237.30708639\n",
      "  23.98524021 378.19622248  12.23311283 150.04258069  34.48880212\n",
      "  23.82088658]\n",
      "47-th iteration, loss: 0.05554231600642163, 33 gd steps\n",
      "insert gradient: -0.002437231316482412\n",
      "47-th iteration, new layer inserted. now 53 layers\n",
      "[6.55799629e+01 1.13230967e+02 0.00000000e+00 7.10542736e-15\n",
      " 1.10551921e+02 1.18948948e+02 7.56820324e+01 1.42949205e+02\n",
      " 7.20974133e+01 1.11753005e+02 5.77991954e+01 8.60026979e+01\n",
      " 4.65251336e+01 8.97322804e+01 5.53163863e+01 1.79500517e+02\n",
      " 1.42082934e+02 7.98203656e+01 2.57489434e+01 5.40706690e+01\n",
      " 5.80820406e+01 1.03139727e+02 3.25221957e+01 1.16808194e+01\n",
      " 1.33484104e+02 8.74448964e+01 2.35702780e+01 2.47406557e+01\n",
      " 5.83335602e+01 4.96940472e+01 2.52978345e+01 8.07476477e+01\n",
      " 3.34463770e+01 1.58069758e+01 8.14740241e+01 8.03795024e+00\n",
      " 2.53205312e+01 9.05154893e+01 3.40294120e+01 1.55041429e+01\n",
      " 1.50485166e+02 3.51688472e+01 1.87431954e+02 7.99524023e+00\n",
      " 7.65397149e+01 1.47165047e+01 2.34591979e+02 2.50125942e+01\n",
      " 3.78215062e+02 1.40544450e+01 1.48309068e+02 3.45880615e+01\n",
      " 2.46145848e+01]\n",
      "48-th iteration, loss: 0.05544152091075304, 7 gd steps\n",
      "insert gradient: -0.0032619984154237526\n",
      "48-th iteration, new layer inserted. now 53 layers\n",
      "[6.55827546e+01 1.13234554e+02 5.75690510e-03 3.58657525e-03\n",
      " 1.10557678e+02 1.18952419e+02 7.56851808e+01 1.42950862e+02\n",
      " 7.21006145e+01 1.11755855e+02 5.78030271e+01 8.60027836e+01\n",
      " 4.65239350e+01 8.97325319e+01 5.53168718e+01 1.79498998e+02\n",
      " 1.42080158e+02 7.98187123e+01 2.57444352e+01 5.40676614e+01\n",
      " 5.80777833e+01 1.03137360e+02 3.25185298e+01 1.16785697e+01\n",
      " 1.33479792e+02 8.74411969e+01 2.35595301e+01 2.47306717e+01\n",
      " 5.83222147e+01 4.96840801e+01 2.52711184e+01 8.07325536e+01\n",
      " 3.34030497e+01 1.57696252e+01 8.14349357e+01 8.00430055e+00\n",
      " 2.52739892e+01 9.05016558e+01 3.40083720e+01 1.54928760e+01\n",
      " 1.50471321e+02 3.51602486e+01 1.87422733e+02 7.99143455e+00\n",
      " 7.65307548e+01 1.47136241e+01 2.34584156e+02 2.50100777e+01\n",
      " 3.78211659e+02 1.40531552e+01 1.48306519e+02 3.45867413e+01\n",
      " 2.46137904e+01]\n",
      "49-th iteration, loss: 0.05515084813136453, 34 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "49-th iteration, new layer inserted. now 55 layers\n",
      "[6.47728294e+01 1.13866078e+02 9.27943251e-02 3.75953706e-01\n",
      " 1.10888773e+02 1.19503218e+02 7.44879600e+01 1.42528613e+02\n",
      " 7.22593917e+01 1.12622935e+02 5.90023518e+01 8.55189161e+01\n",
      " 4.53627481e+01 9.01849468e+01 5.66485716e+01 1.78884638e+02\n",
      " 7.10698180e+01 0.00000000e+00 7.10698180e+01 8.02175555e+01\n",
      " 2.58700273e+01 5.34376218e+01 5.69137575e+01 1.02694897e+02\n",
      " 3.22362800e+01 1.22190676e+01 1.33499399e+02 8.74894792e+01\n",
      " 2.36446234e+01 2.41193802e+01 5.86097100e+01 5.06321126e+01\n",
      " 2.52554036e+01 8.04525732e+01 3.37595797e+01 1.56871229e+01\n",
      " 8.03779780e+01 1.06153814e+01 2.39538660e+01 9.04854459e+01\n",
      " 3.39758798e+01 1.48567140e+01 1.50603858e+02 3.52226427e+01\n",
      " 1.88428615e+02 7.16227847e+00 7.66095384e+01 1.46375251e+01\n",
      " 2.33281605e+02 2.56263805e+01 3.78114077e+02 1.45995722e+01\n",
      " 1.47517439e+02 3.43524464e+01 2.47076830e+01]\n",
      "50-th iteration, loss: 0.053821343112961446, 26 gd steps\n",
      "insert gradient: -0.0018191457251214185\n",
      "50-th iteration, new layer inserted. now 57 layers\n",
      "[6.52064482e+01 1.11821411e+02 2.06179985e-01 4.47530569e-02\n",
      " 1.09398122e+02 1.20274792e+02 7.23320359e+01 1.46237228e+02\n",
      " 7.42006115e+01 1.07294982e+02 6.18008730e+01 8.56904653e+01\n",
      " 4.32465547e+01 9.10564451e+01 5.95473056e+01 1.27229138e+02\n",
      " 0.00000000e+00 4.24097126e+01 6.74104778e+01 1.53844982e+01\n",
      " 6.17612748e+01 9.00677644e+01 2.95875125e+01 4.35903745e+01\n",
      " 5.65214626e+01 1.05579768e+02 2.66711987e+01 1.25019867e+01\n",
      " 1.38236885e+02 8.86316943e+01 2.27442476e+01 1.78255076e+01\n",
      " 6.22413098e+01 5.75193541e+01 2.48253208e+01 7.97263111e+01\n",
      " 3.45898438e+01 1.32992322e+01 7.65285858e+01 2.15928120e+01\n",
      " 1.90450906e+01 9.27141250e+01 3.38074698e+01 9.09752837e+00\n",
      " 1.52618388e+02 3.77156129e+01 1.94689075e+02 2.62191388e+00\n",
      " 7.68540410e+01 1.30134316e+01 2.27288615e+02 2.93771997e+01\n",
      " 3.79269682e+02 1.61460735e+01 1.44651172e+02 3.33804177e+01\n",
      " 2.54339093e+01]\n",
      "51-th iteration, loss: 0.05339612923120603, 20 gd steps\n",
      "insert gradient: -0.0012969377058091264\n",
      "51-th iteration, new layer inserted. now 55 layers\n",
      "[ 65.13620871 113.01362547 108.63141604 120.14043437  73.45498614\n",
      " 146.7399869   74.47447455 109.31971447  58.16048357  85.50678707\n",
      "  45.80083258  91.07112446  57.40138725 125.96153865   1.2369284\n",
      "  41.1289554   68.95520529  16.50719209  60.98910569  91.2241215\n",
      "  30.84899564  41.47028121  55.68834397 105.59976856  26.56341568\n",
      "  12.48690351 138.17580874  88.68947472  22.73107818  17.41297376\n",
      "  62.64436241  58.29193996  25.00032629  79.85185961  35.05532289\n",
      "  12.73097538  76.29405161  22.9089555   18.27833083  92.92275259\n",
      "  33.44539168   8.7273582  152.72014606  37.69876544 195.45173988\n",
      "   2.88072058  77.22695253  13.10654105 226.53694295  29.83929801\n",
      " 379.07759447  16.20607826 144.46488719  33.28368522  25.20346878]\n",
      "52-th iteration, loss: 0.05258201050628064, 21 gd steps\n",
      "insert gradient: -0.0032172405298828122\n",
      "52-th iteration, new layer inserted. now 57 layers\n",
      "[6.69014431e+01 1.14276746e+02 1.06663853e+02 1.19092297e+02\n",
      " 7.24750392e+01 1.50198452e+02 7.37117282e+01 1.12914221e+02\n",
      " 5.56627195e+01 8.53810506e+01 4.68671080e+01 9.22184897e+01\n",
      " 5.87520854e+01 1.16924524e+02 8.55508624e+00 3.12044855e+01\n",
      " 7.53766446e+01 2.35478177e+01 4.91496573e+01 9.53721952e+01\n",
      " 0.00000000e+00 7.10542736e-15 3.84714628e+01 2.96913801e+01\n",
      " 5.41108953e+01 1.06331397e+02 2.63522127e+01 1.18359792e+01\n",
      " 1.38106025e+02 8.82005947e+01 2.35967508e+01 1.41600308e+01\n",
      " 6.50510272e+01 6.32652962e+01 2.43821998e+01 7.87532174e+01\n",
      " 3.72869889e+01 9.92981877e+00 7.43721953e+01 3.11441946e+01\n",
      " 1.46018834e+01 9.41181139e+01 3.30242483e+01 6.86042599e+00\n",
      " 1.53373603e+02 3.78778734e+01 1.99553661e+02 9.86655194e-01\n",
      " 7.79420538e+01 1.26173366e+01 2.23332598e+02 3.11446536e+01\n",
      " 3.78147132e+02 1.66722238e+01 1.43456193e+02 3.32131126e+01\n",
      " 2.53896232e+01]\n",
      "53-th iteration, loss: 0.05141289610261161, 55 gd steps\n",
      "insert gradient: -0.0016517573269637762\n",
      "53-th iteration, new layer inserted. now 57 layers\n",
      "[6.57413841e+01 1.14472690e+02 1.05003848e+02 1.23310357e+02\n",
      " 0.00000000e+00 7.10542736e-15 7.28385812e+01 1.49238858e+02\n",
      " 7.39776454e+01 1.11004344e+02 5.88076630e+01 8.59321771e+01\n",
      " 4.55400066e+01 9.33456789e+01 5.84214635e+01 1.13585324e+02\n",
      " 1.62662425e+01 2.34131127e+01 7.38653605e+01 3.26199853e+01\n",
      " 4.22064980e+01 9.49430163e+01 4.62207885e+01 2.29397504e+01\n",
      " 5.30048144e+01 1.04893851e+02 2.92606954e+01 8.79925160e+00\n",
      " 1.37104127e+02 8.69767312e+01 2.53659419e+01 1.21017822e+01\n",
      " 6.65797804e+01 6.75960338e+01 2.37847309e+01 7.64419674e+01\n",
      " 4.06731644e+01 6.91912769e+00 7.29945989e+01 3.47476945e+01\n",
      " 1.26710384e+01 9.41061297e+01 3.42854234e+01 6.48678126e+00\n",
      " 1.53669588e+02 3.61454571e+01 2.01461929e+02 1.25165776e+00\n",
      " 7.84520085e+01 1.41350674e+01 2.22655417e+02 3.14081285e+01\n",
      " 3.77810850e+02 1.68362700e+01 1.43281106e+02 3.34890465e+01\n",
      " 2.52508070e+01]\n",
      "54-th iteration, loss: 0.051290683716119834, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "54-th iteration, new layer inserted. now 59 layers\n",
      "[6.52312855e+01 1.14364538e+02 1.05200948e+02 1.23395092e+02\n",
      " 2.17248222e-01 7.67696770e-02 7.30593267e+01 1.49348191e+02\n",
      " 7.41464993e+01 1.11121666e+02 5.88754368e+01 8.59521496e+01\n",
      " 4.55199672e+01 9.32716346e+01 5.82190078e+01 1.13562939e+02\n",
      " 1.65777396e+01 2.34519033e+01 7.39006046e+01 3.28222049e+01\n",
      " 4.22242067e+01 9.49157611e+01 4.62110547e+01 2.29337564e+01\n",
      " 5.29078268e+01 1.04801303e+02 2.93758699e+01 8.85897632e+00\n",
      " 9.14036047e+01 0.00000000e+00 4.57018023e+01 8.69790482e+01\n",
      " 2.54729813e+01 1.21114416e+01 6.66497064e+01 6.76981774e+01\n",
      " 2.38586712e+01 7.64125032e+01 4.06773313e+01 6.84464968e+00\n",
      " 7.29464456e+01 3.47560209e+01 1.25572707e+01 9.40876155e+01\n",
      " 3.43302661e+01 6.50201767e+00 1.53688189e+02 3.61226489e+01\n",
      " 2.01508062e+02 1.28348915e+00 7.84801201e+01 1.41970536e+01\n",
      " 2.22660768e+02 3.14179667e+01 3.77781747e+02 1.68189792e+01\n",
      " 1.43240709e+02 3.34512660e+01 2.51943581e+01]\n",
      "55-th iteration, loss: 0.050877160921118075, 19 gd steps\n",
      "insert gradient: -0.002602404131540555\n",
      "55-th iteration, new layer inserted. now 57 layers\n",
      "[ 64.6293192  114.35738052 105.32821349 123.96679407  73.69211291\n",
      " 149.04413307  73.67208697 111.67628173  59.14694952  86.11191957\n",
      "  45.673133    93.45135064  58.20810326 112.91119246  18.68075055\n",
      "  21.78398272  73.24357545  34.61855447  40.81691227  94.61261997\n",
      "  47.2107102   22.56239854  52.55761698 103.86023845  29.66202999\n",
      "   7.04214535  90.32919205   4.50287988  44.63491804  86.83722905\n",
      "  25.89305804  11.40907094  67.12705114  68.95626227  23.68978014\n",
      "  75.7192034   41.56744001   5.87146335  72.73692128  35.19588876\n",
      "  12.16674242  94.03802087  34.88842739   6.76143457 153.93877605\n",
      "  35.6191092  201.80790018   1.19543914  78.46748947  14.94256774\n",
      " 222.48340698  31.30169711 377.64364016  16.72105789 143.12052159\n",
      "  33.52330291  24.97502845]\n",
      "56-th iteration, loss: 0.05005462794711026, 20 gd steps\n",
      "insert gradient: -0.0034576800333963958\n",
      "56-th iteration, new layer inserted. now 51 layers\n",
      "[ 64.31732539 113.91787373 105.66992256 125.9472752   72.90619851\n",
      " 148.88732754  73.58129637 112.48641643  59.44014318  88.25057761\n",
      "  45.19337691  93.72250231  58.63612539 109.39302384  27.61752667\n",
      "  15.91192235  70.05072657  45.80560382  32.83279107  92.40590612\n",
      "  54.24961477  19.30237479  50.92950728 100.05521806 123.48199233\n",
      "  16.81380356  36.27391721  88.95566342  26.86164296   6.57281092\n",
      "  71.46954247  77.00328683  20.39263249  71.55738442 122.38892984\n",
      "  35.18499313  11.43643448  94.15164663  37.66899558   5.58411936\n",
      " 154.93743554  32.84011109 280.97098483  18.86032153 221.50449623\n",
      "  31.36538928 377.78497419  17.10711908 142.96085724  34.91381242\n",
      "  23.67916333]\n",
      "57-th iteration, loss: 0.04953724895178556, 10 gd steps\n",
      "insert gradient: -0.0012765592109529162\n",
      "57-th iteration, new layer inserted. now 53 layers\n",
      "[6.44003783e+01 1.13966684e+02 1.05738132e+02 1.25993903e+02\n",
      " 7.30010976e+01 1.48929758e+02 0.00000000e+00 1.42108547e-14\n",
      " 7.36728971e+01 1.12538059e+02 5.95020315e+01 8.82643715e+01\n",
      " 4.51960888e+01 9.37247218e+01 5.86328313e+01 1.09388236e+02\n",
      " 2.76052083e+01 1.58856335e+01 7.00278590e+01 4.58038534e+01\n",
      " 3.27901882e+01 9.23568302e+01 5.41018653e+01 1.92417069e+01\n",
      " 5.07568834e+01 9.99532092e+01 1.23390329e+02 1.67896420e+01\n",
      " 3.62289522e+01 8.89444351e+01 2.68436454e+01 6.55343544e+00\n",
      " 7.14491160e+01 7.70007892e+01 2.03594069e+01 7.15279909e+01\n",
      " 1.22334669e+02 3.51363663e+01 1.13208314e+01 9.41209009e+01\n",
      " 3.76462037e+01 5.56512328e+00 1.54914118e+02 3.28277654e+01\n",
      " 2.80970211e+02 1.88615681e+01 2.21505813e+02 3.13704594e+01\n",
      " 3.77785419e+02 1.71097294e+01 1.42955780e+02 3.49129812e+01\n",
      " 2.36755074e+01]\n",
      "58-th iteration, loss: 0.049471767199886475, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "58-th iteration, new layer inserted. now 55 layers\n",
      "[6.46397289e+01 1.14040438e+02 1.05736673e+02 1.26017784e+02\n",
      " 7.30803595e+01 1.48933860e+02 7.43532128e-02 1.92816435e-03\n",
      " 7.37478786e+01 1.12596014e+02 5.95687440e+01 8.82750370e+01\n",
      " 4.51831693e+01 9.37178116e+01 5.86112775e+01 1.09396828e+02\n",
      " 2.77026184e+01 1.58866979e+01 7.00671952e+01 4.59092372e+01\n",
      " 3.28735379e+01 9.23625339e+01 5.40653293e+01 1.92917441e+01\n",
      " 5.06730210e+01 9.98790062e+01 1.23418439e+02 1.69208808e+01\n",
      " 3.63025969e+01 8.90020976e+01 2.69129801e+01 6.55810912e+00\n",
      " 7.15146350e+01 7.70746753e+01 2.04212701e+01 7.15020362e+01\n",
      " 1.22304732e+02 3.50862517e+01 1.11767752e+01 9.41030293e+01\n",
      " 3.77178369e+01 5.59577033e+00 1.54967554e+02 3.28484946e+01\n",
      " 2.81039170e+02 1.89170044e+01 2.21562686e+02 3.14062702e+01\n",
      " 1.88902120e+02 0.00000000e+00 1.88902120e+02 1.71339194e+01\n",
      " 1.42949001e+02 3.49226746e+01 2.36718815e+01]\n",
      "59-th iteration, loss: 0.04910247605090804, 19 gd steps\n",
      "insert gradient: -0.0032643547629568003\n",
      "59-th iteration, new layer inserted. now 55 layers\n",
      "[6.61143746e+01 1.14371354e+02 1.05227388e+02 1.25787710e+02\n",
      " 7.34692789e+01 1.48798424e+02 4.63132712e-01 2.30033782e-03\n",
      " 7.41366791e+01 1.12101964e+02 5.88338388e+01 8.85352098e+01\n",
      " 4.63821869e+01 9.40106383e+01 5.74893645e+01 1.08174587e+02\n",
      " 3.04346503e+01 1.43938625e+01 6.92249753e+01 4.79228926e+01\n",
      " 3.21080432e+01 9.21886168e+01 5.50247925e+01 2.02408723e+01\n",
      " 4.95495470e+01 9.83974285e+01 1.23852033e+02 1.95929035e+01\n",
      " 3.48276640e+01 9.00760218e+01 2.72980078e+01 4.67039653e+00\n",
      " 7.20733653e+01 7.89324895e+01 2.03098042e+01 6.99656288e+01\n",
      " 1.22110650e+02 3.49394991e+01 1.10158824e+01 9.40589132e+01\n",
      " 3.88046621e+01 4.83079275e+00 1.55266776e+02 3.25446588e+01\n",
      " 2.81492877e+02 1.95126504e+01 2.21243492e+02 3.13907789e+01\n",
      " 1.89074302e+02 4.72851955e+00 1.88856140e+02 1.72878875e+01\n",
      " 1.41930751e+02 3.51587402e+01 2.38290429e+01]\n",
      "60-th iteration, loss: 0.04903878872599407, 14 gd steps\n",
      "insert gradient: -0.0021415316859674737\n",
      "60-th iteration, new layer inserted. now 53 layers\n",
      "[ 66.05435442 114.46559192 105.32058435 125.94914189  73.51965974\n",
      " 148.75294602  74.28049417 112.25169774  59.36986418  88.53795399\n",
      "  46.05228196  94.04273916  57.93069651 108.18564391  30.75558862\n",
      "  14.23181516  69.18495939  48.06879717  31.97713688  92.1543344\n",
      "  55.07256237  20.37553814  49.36258295  98.24436337 123.92163227\n",
      "  19.89060102  34.78002596  90.22564043  27.42141697   4.4871281\n",
      "  72.20403898  79.15295206  20.43371456  69.89486119 122.14918306\n",
      "  34.83063722  10.86133349  94.05296155  38.97837799   4.84589913\n",
      " 155.38553315  32.55303256 281.59296486  19.60347966 221.28760337\n",
      "  31.37920247 189.22412658   5.06649604 188.84795999  17.28228741\n",
      " 141.83619195  35.1491973   23.83431619]\n",
      "61-th iteration, loss: 0.049009322936502675, 16 gd steps\n",
      "insert gradient: -0.00042801238979902343\n",
      "61-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.89978151 114.5384282  105.42615244 125.99871583  73.43525695\n",
      " 148.75110772  74.23618316 112.31227021  59.70079436  88.57903655\n",
      "  45.79280569  94.02822992  58.2223509  108.03601855  31.3145722\n",
      "  13.88440638  69.03994293  48.37439399  31.73993724  92.08415911\n",
      "  55.2229201   20.53248863  49.11819559  98.05521163 124.0807904\n",
      "  20.36241848  34.51120398  90.4492989   27.57847153   4.05645011\n",
      "  72.37445433  79.50004266  20.44432072  69.70867885 122.31816605\n",
      "  34.6984052   10.79888913  94.06676049  39.20444047   4.85768121\n",
      " 155.52033292  32.53337601 281.73271063  19.72219928 221.36732059\n",
      "  31.32099596 189.53700553   5.6378938  188.829363    17.25045952\n",
      " 141.65785913  35.17739086  23.93150439]\n",
      "62-th iteration, loss: 0.048983065158333526, 8 gd steps\n",
      "insert gradient: -0.0009396678950882697\n",
      "62-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.89993885 114.53904664 105.42752554 125.99931226  73.43579783\n",
      " 148.75171977  74.2370435  112.31264623  59.70162163  88.57965147\n",
      "  45.79324709  94.02864398  58.22346974 108.03589797  31.31850052\n",
      "  13.88459731  69.04117692  48.37719317  31.741614    92.08526758\n",
      "  55.22627442  20.53394338  49.12006251  98.05624171 124.08359137\n",
      "  20.36555737  34.51020008  90.44922712  27.57377095   4.04955962\n",
      "  72.37026178  79.49656278  20.42698438  69.69653853 122.28800557\n",
      "  34.66722207  10.73685731  94.04768945  39.18295624   4.84753637\n",
      " 155.50102003  32.52328334 281.72340172  19.71868496 221.36183502\n",
      "  31.31860836 189.53585714   5.63880375 188.82618989  17.24851169\n",
      " 141.65524168  35.17638584  23.93117354]\n",
      "63-th iteration, loss: 0.0486672866807989, 49 gd steps\n",
      "insert gradient: -0.0005853690868490365\n",
      "63-th iteration, new layer inserted. now 53 layers\n",
      "[ 66.4945953  114.87278812 105.42094058 126.32530597  72.7816997\n",
      " 148.82636888  75.04458474 111.92612809  59.46523225  89.49436447\n",
      "  45.74320203  94.79038224  58.87850828 104.11805229  37.30373833\n",
      "  11.27744902  65.75840862  52.63181912  29.79618263  92.05651827\n",
      "  58.31123067  22.08848202  45.52504557  95.83882678 125.24897849\n",
      "  25.94946547  30.37237742  93.46540172 102.00504296  81.86685691\n",
      "  19.76355395  68.33248727 121.85276781  34.68320764  10.97394647\n",
      "  93.86530964  40.72004646   3.84785557 155.76630853  32.19715461\n",
      " 282.46338403  20.31141856 165.11850154   0.          55.03950051\n",
      "  30.54530255 193.58501569   9.0111173  186.34019403  16.66611684\n",
      " 139.99159312  34.73650154  23.93383369]\n",
      "64-th iteration, loss: 0.048378917720313816, 41 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "64-th iteration, new layer inserted. now 55 layers\n",
      "[ 66.52666011 115.01290472 106.52805867 124.92902466  73.19861794\n",
      " 147.64147614  74.75977938 112.46964748  60.5813046   89.64208425\n",
      "  45.37913088  93.96403933  59.52695158 104.56676781  39.99913466\n",
      "  10.33602068  65.18205565  53.63256804  28.56942585  91.24081701\n",
      "  58.31716395  23.32518244  44.51615597  96.66306876 125.71783617\n",
      "  28.02426062  28.31798713  95.31082345 101.60364565  82.85217557\n",
      "  19.46796108  68.85464638 121.30766705  33.5337806   11.51371161\n",
      "  93.5875844   40.98675304   5.35730747 155.7699895   31.11928797\n",
      " 284.52090732  19.37309995  81.62421585   0.          81.62421585\n",
      "   7.84642798  48.98178233  33.69972181 196.77307296  11.89837071\n",
      " 181.78418664  13.54289678 141.34312276  35.59284885  22.95076204]\n",
      "65-th iteration, loss: 0.04808366056094933, 39 gd steps\n",
      "insert gradient: -0.00037934941597410777\n",
      "65-th iteration, new layer inserted. now 55 layers\n",
      "[ 64.51389788 114.3191852  106.61826007 125.61317807  74.47480801\n",
      " 144.89284936  75.116685   112.92046695  61.92014875  89.76889608\n",
      "  44.47825393  93.353729    60.01584756 104.13681753  43.52923712\n",
      "   8.55612338  65.21052179  53.5574723   26.9865699   91.5150405\n",
      "  59.94642271  22.33655272  43.46816075  97.44432165 125.48812745\n",
      "  26.64928216  27.68615732  96.73658344 102.56438424  83.54789722\n",
      "  19.05652209  69.52635559 119.73790715  31.03599524  12.98307307\n",
      "  93.59857659  41.39596567   6.5747925  155.55824569  29.90462414\n",
      " 289.49745561  19.53033022  77.96628659  10.00035936  78.16248267\n",
      "  12.97895407  42.06424421  35.30133276 197.89531663  13.53150646\n",
      " 178.111567    13.65622441 143.01585777  34.48974987  22.93917429]\n",
      "66-th iteration, loss: 0.047866833225342846, 19 gd steps\n",
      "insert gradient: -0.006162744709809429\n",
      "66-th iteration, new layer inserted. now 55 layers\n",
      "[ 67.11188909 114.77613795 106.1770812  125.30254563  74.01596151\n",
      " 144.98116169  75.90981075 112.66842921  60.18153028  89.96216577\n",
      "  45.67927752  93.45738454  59.00753842 103.81750967  43.4340595\n",
      "   8.59220971  65.30382772  53.9242728   27.16103038  91.49871214\n",
      "  59.93489356  22.33760911  43.43383325  97.41274992 125.29034219\n",
      "  26.57774589  27.80594699  96.85136906 103.01207935  83.56422159\n",
      "  18.77392405  69.36223285 119.26874401  30.79164935  13.29584016\n",
      "  93.54026106  41.44054223   6.61970501 155.44585817  29.66104057\n",
      " 289.91752689  19.77666391  77.66224243  10.37651955  78.05268181\n",
      "  13.55383144  41.93693313  35.64128966 198.10200387  13.7305526\n",
      " 177.95189096  13.89539329 143.2161331   34.37927171  22.89494919]\n",
      "67-th iteration, loss: 0.04771106343780346, 53 gd steps\n",
      "insert gradient: -0.000830217299199759\n",
      "67-th iteration, new layer inserted. now 57 layers\n",
      "[ 66.87128376 114.94335732 106.15749751 125.68907582  74.63017621\n",
      " 143.89475958  75.98251635 113.18213495  60.03954284  90.15537906\n",
      "  45.91953861  93.19152571  58.70650813 103.40183926  44.12986403\n",
      "   8.444864    65.63828073  54.64447458  26.51927949  91.29422626\n",
      "  60.57248397  22.18850405  43.19074374  97.55861732 124.6569135\n",
      "  25.90558607  28.17200272  97.16870654 103.76551502  83.81624257\n",
      "  18.78489024  69.25080229 117.51437588  29.61953422  14.87646943\n",
      "  93.62880906  42.09032604   6.74022562 155.25404867  28.56752862\n",
      " 219.00946266   0.          73.00315422  20.27170976  75.89594646\n",
      "  11.57827147  76.82407734  15.3255849   40.76765648  36.67277949\n",
      " 198.81709263  14.43741631 177.0201053   14.22100341 143.80912608\n",
      "  33.55636032  22.87449757]\n",
      "68-th iteration, loss: 0.04706896160593492, 36 gd steps\n",
      "insert gradient: -0.0030543968211481677\n",
      "68-th iteration, new layer inserted. now 57 layers\n",
      "[ 64.69519976 113.97886106 106.46650869 126.36987678  75.36604888\n",
      " 141.03621822  78.05730767 114.72411352  60.12774148  90.32202276\n",
      "  46.07163518  92.48474471  58.20251054 102.76103075  46.11792552\n",
      "   7.03881938  66.14067981  56.4058378   24.7553927   91.50838589\n",
      "  64.19560296  21.50725835  40.78813834  97.86139673 120.87795542\n",
      "  24.65620521  29.96020172  97.05855579 105.95936986  83.73769805\n",
      "  19.36665118  68.03450837 110.35734119  31.50962478  19.2163146\n",
      "  93.59093571  43.94116889   6.83903712 152.84974049  25.86782952\n",
      " 219.90741336  10.7945149   69.54095606  23.16950693  71.02875069\n",
      "  13.65200333  73.254237    21.14413432  38.14468049  37.93963108\n",
      " 201.58598978  17.99488744 175.48897088  13.05644644 145.64866838\n",
      "  31.20248007  21.90689217]\n",
      "69-th iteration, loss: 0.046989561183837854, 14 gd steps\n",
      "insert gradient: -0.012043573201814437\n",
      "69-th iteration, new layer inserted. now 59 layers\n",
      "[6.62816127e+01 1.14491300e+02 1.06318337e+02 1.26410681e+02\n",
      " 7.50398903e+01 1.40788413e+02 7.76351625e+01 1.14650934e+02\n",
      " 6.04714205e+01 9.04108981e+01 4.57879667e+01 9.23189775e+01\n",
      " 5.82127816e+01 1.02804338e+02 4.62214727e+01 6.83864212e+00\n",
      " 6.61754163e+01 5.65214754e+01 2.47347363e+01 9.15337398e+01\n",
      " 6.42548307e+01 2.15400944e+01 4.06236304e+01 9.78460834e+01\n",
      " 1.20589345e+02 2.47095325e+01 3.02128056e+01 9.71093079e+01\n",
      " 1.06216658e+02 8.38286206e+01 1.93960600e+01 6.79416677e+01\n",
      " 1.09985736e+02 3.17139044e+01 0.00000000e+00 3.55271368e-15\n",
      " 1.92531809e+01 9.35551332e+01 4.39810706e+01 6.79966557e+00\n",
      " 1.52713272e+02 2.58376309e+01 2.20275023e+02 1.13063245e+01\n",
      " 6.92803304e+01 2.33149866e+01 7.08984164e+01 1.37188490e+01\n",
      " 7.31631491e+01 2.13750433e+01 3.80230461e+01 3.79357466e+01\n",
      " 2.01696680e+02 1.81293041e+01 1.75402194e+02 1.28449342e+01\n",
      " 1.45654575e+02 3.10914343e+01 2.17970780e+01]\n",
      "70-th iteration, loss: 0.046956390238545476, 6 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "70-th iteration, new layer inserted. now 61 layers\n",
      "[6.62793155e+01 1.14490330e+02 1.06317930e+02 1.26410059e+02\n",
      " 7.50397021e+01 1.40788631e+02 7.76362038e+01 1.14650924e+02\n",
      " 6.04710258e+01 9.04108477e+01 4.57875180e+01 9.23183787e+01\n",
      " 5.82118573e+01 1.02804210e+02 4.62229847e+01 6.83987254e+00\n",
      " 6.61766399e+01 5.65224737e+01 2.47358993e+01 9.15341907e+01\n",
      " 6.42559503e+01 2.15415649e+01 4.06250750e+01 9.78469584e+01\n",
      " 1.20591152e+02 2.47119599e+01 3.02174201e+01 9.71115942e+01\n",
      " 1.06224127e+02 8.38342077e+01 1.94131420e+01 6.79528414e+01\n",
      " 1.10015544e+02 3.17386854e+01 3.10045836e-02 2.47810586e-02\n",
      " 1.92841854e+01 9.35652381e+01 4.39917905e+01 6.80418006e+00\n",
      " 1.01814631e+02 0.00000000e+00 5.09073154e+01 2.58414249e+01\n",
      " 2.20280695e+02 1.13100644e+01 6.92828800e+01 2.33169128e+01\n",
      " 7.08997764e+01 1.37200616e+01 7.31641424e+01 2.13766904e+01\n",
      " 3.80239700e+01 3.79363774e+01 2.01697781e+02 1.81297065e+01\n",
      " 1.75402415e+02 1.28446235e+01 1.45654799e+02 3.10913398e+01\n",
      " 2.17969413e+01]\n",
      "71-th iteration, loss: 0.04638190899302238, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "71-th iteration, new layer inserted. now 63 layers\n",
      "[6.60261227e+01 1.14635001e+02 1.06285000e+02 1.26434599e+02\n",
      " 7.54627772e+01 1.40122909e+02 7.77443425e+01 1.14786354e+02\n",
      " 6.10199628e+01 9.10471720e+01 4.56845183e+01 9.15427952e+01\n",
      " 5.76834955e+01 1.02426115e+02 4.75414495e+01 5.98323243e+00\n",
      " 6.64835901e+01 5.73216948e+01 2.40705904e+01 9.13846169e+01\n",
      " 6.47392067e+01 2.21932993e+01 3.95123083e+01 9.75942242e+01\n",
      " 1.18229785e+02 2.59044184e+01 3.09309971e+01 9.68259365e+01\n",
      " 1.07357233e+02 8.45452497e+01 1.94156741e+01 6.68582651e+01\n",
      " 1.06344869e+02 3.38069936e+01 1.13948728e-01 2.07801413e+00\n",
      " 1.94280320e+01 9.30308913e+01 4.35013757e+01 4.63468712e+00\n",
      " 9.95033666e+01 7.25430279e+00 4.84362672e+01 2.80750072e+01\n",
      " 2.22887082e+02 1.46663849e+01 6.66152606e+01 2.43437575e+01\n",
      " 6.91255005e+01 1.40906773e+01 7.22152682e+01 2.33606835e+01\n",
      " 3.70628105e+01 3.80964793e+01 2.03075475e+02 1.90983957e+01\n",
      " 1.16644996e+02 0.00000000e+00 5.83224979e+01 1.08885090e+01\n",
      " 1.45900521e+02 3.07061624e+01 2.10950028e+01]\n",
      "72-th iteration, loss: 0.04461689641935676, 33 gd steps\n",
      "insert gradient: -0.000727121688455371\n",
      "72-th iteration, new layer inserted. now 61 layers\n",
      "[6.43338620e+01 1.13724515e+02 0.00000000e+00 1.42108547e-14\n",
      " 1.05460015e+02 1.26516050e+02 7.55567348e+01 1.39357553e+02\n",
      " 7.95614575e+01 1.16414735e+02 6.14223710e+01 9.18264133e+01\n",
      " 4.61277602e+01 8.94148102e+01 5.69619991e+01 1.01851245e+02\n",
      " 1.19885056e+02 5.90937097e+01 2.32867231e+01 9.15475873e+01\n",
      " 6.49279768e+01 2.87677905e+01 3.34584748e+01 9.68746675e+01\n",
      " 1.11365203e+02 3.60341766e+01 3.01170065e+01 9.55637389e+01\n",
      " 1.08580132e+02 9.18517937e+01 2.04609313e+01 6.08600093e+01\n",
      " 9.82364653e+01 4.29985686e+01 1.34736875e-02 1.12641543e+01\n",
      " 1.55677063e+01 9.22630338e+01 1.45569967e+02 1.91524147e+01\n",
      " 3.74195390e+01 3.67195289e+01 2.24370411e+02 2.66880584e+01\n",
      " 5.94249818e+01 2.94730047e+01 6.51444984e+01 1.11238386e+01\n",
      " 7.18387782e+01 2.97968262e+01 3.42221160e+01 3.51854827e+01\n",
      " 2.03770610e+02 1.64771004e+01 1.13658466e+02 1.61463979e+01\n",
      " 5.22760063e+01 1.24370921e+01 1.43678866e+02 3.31335810e+01\n",
      " 1.68397932e+01]\n",
      "73-th iteration, loss: 0.04451945216561401, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "73-th iteration, new layer inserted. now 63 layers\n",
      "[6.47761264e+01 1.13794906e+02 9.34532649e-02 7.72922075e-04\n",
      " 1.05584681e+02 1.26579650e+02 7.57517577e+01 1.39405424e+02\n",
      " 7.93855296e+01 1.16149596e+02 6.13122685e+01 9.19690054e+01\n",
      " 4.61234257e+01 8.94329642e+01 5.71493718e+01 1.01709358e+02\n",
      " 1.19616124e+02 5.91792011e+01 2.34713655e+01 9.15833654e+01\n",
      " 6.50004832e+01 2.91075580e+01 3.32366946e+01 9.69732900e+01\n",
      " 1.11599583e+02 3.65891364e+01 2.98372050e+01 9.54846371e+01\n",
      " 1.08585110e+02 9.22332041e+01 2.05876033e+01 6.06191852e+01\n",
      " 9.80274710e+01 4.34222807e+01 1.83449108e-02 1.16878663e+01\n",
      " 1.50806032e+01 9.21824047e+01 1.45562743e+02 1.94236533e+01\n",
      " 3.73532365e+01 3.70423139e+01 2.24235319e+02 2.72029028e+01\n",
      " 5.92433631e+01 2.96975144e+01 6.50310682e+01 1.08050625e+01\n",
      " 7.17996710e+01 3.01142220e+01 3.42975467e+01 3.50792938e+01\n",
      " 6.79903419e+01 0.00000000e+00 1.35980684e+02 1.61071912e+01\n",
      " 1.13818936e+02 1.67086660e+01 5.18158291e+01 1.23500640e+01\n",
      " 1.43442802e+02 3.34582612e+01 1.66220802e+01]\n",
      "74-th iteration, loss: 0.044176688460218355, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "74-th iteration, new layer inserted. now 65 layers\n",
      "[6.48206038e+01 1.13731620e+02 1.00616034e-01 4.67838697e-02\n",
      " 1.05522467e+02 1.26121597e+02 7.55944509e+01 1.40194602e+02\n",
      " 7.98764524e+01 1.15712512e+02 6.14388094e+01 9.25736193e+01\n",
      " 4.60088369e+01 8.91569512e+01 5.73978909e+01 1.01256780e+02\n",
      " 1.18727113e+02 5.95303528e+01 2.39191848e+01 9.16842362e+01\n",
      " 6.54272777e+01 3.10809080e+01 3.14578284e+01 9.70930613e+01\n",
      " 1.11686067e+02 3.87807651e+01 2.85428996e+01 9.51813426e+01\n",
      " 1.08270107e+02 9.43250804e+01 2.12624421e+01 5.91978367e+01\n",
      " 9.67307374e+01 4.58428239e+01 2.17170215e-02 1.41083807e+01\n",
      " 1.36547187e+01 9.19918753e+01 1.45076755e+02 2.09073517e+01\n",
      " 3.68556626e+01 3.82996798e+01 2.23891456e+02 3.00917546e+01\n",
      " 5.81239592e+01 3.03409588e+01 6.45073081e+01 9.54615344e+00\n",
      " 7.16379051e+01 3.14565927e+01 3.45410343e+01 3.44437763e+01\n",
      " 6.80372017e+01 1.31719488e+00 9.07999918e+01 0.00000000e+00\n",
      " 4.53999959e+01 1.39742677e+01 1.14281132e+02 1.95396013e+01\n",
      " 4.94466845e+01 1.18307292e+01 1.42195649e+02 3.47178962e+01\n",
      " 1.55646680e+01]\n",
      "75-th iteration, loss: 0.04414171034064339, 15 gd steps\n",
      "insert gradient: -0.0007888669361375875\n",
      "75-th iteration, new layer inserted. now 65 layers\n",
      "[6.48339603e+01 1.13737876e+02 9.49695409e-02 3.57045578e-02\n",
      " 1.05524303e+02 1.26119641e+02 7.56029972e+01 1.40225672e+02\n",
      " 7.98940116e+01 1.15710341e+02 6.14439727e+01 9.25899462e+01\n",
      " 4.60139391e+01 8.91481583e+01 5.74117754e+01 1.01265016e+02\n",
      " 1.18725101e+02 5.95670934e+01 2.39790851e+01 9.17069160e+01\n",
      " 6.54773914e+01 3.12056747e+01 3.14161328e+01 9.71072477e+01\n",
      " 1.11692118e+02 3.88784922e+01 2.85056729e+01 9.51763316e+01\n",
      " 1.08269446e+02 9.44191502e+01 2.12968874e+01 5.91353393e+01\n",
      " 9.66887644e+01 6.01557200e+01 1.35476301e+01 9.19671471e+01\n",
      " 1.45049807e+02 2.09799686e+01 3.68447186e+01 3.83325056e+01\n",
      " 2.23879301e+02 3.02098390e+01 5.80692165e+01 3.03611781e+01\n",
      " 6.44811170e+01 9.48391934e+00 7.16229287e+01 3.15128447e+01\n",
      " 3.45511090e+01 3.44091168e+01 6.80191599e+01 1.33451781e+00\n",
      " 9.08003063e+01 0.00000000e+00 7.10542736e-15 3.83999382e-01\n",
      " 4.54003247e+01 1.38754286e+01 1.14287619e+02 1.96557514e+01\n",
      " 4.93464450e+01 1.18108130e+01 1.42137826e+02 3.47624439e+01\n",
      " 1.55143401e+01]\n",
      "76-th iteration, loss: 0.041699616563579, 39 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "76-th iteration, new layer inserted. now 61 layers\n",
      "[6.35505244e+01 1.13418887e+02 1.06733052e+02 1.21796185e+02\n",
      " 7.07638971e+01 1.45563651e+02 8.10160746e+01 1.16660817e+02\n",
      " 6.27971760e+01 9.43606447e+01 4.62223371e+01 8.34524421e+01\n",
      " 5.78250878e+01 1.02647120e+02 1.09094920e+02 6.72126729e+01\n",
      " 2.76018083e+01 8.82271665e+01 6.55520594e+01 5.42195346e+01\n",
      " 1.57528644e+01 9.85293655e+01 1.10717275e+02 5.87252885e+01\n",
      " 1.75851051e+01 9.42233054e+01 1.03915560e+02 1.17463960e+02\n",
      " 2.66247308e+01 4.08467914e+01 8.72409590e+01 1.11613107e+02\n",
      " 2.59526555e+00 2.66792111e+01 0.00000000e+00 5.33584223e+01\n",
      " 1.39085470e+02 3.14727364e+01 3.72484061e+01 3.23344592e+01\n",
      " 2.23152083e+02 5.03420251e+01 4.35765397e+01 3.74331393e+01\n",
      " 6.36193944e+01 1.88523650e-01 7.20373858e+01 4.91959631e+01\n",
      " 3.04788425e+01 3.25765354e+01 1.50778840e+02 2.45333981e+01\n",
      " 3.98459339e+01 1.94648258e+01 1.09621479e+02 3.13203501e+01\n",
      " 4.25135025e+01 1.79025618e+01 1.34130934e+02 3.13015641e+01\n",
      " 9.51444407e+00]\n",
      "77-th iteration, loss: 0.0404387554826316, 28 gd steps\n",
      "insert gradient: -0.0007756447012198565\n",
      "77-th iteration, new layer inserted. now 57 layers\n",
      "[ 63.39922284 113.06495586 106.76013795 122.22913309  70.6376082\n",
      " 141.96923165  84.77371137 115.64908761  63.31817961  95.15289468\n",
      "  45.07779376  85.06973396  57.01834544 102.22506728 107.57215291\n",
      "  70.74586424  28.28075872  86.77001617  65.26713341  61.78564444\n",
      "  11.5852618   97.60108826 111.3614333   62.89189515  15.7092553\n",
      "  93.17346039 103.98560213 120.31334996  29.52244991  38.54771077\n",
      "  85.93471513 129.16853388  12.03149434  44.83011164 141.89948008\n",
      "  34.29454046  38.07409899  26.98222818 223.89244459  53.81842868\n",
      "  42.14599044  38.13911729 133.78219601  54.50527092  31.36195869\n",
      "  31.25621429 146.17558599  26.53967288  40.04080581  17.34336852\n",
      " 109.74587739  31.74547328  39.98899768  19.283357   135.11943774\n",
      "  30.08339509   9.96611955]\n",
      "78-th iteration, loss: 0.039368050591938306, 31 gd steps\n",
      "insert gradient: -0.0015739002374710012\n",
      "78-th iteration, new layer inserted. now 59 layers\n",
      "[6.38693983e+01 1.12408490e+02 1.05576041e+02 1.21402258e+02\n",
      " 7.07595226e+01 1.37316157e+02 8.83193961e+01 1.16925892e+02\n",
      " 6.12504489e+01 9.59001350e+01 4.61678340e+01 8.29215645e+01\n",
      " 5.63309072e+01 1.02687273e+02 1.05129529e+02 7.51920722e+01\n",
      " 2.90671903e+01 8.41509883e+01 6.41695320e+01 7.20413441e+01\n",
      " 6.61487164e+00 9.67946954e+01 1.12742247e+02 6.81168650e+01\n",
      " 1.26598269e+01 9.14298225e+01 1.04393969e+02 1.23209324e+02\n",
      " 3.11544054e+01 3.46774383e+01 0.00000000e+00 3.55271368e-15\n",
      " 8.64720377e+01 1.25699169e+02 1.54045736e+01 3.24994417e+01\n",
      " 1.46308399e+02 3.56567266e+01 3.99592654e+01 2.01217258e+01\n",
      " 2.26278868e+02 5.60960129e+01 3.94711707e+01 3.82746328e+01\n",
      " 1.32673516e+02 5.95394953e+01 3.10691302e+01 2.86312045e+01\n",
      " 1.43001991e+02 2.97872585e+01 3.93977850e+01 1.56575695e+01\n",
      " 1.11887340e+02 3.21884588e+01 3.71607417e+01 2.24089388e+01\n",
      " 1.36662125e+02 2.82803487e+01 9.99150942e+00]\n",
      "79-th iteration, loss: 0.03814204512982337, 51 gd steps\n",
      "insert gradient: -0.0005524824031856615\n",
      "79-th iteration, new layer inserted. now 59 layers\n",
      "[6.38123917e+01 1.11125000e+02 1.06223220e+02 1.18586612e+02\n",
      " 7.03492661e+01 1.29134233e+02 9.14424641e+01 1.17827267e+02\n",
      " 6.12099685e+01 9.60000094e+01 4.60734733e+01 8.00137097e+01\n",
      " 5.56435621e+01 1.02810093e+02 0.00000000e+00 1.42108547e-14\n",
      " 1.01407089e+02 8.11582748e+01 3.05970248e+01 7.57863859e+01\n",
      " 6.44474351e+01 1.82508465e+02 1.14461455e+02 7.50912161e+01\n",
      " 8.41920337e+00 8.73618726e+01 1.05110233e+02 1.26581880e+02\n",
      " 3.41908661e+01 2.82836895e+01 1.65866847e-01 7.92637283e-03\n",
      " 8.62761755e+01 1.23991830e+02 1.82533100e+01 2.04210232e+01\n",
      " 1.53898987e+02 3.47078977e+01 4.36383570e+01 7.23441280e+00\n",
      " 2.31527400e+02 5.71758000e+01 3.46784267e+01 4.09233955e+01\n",
      " 1.30831427e+02 6.77839274e+01 2.88753769e+01 2.61658364e+01\n",
      " 1.37046711e+02 3.76504376e+01 3.21020655e+01 1.84225747e+01\n",
      " 1.19806808e+02 3.58288184e+01 3.23382375e+01 3.16836389e+01\n",
      " 1.35384624e+02 2.88034321e+01 9.17511818e+00]\n",
      "80-th iteration, loss: 0.03780106695983633, 62 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "80-th iteration, new layer inserted. now 61 layers\n",
      "[6.33243591e+01 1.10330588e+02 1.06482246e+02 1.17834223e+02\n",
      " 7.02103714e+01 1.27417593e+02 9.10334612e+01 1.17621187e+02\n",
      " 6.13794845e+01 9.56369837e+01 4.61193580e+01 7.90375864e+01\n",
      " 5.54789372e+01 1.02063590e+02 2.63855962e-02 3.17625726e-03\n",
      " 1.00853562e+02 8.20200033e+01 3.09734152e+01 7.22884568e+01\n",
      " 6.62835156e+01 1.20414642e+02 0.00000000e+00 6.02073209e+01\n",
      " 1.13827783e+02 7.64351518e+01 8.01150557e+00 8.53561317e+01\n",
      " 1.04662036e+02 1.27500049e+02 3.46706919e+01 2.58166962e+01\n",
      " 5.98659323e-01 1.90938479e-02 8.67089667e+01 1.23975786e+02\n",
      " 1.89530381e+01 1.77132579e+01 1.54982427e+02 3.28008744e+01\n",
      " 4.61848773e+01 3.06652211e+00 2.33513229e+02 5.65295866e+01\n",
      " 3.29385523e+01 4.25635158e+01 1.29603854e+02 6.99422798e+01\n",
      " 2.81473035e+01 2.61946235e+01 1.33856375e+02 4.11338509e+01\n",
      " 2.84357682e+01 2.13399347e+01 1.22499765e+02 3.69009126e+01\n",
      " 3.05811657e+01 3.59045859e+01 1.34371449e+02 3.07515369e+01\n",
      " 8.89813971e+00]\n",
      "81-th iteration, loss: 0.037788912490191874, 7 gd steps\n",
      "insert gradient: -0.0008549664036493388\n",
      "81-th iteration, new layer inserted. now 63 layers\n",
      "[6.33199681e+01 1.10328721e+02 1.06480828e+02 1.17833142e+02\n",
      " 7.02099823e+01 1.27417225e+02 9.10341668e+01 1.17621495e+02\n",
      " 6.13794587e+01 9.56368343e+01 4.61189305e+01 7.90366481e+01\n",
      " 5.54769847e+01 1.02062589e+02 2.58170457e-02 2.12263718e-03\n",
      " 1.00853000e+02 8.20195645e+01 3.09721462e+01 7.22861649e+01\n",
      " 6.62797514e+01 1.20411825e+02 0.00000000e+00 1.42108547e-14\n",
      " 6.88905871e-03 6.02045045e+01 1.13822864e+02 7.64317449e+01\n",
      " 8.00056887e+00 8.53505890e+01 1.04649291e+02 1.27491989e+02\n",
      " 3.46541337e+01 2.58018400e+01 5.71565201e-01 4.50541022e-03\n",
      " 8.66818646e+01 1.23965164e+02 1.89307062e+01 1.77011141e+01\n",
      " 1.54964118e+02 3.27916109e+01 4.61723038e+01 3.05684040e+00\n",
      " 2.33500180e+02 5.65242679e+01 3.29324023e+01 4.25623238e+01\n",
      " 1.29600764e+02 6.99419284e+01 2.81462441e+01 2.61944639e+01\n",
      " 1.33854368e+02 4.11345864e+01 2.84339074e+01 2.13404604e+01\n",
      " 1.22499633e+02 3.69007304e+01 3.05801436e+01 3.59060364e+01\n",
      " 1.34370861e+02 3.07525880e+01 8.89829858e+00]\n",
      "82-th iteration, loss: 0.037531515531793916, 33 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "82-th iteration, new layer inserted. now 61 layers\n",
      "[6.31808199e+01 1.10019290e+02 1.06212290e+02 1.16998294e+02\n",
      " 7.03637492e+01 1.27192051e+02 9.13848676e+01 1.18464687e+02\n",
      " 6.09832009e+01 9.56039438e+01 4.64004382e+01 7.87036976e+01\n",
      " 5.50185230e+01 1.01621327e+02 1.00688638e+02 8.23763606e+01\n",
      " 3.16097436e+01 7.10410265e+01 6.61329622e+01 1.16252021e+02\n",
      " 2.41615120e+00 5.62038284e+01 1.14773988e+02 7.75664271e+01\n",
      " 7.95034167e+00 8.33490554e+01 1.05067042e+02 1.28326284e+02\n",
      " 3.54903834e+01 2.40106648e+01 6.09586827e-01 3.93965028e-02\n",
      " 8.67201408e+01 1.23799992e+02 1.99154130e+01 1.61406451e+01\n",
      " 1.55310075e+02 3.15338212e+01 4.77836199e+01 4.74077614e-03\n",
      " 7.82041203e+01 0.00000000e+00 1.56408241e+02 5.63734732e+01\n",
      " 3.20688268e+01 4.40396875e+01 1.28446188e+02 7.11438492e+01\n",
      " 2.79748523e+01 2.61059584e+01 1.31712968e+02 4.40382187e+01\n",
      " 2.61114444e+01 2.33549021e+01 1.24094842e+02 3.78081972e+01\n",
      " 2.95197815e+01 3.86004302e+01 1.33476700e+02 3.29797551e+01\n",
      " 8.64911601e+00]\n",
      "83-th iteration, loss: 0.03728917008040184, 53 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "83-th iteration, new layer inserted. now 59 layers\n",
      "[ 62.78017743 109.77321532 105.83773182 117.02124347  70.49954836\n",
      " 126.55530026  91.08927333 118.31775108  61.03930288  95.3573439\n",
      "  46.18633344  78.31118468  55.04791277 101.32984954 100.44354902\n",
      "  82.27535864  31.48905935  70.2900218   66.96921602 114.1097421\n",
      "   2.88731196  54.1030249  114.80266705  77.73942288   8.1591092\n",
      "  82.10610694 105.10617884  43.03356631   0.          86.06713262\n",
      "  35.43853887  22.4106329   88.4082471  122.87625984  21.02498967\n",
      "  14.63310701 154.97117655  30.84384403 125.20870611   5.09900341\n",
      " 155.79973938  55.53483575  31.43462146  45.25925687 127.35024195\n",
      "  71.14608991  27.71880489  26.56528966 130.94933575  45.41924134\n",
      "  25.04793904  24.38795805 124.59020367  37.9849919   29.19861161\n",
      "  40.20454992 133.0416261   34.67009864   9.22198442]\n",
      "84-th iteration, loss: 0.03642841929952735, 36 gd steps\n",
      "insert gradient: -0.003583770627329549\n",
      "84-th iteration, new layer inserted. now 59 layers\n",
      "[ 60.57989274 111.23465319 100.66777566 118.41615688  71.47379553\n",
      " 124.42439558  94.68489485 117.85616342  60.34477976  95.28030147\n",
      "  45.64530977  80.35175519  52.47575735  99.47888238 100.37809905\n",
      "  83.81882454  33.07285626  69.14191253  61.66588282 106.18829367\n",
      "   3.08057234  46.75374611 120.45643851  78.75979803   6.90477447\n",
      "  79.70864317 103.38274302  39.8322768    6.01311404  86.59645439\n",
      "  39.85546635  13.44311346  92.03977031 118.19257104  29.41951071\n",
      "   9.56512424 150.8791166   33.86775411 121.68278548  18.26410794\n",
      " 151.03372081  54.53407402  27.86820339  54.13622387 123.23079073\n",
      "  69.93197791  25.06752135  29.85921989 130.22054498  54.16163204\n",
      "  19.14168046  32.59307845 127.70942861  38.9432673   27.31238207\n",
      "  48.40389062 131.21848489  44.85953249  12.14570463]\n",
      "85-th iteration, loss: 0.03570580086635117, 57 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "85-th iteration, new layer inserted. now 59 layers\n",
      "[ 61.56974672 110.03063831 100.67033061 117.68397145  71.20827202\n",
      " 124.23450414  93.91579964 117.50679452  59.45123236  95.82122221\n",
      "  46.56977156  79.01092358  51.15401299  97.82201287  99.91808684\n",
      "  85.48607717  33.82726845  67.65636556  57.60008488 159.03373075\n",
      " 118.78978994  78.934629     7.53070811  80.32185138 101.6226689\n",
      "  41.49506047   5.12466591  89.38961362  41.53924781   5.36156551\n",
      "  98.20059601 115.82619824  30.95577752   7.39661972 100.83800263\n",
      "   0.          50.41900131  31.37900172 120.94508774  19.74343724\n",
      " 151.99708867  55.40172193  26.75794321  54.90839846 121.62678668\n",
      "  68.71317724  24.78927384  29.98120753 129.05347176  55.20984768\n",
      "  17.37634169  32.61866245 127.82998424  39.71424555  26.23442669\n",
      "  49.0205358  130.72311944  42.61308677  11.2506522 ]\n",
      "86-th iteration, loss: 0.034796789017117254, 23 gd steps\n",
      "insert gradient: -0.010101364572950229\n",
      "86-th iteration, new layer inserted. now 61 layers\n",
      "[6.05411377e+01 1.09750397e+02 1.01504775e+02 1.17354894e+02\n",
      " 7.06508855e+01 1.23879376e+02 9.28915210e+01 1.17213848e+02\n",
      " 6.07013155e+01 9.60565806e+01 4.67512160e+01 7.83190819e+01\n",
      " 5.06568612e+01 9.69712146e+01 9.99065506e+01 8.64401818e+01\n",
      " 3.40707454e+01 6.64299446e+01 5.68027992e+01 1.58539352e+02\n",
      " 1.17197263e+02 7.90255066e+01 0.00000000e+00 7.10542736e-15\n",
      " 7.34581820e+00 7.95639157e+01 1.00786301e+02 4.28787178e+01\n",
      " 5.22620756e+00 9.11259820e+01 4.25264105e+01 2.43850939e-01\n",
      " 1.01160019e+02 1.12321641e+02 2.91323366e+01 3.14518493e+00\n",
      " 9.76306157e+01 1.26173765e+01 4.93507909e+01 3.50220675e+01\n",
      " 1.21393046e+02 2.09065475e+01 1.51048185e+02 5.62078474e+01\n",
      " 2.65056733e+01 5.59091062e+01 1.21127995e+02 6.77612474e+01\n",
      " 2.44660041e+01 3.06501287e+01 1.29504545e+02 5.53293488e+01\n",
      " 1.65041720e+01 3.25304864e+01 1.27801749e+02 3.98461020e+01\n",
      " 2.58531346e+01 4.99106155e+01 1.30055806e+02 4.14314515e+01\n",
      " 1.17810730e+01]\n",
      "87-th iteration, loss: 0.03413892235443817, 78 gd steps\n",
      "insert gradient: -0.0003071412388072105\n",
      "87-th iteration, new layer inserted. now 57 layers\n",
      "[ 61.31644386 109.19343272 101.81005372 116.31760488  69.90222006\n",
      " 124.33073708  92.4399069  117.56955659  59.91215193  96.03433233\n",
      "  47.07043172  77.73105378  50.81173948  96.74787962  98.85297662\n",
      "  87.75692545  35.0563331   64.64996977  55.87818586 155.9609142\n",
      " 115.11021852  77.92650152   7.35450289  79.72419332 102.53188383\n",
      "  46.92635042   3.90508883  93.60964071 144.49785988 110.20070241\n",
      " 122.77810657  21.29218533  48.17726074  37.90637779  90.39913962\n",
      "   0.          30.13304654  21.3951575  150.2219798   57.04811253\n",
      "  25.28499922  56.40000479 122.05845652  67.76038079  23.53510493\n",
      "  30.01942703 129.83237475  55.0465278   16.15994391  32.96943567\n",
      " 127.69082626  39.22916473  25.34486743  50.55794242 129.64019822\n",
      "  40.22500599  11.95365271]\n",
      "88-th iteration, loss: 0.03333132136768766, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "88-th iteration, new layer inserted. now 57 layers\n",
      "[ 62.04749027 108.96999748 103.38759318 116.01159851  68.31546059\n",
      " 124.96438377  91.77717659 117.88801063  60.99137567  97.15859444\n",
      "  47.78763898  76.38467909  50.82830087  96.56821876  98.18294055\n",
      "  90.88778781  36.93084588  60.76193336  55.54006241  37.85999128\n",
      "   0.         113.57997385 110.97168573  73.69991128   8.74166653\n",
      "  82.63705664 108.14012024 155.99608458 143.35226423 104.27311575\n",
      " 121.02697022  30.43915044  43.61958609  44.39087712  88.26453508\n",
      "   8.81883972  28.52411836  23.57708539 145.79081419  57.74620938\n",
      "  23.30198165  58.56168034 123.69391051  68.55198125  24.46120001\n",
      "  27.81836616 128.54647331  53.88241741  16.17828975  33.57556101\n",
      " 127.54192128  38.122567    25.1839645   52.51215118 130.07174275\n",
      "  38.15280225  12.18149337]\n",
      "89-th iteration, loss: 0.03325827371175627, 526 gd steps\n",
      "insert gradient: -0.0007401169339636648\n",
      "89-th iteration, new layer inserted. now 57 layers\n",
      "[ 62.15306953 109.04499802 103.27815483 116.03119372  68.34542035\n",
      " 125.09162841  91.93589672 117.94681455  60.92170413  97.18555742\n",
      "  47.75229539  76.33083656  50.93272637  96.64157182  97.96117666\n",
      "  91.03525361  36.92988102  60.62079725  55.85916831 150.82606407\n",
      " 111.13280366  73.44822672   8.9676317   82.8076566  108.15440398\n",
      " 156.02869273 107.50277959   0.          35.83425986 104.07550227\n",
      " 121.0154918   30.75737164  43.47142294  44.61884927  88.16797345\n",
      "   9.18948851  28.63828321  23.80309002 145.39818858  57.76983559\n",
      "  23.18295443  58.68642312 123.92976527  68.59215705  24.4497863\n",
      "  27.76441464 128.62416481  53.8505392   16.24952273  33.70594417\n",
      " 127.56936096  38.04723073  25.06477249  52.54819328 130.12502397\n",
      "  38.02432312  12.11300254]\n",
      "90-th iteration, loss: 0.03189672410618765, 36 gd steps\n",
      "insert gradient: -0.001324776423409779\n",
      "90-th iteration, new layer inserted. now 59 layers\n",
      "[6.25960680e+01 1.09902510e+02 1.01415369e+02 1.16775160e+02\n",
      " 0.00000000e+00 1.42108547e-14 6.71169317e+01 1.23513877e+02\n",
      " 9.52728385e+01 1.16702438e+02 6.05291306e+01 9.82118040e+01\n",
      " 4.93697479e+01 7.47851894e+01 5.04107992e+01 9.69442745e+01\n",
      " 9.52330003e+01 9.93408665e+01 3.93169701e+01 5.03241791e+01\n",
      " 6.43478312e+01 1.34730843e+02 1.11546756e+02 6.40046458e+01\n",
      " 1.37066831e+01 8.32115892e+01 1.08912153e+02 1.51299642e+02\n",
      " 1.03669312e+02 2.20358184e+01 2.98189052e+01 9.65534988e+01\n",
      " 1.16115306e+02 4.89303067e+01 3.97441000e+01 4.57012309e+01\n",
      " 8.44190077e+01 2.10214758e+01 2.77616148e+01 3.49532089e+01\n",
      " 1.36158702e+02 5.86237642e+01 1.87535253e+01 6.38035654e+01\n",
      " 1.27889279e+02 6.97550262e+01 2.37951188e+01 2.69376333e+01\n",
      " 1.26313228e+02 5.01480517e+01 1.62033645e+01 3.95062085e+01\n",
      " 1.27445984e+02 3.58104473e+01 2.53828952e+01 5.50927785e+01\n",
      " 1.31976552e+02 3.55960397e+01 1.30814148e+01]\n",
      "91-th iteration, loss: 0.030717416056550484, 37 gd steps\n",
      "insert gradient: -0.0016589323568873345\n",
      "91-th iteration, new layer inserted. now 59 layers\n",
      "[6.20035138e+01 1.09767914e+02 1.00557654e+02 1.16965132e+02\n",
      " 0.00000000e+00 1.42108547e-14 6.56434387e+01 1.21171737e+02\n",
      " 9.61515625e+01 1.16117826e+02 6.12504623e+01 9.90226001e+01\n",
      " 4.94859231e+01 7.38396358e+01 4.93298493e+01 9.58114527e+01\n",
      " 9.40799803e+01 1.01952683e+02 4.23821253e+01 4.73814786e+01\n",
      " 6.17957169e+01 1.23687012e+02 1.08501319e+02 6.47585277e+01\n",
      " 1.47557791e+01 8.33469639e+01 1.07836148e+02 1.57534453e+02\n",
      " 1.00056004e+02 2.70947617e+01 2.88777542e+01 9.42812707e+01\n",
      " 1.13311590e+02 5.61182918e+01 3.86539596e+01 4.27688366e+01\n",
      " 8.76945223e+01 2.38501589e+01 2.26743540e+01 4.09415179e+01\n",
      " 1.33918513e+02 5.69487201e+01 1.71404515e+01 6.77521022e+01\n",
      " 1.27584143e+02 7.00377343e+01 2.20180354e+01 2.80988408e+01\n",
      " 1.26099149e+02 4.73237810e+01 1.56899063e+01 4.26171750e+01\n",
      " 1.27700658e+02 3.49258174e+01 2.53658753e+01 5.63609619e+01\n",
      " 1.32406631e+02 3.62221607e+01 1.35015777e+01]\n",
      "92-th iteration, loss: 0.02897999458268207, 73 gd steps\n",
      "insert gradient: -0.0017321894312929986\n",
      "92-th iteration, new layer inserted. now 59 layers\n",
      "[6.17838403e+01 1.08418609e+02 1.00986451e+02 1.12941108e+02\n",
      " 0.00000000e+00 1.42108547e-14 6.40020500e+01 1.17369706e+02\n",
      " 9.49245988e+01 1.17476930e+02 6.04744311e+01 9.85303736e+01\n",
      " 4.96007085e+01 7.18376016e+01 4.74610952e+01 9.42169853e+01\n",
      " 9.18090953e+01 1.03372173e+02 4.47992900e+01 4.12906123e+01\n",
      " 6.48065009e+01 1.11923077e+02 1.03829620e+02 6.80828190e+01\n",
      " 1.71842067e+01 7.82648531e+01 9.37668006e+01 1.72312026e+02\n",
      " 1.00084661e+02 4.42667140e+01 1.99479710e+01 8.88969888e+01\n",
      " 1.10959491e+02 7.06990780e+01 3.59679164e+01 3.34597590e+01\n",
      " 9.35206497e+01 3.20838581e+01 1.52392596e+01 5.03017193e+01\n",
      " 1.25813837e+02 5.24616415e+01 1.59732393e+01 7.60866643e+01\n",
      " 1.25274984e+02 6.74942968e+01 1.95212977e+01 3.06904259e+01\n",
      " 1.24118850e+02 4.34045670e+01 1.61291643e+01 4.91385706e+01\n",
      " 1.25094950e+02 3.34953429e+01 2.63339263e+01 5.71618008e+01\n",
      " 1.29986751e+02 3.66468060e+01 1.54314142e+01]\n",
      "93-th iteration, loss: 0.028504273882402054, 36 gd steps\n",
      "insert gradient: -0.001689391110724374\n",
      "93-th iteration, new layer inserted. now 57 layers\n",
      "[ 61.97093401 108.27577313 101.12008712 112.65177835  62.96765988\n",
      " 117.12819779  95.15541433 117.92073662  61.03869377  98.89418383\n",
      "  49.66509148  71.58461958  47.063568    94.07089309  91.36177702\n",
      " 102.70603145  47.09833878  38.10410979  66.0828788  107.02231215\n",
      " 102.98445153  70.84563275  18.97845033  72.54984832  89.06901831\n",
      " 174.62509805 104.20438851  51.60129741  16.49221824  84.58180012\n",
      " 110.64527147  75.00631628  34.93972917  31.25001951  91.59765902\n",
      "  36.17207255  14.88217629  57.47455652 123.63948863  51.60814723\n",
      "  14.73694208  78.3523493  123.91331951  65.58607138  19.52090473\n",
      "  31.52842385 121.6983329   41.35915388  16.60469134  52.77497639\n",
      " 123.24165604  32.95859992  27.53171042  56.10132212 128.32852881\n",
      "  37.4237993   16.53371712]\n",
      "94-th iteration, loss: 0.02800438143513536, 49 gd steps\n",
      "insert gradient: -0.0014314129782704741\n",
      "94-th iteration, new layer inserted. now 57 layers\n",
      "[ 62.11215396 108.5181029  101.60244759 112.63727195  62.73079493\n",
      " 114.52970876  97.19608888 117.02588303  62.379987   100.17999235\n",
      "  50.53740206  71.31778738  45.95232853  93.25540431  91.07636767\n",
      " 102.73003152  52.84669752  32.62776527  67.83372364  95.80582222\n",
      " 104.65537756  74.53590855  22.50961064  60.47137104  86.42908649\n",
      " 175.41164941 105.55701856  61.49949678  15.21744136  75.44515823\n",
      " 111.53590108  82.0313852   35.31298775  24.39957787  88.94159842\n",
      "  46.80418829   9.90162588  75.23022108 118.9763829   48.8876433\n",
      "  13.63000378  81.66521388 123.4558566   58.27203504  19.69495297\n",
      "  31.98429629 119.89298323  40.40970927  17.50431343  58.80373163\n",
      " 121.10483191  31.54918014  30.71383587  51.82053615 127.2131722\n",
      "  40.94299708  17.18325764]\n",
      "95-th iteration, loss: 0.02764025977707056, 64 gd steps\n",
      "insert gradient: -0.000583838631262392\n",
      "95-th iteration, new layer inserted. now 59 layers\n",
      "[6.20377046e+01 1.08268555e+02 0.00000000e+00 1.42108547e-14\n",
      " 9.92422759e+01 1.14662905e+02 6.40776955e+01 1.11815531e+02\n",
      " 1.01024947e+02 1.13393290e+02 6.16198551e+01 1.00693581e+02\n",
      " 5.16617960e+01 7.24080264e+01 4.48724176e+01 9.12249224e+01\n",
      " 9.01476278e+01 1.05781324e+02 5.85113974e+01 3.04756196e+01\n",
      " 6.31213236e+01 9.46731394e+01 1.04629433e+02 7.74365050e+01\n",
      " 2.35879715e+01 5.58781043e+01 8.59214723e+01 1.77286727e+02\n",
      " 1.05774243e+02 6.51484153e+01 1.49320764e+01 7.12824176e+01\n",
      " 1.10842285e+02 8.42712169e+01 3.65769077e+01 1.69784198e+01\n",
      " 9.29309148e+01 5.28130511e+01 6.09743766e+00 8.01335439e+01\n",
      " 1.12701824e+02 4.43889142e+01 1.65844530e+01 8.46903720e+01\n",
      " 1.23276312e+02 5.45535790e+01 2.06882851e+01 3.10690951e+01\n",
      " 1.20290537e+02 3.68194220e+01 1.95657692e+01 5.85202003e+01\n",
      " 1.18437478e+02 3.54739724e+01 3.26614970e+01 5.02753096e+01\n",
      " 1.27258312e+02 4.56685780e+01 1.62765007e+01]\n",
      "96-th iteration, loss: 0.027350177613993634, 45 gd steps\n",
      "insert gradient: -0.0008792879745317066\n",
      "96-th iteration, new layer inserted. now 59 layers\n",
      "[6.30256897e+01 1.08625384e+02 1.46298024e-01 6.18127650e-04\n",
      " 1.00061230e+02 1.13871525e+02 6.37122352e+01 1.12727678e+02\n",
      " 9.95671631e+01 1.14005045e+02 6.10057404e+01 1.01251518e+02\n",
      " 5.20970072e+01 7.21223597e+01 4.46352137e+01 9.16290688e+01\n",
      " 8.95080546e+01 1.07644721e+02 5.92686528e+01 3.20789716e+01\n",
      " 5.92704798e+01 9.47308836e+01 1.04692629e+02 7.93849351e+01\n",
      " 2.35115776e+01 5.37177610e+01 8.64975159e+01 1.76672555e+02\n",
      " 1.05487706e+02 6.54863593e+01 1.46786161e+01 7.09197588e+01\n",
      " 1.11230863e+02 8.51829862e+01 3.61666373e+01 1.40285921e+01\n",
      " 9.68631581e+01 5.15653670e+01 4.94480600e+00 8.06376032e+01\n",
      " 1.09913075e+02 4.57406330e+01 1.83727210e+01 8.50035632e+01\n",
      " 1.23621389e+02 5.41668026e+01 2.18690837e+01 2.85576840e+01\n",
      " 1.17954521e+02 3.41816441e+01 2.19532597e+01 5.97201849e+01\n",
      " 1.19142268e+02 3.93507723e+01 3.34415511e+01 5.09721983e+01\n",
      " 1.29063616e+02 4.85923518e+01 1.83319006e+01]\n",
      "97-th iteration, loss: 0.027201198790896365, 48 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "97-th iteration, new layer inserted. now 59 layers\n",
      "[ 61.83087583 108.23834255 100.89776009 113.69961625  63.72846124\n",
      " 112.78754786  99.72716584 114.45302049  61.83855972 101.28869014\n",
      "  52.21682022  72.22360974  44.42615886  91.63551444  89.13529384\n",
      " 109.14273536  60.06160288  32.70079641  58.14054276  95.13690367\n",
      " 104.65988363  80.61752939  23.66500589  52.41942243  86.55973984\n",
      " 176.37101517 105.54205594  65.99887987  14.63904463  70.56913369\n",
      " 110.93793873  85.72587192  35.16907008  12.51461094  99.9362261\n",
      "  50.5157732    4.62132075  80.84625385 108.63858306  47.04774683\n",
      "  18.94372225  84.86598441  82.41710639   0.          41.2085532\n",
      "  53.07769377  21.99181839  27.48912581 116.91540124  34.39502578\n",
      "  23.43738998  60.23127226 119.07612995  41.74616963  33.08584131\n",
      "  50.32894897 129.29077532  49.46043495  18.57500441]\n",
      "98-th iteration, loss: 0.027053134033867988, 34 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "98-th iteration, new layer inserted. now 61 layers\n",
      "[ 61.92832231 108.03510893 101.00072691 113.40329938  63.68887236\n",
      " 112.89559432  99.80655769 114.69343769  61.74089431 101.42243111\n",
      "  52.38663632  72.33966463  44.21928233  91.59437162  88.75758854\n",
      " 110.41582686  60.47234671  33.14462543  56.95352513  95.2727565\n",
      " 104.54722083  81.58178545  23.7065543   51.31209132  86.75530668\n",
      " 176.35818288 105.38606535  66.54779564  14.60126294  70.10812167\n",
      "  73.84866802   0.          36.92433401  86.29516389  34.05196746\n",
      "  11.20196153 102.43969084  49.63127863   4.35930226  80.93250043\n",
      " 108.11186782  48.03228784  19.2979783   84.39445412  80.50084396\n",
      "   4.72525101  39.32881514  52.32396656  21.84201515  26.91612696\n",
      " 116.8938796   36.17957436  23.96807432  59.87905232 118.90057329\n",
      "  43.63377609  32.99318453  49.91632636 129.2858466   50.19012773\n",
      "  18.80236833]\n",
      "99-th iteration, loss: 0.026929082751958457, 20 gd steps\n",
      "insert gradient: -0.00018753695075955683\n",
      "99-th iteration, new layer inserted. now 63 layers\n",
      "[6.15020180e+01 1.07910146e+02 0.00000000e+00 1.42108547e-14\n",
      " 1.01196922e+02 1.13174446e+02 6.37508194e+01 1.12892763e+02\n",
      " 9.95177771e+01 1.14920884e+02 6.20134647e+01 1.01511395e+02\n",
      " 5.24857947e+01 7.24437955e+01 4.40288686e+01 9.14775194e+01\n",
      " 8.83737284e+01 1.11715815e+02 6.06992929e+01 3.38270896e+01\n",
      " 5.57304223e+01 9.52878735e+01 1.04606388e+02 8.24775732e+01\n",
      " 2.35966530e+01 5.02271788e+01 8.69357753e+01 1.76371521e+02\n",
      " 1.05226749e+02 6.71502525e+01 1.46402690e+01 6.93899403e+01\n",
      " 7.26982332e+01 2.92424263e+00 3.57054343e+01 8.67940724e+01\n",
      " 3.27496529e+01 9.96817120e+00 1.04435016e+02 4.90729465e+01\n",
      " 4.40702911e+00 8.10946262e+01 1.08396703e+02 4.89850112e+01\n",
      " 1.94672748e+01 8.38752188e+01 7.86282876e+01 7.52051845e+00\n",
      " 3.76922092e+01 5.22579491e+01 2.15673853e+01 2.65816698e+01\n",
      " 1.17556970e+02 3.84179997e+01 2.36660557e+01 5.92114702e+01\n",
      " 1.19097147e+02 4.54368854e+01 3.26481318e+01 4.95768882e+01\n",
      " 1.29611002e+02 5.08481101e+01 1.87790836e+01]\n",
      "0-th iteration, loss: 0.7704810422680824, 18 gd steps\n",
      "insert gradient: -0.5089149502346435\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  82.55280887    0.         3343.38875909]\n",
      "1-th iteration, loss: 0.5718283527186055, 13 gd steps\n",
      "insert gradient: -0.2364127948054859\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  52.19964602  108.92218374  162.12854749    0.         3161.50667609]\n",
      "2-th iteration, loss: 0.4902935830533684, 16 gd steps\n",
      "insert gradient: -0.2783222052399349\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  75.32244981   95.78520156   60.73459356    0.           49.48744661\n",
      "  122.48908096 3139.09006405]\n",
      "3-th iteration, loss: 0.4398048327992872, 17 gd steps\n",
      "insert gradient: -0.15016824181056496\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  74.37913483  103.40316449   56.33939367   27.12245336   40.56254496\n",
      "   94.22640576 3141.70824807]\n",
      "4-th iteration, loss: 0.41929808382551537, 107 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  72.75088162  124.61643232   55.87003371   75.97122657   27.26686039\n",
      "   84.27999383  177.63830124    0.         2931.03197043]\n",
      "5-th iteration, loss: 0.3757909102179485, 31 gd steps\n",
      "insert gradient: -0.0344786636307283\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  66.46951708  128.42938125   56.45104348   73.27251911   25.7884192\n",
      "   79.07700689  138.63017663   83.40372444  324.78729413    0.\n",
      " 2598.29835306]\n",
      "6-th iteration, loss: 0.3680198349871773, 23 gd steps\n",
      "insert gradient: -0.12517891330342504\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  64.93984855  125.8932767    58.44212062   76.77162207   24.00871161\n",
      "   81.04171011  124.68284214  108.15658633  145.98819594    0.\n",
      "  175.18583513   14.94578705 2592.36023918]\n",
      "7-th iteration, loss: 0.3506812676991443, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  54.657058      6.51043646    0.          110.6774199    69.29474146\n",
      "   77.05917041   21.57572487   78.19807682  117.84460728  108.29975766\n",
      "  129.97123865   37.69703179  169.2119366    17.45672699 2597.4266685 ]\n",
      "8-th iteration, loss: 0.2097211978293812, 36 gd steps\n",
      "insert gradient: -0.06224410760661645\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  47.29183196  146.17135977   40.36309307   82.31847106   49.17840236\n",
      "  108.81610444  102.34005334  120.69511967   96.53161415  112.37114601\n",
      "  111.15002928   76.130453    434.76823637    0.         2173.84118187]\n",
      "9-th iteration, loss: 0.15549762661020466, 153 gd steps\n",
      "insert gradient: -0.022296343028364096\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  69.60995774   97.29209402   45.92671519   79.31611032   56.83827245\n",
      "  110.61847398  103.4944756    95.29045799  106.81589512   91.6583976\n",
      "  104.19519545   96.18701113  122.33305472    0.          269.13272038\n",
      "  105.75926527 2164.22905181]\n",
      "10-th iteration, loss: 0.13323146307047554, 156 gd steps\n",
      "insert gradient: -0.022195429148237588\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  62.37685746   99.81202547   47.4190693    79.36622169   52.76538377\n",
      "  130.883026     94.39311314  106.25578978  101.08647389   97.34492887\n",
      "  102.81789669   93.86291791  111.53987675   51.10098162   45.81263368\n",
      "    0.          167.97965684  178.87294766 2158.29416664]\n",
      "11-th iteration, loss: 0.12150246242387557, 90 gd steps\n",
      "insert gradient: -0.01605830453572988\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  58.07586113   97.52213313   49.26845237   83.95524302   54.53009941\n",
      "  111.36910992   97.32822993  107.30849796  101.56601668   99.75986361\n",
      "  102.44584621   94.41499479  105.62813546   80.25490638   13.89609669\n",
      "   58.0336008   165.90220385  173.87537576 2137.46894485]\n",
      "12-th iteration, loss: 0.11938921253330467, 77 gd steps\n",
      "insert gradient: -0.00921005224988811\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  57.00183565   97.03839225   49.93531134   84.46409549   55.94626884\n",
      "  110.23950347   98.164126    106.52965202  101.54832864  100.71366001\n",
      "  102.30914499   95.96280759  105.85178716   81.41766183   13.18486095\n",
      "   58.47717489  165.53621184  104.43599969    0.           74.59714264\n",
      " 2113.89011731]\n",
      "13-th iteration, loss: 0.11856003420727318, 16 gd steps\n",
      "insert gradient: -0.00923969374051364\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  57.36480805   95.10386326   50.58096412   86.03271993   55.10446312\n",
      "  108.22775382   98.45753568  107.84791949  100.67371904  102.93769231\n",
      "  101.63634748   98.61084766  104.57842758   86.66132497   11.64922471\n",
      "   59.28060066  120.90087182    0.           45.33782693   92.45605448\n",
      "    7.29651691   62.57575779 2113.67718134]\n",
      "14-th iteration, loss: 0.11417167150267234, 29 gd steps\n",
      "insert gradient: -0.020625576183710786\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  55.86537984   94.44573455   51.92378401   95.79325724   55.75212394\n",
      "  102.36639582  102.46620206  104.16843173  102.8042792   105.10098816\n",
      "  102.45817128  101.02548521  104.01396051   97.35830721    7.79971675\n",
      "   49.41461749  112.61535724   31.69342788   30.81599235   83.81302909\n",
      "   15.39380892   60.74768869 2108.44334494]\n",
      "15-th iteration, loss: 0.10265044435119844, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  54.55024878   96.55756243   56.71595295   99.58754943   57.87431645\n",
      "   97.70182317  108.63001993   96.4015268   109.77254518   98.18659702\n",
      "  108.14750665  100.00157476  102.58710678  158.17511953  108.40896261\n",
      "   93.98484433   19.76635533   55.07359167   45.03944573   28.93224872\n",
      "  188.94304241    0.         1889.43042414]\n",
      "16-th iteration, loss: 0.1009657594876792, 94 gd steps\n",
      "insert gradient: -0.007444667605329914\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  54.9447993    97.59274226   57.41023066   99.70612164   56.68566741\n",
      "   97.0777868   109.82694594   96.14521384  111.30747094   94.55458562\n",
      "  109.10095187   97.38505725  102.74248178  159.0149235   106.49419288\n",
      "  102.52929662   23.93271633   35.25582356   59.1020511    22.51790348\n",
      "   54.77413991    0.          127.80632645   13.22662342 1873.25106051]\n",
      "17-th iteration, loss: 0.09596870066540336, 117 gd steps\n",
      "insert gradient: -0.008515346272021234\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  55.17339325   97.73943241   57.47472532   99.18093437   55.72122771\n",
      "   94.22645442  110.39842662   93.40907037  112.13188508   91.30893239\n",
      "  109.66529328   91.76961936  102.543491    158.66507108   77.1911181\n",
      "    0.           22.05460517  121.89544858   35.06194329   15.32924595\n",
      "   54.61810087   81.99439206   14.48795507   49.72550694  116.9454752\n",
      "   11.92298443 1866.03613837]\n",
      "18-th iteration, loss: 0.09427273094863806, 41 gd steps\n",
      "insert gradient: -0.0039061302469512627\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[  55.56721085   98.44519627   57.48124991   98.21539882   55.13546155\n",
      "   93.5816881   111.04801574   93.29301193   69.55069448    0.\n",
      "   41.73041669   92.50677432  108.85741329   94.36103362  101.9589085\n",
      "  154.58805415   71.00322739   16.66415753   17.62430237  114.06857129\n",
      "   41.37927762    9.58373865   55.40473447   89.53068139   12.27210313\n",
      "   47.33429894  118.67844267   12.71789618 1864.99664279]\n",
      "19-th iteration, loss: 0.09344689984516635, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[  55.90897333   98.73493405   57.98185527   97.93428942   54.76139336\n",
      "   94.60274392  111.27925826   95.49097577   60.86769634   11.93322159\n",
      "   38.33808531   94.50722453  108.6033514    96.86361389  100.65669206\n",
      "  155.207671     69.85632912   18.91975265   17.68041571  111.0544555\n",
      "   45.60043974    4.93873852   56.0942865    94.2479715    11.19969116\n",
      "   44.9556132   117.27525831   13.99411103  465.6029694     0.\n",
      " 1396.80890821]\n",
      "20-th iteration, loss: 0.09290828273527393, 25 gd steps\n",
      "insert gradient: -0.002805619361000208\n",
      "20-th iteration, new layer inserted. now 33 layers\n",
      "[  55.9862789    99.40011963   57.42164795   98.09785833   54.71753656\n",
      "   95.23156377  111.0741875    95.75187923   59.34894783   14.12501617\n",
      "   37.78453029   94.49660136   61.78033811    0.           46.33525358\n",
      "   98.59812998   99.73610155  154.67686111   69.34865282   18.61425286\n",
      "   17.76232388  110.97577973   47.33154031    2.38974388   57.08381268\n",
      "   96.24236497   10.46694586   44.73714198  116.21182724   14.39827922\n",
      "  462.7307214     5.81333623 1394.04449737]\n",
      "21-th iteration, loss: 0.0922865378017095, 24 gd steps\n",
      "insert gradient: -0.003682440651023852\n",
      "21-th iteration, new layer inserted. now 33 layers\n",
      "[5.60856328e+01 9.92020498e+01 0.00000000e+00 1.06581410e-14\n",
      " 5.72767061e+01 9.78427630e+01 5.54402478e+01 9.59314163e+01\n",
      " 1.10876251e+02 9.58559561e+01 6.00624472e+01 1.38952185e+01\n",
      " 3.71445253e+01 9.66777406e+01 5.84308274e+01 8.18672762e+00\n",
      " 4.04982384e+01 1.01884247e+02 9.94223556e+01 1.53487278e+02\n",
      " 7.16730172e+01 1.83526953e+01 1.63967082e+01 1.12477403e+02\n",
      " 1.07709404e+02 9.86749644e+01 1.02243261e+01 4.42125188e+01\n",
      " 1.14598459e+02 1.35944269e+01 4.59530787e+02 7.32770888e+00\n",
      " 1.39133452e+03]\n",
      "22-th iteration, loss: 0.09205531224243182, 16 gd steps\n",
      "insert gradient: -0.0035791592823600543\n",
      "22-th iteration, new layer inserted. now 33 layers\n",
      "[5.61775167e+01 9.94304645e+01 0.00000000e+00 1.77635684e-14\n",
      " 5.72803293e+01 9.88810317e+01 5.54293414e+01 9.60976077e+01\n",
      " 1.11368824e+02 9.56294665e+01 5.92382682e+01 1.55297400e+01\n",
      " 3.66712555e+01 9.68635057e+01 6.08362329e+01 1.02488772e+01\n",
      " 3.65824693e+01 1.02568123e+02 9.95419340e+01 1.53145001e+02\n",
      " 7.21048411e+01 1.91864232e+01 1.62704515e+01 1.12469060e+02\n",
      " 1.07345372e+02 9.99340614e+01 9.96935815e+00 4.40234949e+01\n",
      " 1.14842425e+02 1.39803749e+01 4.57127678e+02 8.21251275e+00\n",
      " 1.38944392e+03]\n",
      "23-th iteration, loss: 0.09170467595704648, 60 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  56.259904     99.47920975   57.74744836   99.13475586   55.80316645\n",
      "   96.43049385  111.73843386   95.89486299   59.24166702   17.49825434\n",
      "   35.46193129   96.90227623   60.72053509   13.56689094   34.06941867\n",
      "  103.06990812   99.09950589  153.86600374   71.3681133    21.02543164\n",
      "   16.52570705  111.603407    107.05552895  101.22300174    9.0599691\n",
      "   43.3840252   115.1486189    15.14627673   64.56932545    0.\n",
      "  387.4159527     8.45305301 1385.75595807]\n",
      "24-th iteration, loss: 0.09094214922878083, 26 gd steps\n",
      "insert gradient: -0.005719326099477558\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[5.60238023e+01 9.83570219e+01 0.00000000e+00 3.55271368e-15\n",
      " 5.77013756e+01 1.00314893e+02 5.57957480e+01 9.66192208e+01\n",
      " 1.11443962e+02 9.63687533e+01 5.99767147e+01 1.80841621e+01\n",
      " 3.41283255e+01 9.77674503e+01 6.14934826e+01 1.52612439e+01\n",
      " 3.18095130e+01 1.03499622e+02 9.87075515e+01 1.54321075e+02\n",
      " 7.02893406e+01 2.24812077e+01 1.69463816e+01 1.10165902e+02\n",
      " 1.07689337e+02 1.03275564e+02 8.17918692e+00 4.29885555e+01\n",
      " 1.18499075e+02 1.96132771e+01 5.47983351e+01 1.10143397e+01\n",
      " 3.81103185e+02 7.63086495e+00 1.38394555e+03]\n",
      "25-th iteration, loss: 0.09003066519574791, 79 gd steps\n",
      "insert gradient: -0.0017631380659507846\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[5.64866027e+01 9.89960656e+01 0.00000000e+00 1.06581410e-14\n",
      " 5.71919448e+01 1.00456634e+02 5.63698635e+01 9.59980658e+01\n",
      " 1.11720231e+02 9.52348878e+01 5.92903721e+01 1.93515975e+01\n",
      " 3.36407660e+01 9.73507478e+01 6.17359090e+01 1.68132666e+01\n",
      " 3.02729608e+01 1.03777584e+02 9.76424632e+01 1.55898201e+02\n",
      " 6.98875062e+01 2.48224274e+01 1.65500031e+01 1.08231723e+02\n",
      " 1.06229780e+02 1.03987625e+02 6.58836788e+00 4.28144973e+01\n",
      " 1.18767650e+02 2.93437272e+01 4.27471841e+01 1.58663011e+01\n",
      " 3.77525950e+02 6.24389287e+00 1.38191207e+03]\n",
      "26-th iteration, loss: 0.0896286909011799, 58 gd steps\n",
      "insert gradient: -0.0004949062315326453\n",
      "26-th iteration, new layer inserted. now 35 layers\n",
      "[ 55.79896241  98.73595332  57.60623806 100.5576011   56.26065904\n",
      "  95.70103612 111.74766816  94.92042474  59.09693905  19.7189673\n",
      "  33.41902855  97.2649418   61.72269886  17.18414724  29.78658327\n",
      " 103.80136563  97.3020241  156.39945489  69.24538841  25.312537\n",
      "  16.72882195 107.13644107 105.75726903 104.13087165   6.00446597\n",
      "  43.02334422 118.09496545  32.5497975   39.36729411  17.64517454\n",
      " 374.51487322   5.3718157  788.45987739   0.         591.34490804]\n",
      "27-th iteration, loss: 0.08929801584815952, 24 gd steps\n",
      "insert gradient: -0.002135473763688713\n",
      "27-th iteration, new layer inserted. now 37 layers\n",
      "[5.58668051e+01 9.86916100e+01 5.75426425e+01 1.01092517e+02\n",
      " 5.70738427e+01 9.55868222e+01 1.12035399e+02 9.44771304e+01\n",
      " 5.90885258e+01 0.00000000e+00 1.42108547e-14 2.06249037e+01\n",
      " 3.28531583e+01 9.69516933e+01 6.20140293e+01 1.80358287e+01\n",
      " 2.90840698e+01 1.03645628e+02 9.69328958e+01 1.57606374e+02\n",
      " 6.82234228e+01 2.65917364e+01 1.73674574e+01 1.04696971e+02\n",
      " 1.05551980e+02 1.04740129e+02 4.98759470e+00 4.38550200e+01\n",
      " 1.17754125e+02 3.90101023e+01 3.55918973e+01 2.22079168e+01\n",
      " 3.69788355e+02 5.27013762e+00 7.86736209e+02 2.31442011e+00\n",
      " 5.89490822e+02]\n",
      "28-th iteration, loss: 0.08869509741925524, 103 gd steps\n",
      "insert gradient: -0.0012358555054545381\n",
      "28-th iteration, new layer inserted. now 37 layers\n",
      "[ 55.89077192  98.76458888  57.99934863 101.78407021  56.77539818\n",
      "  95.08447885  56.46513747   0.          56.46513747  93.9219752\n",
      "  58.68876982  23.69149604  31.48091953  96.53191232  62.50516373\n",
      "  20.48971221  27.3521685  102.44391535  96.4601242  160.00629368\n",
      "  67.83834508  29.08732883  17.20113835 101.84713299 104.8988385\n",
      " 105.78804907   3.57158149  45.48123766 118.36536313  42.4533013\n",
      "  33.02554775  23.6976483  362.6225224    4.0600982  782.67592318\n",
      "   2.30299599 585.61614078]\n",
      "29-th iteration, loss: 0.08846676148452189, 22 gd steps\n",
      "insert gradient: -0.0013483867390883552\n",
      "29-th iteration, new layer inserted. now 39 layers\n",
      "[5.59184730e+01 9.90638671e+01 5.82605500e+01 1.01713391e+02\n",
      " 5.74137057e+01 9.54501431e+01 5.41780220e+01 4.56939762e+00\n",
      " 5.41546656e+01 9.44106638e+01 5.95083999e+01 2.36438133e+01\n",
      " 0.00000000e+00 8.88178420e-16 3.06896911e+01 9.71674564e+01\n",
      " 6.29477803e+01 2.09890940e+01 2.64579900e+01 1.02601107e+02\n",
      " 9.59537699e+01 1.60258834e+02 6.79440367e+01 2.96752805e+01\n",
      " 1.68191813e+01 1.01542872e+02 1.04177483e+02 1.06478829e+02\n",
      " 2.78559617e+00 4.63794811e+01 1.18705773e+02 4.33161457e+01\n",
      " 3.27492272e+01 2.41139063e+01 3.61636967e+02 3.80979165e+00\n",
      " 7.81688908e+02 2.48617819e+00 5.84697306e+02]\n",
      "30-th iteration, loss: 0.08828187600632265, 49 gd steps\n",
      "insert gradient: -0.004721988368581472\n",
      "30-th iteration, new layer inserted. now 37 layers\n",
      "[ 56.11486094  99.09430952  58.07357831 101.54129261  56.68301413\n",
      "  95.61210272  53.83551256   5.48499452  53.52219403  94.67569545\n",
      "  59.4105535   24.30411419  30.13137565  97.49753962  63.24627555\n",
      "  21.50724052  25.52899334 103.15409643  95.30190239 161.22227341\n",
      "  67.74418296  30.6263493   16.51842962 101.17179486 103.45355633\n",
      " 107.37391792   2.25852127  47.55040954 118.87385445  43.97766743\n",
      "  32.09412733  24.03852819 360.35859809   3.05535619 780.2885989\n",
      "   2.30779015 583.21096428]\n",
      "31-th iteration, loss: 0.0880573346166351, 56 gd steps\n",
      "insert gradient: -0.0016251872790149224\n",
      "31-th iteration, new layer inserted. now 39 layers\n",
      "[5.59939954e+01 9.91561032e+01 5.83233464e+01 1.01670532e+02\n",
      " 5.65548629e+01 9.54650100e+01 5.34886887e+01 6.81826559e+00\n",
      " 5.23615403e+01 9.47284457e+01 5.92692328e+01 2.50303542e+01\n",
      " 2.92882108e+01 9.79237310e+01 6.33065200e+01 2.20477026e+01\n",
      " 2.44556306e+01 1.04225917e+02 9.43287167e+01 1.61891921e+02\n",
      " 6.76140247e+01 3.19279983e+01 1.57323366e+01 1.00988277e+02\n",
      " 1.01873023e+02 1.09332018e+02 7.27484464e-01 4.97450319e+01\n",
      " 7.93986948e+01 0.00000000e+00 3.96993474e+01 4.57967816e+01\n",
      " 3.14604044e+01 2.48161847e+01 3.59074394e+02 3.00221317e+00\n",
      " 7.78333130e+02 2.52727356e+00 5.81212824e+02]\n",
      "32-th iteration, loss: 0.08779939633237789, 23 gd steps\n",
      "insert gradient: -0.0018443506376415878\n",
      "32-th iteration, new layer inserted. now 41 layers\n",
      "[5.58391882e+01 9.94472846e+01 0.00000000e+00 7.10542736e-15\n",
      " 5.84021333e+01 1.01505047e+02 5.60525660e+01 9.52052583e+01\n",
      " 5.37657851e+01 7.30797033e+00 5.15129823e+01 9.50080171e+01\n",
      " 5.96320860e+01 2.58781284e+01 2.81849420e+01 9.85453480e+01\n",
      " 6.38521668e+01 2.27987614e+01 2.30956655e+01 1.05478611e+02\n",
      " 9.31325475e+01 1.62858033e+02 6.71879191e+01 3.37613226e+01\n",
      " 1.50999991e+01 1.00945166e+02 1.00577220e+02 1.10247196e+02\n",
      " 3.09025554e-01 5.05897230e+01 7.76211317e+01 5.72101351e+00\n",
      " 3.69541867e+01 5.07558096e+01 3.03063327e+01 2.44848872e+01\n",
      " 3.59170927e+02 3.41681907e+00 7.77334779e+02 2.70501160e+00\n",
      " 5.80164467e+02]\n",
      "33-th iteration, loss: 0.08768114624303872, 27 gd steps\n",
      "insert gradient: -0.0008256102656683247\n",
      "33-th iteration, new layer inserted. now 43 layers\n",
      "[5.60882606e+01 9.92550298e+01 1.89903519e-01 3.15345277e-02\n",
      " 5.83721515e+01 1.01528781e+02 5.62542805e+01 9.51061322e+01\n",
      " 5.37055644e+01 7.90600945e+00 5.12676588e+01 9.47371037e+01\n",
      " 5.92951304e+01 2.65778069e+01 2.80344369e+01 9.84247671e+01\n",
      " 6.36555758e+01 2.32438632e+01 2.27293857e+01 1.05809512e+02\n",
      " 9.28340289e+01 1.63601440e+02 6.69205197e+01 3.43481027e+01\n",
      " 1.48616566e+01 1.00788580e+02 1.00428917e+02 1.10359122e+02\n",
      " 4.34874279e-01 5.06589411e+01 7.78119221e+01 7.20545107e+00\n",
      " 3.53773598e+01 5.22466353e+01 2.96862979e+01 2.43721111e+01\n",
      " 2.87073671e+02 0.00000000e+00 7.17684176e+01 3.48120342e+00\n",
      " 7.76452508e+02 2.53793144e+00 5.79263678e+02]\n",
      "34-th iteration, loss: 0.08738705071393829, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 43 layers\n",
      "[5.58456431e+01 9.93019436e+01 5.88173003e+01 1.01702065e+02\n",
      " 5.62816280e+01 9.51420233e+01 5.40670208e+01 8.43343855e+00\n",
      " 5.05297180e+01 9.48558785e+01 5.91981123e+01 2.78685585e+01\n",
      " 2.71370039e+01 9.87533538e+01 6.38565379e+01 2.45199448e+01\n",
      " 2.13473779e+01 1.06956391e+02 9.17763555e+01 1.65413816e+02\n",
      " 6.61817625e+01 3.62684319e+01 1.44578644e+01 1.00376322e+02\n",
      " 9.97317738e+01 1.10777085e+02 3.36738203e-01 5.09047939e+01\n",
      " 7.97795532e+01 1.08919432e+01 3.13574920e+01 5.74863459e+01\n",
      " 2.79465473e+01 2.42697928e+01 1.14412553e+02 0.00000000e+00\n",
      " 1.71618830e+02 2.80763158e+00 7.05276413e+01 4.39264492e+00\n",
      " 7.74628129e+02 2.51496926e+00 5.77431302e+02]\n",
      "35-th iteration, loss: 0.08694014836367227, 31 gd steps\n",
      "insert gradient: -0.0019136096154416219\n",
      "35-th iteration, new layer inserted. now 43 layers\n",
      "[5.62495259e+01 9.95897348e+01 5.87539391e+01 1.01855403e+02\n",
      " 5.64835822e+01 9.51556946e+01 5.40957417e+01 9.18369696e+00\n",
      " 4.99355510e+01 9.47256804e+01 5.90588069e+01 2.92947483e+01\n",
      " 2.64176574e+01 9.86403721e+01 6.37449611e+01 2.55638294e+01\n",
      " 2.04485065e+01 1.07512546e+02 9.10386865e+01 1.66615041e+02\n",
      " 6.57037033e+01 3.74456360e+01 1.40742397e+01 9.99809035e+01\n",
      " 9.89550059e+01 1.11222022e+02 7.82072683e-02 5.13106555e+01\n",
      " 8.06765688e+01 1.35047323e+01 2.89207804e+01 6.03767733e+01\n",
      " 2.61682346e+01 2.22791494e+01 1.14399452e+02 6.46053063e+00\n",
      " 1.71198945e+02 4.04709410e+00 6.96676958e+01 4.16673430e+00\n",
      " 7.73345631e+02 2.54335878e+00 5.76137860e+02]\n",
      "36-th iteration, loss: 0.08518554137686982, 69 gd steps\n",
      "insert gradient: -0.0018213000575682881\n",
      "36-th iteration, new layer inserted. now 41 layers\n",
      "[ 56.35901496 100.35547138  59.35717347 102.90371323  56.86080065\n",
      "  96.27166931  55.0396857   15.65344799  44.51756619  94.29398979\n",
      "  56.9100495   40.54896509  21.82589016  97.44866063  62.2470641\n",
      "  33.27697334  14.55563934 111.73390002  88.24796386 173.45279082\n",
      "  64.57846569  48.87993226   9.48226201  98.6331831   92.24309894\n",
      " 169.22335353  83.96245517  29.48400663  17.93022083  77.71739734\n",
      "  19.24039724  15.58717691 123.54544507  14.47354347 173.55507109\n",
      "   7.77937731  64.84775431   5.29235675 765.26151046   2.5716049\n",
      " 568.36928311]\n",
      "37-th iteration, loss: 0.08248958592145217, 86 gd steps\n",
      "insert gradient: -0.00798555349643998\n",
      "37-th iteration, new layer inserted. now 43 layers\n",
      "[5.64673719e+01 1.01530051e+02 5.82259923e+01 1.01775149e+02\n",
      " 5.88935202e+01 9.93360717e+01 5.52710042e+01 2.35418328e+01\n",
      " 3.79101284e+01 9.33552819e+01 5.26963173e+01 5.86509557e+01\n",
      " 1.63003731e+01 9.18884625e+01 5.75223678e+01 4.75602371e+01\n",
      " 9.10352485e+00 1.06706040e+02 8.71892881e+01 1.38710264e+02\n",
      " 0.00000000e+00 3.46775661e+01 6.66695803e+01 7.03258817e+01\n",
      " 1.13614190e-01 1.01887541e+02 8.20762918e+01 1.66906576e+02\n",
      " 9.08404105e+01 5.31211624e+01 1.86702090e+00 9.53762992e+01\n",
      " 2.79379244e+00 2.14218750e+01 1.36326609e+02 1.80855811e+01\n",
      " 1.79662841e+02 8.63982493e+00 6.07390796e+01 5.38387796e+00\n",
      " 7.57048228e+02 2.26091993e+00 5.63258048e+02]\n",
      "38-th iteration, loss: 0.08146191071581461, 17 gd steps\n",
      "insert gradient: -0.0024392342255724914\n",
      "38-th iteration, new layer inserted. now 41 layers\n",
      "[5.59740235e+01 1.01241689e+02 5.83246922e+01 1.01886435e+02\n",
      " 6.04547359e+01 9.83143173e+01 5.56513101e+01 2.57721785e+01\n",
      " 3.62827082e+01 9.19448791e+01 5.37175972e+01 6.12383612e+01\n",
      " 1.52945019e+01 9.13564155e+01 5.84735429e+01 5.09422794e+01\n",
      " 7.04635811e+00 1.06522245e+02 8.65569229e+01 1.33849404e+02\n",
      " 6.59709449e+00 3.06730678e+01 6.25795320e+01 1.75854567e+02\n",
      " 8.29580988e+01 1.67873361e+02 9.05187193e+01 5.48913470e+01\n",
      " 6.17645834e-01 9.72072832e+01 2.54111366e+00 2.32543324e+01\n",
      " 1.37186056e+02 1.65995602e+01 1.79214333e+02 8.57715206e+00\n",
      " 5.98808764e+01 5.06511835e+00 7.55992978e+02 1.98178072e+00\n",
      " 5.62679522e+02]\n",
      "39-th iteration, loss: 0.0791562020757095, 303 gd steps\n",
      "insert gradient: -0.0025402653916176236\n",
      "39-th iteration, new layer inserted. now 39 layers\n",
      "[ 55.93985702 101.17146574  59.12413839 102.71721019  57.5369249\n",
      "  97.56677523  54.51929735  32.57347282  31.95206487  92.46582279\n",
      "  52.67693044  74.07483971  11.07184189  88.24406423  56.29055407\n",
      " 172.89900373  87.64152377 114.41791574  15.65342728  29.02957745\n",
      "  61.37784717 168.45506712  84.49533203 165.5765767   89.10130186\n",
      " 156.87971925   2.64739901  25.19250237  95.34739779   0.\n",
      "  47.6736989   11.89320292 181.57152539   6.82673801  56.28481951\n",
      "   3.57995954 751.35981343   2.33401701 559.54843265]\n",
      "40-th iteration, loss: 0.07484591874624687, 55 gd steps\n",
      "insert gradient: -0.01948318405561475\n",
      "40-th iteration, new layer inserted. now 37 layers\n",
      "[ 56.63433554 100.40824837  59.79820645 104.36751212  57.61008122\n",
      "  94.93296983  54.85830293  45.32306972  26.82620642  88.62862647\n",
      "  54.24120593  85.64493559  11.51580606  76.15273664  52.92891496\n",
      " 167.11858936  93.06028757 109.55970068  26.54786124  15.15250133\n",
      "  61.11896333 149.62819767  91.46683868 149.81751846  95.90797413\n",
      " 146.89870696  11.60114202  19.71543427  94.78413987  27.40512566\n",
      "  35.51507629  22.16179709 189.66361081   6.41644338 787.29611969\n",
      "   3.13891617 552.42159155]\n",
      "41-th iteration, loss: 0.07339318779394613, 17 gd steps\n",
      "insert gradient: -0.00292849637405226\n",
      "41-th iteration, new layer inserted. now 39 layers\n",
      "[5.75629485e+01 1.01185943e+02 0.00000000e+00 2.13162821e-14\n",
      " 6.04868478e+01 1.04539561e+02 5.83136713e+01 9.53175871e+01\n",
      " 5.46593234e+01 4.70457454e+01 2.62186722e+01 8.80700304e+01\n",
      " 5.39100395e+01 8.65463168e+01 1.19006073e+01 7.44895588e+01\n",
      " 5.35062635e+01 1.65147125e+02 9.38003938e+01 1.09733725e+02\n",
      " 2.83510743e+01 1.33080165e+01 6.11370717e+01 1.47193027e+02\n",
      " 9.31229321e+01 1.49234335e+02 9.75594674e+01 1.45917174e+02\n",
      " 1.36393540e+01 1.83915824e+01 9.55208931e+01 2.90668320e+01\n",
      " 3.41902138e+01 2.12470506e+01 1.89176851e+02 5.13261706e+00\n",
      " 7.85831091e+02 2.14122167e+00 5.51129254e+02]\n",
      "42-th iteration, loss: 0.0695860175838639, 37 gd steps\n",
      "insert gradient: -0.0021347055042814198\n",
      "42-th iteration, new layer inserted. now 39 layers\n",
      "[5.68539131e+01 1.01231313e+02 2.17646952e-01 5.54028070e-02\n",
      " 5.99040723e+01 1.05142480e+02 5.98011309e+01 9.68075609e+01\n",
      " 5.33938358e+01 5.93299940e+01 2.23148120e+01 8.39792097e+01\n",
      " 5.21007376e+01 9.23319807e+01 1.68859902e+01 5.55849424e+01\n",
      " 5.32277201e+01 1.55346950e+02 9.79398339e+01 1.13814079e+02\n",
      " 1.02485979e+02 1.32608014e+02 9.65651049e+01 1.46379889e+02\n",
      " 9.91865860e+01 1.43798548e+02 1.74978680e+01 8.26615556e+00\n",
      " 9.95717586e+01 4.09733540e+01 3.01515022e+01 2.25934477e+01\n",
      " 1.27506917e+02 0.00000000e+00 6.37534587e+01 1.59550423e+00\n",
      " 7.82944743e+02 9.03306170e-01 5.49242133e+02]\n",
      "43-th iteration, loss: 0.06753559924498258, 90 gd steps\n",
      "insert gradient: -0.0008984008493714478\n",
      "43-th iteration, new layer inserted. now 35 layers\n",
      "[5.62388862e+01 1.00742201e+02 1.51275169e-01 3.16972169e-02\n",
      " 5.96143005e+01 1.05247918e+02 6.04082659e+01 9.76983390e+01\n",
      " 5.25144992e+01 6.33246983e+01 2.18127983e+01 8.10563883e+01\n",
      " 5.15457888e+01 8.82624953e+01 2.35029913e+01 4.08560720e+01\n",
      " 5.62678285e+01 1.50816859e+02 9.85228327e+01 1.15994685e+02\n",
      " 1.03370442e+02 1.24374164e+02 9.90878176e+01 1.41942512e+02\n",
      " 9.83501552e+01 1.51343411e+02 1.15462626e+02 5.60226953e+01\n",
      " 2.32367695e+01 2.85013151e+01 1.27868889e+02 1.12307336e+01\n",
      " 5.77310142e+01 2.63919860e+00 1.32512893e+03]\n",
      "44-th iteration, loss: 0.06740970944303527, 23 gd steps\n",
      "insert gradient: -0.0007297225758970006\n",
      "44-th iteration, new layer inserted. now 37 layers\n",
      "[5.59609766e+01 1.00745505e+02 1.02006404e-01 2.47905297e-02\n",
      " 5.95683521e+01 1.04775961e+02 6.03810916e+01 9.80601783e+01\n",
      " 5.28745395e+01 6.32005121e+01 2.15701569e+01 8.11110618e+01\n",
      " 5.16120528e+01 8.79133173e+01 2.35117366e+01 4.11431506e+01\n",
      " 5.57689373e+01 1.51091008e+02 8.20695493e+01 0.00000000e+00\n",
      " 1.64139099e+01 1.15643482e+02 1.03218753e+02 1.24553738e+02\n",
      " 9.92470384e+01 1.40969144e+02 9.81961337e+01 1.52261946e+02\n",
      " 1.14606275e+02 5.97673004e+01 2.20355800e+01 3.03828788e+01\n",
      " 1.28301015e+02 1.19330575e+01 5.50864931e+01 4.02258700e+00\n",
      " 1.32348427e+03]\n",
      "45-th iteration, loss: 0.06732188817837922, 22 gd steps\n",
      "insert gradient: -0.001253691042710537\n",
      "45-th iteration, new layer inserted. now 39 layers\n",
      "[5.60331810e+01 1.00517376e+02 2.67158517e-01 5.51517969e-02\n",
      " 5.93921248e+01 1.05066890e+02 6.05646945e+01 9.78890959e+01\n",
      " 5.26025432e+01 6.30449435e+01 0.00000000e+00 1.06581410e-14\n",
      " 2.16693877e+01 8.11644328e+01 5.15938223e+01 8.77360781e+01\n",
      " 2.36506631e+01 4.13127272e+01 5.55291921e+01 1.50950986e+02\n",
      " 8.16054756e+01 1.77873125e+00 1.60550445e+01 1.14860831e+02\n",
      " 1.03128648e+02 1.25243934e+02 9.92703428e+01 1.40507382e+02\n",
      " 9.80749683e+01 1.52800838e+02 1.14099458e+02 6.17077677e+01\n",
      " 2.13912993e+01 3.11909859e+01 1.28198258e+02 1.24521973e+01\n",
      " 5.34616213e+01 4.91305658e+00 1.32246982e+03]\n",
      "46-th iteration, loss: 0.06728240566714971, 24 gd steps\n",
      "insert gradient: -0.0005237976604314784\n",
      "46-th iteration, new layer inserted. now 37 layers\n",
      "[  56.2499511   100.68416068   59.59951373  104.7805737    60.3783177\n",
      "   97.9473104    52.64166945   62.9992065    21.74685378   81.19826251\n",
      "   51.51569732   87.68709621   23.68943204   41.31078737   55.51694319\n",
      "  150.89345065   81.4761861     2.10113968   16.10779808  114.46848431\n",
      "  103.06002488  125.58092514   99.18746556  140.31558408   98.001682\n",
      "  153.00397372   75.94534666    0.           37.97267333   62.38207459\n",
      "   21.20332097   31.43680196  128.10821743   12.61684082   52.81689946\n",
      "    5.1812176  1322.02189565]\n",
      "47-th iteration, loss: 0.0670399321054332, 26 gd steps\n",
      "insert gradient: -0.0006345416130138511\n",
      "47-th iteration, new layer inserted. now 37 layers\n",
      "[  56.28168527  100.27359137   60.08388318  105.15184346   59.61847315\n",
      "   97.90352212   53.201556     62.77994568   21.6481959    81.48365034\n",
      "   51.42058161   87.31973273   24.10981252   41.05921141   54.9311214\n",
      "  151.21982606   80.07656022    4.15030365   17.28099019  110.44813085\n",
      "  103.2397357   127.14429814   98.60510055  140.34285052   97.47735679\n",
      "  153.36662657   73.1954175     4.56984111   34.59086056   73.85041562\n",
      "   18.01112998   34.00299849  128.71015403   14.9589579    48.88480762\n",
      "    7.18882738 1320.24187378]\n",
      "48-th iteration, loss: 0.0669969716877493, 18 gd steps\n",
      "insert gradient: -0.0005515185940924426\n",
      "48-th iteration, new layer inserted. now 37 layers\n",
      "[  56.25816777  100.53992467   59.45751185  104.80975921   60.31594731\n",
      "   97.89952417   52.58242459   62.79193018   21.62744937   81.42345723\n",
      "   51.65876375   87.41295904   23.90806064   40.99562034   55.22808359\n",
      "  151.01423909   79.83470267    4.25233034   17.37966528  110.25891632\n",
      "  103.11322703  127.42377655   98.52980864  140.24827605   97.41335396\n",
      "  153.52461835   73.13258721    4.92697005   34.23046904   74.62068012\n",
      "   17.8118678    33.99378543  128.6912289    15.38388653   48.52354383\n",
      "    7.21118941 1319.86808204]\n",
      "49-th iteration, loss: 0.06692681771641319, 26 gd steps\n",
      "insert gradient: -0.0009335025608869078\n",
      "49-th iteration, new layer inserted. now 39 layers\n",
      "[5.62864068e+01 1.00406086e+02 5.93914944e+01 1.04940243e+02\n",
      " 6.04900369e+01 9.78040676e+01 5.25283685e+01 6.28619495e+01\n",
      " 2.17060810e+01 8.11896618e+01 5.16495174e+01 8.74302844e+01\n",
      " 0.00000000e+00 7.10542736e-15 2.39437220e+01 4.08033808e+01\n",
      " 5.53341187e+01 1.50829052e+02 7.89958191e+01 4.70823951e+00\n",
      " 1.80952151e+01 1.09523261e+02 1.02983133e+02 1.28215111e+02\n",
      " 9.82512344e+01 1.40238702e+02 9.71899701e+01 1.53727520e+02\n",
      " 7.29539217e+01 6.20164464e+00 3.26473977e+01 7.83272760e+01\n",
      " 1.68689378e+01 3.41029689e+01 1.28822329e+02 1.63077090e+01\n",
      " 4.74592574e+01 7.60218222e+00 1.31856064e+03]\n",
      "50-th iteration, loss: 0.06637374420464415, 174 gd steps\n",
      "insert gradient: -0.0001744359790404071\n",
      "50-th iteration, new layer inserted. now 39 layers\n",
      "[  55.85667302   99.95393449   59.31071666  104.52578399   59.934462\n",
      "   97.42066099   52.43508553   62.54957383   21.67111552   80.85192738\n",
      "   51.27090734   86.90592163   24.16809647   40.10012      55.15358591\n",
      "  150.30004948   76.57263573    5.49153598   19.67922043  107.93500782\n",
      "  102.29137354  128.89517333   97.27469885  140.07697123   96.24305324\n",
      "  153.17579655   73.18425499    8.27832009   28.6896979    88.79549223\n",
      "   14.48434395   34.17220598   85.29361201    0.           42.64680601\n",
      "   17.28228512   45.07823133    7.9338547  1312.68484825]\n",
      "51-th iteration, loss: 0.0659564352473756, 44 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "51-th iteration, new layer inserted. now 41 layers\n",
      "[ 55.86306094  99.73016395  59.05141096 104.1643578   59.48382578\n",
      "  97.09906258  52.21019127  62.5636145   21.55521552  80.37724717\n",
      "  51.00485848  86.93999318  24.26064712  39.33907617  55.03388183\n",
      " 149.2528497   76.30856341   5.40093461  19.63747532 108.09508936\n",
      " 101.9994404  127.90996247  96.87890825 140.03900953  95.60220673\n",
      " 152.89658988  73.24146882   8.40965806  27.64869408  90.80525841\n",
      "  14.10230818  33.66762083  84.41874093   3.62357671  41.33275154\n",
      "  19.29940505  44.23212679   8.11084131 524.69295021   0.\n",
      " 787.03942531]\n",
      "52-th iteration, loss: 0.06586943991966036, 20 gd steps\n",
      "insert gradient: -0.002566208378400187\n",
      "52-th iteration, new layer inserted. now 43 layers\n",
      "[ 55.57537167  99.6751198   59.20029064 104.32394058  59.56054872\n",
      "  97.02694069  52.05248392  62.51574374  21.70161783  80.22153656\n",
      "  50.84157528  86.71977273  24.33264931  39.22027377  54.92578117\n",
      " 149.21324469  76.09063917   5.50626081  19.59245442 108.06821355\n",
      " 101.91058733 128.08833567  96.70944374 139.87773413  95.50517613\n",
      " 153.15553015  73.22641228   8.74355668  27.0666882   91.71353347\n",
      "  14.16891399  33.05453969  84.01610238   4.96743905  40.3835354\n",
      "  20.70324004  43.79340543   8.51682456 523.86742601   2.23349637\n",
      " 628.89417011   0.         157.22354253]\n",
      "53-th iteration, loss: 0.06533681415635591, 22 gd steps\n",
      "insert gradient: -0.006185084793679856\n",
      "53-th iteration, new layer inserted. now 43 layers\n",
      "[ 55.62445934 100.06446839  58.9154478  103.06734137  59.00362424\n",
      "  96.99483044  52.07864785  62.424746    21.78262975  80.18085031\n",
      "  51.15494282  86.51902034  24.44085376  39.32790715  54.81421169\n",
      " 149.26731891  75.57848806   5.86940115  19.55297105 107.61508624\n",
      " 101.96746528 127.90873124  96.36336224 140.7360838   95.05299071\n",
      " 153.07219331  73.37893051   9.81535109  25.65490978  93.3991752\n",
      "  13.90345031  31.92769044  83.64270275   7.1912726   38.33808988\n",
      "  22.7491767   42.23941825   9.35083119 522.57483345   2.99358998\n",
      " 626.60470787   9.00337979 155.01898446]\n",
      "54-th iteration, loss: 0.06484648639271266, 39 gd steps\n",
      "insert gradient: -0.005159396527042583\n",
      "54-th iteration, new layer inserted. now 45 layers\n",
      "[ 55.46625088  99.41034492  58.69484848 103.70564563  59.05498813\n",
      "  96.59286788  51.69939791  62.10530209  21.74631398  80.067464\n",
      "  50.73201513  86.45181891  24.33712323  38.94854801  54.26402377\n",
      " 148.48320436  75.42349105   5.71407211  19.34435812 107.5798337\n",
      " 101.69824405 127.01059624  95.89896674 140.75851308  94.63513914\n",
      " 152.28689788  73.54161038   9.71980315  24.78981793  94.09451649\n",
      "  13.2643749   31.66862053  83.88245278   7.51523679  37.46367179\n",
      "  23.16441797  41.25576373   9.22082167 522.46508212   3.24084697\n",
      " 625.95041037  10.24385782 123.26318774   0.          30.81579693]\n",
      "55-th iteration, loss: 0.060478145346743006, 32 gd steps\n",
      "insert gradient: -0.002905132135405012\n",
      "55-th iteration, new layer inserted. now 47 layers\n",
      "[5.44427606e+01 9.72644502e+01 5.68882140e+01 1.03530039e+02\n",
      " 5.83603781e+01 9.58014862e+01 5.05833440e+01 6.20007097e+01\n",
      " 0.00000000e+00 3.55271368e-15 2.06717329e+01 7.80276299e+01\n",
      " 4.93416736e+01 8.47622234e+01 2.45201179e+01 3.60380608e+01\n",
      " 5.37666364e+01 1.46258327e+02 7.42551381e+01 6.44207175e+00\n",
      " 1.78770209e+01 1.06723883e+02 1.00367265e+02 1.21650835e+02\n",
      " 9.46494365e+01 1.38844510e+02 9.27747277e+01 1.47841547e+02\n",
      " 7.75146811e+01 8.33418987e+00 1.87514595e+01 1.02925875e+02\n",
      " 1.17625306e+01 2.90081279e+01 8.92133293e+01 1.33458280e+01\n",
      " 2.85164492e+01 2.77626178e+01 3.59365824e+01 1.14895023e+01\n",
      " 5.28976330e+02 4.73459135e+00 6.23658254e+02 4.36278328e+00\n",
      " 1.20776089e+02 2.66668692e+01 1.57641351e+01]\n",
      "56-th iteration, loss: 0.05571503043851803, 62 gd steps\n",
      "insert gradient: -0.0019323666738522093\n",
      "56-th iteration, new layer inserted. now 45 layers\n",
      "[ 50.10613253  90.24966808  52.75181708 103.59268489  55.87764774\n",
      "  91.5396302   49.51256633  56.72599426  19.36252401  75.55017091\n",
      "  49.04549862  77.39352372  25.60436619  28.37683129  54.11353216\n",
      " 134.28207653  79.65384806   1.24044346   8.04639644 120.71920199\n",
      "  93.87884233 113.44177154  92.03608255 129.62592347  88.15252296\n",
      " 139.43438898  76.53000045   2.8628397   17.47846327 109.97324751\n",
      "   8.79142453  22.42516577  90.57522007  37.75215466  17.22042251\n",
      "  45.88607206  24.3353245   15.61955343 527.64317739   2.46289261\n",
      " 625.11398456   4.05964205 125.17531759  27.26866189  15.71983   ]\n",
      "57-th iteration, loss: 0.047206355435813585, 743 gd steps\n",
      "insert gradient: -0.000581701925917048\n",
      "57-th iteration, new layer inserted. now 41 layers\n",
      "[5.26374754e+01 9.19980348e+01 5.13527467e+01 9.42454577e+01\n",
      " 5.36223823e+01 9.42913106e+01 5.28481694e+01 7.15500496e+01\n",
      " 1.18254561e+01 7.49334602e+01 4.71638282e+01 8.32498423e+01\n",
      " 3.91067876e+01 1.82020059e+01 4.55987386e+01 9.34766238e+01\n",
      " 9.47986780e+01 1.16373903e+02 8.56622880e+01 1.34776898e+02\n",
      " 8.40878765e+01 1.34860948e+02 8.73949701e+01 1.33045871e+02\n",
      " 9.08011117e+01 1.27044849e+02 8.14858883e-01 2.97512714e+01\n",
      " 1.08197174e+02 5.91847748e+01 1.26139740e+01 5.75189967e+01\n",
      " 1.71711524e+01 1.61443838e+01 5.30122505e+02 1.35946778e-01\n",
      " 6.27233623e+02 5.95007203e+00 1.23193103e+02 2.70164006e+01\n",
      " 1.31333175e+01]\n",
      "58-th iteration, loss: 0.044698592527005435, 79 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "58-th iteration, new layer inserted. now 41 layers\n",
      "[5.30588854e+01 9.21728924e+01 5.10261725e+01 9.10225425e+01\n",
      " 5.19111888e+01 9.35817314e+01 5.32773489e+01 8.03173654e+01\n",
      " 1.27892678e+01 5.94744171e+01 4.84706322e+01 8.35299221e+01\n",
      " 4.86541812e+01 4.44515807e+00 4.85273398e+01 9.10026825e+01\n",
      " 9.62130292e+01 1.04315449e+02 9.00850577e+01 1.22800519e+02\n",
      " 8.45051911e+01 1.36981410e+02 8.38896745e+01 1.44769543e+02\n",
      " 9.97287245e+01 1.33744570e+02 4.79050409e+00 3.90138757e+01\n",
      " 1.02260221e+02 1.44545795e+02 6.18760194e-01 2.41501582e+01\n",
      " 1.05157425e+02 0.00000000e+00 4.20629699e+02 5.77399347e+00\n",
      " 6.20595720e+02 7.77586118e+00 1.21196670e+02 2.72005788e+01\n",
      " 1.39116278e+01]\n",
      "59-th iteration, loss: 0.04270227752732181, 26 gd steps\n",
      "insert gradient: -0.0012643160940039556\n",
      "59-th iteration, new layer inserted. now 37 layers\n",
      "[ 52.32535204  92.05776035  50.5740202   89.28039818  51.87575608\n",
      "  93.048408    53.77726519  83.42702331  13.26065879  53.82930363\n",
      "  48.34751601  83.37638107 100.49602947  91.57596903  95.25878645\n",
      " 104.31819349  90.44346458 119.15462191  85.4078025  135.53226179\n",
      "  83.85986859 149.13455851 104.67493122 189.1757963   97.38059694\n",
      " 139.5914636    2.84479862  18.96530777 108.24268724  16.85018849\n",
      " 412.49798667   5.0590192  613.23764993   5.51971216 120.798936\n",
      "  26.79735597  15.66108847]\n",
      "60-th iteration, loss: 0.04241000707751222, 32 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "60-th iteration, new layer inserted. now 39 layers\n",
      "[ 52.54004451  91.89694825  50.68579715  90.44045692  51.55941447\n",
      "  92.97206871  52.9674466   83.70486056  13.94283807  52.03434693\n",
      "  49.47995527  83.55543559 100.86831671  91.22560353  95.45087206\n",
      " 104.66562587  90.2688346  119.49530258  84.77039014 136.40042025\n",
      "  83.31602096 150.48749318 104.8238058  185.90301973  96.78171515\n",
      " 140.48427459   0.69119451  19.27683403 112.34321202  19.82515054\n",
      " 136.39967197   0.         272.79934394   4.18823592 611.42148416\n",
      "   5.98836869 121.44426212  26.58089463  15.91844288]\n",
      "61-th iteration, loss: 0.04196340830217436, 33 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "61-th iteration, new layer inserted. now 39 layers\n",
      "[ 52.40398955  92.07878713  50.97742219  90.54799245  52.0068958\n",
      "  92.55031081  52.74781921  83.7498059   14.82989286  49.86297686\n",
      "  50.43437554  83.4449641  101.20170282  90.66515495  95.62531208\n",
      " 105.21322033  89.96677955 120.38207212  84.39403386 136.96831635\n",
      "  82.97512684 152.82939623 105.61525642 184.86679201  95.32915398\n",
      " 161.32313545 114.43350677  23.98604885  44.68211506   0.\n",
      "  89.36423013   9.22441255 266.26603625   3.84372572 608.22707737\n",
      "   7.17273576 123.64061247  27.54237052  15.21785796]\n",
      "62-th iteration, loss: 0.04178814271083195, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "62-th iteration, new layer inserted. now 41 layers\n",
      "[ 51.48882     91.72409699  50.86659723  90.88832097  52.18496178\n",
      "  93.22646746  53.37989473  83.78336041  14.818588    49.4368997\n",
      "  50.29808988  83.70977836 101.19796145  90.51970219  95.82979178\n",
      " 104.37876792  90.28522935 119.99753446  84.48816826 136.99614001\n",
      "  83.1184799  152.78657304 105.57578384 185.3389613   95.72602619\n",
      " 160.95297206 113.69465612  26.19844027  43.32189108   5.87703297\n",
      "  88.30937809   8.58779014  52.82869365   0.         211.31477459\n",
      "   3.42994592 607.02629429   7.73426003 124.23134486  28.03046915\n",
      "  14.99731783]\n",
      "63-th iteration, loss: 0.041631228038615645, 16 gd steps\n",
      "insert gradient: -0.0005067775735448634\n",
      "63-th iteration, new layer inserted. now 43 layers\n",
      "[ 51.88475098  91.76275171  50.78807648  90.81314055  52.01683573\n",
      "  92.89103917  52.93101729  83.81353948  14.82918973  49.38783367\n",
      "  50.57604618  83.85616952 101.12413029  90.52082958  95.74874537\n",
      " 104.30517396  90.19631776 119.98646826  84.48545336 136.88476276\n",
      "  83.22023988 152.77267656 105.48558173 185.3674966   95.71092994\n",
      " 160.77602868 113.34307981  26.96065998  25.40616046   0.\n",
      "  16.93744031   6.83908117  87.98792268   8.71287972  52.34027209\n",
      "   2.14524365 210.83611973   3.35817754 606.77350971   7.83080338\n",
      " 124.4984804   28.15863833  14.97108078]\n",
      "64-th iteration, loss: 0.04137112946455506, 15 gd steps\n",
      "insert gradient: -0.0007931978931652793\n",
      "64-th iteration, new layer inserted. now 43 layers\n",
      "[ 51.30088359  91.4648175   50.83676945  90.94055368  52.02511209\n",
      "  92.72050085  53.05051408  84.1574151   14.54777125  49.0460446\n",
      "  51.01552592  84.17602558 100.91934717  90.49131013  95.68352018\n",
      " 104.21693563  90.15917552 119.94587457  84.5333041  136.57886436\n",
      "  83.32872417 152.69924508 105.11030641 185.45778368  96.09683144\n",
      " 160.21487175 111.89412115  30.21334484  23.85330023   3.75150536\n",
      "  15.39691798   9.10847607  87.71398403   9.71955705  50.71248044\n",
      "   5.44939433 209.37748032   2.98450683 605.90434931   7.93165108\n",
      " 125.14422038  28.63354808  14.87962947]\n",
      "65-th iteration, loss: 0.041269682060075696, 17 gd steps\n",
      "insert gradient: -0.0006769502654606396\n",
      "65-th iteration, new layer inserted. now 43 layers\n",
      "[ 51.52807864  91.52298815  50.77906718  90.93451864  51.99517287\n",
      "  92.64280358  52.86853336  84.31144291  14.53910988  49.00438134\n",
      "  51.04859862  84.22353351 100.93722799  90.44239552  95.69270917\n",
      " 104.17112854  90.14306453 119.90274723  84.57756526 136.40893549\n",
      "  83.3708653  152.62495169 104.89258197 185.5499445   96.25270671\n",
      " 159.91239843 111.01760299  31.51840475  23.28342846   4.79392977\n",
      "  14.89506595   9.71716691  87.67455496  10.14419992  49.96430327\n",
      "   6.24382088 208.8447255    2.78474039 605.55268308   7.92838989\n",
      " 125.37964111  28.81443145  14.81119568]\n",
      "66-th iteration, loss: 0.04114956183238527, 20 gd steps\n",
      "insert gradient: -0.0006786764429288177\n",
      "66-th iteration, new layer inserted. now 45 layers\n",
      "[5.18368300e+01 9.16686623e+01 5.09070677e+01 9.09051969e+01\n",
      " 5.16395887e+01 9.25052474e+01 5.27641368e+01 8.44967934e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.43315829e+01 4.89232180e+01\n",
      " 5.11072867e+01 8.42524203e+01 1.00764310e+02 9.03199138e+01\n",
      " 9.56409753e+01 1.04094434e+02 9.00991002e+01 1.19841364e+02\n",
      " 8.46187957e+01 1.36194941e+02 8.34710769e+01 1.52526884e+02\n",
      " 1.04678282e+02 1.85677316e+02 9.64950606e+01 1.59586891e+02\n",
      " 1.10030100e+02 3.33083214e+01 2.26197185e+01 6.16355873e+00\n",
      " 1.43727154e+01 1.05046167e+01 8.77678971e+01 1.06783986e+01\n",
      " 4.89577503e+01 7.19066522e+00 2.08226666e+02 2.57891132e+00\n",
      " 6.05139863e+02 7.98452970e+00 1.25706236e+02 2.90246760e+01\n",
      " 1.47346656e+01]\n",
      "67-th iteration, loss: 0.04092303053575768, 22 gd steps\n",
      "insert gradient: -0.001393188755523479\n",
      "67-th iteration, new layer inserted. now 47 layers\n",
      "[5.22101729e+01 9.17823099e+01 5.09733380e+01 9.09107970e+01\n",
      " 0.00000000e+00 7.10542736e-15 5.12768871e+01 9.21800987e+01\n",
      " 5.23882899e+01 8.49892682e+01 4.61617219e-02 4.66044172e-01\n",
      " 1.40993408e+01 4.88819322e+01 5.13389124e+01 8.44485278e+01\n",
      " 1.00972827e+02 9.00417679e+01 9.58775141e+01 1.03787118e+02\n",
      " 9.01893967e+01 1.19604008e+02 8.48137102e+01 1.35694672e+02\n",
      " 8.37579301e+01 1.52207105e+02 1.04253857e+02 1.85926766e+02\n",
      " 9.70192889e+01 1.58872426e+02 1.07701897e+02 3.82472447e+01\n",
      " 2.06356683e+01 9.84289745e+00 1.31110903e+01 1.23189699e+01\n",
      " 8.83839268e+01 1.20463401e+01 4.60868343e+01 9.58906551e+00\n",
      " 2.06771123e+02 2.00037931e+00 6.04136697e+02 8.10275715e+00\n",
      " 1.26534858e+02 2.94194801e+01 1.45682437e+01]\n",
      "68-th iteration, loss: 0.040433348051829306, 56 gd steps\n",
      "insert gradient: -0.001422025602750386\n",
      "68-th iteration, new layer inserted. now 43 layers\n",
      "[ 51.33297053  91.41888438  50.68920679  90.92849518  51.30254869\n",
      "  92.05025728  52.41690987  86.75556657  13.42228925  48.84130578\n",
      "  51.60951564  84.4134119  100.70832974  88.87098489  95.89771945\n",
      " 102.48801445  90.2436149  118.87413092  84.95974984 134.80415718\n",
      "  83.94026093 151.3437557  103.28625109 186.05980149  97.63127958\n",
      " 157.26168024 102.54021257  48.54804467  14.92922353  17.85819489\n",
      "  11.12540145  15.551692    91.54457722  16.27136783  39.45893806\n",
      "  14.48833671 204.18175196   0.99739127 601.98227264   8.01970955\n",
      " 127.49436081  29.64516528  14.2956963 ]\n",
      "69-th iteration, loss: 0.04040629337588685, 19 gd steps\n",
      "insert gradient: -0.0003059721043266245\n",
      "69-th iteration, new layer inserted. now 45 layers\n",
      "[ 51.46492465  91.48458354  50.7741829   90.90379111  51.19507781\n",
      "  92.01956384  52.40968043  86.74680031  13.33789183  48.85501561\n",
      "  51.59325293  84.36201513 100.61725779  88.82338907  95.8467612\n",
      " 102.4678944   90.22512479 118.8738788   84.95917133 134.78310804\n",
      "  83.93587427 151.28225201 103.20062018 186.05713063  97.64989167\n",
      " 157.21891372 102.34957514  48.82281039  14.72966899  18.11019679\n",
      "  11.08930005  15.67396283  91.67950433  16.4682278   39.23607997\n",
      "  14.64539118 163.26461023   0.          40.81615256   0.96309268\n",
      " 601.88525436   8.01675284 127.48700672  29.64197576  14.27600556]\n",
      "70-th iteration, loss: 0.039640467852019905, 25 gd steps\n",
      "insert gradient: -0.0037578730803693697\n",
      "70-th iteration, new layer inserted. now 47 layers\n",
      "[5.18052867e+01 9.07933477e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.93277908e+01 9.12553221e+01 5.16232770e+01 9.08792162e+01\n",
      " 5.19623568e+01 8.92973998e+01 1.26768733e+01 4.72663597e+01\n",
      " 5.27468249e+01 8.46454633e+01 1.00982406e+02 8.51111211e+01\n",
      " 9.72767274e+01 9.75214633e+01 9.11912933e+01 1.16282795e+02\n",
      " 8.49926209e+01 1.33441749e+02 8.39169938e+01 1.49994989e+02\n",
      " 1.03110455e+02 1.85942378e+02 9.70567824e+01 1.52564083e+02\n",
      " 9.44521928e+01 6.19590834e+01 3.92461116e+00 3.59905402e+01\n",
      " 6.72676728e+00 2.49480894e+01 1.00642282e+02 2.94254730e+01\n",
      " 3.13614486e+01 2.47309774e+01 1.56795069e+02 7.42440432e+00\n",
      " 3.79918207e+01 5.25380861e+00 5.95892073e+02 7.88685891e+00\n",
      " 1.27393912e+02 2.97341974e+01 1.38721614e+01]\n",
      "71-th iteration, loss: 0.03949265863967348, 16 gd steps\n",
      "insert gradient: -0.0006093789144938625\n",
      "71-th iteration, new layer inserted. now 47 layers\n",
      "[5.15712377e+01 9.08960961e+01 4.39906687e-01 3.68975733e-02\n",
      " 4.98095794e+01 9.10652624e+01 5.10308859e+01 9.08103929e+01\n",
      " 5.20368230e+01 8.92390458e+01 1.24702324e+01 4.71311134e+01\n",
      " 5.26276633e+01 8.45205023e+01 1.00603400e+02 8.50369725e+01\n",
      " 9.69845053e+01 9.74772813e+01 9.09874594e+01 1.16259620e+02\n",
      " 8.49050042e+01 1.33396394e+02 8.38515187e+01 1.49944443e+02\n",
      " 1.02943775e+02 1.85886971e+02 9.69394862e+01 1.52427097e+02\n",
      " 9.40813878e+01 6.20620557e+01 4.15094393e+00 3.61066388e+01\n",
      " 6.72265061e+00 2.50883813e+01 1.00665008e+02 2.95363806e+01\n",
      " 3.12468694e+01 2.47327579e+01 1.56666560e+02 7.44486041e+00\n",
      " 3.80168533e+01 5.30265204e+00 5.95806747e+02 7.84415577e+00\n",
      " 1.27356041e+02 2.97673509e+01 1.38653530e+01]\n",
      "72-th iteration, loss: 0.03876653430795891, 54 gd steps\n",
      "insert gradient: -0.001194894197101371\n",
      "72-th iteration, new layer inserted. now 47 layers\n",
      "[4.95467930e+01 8.98553118e+01 5.09129538e+01 8.96057754e+01\n",
      " 0.00000000e+00 7.10542736e-15 4.92887078e+01 8.90301943e+01\n",
      " 5.16778830e+01 8.96196686e+01 1.15402973e+01 4.57698901e+01\n",
      " 5.34162269e+01 8.31854515e+01 9.89482473e+01 8.34303575e+01\n",
      " 9.52061996e+01 9.77683362e+01 8.88300273e+01 1.17243082e+02\n",
      " 8.33727603e+01 1.32298768e+02 8.33549592e+01 1.48886620e+02\n",
      " 1.01436671e+02 1.84210912e+02 9.57058269e+01 1.49559278e+02\n",
      " 8.96723209e+01 6.69103217e+01 2.41090724e+00 4.10634152e+01\n",
      " 3.52349158e+00 2.96507137e+01 1.03249668e+02 3.35879498e+01\n",
      " 2.89329304e+01 2.74705891e+01 1.51553268e+02 1.06588547e+01\n",
      " 3.81417015e+01 8.96685993e+00 5.91373457e+02 6.88485801e+00\n",
      " 1.24865172e+02 2.95824550e+01 1.32376906e+01]\n",
      "73-th iteration, loss: 0.0378502346860177, 30 gd steps\n",
      "insert gradient: -0.002666383864196614\n",
      "73-th iteration, new layer inserted. now 49 layers\n",
      "[4.88270620e+01 8.88066951e+01 5.01318151e+01 8.90542023e+01\n",
      " 3.33419717e-01 1.98626438e-01 4.82185904e+01 8.80888481e+01\n",
      " 5.10679512e+01 8.93149064e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.13750880e+01 4.31101890e+01 5.30489212e+01 8.27381156e+01\n",
      " 9.74921095e+01 8.08349840e+01 9.40968970e+01 9.48109958e+01\n",
      " 8.73015326e+01 1.15869270e+02 8.16128756e+01 1.31013598e+02\n",
      " 8.17290737e+01 1.47873990e+02 1.00874969e+02 1.82705846e+02\n",
      " 9.44547553e+01 1.47559960e+02 8.72096989e+01 7.04931937e+01\n",
      " 4.90421435e-01 4.42412839e+01 2.50880571e+00 3.31149097e+01\n",
      " 1.03571423e+02 3.50890624e+01 2.74882449e+01 2.97337190e+01\n",
      " 1.47276500e+02 1.41524492e+01 3.80107061e+01 1.27488797e+01\n",
      " 5.88056655e+02 5.71433589e+00 1.22731272e+02 2.86827824e+01\n",
      " 1.31717693e+01]\n",
      "74-th iteration, loss: 0.03654902447875046, 36 gd steps\n",
      "insert gradient: -0.0027426269387799017\n",
      "74-th iteration, new layer inserted. now 43 layers\n",
      "[ 47.97641509  86.84683466  48.41546908  87.43091427  48.99022662\n",
      "  86.3172407   49.47673969  88.15274311  12.99792758  39.19413804\n",
      "  52.21465261  82.37247294  95.95814566  78.76155351  93.43494084\n",
      "  91.40962335  86.2747278  114.21526077  80.26495687 129.81426577\n",
      "  80.4638088  146.3724205   99.58961296 180.35381822  92.96072748\n",
      " 145.57958126  85.34926479 120.1294892    1.96687829  36.15595529\n",
      " 103.0769052   35.1420412   25.72241855  31.68275121 141.14319841\n",
      "  17.03659442  35.73317759  15.72167215 584.9318499    5.18467304\n",
      " 120.16900764  27.77262639  12.62632489]\n",
      "75-th iteration, loss: 0.0360499541801607, 17 gd steps\n",
      "insert gradient: -0.0019804582095267007\n",
      "75-th iteration, new layer inserted. now 43 layers\n",
      "[ 46.28001168  85.99263383  48.07078845  87.45057175  49.31365296\n",
      "  86.24935891  49.00523596  87.46952426  13.55429633  36.69880617\n",
      "  52.61366599  82.61411596  95.28255112  78.43027857  92.78747717\n",
      "  89.48932196  85.6021617  113.51252693  79.26585909 129.63263098\n",
      "  79.47121583 147.38749425  99.6753902  178.89704869  91.47697033\n",
      " 143.77840956  85.72825841 120.28245433   2.31997919  36.32030853\n",
      " 104.02878029  34.69029641  24.74540351  32.755566   135.437207\n",
      "  19.34247187  33.57384009  17.34391275 583.55034649   5.26321816\n",
      " 118.95810522  27.37700171  12.42662631]\n",
      "76-th iteration, loss: 0.03527307853152127, 18 gd steps\n",
      "insert gradient: -0.0011344932641490944\n",
      "76-th iteration, new layer inserted. now 43 layers\n",
      "[ 47.14689267  86.11456006  47.78843784  86.72748197  47.88393438\n",
      "  85.67366333  48.84614142  87.239155    13.7413144   35.67837601\n",
      "  52.23113674  82.23194228  94.21527332  77.97132603  92.01672592\n",
      "  88.75503092  84.93183198 112.96863026  78.36520631 128.85886148\n",
      "  78.5640177  146.39931583  98.55114562 177.87325558  90.88699041\n",
      " 143.20918517  85.41272578 120.21552075   2.29500855  36.24014672\n",
      " 104.11881616  34.43070594  24.50811521  33.32657112 133.09917956\n",
      "  20.09414509  32.8788731   17.97673695 582.88991517   5.06665749\n",
      " 118.37796706  27.22286115  12.43805742]\n",
      "77-th iteration, loss: 0.03427647084249656, 21 gd steps\n",
      "insert gradient: -0.00330727807520594\n",
      "77-th iteration, new layer inserted. now 43 layers\n",
      "[ 48.69825849  86.25372496  47.74896029  85.36062144  46.08934269\n",
      "  84.42204236  48.09890869  86.35889885  14.54390377  32.22072884\n",
      "  51.87720385  81.72143053  92.51262393  77.40991602  90.83032375\n",
      "  87.32396841  83.89985899 112.20093856  77.47834924 127.64308757\n",
      "  77.93084691 144.80931031  97.03962142 174.85964163  89.75379722\n",
      " 141.86594216  85.47248455 120.74846371   2.87648286  36.81557347\n",
      " 105.01939399  33.99779194  22.97836926  35.22532017 124.68204757\n",
      "  22.78132035  30.68015     20.77412868 580.74987749   5.21063963\n",
      " 116.73830701  26.96331258  12.36011417]\n",
      "78-th iteration, loss: 0.03027735367604586, 161 gd steps\n",
      "insert gradient: -0.0005332488292476078\n",
      "78-th iteration, new layer inserted. now 45 layers\n",
      "[ 46.27400448  84.50555246  44.32708407  82.12663114  44.05259351\n",
      "  82.81414782  45.41610543  83.50289578  18.15472716  20.07970916\n",
      "  52.50298583  82.76645079  86.55822744  80.12815437  50.95711106\n",
      "   0.          33.97140737  85.10038414  76.6567489  116.10932373\n",
      "  71.77587364 126.82172556  74.58202977 141.7566499   94.25477343\n",
      " 165.9542595   84.98716107 135.75215491  85.40958505 122.47624048\n",
      "   2.14216322  39.46880902 102.56367198  58.55846947   8.32171373\n",
      "  59.53350314  97.57314729  41.48089858  17.96103177  34.48980106\n",
      " 565.93636409   6.9919515  111.20906176  26.14619529  12.77930078]\n",
      "79-th iteration, loss: 0.029719108667483955, 45 gd steps\n",
      "insert gradient: -0.0005967743411268476\n",
      "79-th iteration, new layer inserted. now 47 layers\n",
      "[ 46.16092449  84.37324035  43.94374462  81.67414656  43.56125521\n",
      "  82.61620432  45.38398056  83.46784218  19.0187659   17.04865073\n",
      "  53.17036033  82.93865269  85.95251659  81.34137009  50.3722953\n",
      "   2.87629523  31.01903829  84.19215075  75.81636587 116.87588535\n",
      "  70.61564012 127.87154672  74.20124819 142.52045771  95.15846397\n",
      " 164.95062865  83.32473237 135.27217365  86.26993066 121.85660804\n",
      "   2.51978193  39.18642324  97.7672303   65.13292554   3.64953562\n",
      "  66.25281703  92.17838316  46.65312554  15.83935877  40.54375571\n",
      " 338.85829435   0.         225.90552957   6.23968273 112.03148575\n",
      "  25.9953146   13.7450258 ]\n",
      "80-th iteration, loss: 0.02904193475076069, 21 gd steps\n",
      "insert gradient: -0.0007043826025234374\n",
      "80-th iteration, new layer inserted. now 47 layers\n",
      "[4.53509989e+01 8.34541462e+01 4.31364882e+01 8.19154934e+01\n",
      " 4.36464237e+01 8.28207050e+01 4.58259522e+01 8.50951406e+01\n",
      " 1.94844090e+01 1.34058312e+01 5.44995921e+01 8.28944743e+01\n",
      " 8.44817617e+01 8.39491661e+01 5.28723108e+01 4.99420806e+00\n",
      " 2.52331630e+01 8.54265085e+01 7.36349501e+01 1.19716800e+02\n",
      " 6.79365709e+01 1.29962969e+02 7.33009664e+01 1.45770719e+02\n",
      " 9.77033572e+01 1.61986168e+02 8.10268762e+01 1.35530002e+02\n",
      " 8.83809784e+01 1.21317896e+02 3.12346931e+00 3.86190868e+01\n",
      " 9.22587659e+01 7.10889496e+01 1.14213306e-01 7.41210464e+01\n",
      " 8.99076639e+01 5.57312098e+01 1.34861041e+01 4.37364544e+01\n",
      " 3.35626528e+02 6.82678034e+00 2.23323007e+02 4.27437942e+00\n",
      " 1.11688634e+02 2.59532668e+01 1.44917505e+01]\n",
      "81-th iteration, loss: 0.028822127210731446, 28 gd steps\n",
      "insert gradient: -0.0011047830564616902\n",
      "81-th iteration, new layer inserted. now 45 layers\n",
      "[ 45.62974099  83.92925848  43.35864731  81.25319345  42.86433414\n",
      "  82.35663886  44.91403135  84.97864352  19.78721994  12.57893643\n",
      "  54.48069913  83.03960892  84.33434688  84.10523721  52.99022257\n",
      "   5.71840692  24.14080778  85.55806972  72.73131365 120.59776311\n",
      "  67.71854349 130.72083679  72.90095596 146.10791943  97.49519972\n",
      " 161.31087239  80.22805544 135.81282962  88.177674   120.79347583\n",
      "   2.99508973  37.96126415  91.45631268 146.94952223  90.13277726\n",
      "  56.83147588  12.97122308  43.97073314 335.31356724   6.95470429\n",
      " 223.21026763   4.46812011 111.86437755  25.86641506  14.54929944]\n",
      "82-th iteration, loss: 0.02829169445268126, 39 gd steps\n",
      "insert gradient: -0.0006899860189269764\n",
      "82-th iteration, new layer inserted. now 45 layers\n",
      "[ 44.69346129  83.4903705   42.48179352  81.30426058  43.82380015\n",
      "  82.81745923  45.15276238  85.48478997  22.03534979   7.99894395\n",
      "  55.12776158  83.21734761  83.23887747  86.45086592  54.65300134\n",
      "  10.6225536   17.83075889  87.32449345  68.41053749 126.3275851\n",
      "  64.79838446 134.17509983  71.15488915 149.81187216  98.95200651\n",
      " 159.36498743  77.44389805 138.30669538  88.53566651 118.67054445\n",
      "   4.12229248  34.76404318  88.8832275  150.1130257   92.03371728\n",
      "  63.61239444  11.7301235   44.80392912 333.80862436   6.71861368\n",
      " 222.70419591   5.15022969 112.72875203  25.30448121  14.78989195]\n",
      "83-th iteration, loss: 0.027339523511054616, 46 gd steps\n",
      "insert gradient: -0.0016149152739907205\n",
      "83-th iteration, new layer inserted. now 47 layers\n",
      "[4.44723202e+01 8.33557260e+01 0.00000000e+00 7.10542736e-15\n",
      " 4.23484082e+01 8.13665640e+01 4.36123257e+01 8.30083680e+01\n",
      " 4.49641691e+01 8.56284534e+01 2.57214268e+01 9.35134488e-01\n",
      " 5.68122862e+01 8.24471463e+01 8.24660060e+01 8.94379623e+01\n",
      " 5.63346950e+01 1.97731653e+01 1.03123590e+01 9.02390612e+01\n",
      " 6.21220437e+01 1.33084646e+02 6.21986265e+01 1.37043131e+02\n",
      " 6.95891592e+01 1.55369913e+02 9.95143566e+01 1.57815568e+02\n",
      " 7.40345933e+01 1.41523949e+02 8.79058208e+01 1.17284136e+02\n",
      " 4.66752489e+00 3.10094712e+01 8.88597364e+01 1.54152598e+02\n",
      " 9.32277872e+01 6.87415409e+01 1.06920241e+01 4.46578726e+01\n",
      " 3.33476330e+02 5.95217661e+00 2.22593759e+02 5.48337687e+00\n",
      " 1.13870512e+02 2.56709589e+01 1.42515759e+01]\n",
      "84-th iteration, loss: 0.02709584484943967, 21 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "84-th iteration, new layer inserted. now 45 layers\n",
      "[ 45.37292124  83.95187892  43.94983334  81.44628255  42.64869508\n",
      "  82.42263972  43.90679267  85.35297492  84.39330028  82.48135142\n",
      "  82.6985505   89.92998809  56.12519748  21.96596466   8.808451\n",
      "  90.90923671  60.97394543 134.03728481  61.98927918 137.25188362\n",
      "  69.47552736 156.34881322  99.19889803 157.51841736  73.20872633\n",
      " 142.01905327  87.67558256 117.22434186   4.57468639  30.33719236\n",
      "  89.2990956   61.97606436   0.          92.96409655  93.40476186\n",
      "  69.58179365  10.42567061  44.58790511 333.54216037   5.78916968\n",
      " 222.59185821   5.43371903 114.11465344  25.90265348  14.20115418]\n",
      "85-th iteration, loss: 0.026426441906031335, 48 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "85-th iteration, new layer inserted. now 47 layers\n",
      "[ 47.55682072  84.21921445  43.25891376  81.75681495  42.20148292\n",
      "  82.85176545  45.42553599  82.88269056  84.94964151  81.47378228\n",
      "  81.44259504  95.68473498  55.88778758  35.89810196   2.3656875\n",
      "  19.12550775   0.          76.50203099  52.39448571 139.88016174\n",
      "  60.56552422 138.05999164  70.90582799 161.8535856   98.7610559\n",
      " 155.18163331  70.641534   142.30889106  80.70494081 118.68676427\n",
      "   0.79181336  27.56245271  96.86115484  63.61673456   1.80040288\n",
      "  95.08248588  94.7101891   76.80678729   8.95775193  44.58456938\n",
      " 334.12234673   5.0199741  221.96049903   5.24702103 115.40419912\n",
      "  27.87088304  12.95692365]\n",
      "86-th iteration, loss: 0.026147984243640204, 18 gd steps\n",
      "insert gradient: -0.0007514322221848027\n",
      "86-th iteration, new layer inserted. now 49 layers\n",
      "[4.53826610e+01 8.36551167e+01 4.36200993e+01 8.20785530e+01\n",
      " 4.34213132e+01 8.28810252e+01 4.48237884e+01 8.28902980e+01\n",
      " 8.51130040e+01 8.14568800e+01 6.08415211e+01 0.00000000e+00\n",
      " 2.02805070e+01 9.58536011e+01 5.56035280e+01 3.63085898e+01\n",
      " 1.92202306e+00 1.93724047e+01 3.04452953e-01 7.67484391e+01\n",
      " 5.29226186e+01 1.40161982e+02 6.08888008e+01 1.37938331e+02\n",
      " 7.09797993e+01 1.61909295e+02 9.85957855e+01 1.55041456e+02\n",
      " 7.06272477e+01 1.42360180e+02 8.00494709e+01 1.19136076e+02\n",
      " 4.21215864e-01 2.80022197e+01 9.72868324e+01 6.36729303e+01\n",
      " 1.92714140e+00 9.51672341e+01 9.46543061e+01 7.70986645e+01\n",
      " 8.92211091e+00 4.45552282e+01 3.34136760e+02 5.02944344e+00\n",
      " 2.21906657e+02 5.11884835e+00 1.15406483e+02 2.79201436e+01\n",
      " 1.29275946e+01]\n",
      "87-th iteration, loss: 0.02564361229758006, 743 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "87-th iteration, new layer inserted. now 47 layers\n",
      "[ 45.93956232  84.44183579  43.72821129  81.72476979  43.41616458\n",
      "  82.85789072  44.89956758  84.59233523  85.05652388  83.25032407\n",
      "  58.13824624   8.39736655  16.36701485  97.6567933   53.30493959\n",
      "  59.04534031   1.46606149  78.04585115  52.67060595 140.2452988\n",
      "  61.93825726 136.37482952  72.80511034 162.9527354   99.14786803\n",
      " 152.27480821  70.86337223 142.10891749  75.71292529 152.96697286\n",
      "  99.47214813  63.33750175   2.57108669  95.2631494   94.34225138\n",
      "  80.45868126   8.69209137  44.18310997 133.85767855   0.\n",
      " 200.78651782   4.74978286 221.67391018   4.96318235 115.76187328\n",
      "  28.23840966  12.58368269]\n",
      "88-th iteration, loss: 0.0253102641096795, 446 gd steps\n",
      "insert gradient: -0.00033382241490831507\n",
      "88-th iteration, new layer inserted. now 47 layers\n",
      "[ 46.14270657  84.67159784  43.5517004   81.8983688   43.0167593\n",
      "  82.87025496  44.61209465  86.43396432  83.82052478  86.30258687\n",
      "  58.45417303  18.80261707  10.2745032   96.50080968  50.13542443\n",
      " 144.85823143  51.98870978 138.04555888  63.93756277 131.95930334\n",
      "  75.66555436 166.07175049  98.91305377 147.76645019  71.60022208\n",
      " 142.56072813  74.22438476 155.72868813 101.42068476  61.17910317\n",
      "   3.6413284   92.74223168  94.08056791  88.28087676   8.81379573\n",
      "  40.28302642 132.1424442    3.50005991 159.50861747   0.\n",
      "  39.87715437   3.58851682 220.83736949   5.84357092 115.88922365\n",
      "  28.13426509  12.90488076]\n",
      "89-th iteration, loss: 0.025007029901998618, 36 gd steps\n",
      "insert gradient: -0.0003920442484530094\n",
      "89-th iteration, new layer inserted. now 49 layers\n",
      "[ 46.17040549  84.80509161  43.50179988  81.82092701  42.89304867\n",
      "  82.46173209  44.60781686  85.97630684  83.66820424  87.14837676\n",
      "  58.60856988  21.04794049   9.03764036  95.57886305  50.22263794\n",
      " 143.79506781  52.793425   134.9555689   65.75369255 128.52486025\n",
      "  77.60472268 167.96450389  98.43563505 108.52034363   0.\n",
      "  36.17344788  72.09103573 142.56092394  73.71202865 157.99485119\n",
      " 102.23565188  61.6734647    3.87463043  90.62214773  93.04722558\n",
      "  93.35260141   8.81668848  36.55341287 131.2810527    2.4038281\n",
      " 157.79107433  11.04303503  36.14747422  10.51061119 217.36826797\n",
      "   6.5945534  115.29169201  28.50244691  13.0369628 ]\n",
      "90-th iteration, loss: 0.024709338953863225, 21 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "90-th iteration, new layer inserted. now 51 layers\n",
      "[ 45.84233752  84.39377856  43.29287153  81.84591447  43.68627245\n",
      "  82.86687231  45.09118483  86.21340951  83.8559511   87.49624191\n",
      "  58.6471156   24.57040838   7.04760587  97.22653302  49.14924628\n",
      " 143.17092163  54.21585712 130.17116417  68.84958027 124.11022319\n",
      "  81.10091889  85.68713148   0.          85.68713148  98.43644507\n",
      " 103.18964351   4.55817696  30.04143043  69.1505682  143.35362852\n",
      "  74.00101618 158.92995451 102.98626681  62.27010453   4.03406443\n",
      "  88.92962979  92.21599252  97.50511358   8.62357901  34.05176829\n",
      " 132.95489559   1.57171775 159.02225109  14.94301847  33.22297212\n",
      "  14.1946467  213.15162508   6.88798637 114.78281174  29.16064033\n",
      "  12.56902132]\n",
      "91-th iteration, loss: 0.02264056906918264, 240 gd steps\n",
      "insert gradient: -0.00032169441690812226\n",
      "91-th iteration, new layer inserted. now 49 layers\n",
      "[ 48.03983707  85.64664854  44.93342288  81.45047112  43.31702577\n",
      "  82.11886961  45.23486619  85.56079651  83.51925022  92.71987392\n",
      "  55.58680897 143.45126994  47.10433325 128.71775123  69.47681917\n",
      " 106.74966439  80.28495545 109.61187852  90.99165166  81.86463824\n",
      "   5.4412338   76.5054975   98.94331965 100.59354916   9.83917831\n",
      "  21.21523724  66.2351455  108.1235701    0.          36.04119003\n",
      "  75.44257115 161.84398758 103.34395132  63.16995489   3.97335383\n",
      "  85.34412918  90.05627513 105.19938312   7.62560044  29.0267739\n",
      " 297.03937395  23.73353233  29.11453578  21.94972854 203.550442\n",
      "   7.41464073 112.93696328  29.368128    10.92418161]\n",
      "92-th iteration, loss: 0.021610147530292657, 25 gd steps\n",
      "insert gradient: -0.000512656003921886\n",
      "92-th iteration, new layer inserted. now 49 layers\n",
      "[ 49.44472193  86.53160468  46.48608098  80.13957462  43.62314134\n",
      "  80.92901245  46.43853301  83.94890972  82.31685865 100.32334568\n",
      "  51.56875164 140.94912234  57.24795939 105.52722543  81.17878702\n",
      "  94.86170292  84.58591116 107.66422966  93.17490826  84.13673684\n",
      "   7.31004487  69.05913443  98.20066807 104.65191146   8.52187697\n",
      "  17.72501797  69.70855858 105.15477262   3.36415587  32.07885961\n",
      "  76.02061251 163.09542181 100.93936331  66.95428214   1.12046893\n",
      "  85.5801589   84.40142738 112.42286284   3.97388337  31.69035496\n",
      " 299.72838423  27.2502308   26.05455184  25.09894116 198.21168927\n",
      "   6.28998856 109.12709929  27.45118333  11.56340461]\n",
      "93-th iteration, loss: 0.02136723117483573, 30 gd steps\n",
      "insert gradient: -0.00036823578081979236\n",
      "93-th iteration, new layer inserted. now 47 layers\n",
      "[ 48.12847642  85.82941521  45.90217414  80.26309084  44.23471313\n",
      "  81.35909597  46.3983206   84.45032173  82.26492722 100.14626373\n",
      "  51.70341401 140.36305819  57.24281734 103.93200296  81.25655422\n",
      "  95.41923593  84.23092721 107.58438462  93.07080479  84.56015981\n",
      "   7.2519713   68.25216155  97.72386068 105.36320888   7.77501551\n",
      "  16.75832071  70.82632514 104.74383902   3.47449474  30.73559547\n",
      "  76.63388008 163.74851264  99.65341938 154.04788834  82.39076314\n",
      " 113.8559215    3.5918745   33.34383666 299.05298564  26.87091026\n",
      "  26.09524662  26.09931349 198.26839151   7.69153828 109.58261295\n",
      "  27.75952191  10.75413878]\n",
      "94-th iteration, loss: 0.02134269895632784, 16 gd steps\n",
      "insert gradient: -9.464752599300033e-05\n",
      "94-th iteration, new layer inserted. now 49 layers\n",
      "[ 48.07089597  85.81020098  45.97146729  80.28261637  44.37612214\n",
      "  81.43163985  46.44422039  84.47945071  82.13424549 100.1374794\n",
      "  51.71705407 140.22659059  57.33502774 103.63900807  81.27715127\n",
      "  95.52309028  84.14068773 107.61126678  93.02355026  84.67839616\n",
      "   7.21954065  68.0937541   97.62358096 105.56651518   7.59870388\n",
      "  16.57175222  71.132243   104.68483773   3.52375597  30.45183947\n",
      "  76.78178032 163.88727393  99.44575465 154.18749638  82.05552437\n",
      " 114.09700528   3.41246653  33.6014338  298.85906787  26.83142242\n",
      "  26.07223411  26.25693458 148.69398316   0.          49.56466105\n",
      "   7.94272797 109.63636247  27.85981982  10.68139341]\n",
      "95-th iteration, loss: 0.021030526891741425, 28 gd steps\n",
      "insert gradient: -0.00019821232782970635\n",
      "95-th iteration, new layer inserted. now 51 layers\n",
      "[ 48.56285595  84.88002431  45.91647732  79.47462807  45.16845581\n",
      "  80.96664248  46.51049963  84.61797744  81.48119965 101.60829993\n",
      "  51.04064318 136.45556595  61.95142436  96.17663036  83.18264533\n",
      "  95.79368821  83.94688557 107.10774324  92.76795222  88.18248145\n",
      "   7.81951818  63.63753373  71.83337225   0.          23.94445742\n",
      " 112.64335278   1.70477811  11.85312707  80.67982407 102.83501594\n",
      "   7.7558159   21.53478035  74.55783078 166.54824978  98.60419283\n",
      " 154.42500487  80.43894915 116.86373558   2.41421296  34.73382517\n",
      " 296.02917697  28.6465339   25.67517779  29.43691038 145.54378287\n",
      "   6.23619509  45.43647872  14.32444883 111.32509689  29.94186233\n",
      "   9.63224714]\n",
      "96-th iteration, loss: 0.020994401101903606, 17 gd steps\n",
      "insert gradient: -0.0007827561496953951\n",
      "96-th iteration, new layer inserted. now 51 layers\n",
      "[ 46.96378567  85.01488953  46.09124547  79.50120952  44.26945973\n",
      "  81.32163496  47.28504063  84.71293472  80.85987356 101.87751813\n",
      "  51.62125412 136.00326731  62.13964499  96.28238776  83.08246547\n",
      "  95.84287512  83.99611494 107.33521616  92.69841655  88.40691576\n",
      "   7.7658216   63.36888714  71.31380258   1.36929199  23.42312242\n",
      " 112.95012849   1.08009897  11.91059321  81.61015579 103.28259226\n",
      "   8.10516431  21.37614575  74.40861913 166.65307702  98.31456472\n",
      " 154.09293262  80.36049027 116.88856747   2.42408523  34.66196281\n",
      " 295.69419566  28.72505836  25.51156993  29.60462412 145.29808674\n",
      "   6.5282239   44.8956952   14.64917964 111.23766856  30.06228661\n",
      "   9.68771048]\n",
      "97-th iteration, loss: 0.02092990845509193, 22 gd steps\n",
      "insert gradient: -0.00024660410931972145\n",
      "97-th iteration, new layer inserted. now 51 layers\n",
      "[4.81097891e+01 8.50787808e+01 4.52041019e+01 7.95911817e+01\n",
      " 4.50348810e+01 8.16258275e+01 4.66618268e+01 8.46277416e+01\n",
      " 8.08415458e+01 1.02285268e+02 5.15708758e+01 1.35553260e+02\n",
      " 6.23071166e+01 9.62025637e+01 8.29998104e+01 9.57621969e+01\n",
      " 8.39583046e+01 1.07440005e+02 9.27656232e+01 8.87292437e+01\n",
      " 7.84019716e+00 6.30119982e+01 7.06360361e+01 2.51462476e+00\n",
      " 2.27536112e+01 1.12976931e+02 2.19651654e-01 1.17454002e+01\n",
      " 8.27324385e+01 1.03597593e+02 8.41680177e+00 2.09179326e+01\n",
      " 7.40123110e+01 1.66894304e+02 9.82579568e+01 1.53747042e+02\n",
      " 8.03766335e+01 1.17012015e+02 2.52512336e+00 3.46920461e+01\n",
      " 2.95368148e+02 2.88280290e+01 2.53957989e+01 2.98209626e+01\n",
      " 1.45110219e+02 6.95481537e+00 4.43648142e+01 1.50779015e+01\n",
      " 1.11190009e+02 3.01463025e+01 9.72191208e+00]\n",
      "98-th iteration, loss: 0.020881806104402503, 19 gd steps\n",
      "insert gradient: -0.00042186376303599184\n",
      "98-th iteration, new layer inserted. now 49 layers\n",
      "[ 47.82732247  85.04161318  45.47951174  79.52380326  44.53814305\n",
      "  81.7788461   47.1440633   84.77819654  80.44321531 102.82989676\n",
      "  51.68077443 134.93801904  62.84722595  96.1159228   83.12393755\n",
      "  95.74286021  83.90834088 107.63167015  92.89379997  89.38600993\n",
      "   7.8735498   62.39283904  69.82646639   4.00270224  21.95253883\n",
      " 124.79421567  84.18331175 104.0281322    8.95506584  20.17796817\n",
      "  73.34111152 167.14123055  98.08731485 153.25125611  80.60797278\n",
      " 117.15580732   2.5684182   34.66939401 294.87390527  29.13867528\n",
      "  25.39025435  30.13656383 144.90255777   7.75336308  43.59420897\n",
      "  15.83308824 111.14956216  30.18476334   9.6591915 ]\n",
      "99-th iteration, loss: 0.02086784933483656, 13 gd steps\n",
      "insert gradient: -0.00021343271334754482\n",
      "99-th iteration, new layer inserted. now 49 layers\n",
      "[ 47.4019002   84.8973155   45.48891189  79.6101196   44.72324345\n",
      "  81.8420782   47.14252489  84.7697961   80.39180607 102.84191703\n",
      "  51.5636503  134.86188627  62.86421334  96.04938602  83.07077877\n",
      "  95.69073406  83.8980124  107.60564809  92.86527697  89.43665388\n",
      "   7.87619709  62.29760573  69.7224532    4.19278041  21.87476965\n",
      " 124.68386031  84.09998243 104.0269766    9.03407688  20.08557137\n",
      "  73.19855461 167.19031159  98.07488862 153.18463479  80.61819799\n",
      " 117.1901681    2.61727937  34.69051839 294.82072012  29.15910525\n",
      "  25.3640893   30.15532333 144.87535139   7.84165314  43.49100439\n",
      "  15.90225468 111.13904936  30.19022307   9.66771197]\n",
      "0-th iteration, loss: 0.7685138631670256, 18 gd steps\n",
      "insert gradient: -0.4826744511100171\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  83.096669      0.         3365.41509468]\n",
      "1-th iteration, loss: 0.5746331975541318, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[5.11140287e+01 1.26880929e+00 0.00000000e+00 1.02773553e+02\n",
      " 3.34580993e+03]\n",
      "2-th iteration, loss: 0.5700960776321193, 850 gd steps\n",
      "insert gradient: -0.20490237919241944\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[  55.98336985  105.43964074  121.75760461    0.         3206.28358807]\n",
      "3-th iteration, loss: 0.4575875046056896, 62 gd steps\n",
      "insert gradient: -0.1890126931504941\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  68.76912235  100.36107693  112.61619677   86.69892822  193.3927853\n",
      "    0.         2965.35604121]\n",
      "4-th iteration, loss: 0.3841607407012748, 21 gd steps\n",
      "insert gradient: -0.08292702346507685\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  62.97290032  104.59908194  103.74082444   87.53658804  170.4413365\n",
      "   81.30602014  420.18897091    0.         2521.13382549]\n",
      "5-th iteration, loss: 0.3490213637385154, 23 gd steps\n",
      "insert gradient: -0.13742629505668286\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  63.5529369   106.88869692   98.45029336   99.7375511   164.11726793\n",
      "   82.90396569  387.07931739   66.20985451  743.00312266    0.\n",
      " 1764.63241631]\n",
      "6-th iteration, loss: 0.3036313218760732, 55 gd steps\n",
      "insert gradient: -0.11928947385844682\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  63.76585669  106.40489837  100.05909305   97.17553261  159.00572887\n",
      "   91.87429094  379.84212901   78.66925571  290.88556565    0.\n",
      "  420.16803927   81.18566276 1716.75949781]\n",
      "7-th iteration, loss: 0.2523751578171495, 45 gd steps\n",
      "insert gradient: -0.0882877438871076\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  63.92046441  108.36153131  100.18548766   96.27496295  154.33964912\n",
      "  101.18112789  312.04739441    0.           62.40947888   73.53770913\n",
      "  265.62346261   93.39952954  392.0944901    85.0543804  1699.88942132]\n",
      "8-th iteration, loss: 0.20321785171915382, 72 gd steps\n",
      "insert gradient: -0.120018545559348\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  69.46197556  115.9581436    85.98183677   99.53853258  151.71683109\n",
      "   98.54999469  122.13242627    0.          157.02740521   85.40212646\n",
      "   45.98238768   79.74525569  254.14786966   90.0682623   393.66153076\n",
      "   90.2203565  1690.26458843]\n",
      "9-th iteration, loss: 0.13768517937888275, 26 gd steps\n",
      "insert gradient: -0.030613694781803057\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  69.42914305  118.28959059   77.16714074  114.12904549  151.71565246\n",
      "  105.5199673    86.7706129    90.31302989  137.29690256   86.20941479\n",
      "   42.85210562   81.16799117  254.05366235   89.6267495   252.73461039\n",
      "    0.          140.40811688   84.88538614 1698.16526878]\n",
      "10-th iteration, loss: 0.12328002006344164, 74 gd steps\n",
      "insert gradient: -0.022995680799404145\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  64.75408806  125.67049627   78.70333552   98.57850944  157.42604552\n",
      "  100.98738159   86.0667452   101.6564483   131.48787718   86.84227719\n",
      "   45.15475939   75.84081549   63.24823899    0.          189.74471696\n",
      "   98.01466776  242.14219261   37.64497583  123.32090861   69.86351711\n",
      " 1707.29010635]\n",
      "11-th iteration, loss: 0.11316599017581397, 73 gd steps\n",
      "insert gradient: -0.019202240106965925\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  67.02186602  126.63929806   74.99697848   99.05701993  158.65888188\n",
      "  105.99489828   82.81038936  110.30086228  125.12889463   93.27367028\n",
      "   48.27245538   83.81196058   43.55490482   44.51425004   61.22120909\n",
      "    0.          107.13711592  123.69317543  244.19463118   39.63735402\n",
      "  119.69113728   63.72296199 1717.78635275]\n",
      "12-th iteration, loss: 0.10455940929657519, 20 gd steps\n",
      "insert gradient: -0.009288890545089495\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  67.63025808  131.05401692   70.74468039  103.49015714  156.67375247\n",
      "  119.81242956   72.14783675  112.24604091  127.30079553   93.6695827\n",
      "   47.18472215   88.22066744   37.51639263   79.11962657   29.535874\n",
      "   41.48208256   95.54706581  127.3205753   250.52250994   26.4502102\n",
      "  122.01301232   67.16264854 1032.14613186    0.          688.09742124]\n",
      "13-th iteration, loss: 0.09771037399701357, 15 gd steps\n",
      "insert gradient: -0.007890015438996768\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "[ 62.31544581 124.65065334  75.22311638 108.7084602  156.65153381\n",
      " 108.85728581  75.54599198 117.40777206 123.2991578   97.98496016\n",
      "  50.93813161  78.36263634  42.07947643  73.53945767  30.78909042\n",
      "  53.34334244  90.27058336 116.38323559 249.88813952  26.34342742\n",
      " 121.04115812  68.73072306 573.84107338   0.         459.0728587\n",
      "   6.11859538 691.205567  ]\n",
      "14-th iteration, loss: 0.09442969912720216, 22 gd steps\n",
      "insert gradient: -0.01096810673164823\n",
      "14-th iteration, new layer inserted. now 29 layers\n",
      "[ 66.81093008 124.66910069  70.97613599 109.57162484 155.96816733\n",
      " 115.8679587   70.76861325 122.37637082 123.43623122  96.80559738\n",
      "  49.93230792  76.10282928  43.70580601  76.12718739  28.07592903\n",
      "  57.66055635  89.74867618 109.96486474 251.75482083  26.93725978\n",
      " 120.19247405  64.54540825 280.82860978   0.         280.82860978\n",
      "  17.27216787 455.36618951   7.028431   698.80291158]\n",
      "15-th iteration, loss: 0.09051096434838564, 16 gd steps\n",
      "insert gradient: -0.007969064617677053\n",
      "15-th iteration, new layer inserted. now 31 layers\n",
      "[ 68.37884654 121.18048781  72.01723675 111.06755699 156.84800825\n",
      " 108.00348662  73.55345278 121.82109801 123.58866457  97.81438767\n",
      "  50.25267306  75.7751522   42.52025295  77.44811621  28.01541913\n",
      "  56.71180239  91.42616987 107.8509702  253.591962    26.07035898\n",
      "  74.90154776   0.          44.94092866  62.98531577 270.04819836\n",
      "  24.12472854 270.02207147  25.59679053 450.19354551   7.81124582\n",
      " 701.82849177]\n",
      "16-th iteration, loss: 0.08717349575931871, 34 gd steps\n",
      "insert gradient: -0.008397405898772456\n",
      "16-th iteration, new layer inserted. now 33 layers\n",
      "[ 64.44332861 123.78705401  72.22850721 109.60788932 156.07577811\n",
      " 110.05366594  74.77470584 121.66945108 122.65667709  97.19947993\n",
      "  50.0457597   76.82253692  42.14027553  77.59896286  29.57721511\n",
      "  55.26455229  89.14237706 105.88206667 254.03665482  28.58699687\n",
      "  69.5199481   16.7776836   36.52559403  60.86025279 267.08430544\n",
      "  31.58086803 267.76073573  27.6829948  444.52260138   8.89697664\n",
      " 301.97498769   0.         402.63331692]\n",
      "17-th iteration, loss: 0.08203954262576246, 27 gd steps\n",
      "insert gradient: -0.004899796610677888\n",
      "17-th iteration, new layer inserted. now 35 layers\n",
      "[ 63.6895788  127.45793316  71.51666646 106.18849163 156.43819611\n",
      " 110.59905861  73.46218358 123.13083309 121.14197476  98.52535017\n",
      "  51.20858406  75.58054535  41.60719173  79.93934914  31.43417864\n",
      "  50.52682319  86.31946551 110.36559776 146.99058118   0.\n",
      " 110.24293588  30.68059847  59.88308875  32.47559399  28.61139576\n",
      "  63.78101932 261.71159878  35.85257286 271.41029819  30.09270813\n",
      " 429.34731961  16.93468064 292.37499207  25.05927503 398.29677998]\n",
      "18-th iteration, loss: 0.07647759711388596, 51 gd steps\n",
      "insert gradient: -0.0037928143625982464\n",
      "18-th iteration, new layer inserted. now 37 layers\n",
      "[ 63.96137363 125.06118749  70.731822   106.70860574  78.29284829\n",
      "   0.          78.29284829 109.75066073  70.80312249 127.81995795\n",
      " 117.95197672 100.65022536  52.95218998  78.93405423  36.81483434\n",
      "  78.68644811  40.44154302  48.34357836  65.00710646 133.24017973\n",
      " 133.8034267   30.02621374 112.70035861  33.66089067  46.96843858\n",
      "  58.46983551  18.48155019  69.71554142 254.75415071  39.65508607\n",
      " 273.74018289  31.83224912 419.94746046  20.26316959 289.32490184\n",
      "  25.87533079 392.45218833]\n",
      "19-th iteration, loss: 0.07309878441409663, 48 gd steps\n",
      "insert gradient: -0.00696342895714405\n",
      "19-th iteration, new layer inserted. now 39 layers\n",
      "[ 64.93454139 126.61780212  69.45450226 106.18313665  60.09796827\n",
      "  15.95442825  81.94878499 112.42044789  70.67352459 137.11203283\n",
      " 110.03089613 108.07099508  56.02990828  76.84776471  32.57026429\n",
      "  79.48206472  43.55891609  57.97300523  51.02755711 148.3411127\n",
      " 129.0867399   54.22904429 101.82552986  28.56531457  47.33904388\n",
      "  73.54132126  18.89366681  74.28481687 126.55093174   0.\n",
      " 126.55093174  37.09349432 268.99801912  36.06802169 420.34340043\n",
      "  20.41121407 285.62108144  29.15286272 389.83301934]\n",
      "20-th iteration, loss: 0.06870581134880556, 16 gd steps\n",
      "insert gradient: -0.007557752480945692\n",
      "20-th iteration, new layer inserted. now 41 layers\n",
      "[6.56219665e+01 1.24202225e+02 0.00000000e+00 7.10542736e-15\n",
      " 6.80984773e+01 1.02082266e+02 4.97249915e+01 2.98530321e+01\n",
      " 8.27982255e+01 1.15244826e+02 7.05199306e+01 1.39100377e+02\n",
      " 1.05216493e+02 1.12436661e+02 5.71882574e+01 7.50157064e+01\n",
      " 2.99366978e+01 8.08106223e+01 4.47413279e+01 5.71964900e+01\n",
      " 5.02879061e+01 1.50310191e+02 1.26726892e+02 6.35080287e+01\n",
      " 9.93680140e+01 2.53414434e+01 4.71708564e+01 7.63068441e+01\n",
      " 1.97323358e+01 7.62539688e+01 1.23331960e+02 1.15993748e+01\n",
      " 1.24422559e+02 3.29351079e+01 2.67922507e+02 3.64597636e+01\n",
      " 4.19721870e+02 1.96668343e+01 2.84965924e+02 3.03207120e+01\n",
      " 3.89800247e+02]\n",
      "21-th iteration, loss: 0.06112199430813309, 45 gd steps\n",
      "insert gradient: -0.0030774520911685654\n",
      "21-th iteration, new layer inserted. now 41 layers\n",
      "[6.38295724e+01 1.24309500e+02 1.75093470e+00 2.94569897e-01\n",
      " 6.95314856e+01 9.66941448e+01 3.12648042e+01 8.02480179e+01\n",
      " 6.66621407e+01 1.24954647e+02 7.35091502e+01 1.35186313e+02\n",
      " 9.62223443e+01 1.27922312e+02 5.99522511e+01 7.63781568e+01\n",
      " 2.41920527e+01 8.28545710e+01 5.00354438e+01 5.47221026e+01\n",
      " 4.46706319e+01 1.57025463e+02 1.17876960e+02 9.19562731e+01\n",
      " 9.51527452e+01 2.47285853e+01 4.06993990e+01 8.07322361e+01\n",
      " 2.23733001e+01 7.62262596e+01 1.15989804e+02 2.59725600e+01\n",
      " 1.37794330e+02 3.37730595e+01 2.56111353e+02 3.41977191e+01\n",
      " 4.13977862e+02 1.79942877e+01 2.87283161e+02 3.33147629e+01\n",
      " 3.78260966e+02]\n",
      "22-th iteration, loss: 0.06058335151329075, 17 gd steps\n",
      "insert gradient: -0.0056086142588323805\n",
      "22-th iteration, new layer inserted. now 39 layers\n",
      "[ 66.45517211 124.11325721  70.27472761  96.55343987  31.54450452\n",
      "  80.47517219  65.21821362 123.65383158  72.62192376 134.55699189\n",
      "  96.94487658 128.09927485  61.49813601  76.9751489   23.47772224\n",
      "  82.80412884  50.18415145  54.58251352  44.23857801 156.82745467\n",
      " 117.24223139  93.09168941  94.95223764  26.19131566  40.04014389\n",
      "  80.4276374   22.74489172  75.58524497 116.03409866  27.06415607\n",
      " 138.64045056  34.87490892 255.20468597  32.66398742 413.04213534\n",
      "  18.04563607 286.98244189  33.7565747  376.89986774]\n",
      "23-th iteration, loss: 0.059263278614849056, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 41 layers\n",
      "[ 66.60429072 121.74427576  70.2379882   97.08106893  32.30529333\n",
      "  81.91823982  63.74718703 122.04354246  74.7057886  130.1724247\n",
      "  98.04921717 127.20661599  63.48827229  80.02444042  20.90097931\n",
      "  83.17546977  52.07844367  53.88829922  42.01516924  62.57093364\n",
      "   0.          93.85640046 114.71500478 100.63226037  92.40637221\n",
      "  33.83512956  36.47835097  79.94931401  26.32205332  68.16376071\n",
      " 113.8951474   30.77479794 140.66966571  40.54810663 253.38093296\n",
      "  28.73488064 410.71154804  17.16455599 287.02169559  36.2411389\n",
      " 372.89250962]\n",
      "24-th iteration, loss: 0.05925535637369937, 5 gd steps\n",
      "insert gradient: -0.0019505402055013297\n",
      "24-th iteration, new layer inserted. now 43 layers\n",
      "[6.66052832e+01 1.21745809e+02 7.02398691e+01 9.70811752e+01\n",
      " 3.23035199e+01 8.19175568e+01 6.37480087e+01 1.22044624e+02\n",
      " 7.47073169e+01 1.30172246e+02 9.80479072e+01 1.27205882e+02\n",
      " 6.34868292e+01 8.00241411e+01 2.08998289e+01 8.31754444e+01\n",
      " 5.20799710e+01 5.38898036e+01 4.20196207e+01 6.25750677e+01\n",
      " 1.20238652e-02 9.38605346e+01 1.14720276e+02 1.00636180e+02\n",
      " 9.24182339e+01 3.38418008e+01 3.64834947e+01 7.99507113e+01\n",
      " 2.63241360e+01 6.81635508e+01 6.83375122e+01 0.00000000e+00\n",
      " 4.55583415e+01 3.07752892e+01 1.40669725e+02 4.05486677e+01\n",
      " 2.53380691e+02 2.87342598e+01 4.10711232e+02 1.71642228e+01\n",
      " 2.87021375e+02 3.62411114e+01 3.72891921e+02]\n",
      "25-th iteration, loss: 0.05914974380866676, 25 gd steps\n",
      "insert gradient: -0.006347764104692596\n",
      "25-th iteration, new layer inserted. now 41 layers\n",
      "[6.66452664e+01 1.21889301e+02 7.04216807e+01 9.70875515e+01\n",
      " 3.20974585e+01 8.18238568e+01 6.37967987e+01 1.22156494e+02\n",
      " 7.48521292e+01 1.30132860e+02 9.78954255e+01 1.27137133e+02\n",
      " 6.33576887e+01 8.00252271e+01 2.07941874e+01 8.31580300e+01\n",
      " 5.21339229e+01 5.39537074e+01 4.20321244e+01 1.56317011e+02\n",
      " 1.14600616e+02 1.00733499e+02 9.25120377e+01 3.39448826e+01\n",
      " 3.63512398e+01 7.98921558e+01 2.62917386e+01 6.80308223e+01\n",
      " 6.83336498e+01 3.24928799e-01 4.55544926e+01 3.08166007e+01\n",
      " 1.40634438e+02 4.06210648e+01 2.53347840e+02 2.86352527e+01\n",
      " 4.10629184e+02 1.71005767e+01 2.86954366e+02 3.62275970e+01\n",
      " 3.72805140e+02]\n",
      "26-th iteration, loss: 0.05912808704774198, 23 gd steps\n",
      "insert gradient: -0.001781573434588907\n",
      "26-th iteration, new layer inserted. now 43 layers\n",
      "[ 66.62962192 121.90977725  70.44771864  97.08919775  32.06203262\n",
      "  81.80654655  63.81360674 122.18235297  74.88002245 130.12367338\n",
      "  97.86380237 127.12496918  63.33501088  80.0339293   20.77892208\n",
      "  83.15741355  52.15316581  53.97310811  42.03588028 156.30036954\n",
      " 114.56774981 100.76341599  92.54786662  33.97961814  36.31572134\n",
      "  79.87535392  26.28324558  67.99120818  68.32604965   0.41367068\n",
      "  45.54686795  30.82624571 140.62106876  40.64173148 253.33844047\n",
      "  28.60571182 410.60413792  17.08258634 229.54659736   0.\n",
      "  57.38664934  36.22165743 372.77899891]\n",
      "27-th iteration, loss: 0.05581148876034154, 28 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "27-th iteration, new layer inserted. now 45 layers\n",
      "[ 65.67111643 119.335265    70.83438037 100.96693041  31.07622154\n",
      "  80.19067047  65.83340942 120.80780882  78.60078725 126.24250858\n",
      "  97.36026618 124.38666178  64.34578441  88.90325251  15.34016762\n",
      "  82.87336253  57.68291732  56.72619096  37.25975521  58.72048404\n",
      "   0.          88.08072606 113.03046117 110.42853868  95.90514582\n",
      "  44.77822265  28.88703025  78.44999874  33.47768336  60.05385924\n",
      "  62.37038867  19.33661203  35.54205365  37.04734388 135.97141925\n",
      "  47.52029911 255.48058718  26.68366416 409.94639087  15.16207742\n",
      " 225.55756068  10.70076482  50.30374787  39.27298007 371.4923346 ]\n",
      "28-th iteration, loss: 0.05054755516097602, 999 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "28-th iteration, new layer inserted. now 45 layers\n",
      "[ 67.48199234 119.71577489  69.73101914 100.45036038  33.99180169\n",
      "  81.60476448  66.07716527 118.37365309  77.05982616 127.78158363\n",
      " 100.10944948 122.91933772  66.13045774 106.85596513   7.06704111\n",
      "  78.91729683  66.26448656  71.47799476  30.09324838 121.72706241\n",
      "  48.39876653   0.          72.5981498  108.34593096  96.79996218\n",
      "  65.96872982  18.5088584   76.62896762  49.1735558   55.94260928\n",
      "  52.85119077  50.65585971  26.11166128  39.03916144 125.72990375\n",
      "  59.58319627 266.18943747  28.33004132 407.67546764  20.57029656\n",
      " 222.25015619  22.78860066  41.97947552  46.09086543 374.09685697]\n",
      "29-th iteration, loss: 0.0474992259344591, 27 gd steps\n",
      "insert gradient: -0.0021922234772918355\n",
      "29-th iteration, new layer inserted. now 47 layers\n",
      "[6.72136560e+01 1.16903737e+02 7.07420698e+01 1.04670601e+02\n",
      " 3.57436428e+01 7.97743339e+01 6.37381409e+01 1.17968189e+02\n",
      " 7.54582228e+01 1.30469887e+02 9.82992591e+01 1.22233867e+02\n",
      " 6.92557580e+01 1.21580322e+02 3.95026084e+00 7.29706976e+01\n",
      " 0.00000000e+00 3.55271368e-15 5.69544014e+01 9.33134310e+01\n",
      " 3.03662699e+01 8.68406255e+01 5.24738871e+01 1.51680945e+01\n",
      " 6.45597843e+01 1.12719365e+02 9.16389912e+01 9.36403442e+01\n",
      " 4.29039147e+00 8.25649772e+01 6.24649973e+01 5.41578442e+01\n",
      " 3.96694089e+01 7.86144450e+01 2.08042371e+01 4.07063976e+01\n",
      " 1.17466356e+02 7.37402712e+01 2.64984688e+02 2.90817497e+01\n",
      " 4.09087270e+02 2.60022398e+01 2.15571576e+02 3.35854880e+01\n",
      " 3.39482009e+01 5.51699432e+01 3.73232027e+02]\n",
      "30-th iteration, loss: 0.04653814140530261, 35 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 49 layers\n",
      "[6.67921461e+01 1.16880747e+02 6.87823911e+01 1.06091581e+02\n",
      " 3.95532629e+01 7.48685219e+01 6.15985998e+01 1.17444518e+02\n",
      " 7.42442790e+01 1.37039991e+02 9.59237858e+01 1.23228256e+02\n",
      " 6.95911354e+01 1.22766749e+02 1.17170898e+00 7.36054571e+01\n",
      " 1.36299098e+00 3.72706471e-02 5.83657979e+01 9.21081210e+01\n",
      " 3.02972554e+01 8.41487289e+01 5.56014300e+01 1.70536177e+01\n",
      " 6.20553213e+01 1.10570269e+02 9.18334035e+01 9.86137350e+01\n",
      " 9.56397133e-01 8.68398394e+01 6.08549329e+01 5.95759535e+01\n",
      " 3.58957455e+01 8.23101041e+01 2.24129837e+01 3.64300454e+01\n",
      " 1.15829338e+02 7.89390872e+01 2.63036979e+02 2.99775100e+01\n",
      " 4.12413469e+02 3.13520469e+01 5.22029245e+01 0.00000000e+00\n",
      " 1.56608774e+02 3.74670457e+01 3.28877853e+01 5.74694913e+01\n",
      " 3.74513980e+02]\n",
      "31-th iteration, loss: 0.04560079854484833, 25 gd steps\n",
      "insert gradient: -0.0011674667470664957\n",
      "31-th iteration, new layer inserted. now 43 layers\n",
      "[ 68.09788492 115.37936451  69.07829735 104.60833032  40.54251493\n",
      "  74.5613328   62.51677041 116.81921376  74.39223978 136.97296656\n",
      "  95.642779   123.46985977  69.44916465 199.33839329  61.1087843\n",
      "  93.32577172  30.08586298  80.82432512  56.6343846   19.54026774\n",
      "  60.46840377 108.99271769  91.76553338 189.80533057  61.12308172\n",
      "  62.99692375  34.05780819  84.2048367   23.79248104  32.45700423\n",
      " 114.49053467  81.88152028 262.9298828   31.44631487 413.98453577\n",
      "  38.08975712  45.81881023  10.19373599 152.5977959   38.67534066\n",
      "  32.35002705  58.39487624 374.67584841]\n",
      "32-th iteration, loss: 0.045479766130759894, 9 gd steps\n",
      "insert gradient: -0.002170379944883697\n",
      "32-th iteration, new layer inserted. now 45 layers\n",
      "[ 68.09091848 115.37669056  69.08774233 104.61411199  40.57399418\n",
      "  74.58061406  62.52922881 116.8312024   74.42201706 136.9853992\n",
      "  95.6748068  123.4907925   69.4815576  199.35340328  61.13939269\n",
      "  93.34209798  30.11588531  80.83287437  56.66741053  19.56929183\n",
      "  60.49205991 108.99824042  91.7730552  189.80582395  61.11926716\n",
      "  63.00436577  34.04046002  84.18318797  23.70546802  32.36474136\n",
      " 114.37605942  81.82750188 262.77495962  31.36926256 248.27695773\n",
      "   0.         165.51797182  38.03319548  45.70384028  10.16615925\n",
      " 152.49837166  38.64191869  32.32203269  58.39289726 374.675094  ]\n",
      "33-th iteration, loss: 0.04378647520801972, 25 gd steps\n",
      "insert gradient: -0.015271252531403273\n",
      "33-th iteration, new layer inserted. now 47 layers\n",
      "[6.84724442e+01 1.14445561e+02 6.99840861e+01 1.02035053e+02\n",
      " 4.17843913e+01 7.59319651e+01 6.24754917e+01 1.15650784e+02\n",
      " 7.47736997e+01 1.35019026e+02 9.71832654e+01 1.21403089e+02\n",
      " 6.88300426e+01 1.97167278e+02 6.40741414e+01 9.92816908e+01\n",
      " 3.01074172e+01 7.06663245e+01 6.34249893e+01 2.24880925e+01\n",
      " 5.42245408e+01 1.04747886e+02 9.36016336e+01 1.88560041e+02\n",
      " 5.89979267e+01 7.55498263e+01 2.72869922e+01 8.46738822e+01\n",
      " 2.39052232e+01 2.76765716e+01 1.16416410e+02 8.66729950e+01\n",
      " 2.61802345e+02 3.56862573e+01 0.00000000e+00 1.77635684e-15\n",
      " 2.49871268e+02 1.19213060e+01 1.52746493e+02 6.27638789e+01\n",
      " 3.34530588e+01 2.57308144e+01 1.43370685e+02 4.14928745e+01\n",
      " 2.94431819e+01 6.66548439e+01 3.71745787e+02]\n",
      "34-th iteration, loss: 0.043632192433148594, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 49 layers\n",
      "[6.84628920e+01 1.14419004e+02 6.99471530e+01 1.02030523e+02\n",
      " 4.17696876e+01 7.59047468e+01 6.23526653e+01 1.15612045e+02\n",
      " 7.47900248e+01 1.34999435e+02 9.72024546e+01 1.21413335e+02\n",
      " 6.88552785e+01 1.97159611e+02 6.39883954e+01 9.92468254e+01\n",
      " 3.00847462e+01 7.06411623e+01 6.33695326e+01 2.25275837e+01\n",
      " 5.41361321e+01 1.04688624e+02 9.35752098e+01 1.88572266e+02\n",
      " 5.89551204e+01 7.55636231e+01 2.73389595e+01 8.47047580e+01\n",
      " 2.39978795e+01 2.77278172e+01 1.16474777e+02 8.67104223e+01\n",
      " 2.61890678e+02 3.56967836e+01 5.34992774e-02 1.06912454e-02\n",
      " 2.49924692e+02 1.20294205e+01 1.52768865e+02 6.28044768e+01\n",
      " 3.34853618e+01 2.57647647e+01 7.17057954e+01 0.00000000e+00\n",
      " 7.17057954e+01 4.15403522e+01 2.95030589e+01 6.66801586e+01\n",
      " 3.71740936e+02]\n",
      "35-th iteration, loss: 0.04059222076191368, 87 gd steps\n",
      "insert gradient: -0.004485757381306859\n",
      "35-th iteration, new layer inserted. now 49 layers\n",
      "[ 67.76488403 118.62047154  66.89713309 100.9090906   44.28757474\n",
      "  77.38203755  61.33972887 113.79758262  76.40388715 133.93955642\n",
      "  97.92960264 122.59746933  68.45289306 189.83053956  70.10097902\n",
      " 104.92566533  34.21169587  59.02297281  56.41060146  45.97060398\n",
      "  39.97047248  99.53427773  95.19977219 188.33136804  66.2666312\n",
      "  86.01324697  20.27810516  82.5046766   24.56078542  22.05283261\n",
      " 117.08077294  95.16341026 267.33248278  30.07886478   1.65392453\n",
      "   1.5567009  251.19989885  36.48942412 131.96571835  69.50053269\n",
      "  27.65748256  31.77426422  66.46099602  16.21434236  63.57509058\n",
      "  53.87937459  26.81316259  71.00094484 373.96922727]\n",
      "36-th iteration, loss: 0.04035049794616282, 49 gd steps\n",
      "insert gradient: -0.005721478888472481\n",
      "36-th iteration, new layer inserted. now 49 layers\n",
      "[ 66.19286876 116.05924637  69.54789838 101.32960774  43.03262995\n",
      "  77.43544663  62.3675529  114.26156283  76.91040247 132.9887182\n",
      "  97.31921293 123.29386606  67.96256042 190.16642239  69.64564152\n",
      " 105.56524493  36.09915226  54.0522024   56.05351413  52.05038252\n",
      "  37.58217848  99.21577485  95.87723861 187.62576737  65.3377893\n",
      "  90.33147317  19.85957117  80.49933249  28.06255238  19.76952107\n",
      " 114.51505267  96.99296392 266.5230878   29.22093229   2.88902346\n",
      "   2.61831177 250.40077108  37.5745583  131.45214713  73.30203275\n",
      "  25.79942135  34.03374766  66.92998452  16.28132905  62.51206863\n",
      "  56.68594178  24.62594397  73.88285554 374.24820799]\n",
      "37-th iteration, loss: 0.040045001070503364, 85 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "37-th iteration, new layer inserted. now 51 layers\n",
      "[ 66.09753518 115.42748474  69.50697725 101.56888091  43.44370435\n",
      "  77.89223884  61.84543453 113.94233805  76.91741041 134.49370069\n",
      "  97.38762018 123.10194496  68.06003464 189.6211729   70.04449287\n",
      " 105.33633297  37.88073046  50.12646004  56.10714109  57.26650719\n",
      "  35.25776838  98.63383883  96.68152409 188.15252737  63.74527477\n",
      "  95.50924009  20.05108226  75.107511    31.77590983  16.27572584\n",
      " 113.3881865   99.80098538 131.22716178   0.         131.22716178\n",
      "  26.68260129  10.01375837   9.39200769 245.94802738  36.25122447\n",
      " 130.57460914  80.01145501  23.04969339  35.53707779  68.06340586\n",
      "  16.86202057  59.44036904  60.86227698  22.11301652  79.10834791\n",
      " 374.79380217]\n",
      "38-th iteration, loss: 0.03932537207835201, 24 gd steps\n",
      "insert gradient: -0.0014209862970492424\n",
      "38-th iteration, new layer inserted. now 53 layers\n",
      "[ 68.00690757 116.55732591  67.36300945 101.10449551  44.28095423\n",
      "  78.12702373  62.24019537 113.97939629  76.77503456 134.69075172\n",
      "  97.53118205 122.85077444  67.48807942 189.54444476  70.20244401\n",
      " 105.6070578   38.15107476  49.4673653   55.86830686  57.97775024\n",
      "  35.34338237  98.2555039   96.49588713 187.54289298  64.43232686\n",
      "  96.20999086  19.47761283  74.46318709  32.2030267   15.87519172\n",
      "  85.16854835   0.          28.38951612  99.5550225  129.2318406\n",
      "   9.94996273 128.74535692  24.87080981  11.27113554   9.8743599\n",
      " 243.5857595   35.70473745 131.49773622  80.90476591  22.06914975\n",
      "  35.49474444  68.56649458  17.43586537  59.61774572  61.48898921\n",
      "  22.19714888  79.90347469 375.13535426]\n",
      "39-th iteration, loss: 0.038798969895737376, 20 gd steps\n",
      "insert gradient: -0.002380646253436698\n",
      "39-th iteration, new layer inserted. now 55 layers\n",
      "[6.64647579e+01 1.16460819e+02 6.86461456e+01 1.01007782e+02\n",
      " 4.38751927e+01 7.80969781e+01 6.18168010e+01 1.13850580e+02\n",
      " 7.65850738e+01 1.34233379e+02 9.73983064e+01 1.22699401e+02\n",
      " 6.77218734e+01 1.89524739e+02 7.02716833e+01 1.05791100e+02\n",
      " 3.82543775e+01 4.91267571e+01 5.59944125e+01 5.86054782e+01\n",
      " 3.49348385e+01 9.77321458e+01 9.66035642e+01 1.86412599e+02\n",
      " 0.00000000e+00 1.42108547e-14 6.45293004e+01 9.70452177e+01\n",
      " 1.89803033e+01 7.36575990e+01 3.15891721e+01 1.42451295e+01\n",
      " 8.44942132e+01 4.44177941e+00 2.78199705e+01 9.87263002e+01\n",
      " 1.28745778e+02 1.48279107e+01 1.27864794e+02 2.41217889e+01\n",
      " 1.14800945e+01 1.02065782e+01 2.42223030e+02 3.55550613e+01\n",
      " 1.32759376e+02 8.16993843e+01 2.19000877e+01 3.54518806e+01\n",
      " 6.89375868e+01 1.76937166e+01 5.96335818e+01 6.17357540e+01\n",
      " 2.17198852e+01 8.01529127e+01 3.75445742e+02]\n",
      "40-th iteration, loss: 0.03735586773917164, 74 gd steps\n",
      "insert gradient: -0.002203738626018892\n",
      "40-th iteration, new layer inserted. now 53 layers\n",
      "[ 67.04965338 115.49368144  67.53801776  99.34752696  44.42830726\n",
      "  78.8133101   62.14110923 113.26108513  76.95809156 131.42057648\n",
      "  98.04146372 121.94171016  66.9177646  188.60734707  70.4463363\n",
      " 106.98701932  38.91092979  45.94820273  55.50966265  64.16302605\n",
      "  33.1260957   93.27142527  97.0809753  180.92681997  65.8747414\n",
      " 104.50336483  14.57635368  67.82292479  30.53796619  10.61709042\n",
      "  83.19704408  12.48702263  32.1350256   96.55837614 127.42854502\n",
      "  33.40451199 125.66284494  20.84580695  12.41285284   8.69151714\n",
      " 238.46412792  35.13150282 134.83545232  84.86586292  21.16428111\n",
      "  34.7178226   69.16260544  19.50364151  57.75738795  63.24587157\n",
      "  21.12766604  80.66127594 375.85195435]\n",
      "41-th iteration, loss: 0.03680883862556318, 23 gd steps\n",
      "insert gradient: -0.00102693017325994\n",
      "41-th iteration, new layer inserted. now 53 layers\n",
      "[ 66.85698925 114.87676236  67.17696898  98.92363577  44.39911441\n",
      "  78.98464829  62.09932212 113.13627461  76.94888273 130.23660736\n",
      "  98.24146296 121.91833829  66.53189408 187.98129313  70.84737835\n",
      " 107.82179285  39.6521322   44.22091709  55.24088306  66.65593877\n",
      "  32.41718251  91.24664468  96.09199418 177.92756068  67.89590593\n",
      " 106.47548623  12.41602789  64.81145777  30.16445757  11.51162665\n",
      "  81.90977876  16.90083577  33.84548813  95.13286716 126.5270237\n",
      "  40.41380555 125.76029996  19.33758825  13.11348834   8.27356202\n",
      " 237.04366565  35.51318792 135.29779711  86.01153715  20.98789004\n",
      "  33.85291954  69.23120102  20.41646143  56.79431871  63.73158713\n",
      "  21.26767266  80.63129437 376.04091712]\n",
      "42-th iteration, loss: 0.0358158648865581, 25 gd steps\n",
      "insert gradient: -0.04388650908051514\n",
      "42-th iteration, new layer inserted. now 55 layers\n",
      "[ 67.27161061 113.56085818  65.73017556  98.50364556  44.75858299\n",
      "  79.54677216  62.87218913 113.90520639  76.89974005 128.12977782\n",
      "  98.34259756 123.04062706  66.95616079 186.29968017  71.61732732\n",
      " 110.03437962  42.1635996   39.84205604  54.23007426  71.22013774\n",
      "  31.42208738  87.79489698  92.64639015 131.86759553   0.\n",
      "  43.95586518  70.59653    107.46443826   8.86523464  62.59038614\n",
      "  36.41726815  13.47243843  80.89686326  25.21678821  31.10042003\n",
      "  91.18919189 124.66697562  50.07296064 125.64635639  17.61664831\n",
      "  13.91034191   8.34278488 234.313397    36.58878354 136.18934994\n",
      "  88.0032363   21.72353405  31.35192597  69.26470564  21.58372287\n",
      "  54.8249439   64.09885438  21.26828498  80.26963434 376.30723177]\n",
      "43-th iteration, loss: 0.035394002917228926, 494 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 55 layers\n",
      "[ 67.1897835  113.56451148  65.81239954  98.49129184  45.05696883\n",
      "  79.97520163  62.7515161  114.21973433  76.99367812 127.90251582\n",
      "  98.47244053 123.18632683  66.94988338 186.08154643  71.48822491\n",
      " 110.12531675  42.67478205  39.16838456  54.51859908  72.18281887\n",
      "  31.31761097  87.25762783  92.47280426 176.65617329  69.81353703\n",
      " 107.46170578   8.63742171  63.07922873  37.84771511  12.65734923\n",
      "  80.3869957   26.44212213  30.37363125  90.73421398  62.64859696\n",
      "   0.          62.64859696  51.38168498 125.85223036  17.64683575\n",
      "  13.8758289    8.68751471 233.80785194  36.73987512 135.46824781\n",
      "  88.24543727  22.2153765   31.23619465  69.48360544  22.1378832\n",
      "  54.79201467  64.34215213  21.48421041  80.24683948 376.41449867]\n",
      "44-th iteration, loss: 0.0334869769307104, 78 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "44-th iteration, new layer inserted. now 55 layers\n",
      "[ 67.47379092 114.01767275  65.02757514  96.95557101  46.3223404\n",
      "  82.57821529  61.349576   114.65977305  76.85612541 125.98322053\n",
      "  99.37588847 122.623741    67.33568345 185.45611862  71.57635828\n",
      " 109.2825257   53.76180197  22.79154448  54.96161632  88.80776024\n",
      "  29.22700358  74.78930516  89.06245524 187.63028264  63.39646978\n",
      " 107.78745163   6.51032699  61.15435139 134.58590941  35.76961669\n",
      "  26.87307483  90.71310621  61.13324909  13.63882706  48.04444118\n",
      "  71.46693649 124.08426572  21.17146352  21.33769097  15.91317281\n",
      " 218.2657487   35.78098653  68.5768992    0.          68.5768992\n",
      "  90.61314733  25.68312036  27.83205159  69.16667945  27.76976781\n",
      "  45.84297036  68.34043424  23.3669688   75.57017957 377.71731524]\n",
      "45-th iteration, loss: 0.03291986348745844, 27 gd steps\n",
      "insert gradient: -0.0019400729064028596\n",
      "45-th iteration, new layer inserted. now 55 layers\n",
      "[ 68.35196864 113.71746107  64.02906799  96.08403222  46.61864365\n",
      "  83.19084249  61.44445296 113.8479214   76.67903465 126.42980849\n",
      "  98.95905676 122.13485285  66.65169082 185.23558502  71.8275652\n",
      " 108.16067935  55.52324851  20.79842008  55.14530002  91.74609774\n",
      "  29.59273956  69.59399618  87.36763066 189.86045042  63.97197239\n",
      " 108.50380374   5.13484025  61.72064812 133.81013667  38.44993734\n",
      "  26.3174681   89.5010936   63.84612125  15.71160141  43.38308799\n",
      "  78.24831074 122.03030623  23.97032233  22.64658028  24.11425344\n",
      " 208.78104876  36.94171567  67.92118055   6.19379416  66.22956416\n",
      "  91.82704405  28.6034915   25.24902629  69.19418545  29.59616442\n",
      "  41.70493148  70.26556215  23.88203364  73.05584465 380.45853637]\n",
      "46-th iteration, loss: 0.03189837276813349, 128 gd steps\n",
      "insert gradient: -0.001332725943357243\n",
      "46-th iteration, new layer inserted. now 53 layers\n",
      "[ 67.71866143 110.09145464  60.10015437  91.84728587  46.57567284\n",
      "  87.96330444  64.32382054 114.32789582  75.7544961  125.21815976\n",
      "  98.15112436 120.25028128  66.65145611 182.40565268  74.40454984\n",
      " 109.03779318  54.97058089  21.51235924  53.65204422  94.29226095\n",
      "  33.42580266  59.13553048  81.95667798 196.28239222  63.77133328\n",
      " 184.54232207 129.92714961  49.39556918  25.68198167  81.52690464\n",
      "  64.42447099  22.18598078  36.93671495  90.62281541 120.05717761\n",
      "  26.50033782  20.32088993  34.02355181 202.0631602   38.3868358\n",
      "  70.78347235   9.80538277  61.97109929  91.19604093  32.80565047\n",
      "  23.94573332  64.71307171  35.65173874  36.24435111  73.15955275\n",
      "  26.10254296  67.6029983  381.21410277]\n",
      "47-th iteration, loss: 0.03124745111551141, 45 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 55 layers\n",
      "[ 65.43745514 105.43830093  58.57957602  86.47296833  47.31375843\n",
      "  97.47743224  63.49479156 116.62420236  74.54566077 128.7621377\n",
      "  94.9837668  119.58162037  68.61132922 179.12806286  72.79082315\n",
      " 111.53546851  56.52664726  14.93546048  56.75701065  98.40239438\n",
      "  34.12394853  49.95712438  84.72013768 194.85446815  65.1708431\n",
      "  88.35493467   0.          88.35493467 127.32141388  63.97170939\n",
      "  25.89556678  74.4550072   64.59671075  32.17995512  30.37493604\n",
      "  96.59419951 116.10018791  26.00273623  19.87297594  39.19038022\n",
      " 203.48908183  34.15364993  76.90901518  10.18686275  57.5984376\n",
      "  87.13816351  35.39144128  24.83582226  59.58666683  38.73914928\n",
      "  33.62473688  73.70531236  27.91230735  65.22555077 380.86204181]\n",
      "48-th iteration, loss: 0.03087300286367366, 41 gd steps\n",
      "insert gradient: -0.0008821532682215048\n",
      "48-th iteration, new layer inserted. now 55 layers\n",
      "[6.57087007e+01 1.07165091e+02 5.65817575e+01 8.90114846e+01\n",
      " 4.86411546e+01 9.15850674e+01 6.56599580e+01 1.15446251e+02\n",
      " 7.43540097e+01 1.24042484e+02 9.72355029e+01 1.18809178e+02\n",
      " 6.69764682e+01 1.80234666e+02 7.35926771e+01 1.10782271e+02\n",
      " 5.82610302e+01 1.25233315e+01 5.63693624e+01 9.82975519e+01\n",
      " 3.58269980e+01 4.54071247e+01 8.51418066e+01 1.94975933e+02\n",
      " 6.47301821e+01 1.82168553e+02 1.25118588e+02 6.71194307e+01\n",
      " 2.57440159e+01 7.13993647e+01 6.26249866e+01 3.53378771e+01\n",
      " 2.96407032e+01 9.92307323e+01 1.13374149e+02 2.40331561e+01\n",
      " 0.00000000e+00 1.77635684e-15 2.03022595e+01 4.23885949e+01\n",
      " 2.05919605e+02 3.13552201e+01 7.98207572e+01 1.43663630e+01\n",
      " 5.15669188e+01 8.38199917e+01 3.77319743e+01 2.50573834e+01\n",
      " 5.73826796e+01 4.25362770e+01 3.16679146e+01 7.34807432e+01\n",
      " 2.91282585e+01 6.25608452e+01 3.79119007e+02]\n",
      "49-th iteration, loss: 0.030660161937119026, 20 gd steps\n",
      "insert gradient: -0.006226713203228157\n",
      "49-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.53763697 106.13788845  57.40142156  87.86042697  48.17726289\n",
      "  93.63169002  65.30591803 115.39186868  74.24652707 124.75378265\n",
      "  96.46419531 118.7979506   66.97180937 178.70754156  74.19622142\n",
      " 111.03199594  58.76929858  10.95577084  56.72950808  98.8542057\n",
      "  36.04750215  43.59807093  85.50345328 194.5546422   65.38389887\n",
      " 180.15789604 125.42990657  67.7027211   25.45581246  70.64889112\n",
      "  63.19061501  36.70401794  29.15641359  99.67646368 111.98727554\n",
      "  23.32676877  20.65473558  43.54126497 207.16913559  29.72211036\n",
      "  80.1710741   17.00280075  49.27637197  82.66059355  38.40646933\n",
      "  25.13660892  56.63393049  43.4991653   30.85709931  73.56490192\n",
      "  29.8125891   61.44493134 378.21006739]\n",
      "50-th iteration, loss: 0.03062283701166576, 17 gd steps\n",
      "insert gradient: -0.0007200194756458767\n",
      "50-th iteration, new layer inserted. now 53 layers\n",
      "[ 65.39312443 106.06212136  57.32004088  87.80620959  48.09938908\n",
      "  93.60559366  65.27189201 115.38861513  74.24310315 124.74997399\n",
      "  96.48438205 118.79996151  66.96501463 178.70085619  74.1935259\n",
      " 111.02121505  58.74841065  10.92055729  56.70234572  98.84894159\n",
      "  36.04427428  43.58126887  85.49499852 194.54486498  65.38374288\n",
      " 180.15246393 125.40685621  67.69673217  25.43962889  70.63673452\n",
      "  63.19077303  36.70785023  29.14837424  99.68410222 111.98757337\n",
      "  23.35734741  20.71049348  43.5597588  207.19866865  29.70409476\n",
      "  80.18665324  17.00872737  49.24566402  82.64507315  38.39857243\n",
      "  25.12397759  56.61340396  43.50082818  30.823268    73.54788781\n",
      "  29.7929545   61.42617926 378.19322567]\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\rfeng6\\AppData\\Local\\Temp\\ipykernel_21280\\45751605.py\", line 10, in <module>\n",
      "    design.TFNN_train(epoch=100)\n",
      "  File \"c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\design.py\", line 59, in TFNN_train\n",
      "  File \"c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py\", line 48, in LM_optimize_d_simple\n",
      "    stack_J(J, n_arrs_ls, d, target_spec_ls)\n",
      "  File \"c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py\", line 155, in stack_J\n",
      "    get_J(\n",
      "  File \"c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\gets\\get_jacobi.py\", line 49, in get_jacobi_simple\n",
      "    n_A_device = cuda.to_device(n_A)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py\", line 232, in _require_cuda_context\n",
      "    return fn(*args, **kws)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\api.py\", line 120, in to_device\n",
      "    to, new = devicearray.auto_device(obj, stream=stream, copy=copy,\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py\", line 886, in auto_device\n",
      "    devobj.copy_to_device(obj, stream=stream)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py\", line 232, in _require_cuda_context\n",
      "    return fn(*args, **kws)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py\", line 239, in copy_to_device\n",
      "    _driver.host_to_device(self, ary_core, self.alloc_size,\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\", line 3102, in host_to_device\n",
      "    fn(device_pointer(dst), host_pointer(src, readonly=True), size, *varargs)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py\", line 319, in safe_cuda_api_call\n",
      "    retcode = libfn(*args)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# deisgn starting with TiO2\n",
    "for run_num, init_ot in enumerate(np.linspace(1, 5000, 200)):\n",
    "\n",
    "    d_init = np.array([init_ot], dtype='float')\n",
    "    film = FilmSimple('TiO2', 'SiO2', 'SiO2', d_init)\n",
    "    design = make_reflection_design(film)\n",
    "\n",
    "    design.TFNN_train(max(50, run_num), record=True)\n",
    "\n",
    "    fname = f'./raw_result/single_inc/reflection-0_inc-400to1000wls-init_single_layer_TiO2/{run_num}_design.pkl'\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(design, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "design_ls: list[Design] = []\n",
    "\n",
    "for run_num in range(400):\n",
    "    fname = f'./raw_result/single_inc/reflection-0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_design.pkl'\n",
    "\n",
    "    try:\n",
    "        f = open(fname, 'rb')\n",
    "        design_ls.append(pickle.load(f))\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        if e.args[0] != 2:  \n",
    "            print(e.args)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' See cell below for the convergence in one gd optimization after needle. \\nClearly inefficiency in escaping saddle point '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_ot, loss = [], []\n",
    "for d in design_ls:\n",
    "    init_ot.append(d.get_init_ot())\n",
    "    loss.append(d.calculate_loss())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, layout='tight', sharex=True)\n",
    "ax1.plot(init_ot, np.log10(loss))\n",
    "ax1.set_ylabel('log 10 loss')\n",
    "\n",
    "avg_steps = []\n",
    "init_ots = []\n",
    "for desgin in design_ls:\n",
    "    avg_steps.append(np.average([info['step'] for info in desgin.training_info]))\n",
    "    init_ots.append(desgin.get_init_ot())\n",
    "\n",
    "ax2.plot(init_ots, avg_steps)\n",
    "ax2.set_ylabel('avg gd steps after a needle insertion') \n",
    "ax2.set_xlabel('init_ot / nm')\n",
    "\n",
    "''' See cell below for the convergence in one gd optimization after needle. \n",
    "Clearly inefficiency in escaping saddle point '''\n",
    "# TODO: similar experiment for Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 2\n",
      "loss eqot: 0.007744717162379358\n",
      "loss optim: 0.0047574434038447615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot: title={'center': 'refractive index distribution at  701 nm'}, xlabel='position / nm'>,\n",
       " <Figure size 600x100 with 1 Axes>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "f_eqot = copy.deepcopy(f)\n",
    "f_optim = copy.deepcopy(f)\n",
    "from utils.substitutue import equal_optical_thickness, optimal_and_thin_film_approx_substitution_onestep_new\n",
    "from utils.loss import calculate_RMS_f_spec\n",
    "from analyze_utils.structure import plot_layer_thickness\n",
    "\n",
    "\n",
    "idx = 146\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "f = design_ls[idx].film\n",
    "wls_target = np.linspace(400, 1000, 500)\n",
    "\n",
    "d_min = 10.\n",
    "equal_optical_thickness(f_eqot, d_min)\n",
    "optimal_and_thin_film_approx_substitution_onestep_new(f_optim, d_min)\n",
    "print(f'removed {f.get_layer_number() - f_eqot.get_layer_number()}')\n",
    "print(f'loss eqot: {calculate_RMS_f_spec(f_eqot, design_ls[idx].target_specs)}')\n",
    "print(f'loss optim: {calculate_RMS_f_spec(f_optim, design_ls[idx].target_specs)}')\n",
    "\n",
    "ax.plot(f.get_spec(0., wls_target).WLS, f.get_spec(0., wls_target).get_R(), label='train', ls='--', zorder=100, c='C3')\n",
    "\n",
    "wls_dense = np.linspace(400, 1000, 10000)\n",
    "def plot_dense(f, label, c):\n",
    "    ax.plot(wls_dense, f.get_spec(0., wls_dense).get_R(), label=label, zorder=0, c=c)\n",
    "    f.remove_spec_param(wls=wls_dense)\n",
    "plot_dense(f, 'dense sample', 'C2')\n",
    "plot_dense(f_eqot, 'no thin film, eq ot', 'C0')\n",
    "plot_dense(f_optim, 'no thin film, optim', 'C1')\n",
    "\n",
    "ax.set_ylim(0.9, 1.03)\n",
    "ax.set_xlabel('wl / nm')\n",
    "ax.set_ylabel('reflectance')\n",
    "# ax.set_xlim(400, 500)\n",
    "ax.legend()\n",
    "\n",
    "plot_layer_thickness(design_ls[idx].film)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "epoch = []\n",
    "design_idx = 100\n",
    "for info in design_ls[design_idx].training_info:\n",
    "    losses.append(info['loss'])\n",
    "    epoch.append(info['step'])\n",
    "print(len(epoch))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot([sum(epoch[:i]) for i in range(len(epoch))], losses, label=f'design {design_idx}')\n",
    "ax.scatter([sum(epoch[:i]) for i in range(len(epoch))], losses, label='needle insertion', marker='x', c='red', zorder=2)\n",
    "ax.set_xlabel('gd step count')\n",
    "ax.set_ylabel('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99323d5a8e602150fc5547383fe7b71b247ac62304d4d5fb5a3f3a77ad4b3dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
