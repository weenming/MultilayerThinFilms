{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rho = (F_d - F_dnew) / np.dot(h.T, mu * h - g).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design terminated: zero layers\n",
      "0-th iteration, loss: 0.9545036783633585, 6 gd steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert gradient: -1.9865456241651303\n",
      "0-th iteration, new layer inserted. now 2 layers\n",
      "[25.12562814  0.        ]\n",
      "1-th iteration, loss: 0.7465113875206747, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9156536339285143e-15\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[2.98971315e+01 7.52391188e+01 0.00000000e+00 3.70814490e-14]\n",
      "2-th iteration, loss: 0.6598330591444459, 13 gd steps\n",
      "insert gradient: -0.011180187198537962\n",
      "2-th iteration, new layer inserted. now 4 layers\n",
      "[1.20649550e+00 0.00000000e+00 3.26128013e-16 5.52737458e+01]\n",
      "3-th iteration, loss: 0.6598181064726139, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0853385803732504e-16\n",
      "3-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79889184 55.24403845  0.        ]\n",
      "4-th iteration, loss: 0.6598180977378222, 7 gd steps\n",
      "insert gradient: -0.0002516729579486868\n",
      "4-th iteration, new layer inserted. now 2 layers\n",
      "[ 1.79629626 55.23511496]\n",
      "5-th iteration, loss: 0.6598180969660132, 7 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4778355223477435e-15\n",
      "5-th iteration, new layer inserted. now 4 layers\n",
      "[1.79710075e+00 5.52377582e+01 0.00000000e+00 1.38777878e-14]\n",
      "6-th iteration, loss: 0.6598180968911103, 6 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.0712105349610603e-14\n",
      "6-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693521e+00 5.52371452e+01 0.00000000e+00 1.10467191e-13]\n",
      "7-th iteration, loss: 0.6598180968911073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.356503847953933e-15\n",
      "7-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693619e+00 5.52371451e+01 0.00000000e+00 8.60422844e-14]\n",
      "8-th iteration, loss: 0.6598180968911045, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5004352501125248e-14\n",
      "8-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693714e+00 5.52371450e+01 0.00000000e+00 1.56319402e-13]\n",
      "9-th iteration, loss: 0.6598180968911019, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.363573330910042e-15\n",
      "9-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693807e+00 5.52371449e+01 0.00000000e+00 8.60422844e-14]\n",
      "10-th iteration, loss: 0.6598180968910995, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8101009363040587e-16\n",
      "10-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693897 55.23714483  0.        ]\n",
      "11-th iteration, loss: 0.6598180968910972, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9121591517570153e-16\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693985 55.2371447   0.        ]\n",
      "12-th iteration, loss: 0.6598180968910948, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1468047152164157e-16\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969407  55.23714457  0.        ]\n",
      "13-th iteration, loss: 0.6598180968910928, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.224031782464662e-15\n",
      "13-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694153e+00 5.52371444e+01 0.00000000e+00 5.28466160e-14]\n",
      "14-th iteration, loss: 0.6598180968910907, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9892941155157363e-16\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694233 55.23714429  0.        ]\n",
      "15-th iteration, loss: 0.6598180968910888, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.583708300486293e-14\n",
      "15-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694312e+00 5.52371441e+01 0.00000000e+00 1.64424030e-13]\n",
      "16-th iteration, loss: 0.6598180968910871, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1320377413810817e-14\n",
      "16-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694388e+00 5.52371440e+01 0.00000000e+00 1.17350574e-13]\n",
      "17-th iteration, loss: 0.6598180968910853, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0362565054452338e-16\n",
      "17-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694463 55.23714383  0.        ]\n",
      "18-th iteration, loss: 0.6598180968910837, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8272995209182463e-16\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694535 55.23714368  0.        ]\n",
      "19-th iteration, loss: 0.6598180968910822, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0867651663162567e-16\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694606 55.23714352  0.        ]\n",
      "20-th iteration, loss: 0.6598180968910806, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.067160651266847e-15\n",
      "20-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694675e+00 5.52371434e+01 0.00000000e+00 9.30366895e-14]\n",
      "21-th iteration, loss: 0.6598180968910792, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.778350666659227e-15\n",
      "21-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694742e+00 5.52371432e+01 0.00000000e+00 1.01141318e-13]\n",
      "22-th iteration, loss: 0.6598180968910778, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.970134152566637e-16\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694808 55.23714305  0.        ]\n",
      "23-th iteration, loss: 0.6598180968910765, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.186994245199285e-15\n",
      "23-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694871e+00 5.52371429e+01 0.00000000e+00 8.38218384e-14]\n",
      "24-th iteration, loss: 0.6598180968910753, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9505313046758664e-16\n",
      "24-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694934 55.23714274  0.        ]\n",
      "25-th iteration, loss: 0.6598180968910742, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7565438987808222e-16\n",
      "25-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694995 55.23714259  0.        ]\n",
      "26-th iteration, loss: 0.659818096891073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1980288825624532e-16\n",
      "26-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695054 55.23714244  0.        ]\n",
      "27-th iteration, loss: 0.6598180968910718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.783813964193745e-15\n",
      "27-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695112e+00 5.52371423e+01 0.00000000e+00 7.96029909e-14]\n",
      "28-th iteration, loss: 0.659818096891071, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.798331060581357e-15\n",
      "28-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695168e+00 5.52371421e+01 0.00000000e+00 7.96029909e-14]\n",
      "29-th iteration, loss: 0.65981809689107, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5814719355613968e-14\n",
      "29-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695223e+00 5.52371420e+01 0.00000000e+00 1.64424030e-13]\n",
      "30-th iteration, loss: 0.6598180968910691, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1731740719721552e-14\n",
      "30-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695277e+00 5.52371419e+01 0.00000000e+00 1.21458399e-13]\n",
      "31-th iteration, loss: 0.6598180968910682, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7601182191783408e-16\n",
      "31-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695329 55.23714172  0.        ]\n",
      "32-th iteration, loss: 0.6598180968910673, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1159624206210483e-16\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969538  55.23714158  0.        ]\n",
      "33-th iteration, loss: 0.6598180968910665, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1723686954017003e-14\n",
      "33-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695430e+00 5.52371414e+01 0.00000000e+00 1.21458399e-13]\n",
      "34-th iteration, loss: 0.6598180968910657, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.858573286350223e-15\n",
      "34-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695479e+00 5.52371413e+01 0.00000000e+00 7.03881398e-14]\n",
      "35-th iteration, loss: 0.659818096891065, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1168193098152713e-16\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695526 55.23714119  0.        ]\n",
      "36-th iteration, loss: 0.6598180968910643, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.807369573266401e-16\n",
      "36-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695573e+00 5.52371411e+01 0.00000000e+00 6.99440506e-15]\n",
      "37-th iteration, loss: 0.6598180968910637, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8612468418800893e-16\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695618 55.23714094  0.        ]\n",
      "38-th iteration, loss: 0.659818096891063, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1272231093831333e-16\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695662 55.23714081  0.        ]\n",
      "39-th iteration, loss: 0.6598180968910624, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.260055636190348e-15\n",
      "39-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695705e+00 5.52371407e+01 0.00000000e+00 3.25295346e-14]\n",
      "40-th iteration, loss: 0.6598180968910619, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0945556278062414e-16\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695747 55.23714058  0.        ]\n",
      "41-th iteration, loss: 0.6598180968910613, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.559431604874859e-15\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695789e+00 5.52371405e+01 0.00000000e+00 4.58522109e-14]\n",
      "42-th iteration, loss: 0.6598180968910607, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.584862155836908e-15\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695829e+00 5.52371404e+01 0.00000000e+00 6.73905376e-14]\n",
      "43-th iteration, loss: 0.6598180968910603, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7044282020962538e-14\n",
      "43-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695868e+00 5.52371402e+01 0.00000000e+00 1.77746706e-13]\n",
      "44-th iteration, loss: 0.6598180968910599, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.057837662737118e-15\n",
      "44-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695906e+00 5.52371401e+01 0.00000000e+00 9.30366895e-14]\n",
      "45-th iteration, loss: 0.6598180968910594, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.012125159192662e-14\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695943e+00 5.52371400e+01 0.00000000e+00 1.04694031e-13]\n",
      "46-th iteration, loss: 0.659818096891059, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9000377479086705e-16\n",
      "46-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969598  55.23713993  0.        ]\n",
      "47-th iteration, loss: 0.6598180968910585, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.510554523068066e-15\n",
      "47-th iteration, new layer inserted. now 4 layers\n",
      "[1.79696015e+00 5.52371398e+01 0.00000000e+00 5.57331958e-14]\n",
      "48-th iteration, loss: 0.6598180968910582, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8879085328442603e-16\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969605  55.23713973  0.        ]\n",
      "49-th iteration, loss: 0.6598180968910577, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.82067625661617e-16\n",
      "49-th iteration, new layer inserted. now 4 layers\n",
      "[1.79696084e+00 5.52371396e+01 0.00000000e+00 6.99440506e-15]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248375\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.19688805  0.          6.05436823]\n",
      "1-th iteration, loss: 0.7487306396172311, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.0225770581121275e-15\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.27472286e+01 6.23322519e+01 0.00000000e+00 6.37268016e-14\n",
      " 6.05436823e+00]\n",
      "2-th iteration, loss: 0.6614935451217976, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8780096804950728e-16\n",
      "2-th iteration, new layer inserted. now 3 layers\n",
      "[ 8.13284728 54.69001242  6.05436823]\n",
      "3-th iteration, loss: 0.6598229421478641, 20 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.983755953758121e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[1.99893171e+00 5.53284154e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "4-th iteration, loss: 0.6598183930225427, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.487867609514557e-15\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[1.87044516e+00 5.52431740e+01 0.00000000e+00 5.57331958e-14\n",
      " 6.05436823e+00]\n",
      "5-th iteration, loss: 0.6598181362125486, 12 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.3327453423744195e-16\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.82772077 55.23445791  6.05436823]\n",
      "6-th iteration, loss: 0.6598180968947274, 27 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.0973310194762192e-14\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[1.79685097e+00 5.52372964e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "7-th iteration, loss: 0.6598180968914297, 5 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.300776349944795e-08\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[1.79687517e+00 0.00000000e+00 5.89805982e-16 5.52371574e+01\n",
      " 6.05436823e+00]\n",
      "8-th iteration, loss: 0.6598180968913921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.369993456034439e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[1.79687770e+00 1.42961152e-09 0.00000000e+00 6.86213531e-08\n",
      " 2.52829478e-06 5.52371575e+01 6.05436823e+00]\n",
      "9-th iteration, loss: 0.6598180968913434, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.729235551779377e-16\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79688733 55.23715744  6.05436823]\n",
      "10-th iteration, loss: 0.6598180968913293, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.620790945729256e-16\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79688948e+00 5.52371572e+01 0.00000000e+00 6.88338275e-15\n",
      " 6.05436823e+00]\n",
      "11-th iteration, loss: 0.659818096891316, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.897177369990208e-16\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79689157 55.23715694  6.05436823]\n",
      "12-th iteration, loss: 0.6598180968913033, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.079421411969583e-15\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689361e+00 5.52371567e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "13-th iteration, loss: 0.6598180968912915, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8968001770470386e-16\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79689558 55.23715635  6.05436823]\n",
      "14-th iteration, loss: 0.65981809689128, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1159519332459903e-14\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689751e+00 5.52371560e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "15-th iteration, loss: 0.6598180968912691, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.846960872546786e-15\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689938e+00 5.52371557e+01 0.00000000e+00 5.97299987e-14\n",
      " 6.05436823e+00]\n",
      "16-th iteration, loss: 0.6598180968912588, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.951150578501617e-15\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690121e+00 5.52371553e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "17-th iteration, loss: 0.659818096891249, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.4057386078566874e-15\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690298e+00 5.52371550e+01 0.00000000e+00 5.35127498e-14\n",
      " 6.05436823e+00]\n",
      "18-th iteration, loss: 0.6598180968912397, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1195098280413984e-16\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79690471 55.23715464  6.05436823]\n",
      "19-th iteration, loss: 0.6598180968912307, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0592369028021686e-15\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690640e+00 5.52371543e+01 0.00000000e+00 2.81996648e-14\n",
      " 6.05436823e+00]\n",
      "20-th iteration, loss: 0.6598180968912223, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.6160976629067334e-16\n",
      "20-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79690804 55.23715391  6.05436823]\n",
      "21-th iteration, loss: 0.6598180968912142, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1164160175615541e-14\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690964e+00 5.52371535e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "22-th iteration, loss: 0.6598180968912065, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.6331408571684236e-15\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691120e+00 5.52371532e+01 0.00000000e+00 3.48610030e-14\n",
      " 6.05436823e+00]\n",
      "23-th iteration, loss: 0.6598180968911992, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.187726860111706e-16\n",
      "23-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79691273 55.23715282  6.05436823]\n",
      "24-th iteration, loss: 0.6598180968911922, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.635034846991289e-15\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691421e+00 5.52371525e+01 0.00000000e+00 8.79296636e-14\n",
      " 6.05436823e+00]\n",
      "25-th iteration, loss: 0.6598180968911856, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.664244979514424e-17\n",
      "25-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79691566 55.2371521   6.05436823]\n",
      "26-th iteration, loss: 0.6598180968911792, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.908640544581488e-15\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691707e+00 5.52371517e+01 0.00000000e+00 8.08242362e-14\n",
      " 6.05436823e+00]\n",
      "27-th iteration, loss: 0.6598180968911732, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.21448462509023e-15\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691845e+00 5.52371514e+01 0.00000000e+00 9.48130463e-14\n",
      " 6.05436823e+00]\n",
      "28-th iteration, loss: 0.6598180968911674, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.2537483566943716e-15\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691979e+00 5.52371511e+01 0.00000000e+00 5.35127498e-14\n",
      " 6.05436823e+00]\n",
      "29-th iteration, loss: 0.6598180968911619, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.012018982157213e-15\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692110e+00 5.52371507e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "30-th iteration, loss: 0.6598180968911567, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.1405282396704544e-16\n",
      "30-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692238 55.23715038  6.05436823]\n",
      "31-th iteration, loss: 0.6598180968911517, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.997453183682767e-15\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692363e+00 5.52371500e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "32-th iteration, loss: 0.6598180968911469, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.17035931420074e-15\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692484e+00 5.52371497e+01 0.00000000e+00 2.10942375e-14\n",
      " 6.05436823e+00]\n",
      "33-th iteration, loss: 0.6598180968911425, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.013911228755424e-15\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692603e+00 5.52371494e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "34-th iteration, loss: 0.6598180968911381, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4562210047939078e-16\n",
      "34-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692719 55.2371491   6.05436823]\n",
      "35-th iteration, loss: 0.659818096891134, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.3707140467404284e-16\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692832 55.23714879  6.05436823]\n",
      "36-th iteration, loss: 0.6598180968911301, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.037583100720405e-15\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692942e+00 5.52371485e+01 0.00000000e+00 8.08242362e-14\n",
      " 6.05436823e+00]\n",
      "37-th iteration, loss: 0.6598180968911264, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.377671027088762e-15\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693050e+00 5.52371482e+01 0.00000000e+00 6.50590692e-14\n",
      " 6.05436823e+00]\n",
      "38-th iteration, loss: 0.6598180968911228, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.493193568268102e-16\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693155 55.2371479   6.05436823]\n",
      "39-th iteration, loss: 0.6598180968911194, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1129333009102296e-14\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693258e+00 5.52371476e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "40-th iteration, loss: 0.659818096891116, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.252191611747597e-15\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693358e+00 5.52371473e+01 0.00000000e+00 6.50590692e-14\n",
      " 6.05436823e+00]\n",
      "41-th iteration, loss: 0.6598180968911129, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.2209255954305026e-16\n",
      "41-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693456 55.23714707  6.05436823]\n",
      "42-th iteration, loss: 0.6598180968911102, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9807497732101035e-15\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693552e+00 5.52371468e+01 0.00000000e+00 1.88737914e-14\n",
      " 6.05436823e+00]\n",
      "43-th iteration, loss: 0.6598180968911073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.9520768788016154e-15\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693645e+00 5.52371465e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "44-th iteration, loss: 0.6598180968911046, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0206705463414516e-16\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693736 55.23714629  6.05436823]\n",
      "45-th iteration, loss: 0.6598180968911022, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.9458291835299476e-15\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693825e+00 5.52371460e+01 0.00000000e+00 2.81996648e-14\n",
      " 6.05436823e+00]\n",
      "46-th iteration, loss: 0.6598180968910996, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.5864186119777466e-15\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693911e+00 5.52371458e+01 0.00000000e+00 6.72795153e-14\n",
      " 6.05436823e+00]\n",
      "47-th iteration, loss: 0.6598180968910973, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.684204866681388e-15\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693996e+00 5.52371456e+01 0.00000000e+00 4.70734562e-14\n",
      " 6.05436823e+00]\n",
      "48-th iteration, loss: 0.6598180968910952, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.396647831132266e-16\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694078 55.23714533  6.05436823]\n",
      "49-th iteration, loss: 0.6598180968910929, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0162389007964004e-16\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694159 55.2371451   6.05436823]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248406\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.19688805  0.         31.17999637]\n",
      "1-th iteration, loss: 0.7487306396172311, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.099184917260901e-15\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.27472286e+01 6.23322519e+01 0.00000000e+00 5.44009282e-14\n",
      " 3.11799964e+01]\n",
      "2-th iteration, loss: 0.6614935451217976, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.853610500483975e-15\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[8.13284728e+00 5.46900124e+01 0.00000000e+00 4.01900735e-14\n",
      " 3.11799964e+01]\n",
      "3-th iteration, loss: 0.6601012007629534, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.9332941248416625e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[0.00000000e+00 5.65980576e+01 0.00000000e+00 7.12763182e-14\n",
      " 3.11799964e+01]\n",
      "4-th iteration, loss: 0.6598337483318801, 13 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.215411978265948e-15\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[2.13293007e+00 5.54155477e+01 0.00000000e+00 8.59312621e-14\n",
      " 3.11799964e+01]\n",
      "5-th iteration, loss: 0.6598181525924166, 46 gd steps\n",
      "insert gradient: -0.00011824888684448805\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.83508581 55.22751142 31.17999637]\n",
      "6-th iteration, loss: 0.6598180968917238, 20 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.264314567140658e-06\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703945 55.23706697 31.17999637]\n",
      "7-th iteration, loss: 0.6598180968916124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.5524918364192316e-06\n",
      "7-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703922 55.23707317 31.17999637]\n",
      "8-th iteration, loss: 0.6598180968915247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.92619553302848e-06\n",
      "8-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703883 55.23707867 31.17999637]\n",
      "9-th iteration, loss: 0.6598180968914551, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.375031559096475e-06\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703832 55.23708354 31.17999637]\n",
      "10-th iteration, loss: 0.6598180968913996, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8898719009975185e-06\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703770e+00 0.00000000e+00 4.23272528e-16 5.52370879e+01\n",
      " 3.11799964e+01]\n",
      "11-th iteration, loss: 0.6598180968913179, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0256767097898178e-06\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703697e+00 0.00000000e+00 2.53269627e-15 5.52370955e+01\n",
      " 3.11799964e+01]\n",
      "12-th iteration, loss: 0.6598180968912668, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.361855202894515e-06\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703607 55.23710144 31.17999637]\n",
      "13-th iteration, loss: 0.6598180968912477, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.117541239960738e-06\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703506 55.23710378 31.17999637]\n",
      "14-th iteration, loss: 0.6598180968912316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9019701333574368e-06\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703401 55.23710588 31.17999637]\n",
      "15-th iteration, loss: 0.6598180968912178, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7116634577925293e-06\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703294e+00 0.00000000e+00 2.08860707e-15 5.52371078e+01\n",
      " 3.11799964e+01]\n",
      "16-th iteration, loss: 0.6598180968911986, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3506602139284799e-06\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703186 55.23711113 31.17999637]\n",
      "17-th iteration, loss: 0.6598180968911895, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2253054048852317e-06\n",
      "17-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703072 55.23711247 31.17999637]\n",
      "18-th iteration, loss: 0.6598180968911812, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1142972939242786e-06\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702958 55.23711368 31.17999637]\n",
      "19-th iteration, loss: 0.6598180968911738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0159094520524132e-06\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702845 55.23711479 31.17999637]\n",
      "20-th iteration, loss: 0.659818096891167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.286254316031056e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702732e+00 0.00000000e+00 2.10942375e-15 5.52371158e+01\n",
      " 3.11799964e+01]\n",
      "21-th iteration, loss: 0.6598180968911586, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.460414263621368e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702621 55.23711763 31.17999637]\n",
      "22-th iteration, loss: 0.6598180968911531, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.897759499853552e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702508 55.23711837 31.17999637]\n",
      "23-th iteration, loss: 0.659818096891148, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.395461774032062e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702396e+00 0.00000000e+00 3.35148576e-15 5.52371191e+01\n",
      " 3.11799964e+01]\n",
      "24-th iteration, loss: 0.6598180968911421, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.220162840118871e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702287e+00 0.00000000e+00 2.74780199e-15 5.52371203e+01\n",
      " 3.11799964e+01]\n",
      "25-th iteration, loss: 0.659818096891137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.311053140356156e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702177e+00 0.00000000e+00 1.68615122e-15 5.52371213e+01\n",
      " 3.11799964e+01]\n",
      "26-th iteration, loss: 0.6598180968911325, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.606283165024757e-07\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702068e+00 0.00000000e+00 2.10942375e-15 5.52371222e+01\n",
      " 3.11799964e+01]\n",
      "27-th iteration, loss: 0.6598180968911282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0584135104979846e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701960e+00 0.00000000e+00 1.88737914e-15 5.52371229e+01\n",
      " 3.11799964e+01]\n",
      "28-th iteration, loss: 0.6598180968911244, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.631056543952492e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701854e+00 0.00000000e+00 3.18495230e-15 5.52371235e+01\n",
      " 3.11799964e+01]\n",
      "29-th iteration, loss: 0.6598180968911206, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.296301334843892e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970175  55.23712405 31.17999637]\n",
      "30-th iteration, loss: 0.6598180968911174, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2984301618965836e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701647e+00 0.00000000e+00 1.67227343e-15 5.52371243e+01\n",
      " 3.11799964e+01]\n",
      "31-th iteration, loss: 0.659818096891114, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0278873227500972e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701547e+00 0.00000000e+00 2.53269627e-15 5.52371247e+01\n",
      " 3.11799964e+01]\n",
      "32-th iteration, loss: 0.6598180968911109, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8138963988199931e-07\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701449 55.23712515 31.17999637]\n",
      "33-th iteration, loss: 0.659818096891108, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8544921452134612e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701353e+00 0.00000000e+00 2.69922973e-15 5.52371253e+01\n",
      " 3.11799964e+01]\n",
      "34-th iteration, loss: 0.6598180968911053, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6685252802862932e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701260e+00 0.00000000e+00 3.18495230e-15 5.52371257e+01\n",
      " 3.11799964e+01]\n",
      "35-th iteration, loss: 0.6598180968911026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.519861618813484e-07\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701168 55.23712603 31.17999637]\n",
      "36-th iteration, loss: 0.6598180968911, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5775153351916073e-07\n",
      "36-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701079 55.23712619 31.17999637]\n",
      "37-th iteration, loss: 0.6598180968910977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6223860037682135e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700991e+00 0.00000000e+00 2.07472928e-15 5.52371263e+01\n",
      " 3.11799964e+01]\n",
      "38-th iteration, loss: 0.6598180968910954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4674725191795938e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700907e+00 0.00000000e+00 2.53269627e-15 5.52371267e+01\n",
      " 3.11799964e+01]\n",
      "39-th iteration, loss: 0.6598180968910933, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3431900200434232e-07\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700823e+00 0.00000000e+00 1.02695630e-15 5.52371270e+01\n",
      " 3.11799964e+01]\n",
      "40-th iteration, loss: 0.659818096891091, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.242533814663726e-07\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700742 55.23712724 31.17999637]\n",
      "41-th iteration, loss: 0.6598180968910892, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3057771209219962e-07\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700663e+00 0.00000000e+00 8.11850587e-16 5.52371274e+01\n",
      " 3.11799964e+01]\n",
      "42-th iteration, loss: 0.6598180968910873, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2036481789831688e-07\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700585e+00 0.00000000e+00 2.15105711e-16 5.52371276e+01\n",
      " 3.11799964e+01]\n",
      "43-th iteration, loss: 0.6598180968910855, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1203633676906384e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700510e+00 0.00000000e+00 2.13717932e-15 5.52371279e+01\n",
      " 3.11799964e+01]\n",
      "44-th iteration, loss: 0.6598180968910837, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0516522260259463e-07\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700436 55.23712809 31.17999637]\n",
      "45-th iteration, loss: 0.6598180968910822, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1178958427987709e-07\n",
      "45-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700364 55.2371282  31.17999637]\n",
      "46-th iteration, loss: 0.6598180968910806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1714586304332269e-07\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700293e+00 0.00000000e+00 1.88737914e-15 5.52371283e+01\n",
      " 3.11799964e+01]\n",
      "47-th iteration, loss: 0.6598180968910792, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0771957673484245e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700225e+00 0.00000000e+00 4.23272528e-16 5.52371285e+01\n",
      " 3.11799964e+01]\n",
      "48-th iteration, loss: 0.6598180968910778, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0005184769999815e-07\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700158 55.23712876 31.17999637]\n",
      "49-th iteration, loss: 0.6598180968910764, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0548176794838784e-07\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700093 55.23712886 31.17999637]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355636\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.39870033  0.         56.10381223]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2783751519858121e-16\n",
      "1-th iteration, new layer inserted. now 3 layers\n",
      "[42.89280919 62.32063131 56.10381223]\n",
      "2-th iteration, loss: 0.6614980133516497, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.507305171713015e-16\n",
      "2-th iteration, new layer inserted. now 3 layers\n",
      "[ 8.08544013 54.78446635 56.10381223]\n",
      "3-th iteration, loss: 0.6598183807845738, 30 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.8647711019298356e-18\n",
      "3-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.86777631 55.24408677 56.10381223]\n",
      "4-th iteration, loss: 0.6598180970048395, 31 gd steps\n",
      "insert gradient: -9.322925055799605e-05\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[1.79696278e+00 0.00000000e+00 4.23272528e-16 5.52363331e+01\n",
      " 5.61038122e+01]\n",
      "5-th iteration, loss: 0.6598180968912126, 12 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6036420502801359e-06\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703282 55.23710872 56.10381223]\n",
      "6-th iteration, loss: 0.6598180968912016, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.44846919806782e-06\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703172 55.23711031 56.10381223]\n",
      "7-th iteration, loss: 0.6598180968911918, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3112529321602298e-06\n",
      "7-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703061 55.23711175 56.10381223]\n",
      "8-th iteration, loss: 0.6598180968911831, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1898274767332669e-06\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702949e+00 0.00000000e+00 5.89805982e-16 5.52371130e+01\n",
      " 5.61038122e+01]\n",
      "9-th iteration, loss: 0.6598180968911718, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.479138085369971e-07\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702838e+00 0.00000000e+00 5.89805982e-16 5.52371154e+01\n",
      " 5.61038122e+01]\n",
      "10-th iteration, loss: 0.6598180968911629, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.615136892422476e-07\n",
      "10-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702724 55.23711726 56.10381223]\n",
      "11-th iteration, loss: 0.6598180968911572, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.040585346958925e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702609e+00 0.00000000e+00 8.11850587e-16 5.52371180e+01\n",
      " 5.61038122e+01]\n",
      "12-th iteration, loss: 0.6598180968911507, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.728786329349799e-07\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702495 55.2371194  56.10381223]\n",
      "13-th iteration, loss: 0.6598180968911459, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.366354990340606e-07\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702381 55.23711997 56.10381223]\n",
      "14-th iteration, loss: 0.6598180968911412, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.040135754126766e-07\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970227  55.23712051 56.10381223]\n",
      "15-th iteration, loss: 0.6598180968911369, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.745903844858363e-07\n",
      "15-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970216  55.23712101 56.10381223]\n",
      "16-th iteration, loss: 0.6598180968911329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.479945412715544e-07\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702053 55.23712148 56.10381223]\n",
      "17-th iteration, loss: 0.6598180968911289, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2389944214508384e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701947e+00 0.00000000e+00 2.15105711e-16 5.52371219e+01\n",
      " 5.61038122e+01]\n",
      "18-th iteration, loss: 0.6598180968911247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.536456363914294e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701844e+00 0.00000000e+00 8.11850587e-16 5.52371228e+01\n",
      " 5.61038122e+01]\n",
      "19-th iteration, loss: 0.6598180968911209, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.990655607186766e-07\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701742e+00 0.00000000e+00 3.35148576e-15 5.52371235e+01\n",
      " 5.61038122e+01]\n",
      "20-th iteration, loss: 0.6598180968911173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5652337607789277e-07\n",
      "20-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701641 55.23712406 56.10381223]\n",
      "21-th iteration, loss: 0.6598180968911141, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.527909192950026e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701542 55.23712432 56.10381223]\n",
      "22-th iteration, loss: 0.6598180968911111, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.488608351776035e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701445 55.23712457 56.10381223]\n",
      "23-th iteration, loss: 0.6598180968911084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4477267964964485e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701351e+00 0.00000000e+00 3.55965257e-15 5.52371248e+01\n",
      " 5.61038122e+01]\n",
      "24-th iteration, loss: 0.6598180968911054, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1237394617598347e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701259e+00 0.00000000e+00 2.74086309e-15 5.52371253e+01\n",
      " 5.61038122e+01]\n",
      "25-th iteration, loss: 0.6598180968911027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8692172484483314e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701168e+00 0.00000000e+00 1.90819582e-15 5.52371257e+01\n",
      " 5.61038122e+01]\n",
      "26-th iteration, loss: 0.6598180968911002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6681187462588299e-07\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701079e+00 0.00000000e+00 1.46410661e-15 5.52371261e+01\n",
      " 5.61038122e+01]\n",
      "27-th iteration, loss: 0.6598180968910977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5081371253473397e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700992e+00 0.00000000e+00 1.02695630e-15 5.52371264e+01\n",
      " 5.61038122e+01]\n",
      "28-th iteration, loss: 0.6598180968910954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3798307243895285e-07\n",
      "28-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700907 55.23712674 56.10381223]\n",
      "29-th iteration, loss: 0.6598180968910933, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4373106142749183e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700824 55.23712688 56.10381223]\n",
      "30-th iteration, loss: 0.6598180968910913, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4824377960125475e-07\n",
      "30-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700743 55.23712703 56.10381223]\n",
      "31-th iteration, loss: 0.6598180968910893, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5168440570198007e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700664e+00 0.00000000e+00 1.04777298e-15 5.52371272e+01\n",
      " 5.61038122e+01]\n",
      "32-th iteration, loss: 0.6598180968910874, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3657351486862967e-07\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700587 55.23712748 56.10381223]\n",
      "33-th iteration, loss: 0.6598180968910856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4040245408608427e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700512e+00 0.00000000e+00 3.78169718e-15 5.52371276e+01\n",
      " 5.61038122e+01]\n",
      "34-th iteration, loss: 0.659818096891084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2695104700972158e-07\n",
      "34-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700438 55.2371279  56.10381223]\n",
      "35-th iteration, loss: 0.6598180968910823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3096913907176313e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700367e+00 0.00000000e+00 3.55965257e-15 5.52371280e+01\n",
      " 5.61038122e+01]\n",
      "36-th iteration, loss: 0.6598180968910807, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1879261716718422e-07\n",
      "36-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700297 55.23712829 56.10381223]\n",
      "37-th iteration, loss: 0.6598180968910793, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2286911857384983e-07\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700228 55.23712841 56.10381223]\n",
      "38-th iteration, loss: 0.659818096891078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.260090710048752e-07\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700162 55.23712853 56.10381223]\n",
      "39-th iteration, loss: 0.6598180968910766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2833705267791535e-07\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700097e+00 0.00000000e+00 1.02695630e-15 5.52371287e+01\n",
      " 5.61038122e+01]\n",
      "40-th iteration, loss: 0.6598180968910753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1506731582963412e-07\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700034 55.23712891 56.10381223]\n",
      "41-th iteration, loss: 0.6598180968910742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1787797628036033e-07\n",
      "41-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699972 55.23712903 56.10381223]\n",
      "42-th iteration, loss: 0.6598180968910731, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1994712387416712e-07\n",
      "42-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699912 55.23712915 56.10381223]\n",
      "43-th iteration, loss: 0.659818096891072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2137456836738731e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699853e+00 0.00000000e+00 1.67227343e-15 5.52371293e+01\n",
      " 5.61038122e+01]\n",
      "44-th iteration, loss: 0.659818096891071, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0818108625124424e-07\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699796 55.23712951 56.10381223]\n",
      "45-th iteration, loss: 0.65981809689107, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1026968936725593e-07\n",
      "45-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969974  55.23712962 56.10381223]\n",
      "46-th iteration, loss: 0.659818096891069, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1174177457788375e-07\n",
      "46-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699686 55.23712973 56.10381223]\n",
      "47-th iteration, loss: 0.6598180968910681, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.126810585590531e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699632e+00 0.00000000e+00 1.92207361e-15 5.52371298e+01\n",
      " 5.61038122e+01]\n",
      "48-th iteration, loss: 0.6598180968910673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0011173439508334e-07\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699581 55.23713007 56.10381223]\n",
      "49-th iteration, loss: 0.6598180968910664, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0176682799972635e-07\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969953  55.23713017 56.10381223]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235569\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.39870033  0.         81.22944037]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2432348551227353e-16\n",
      "1-th iteration, new layer inserted. now 3 layers\n",
      "[42.89280919 62.32063131 81.22944037]\n",
      "2-th iteration, loss: 0.6614980133516497, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.812223874962927e-15\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[8.08544013e+00 5.47844664e+01 0.00000000e+00 8.08242362e-14\n",
      " 8.12294404e+01]\n",
      "3-th iteration, loss: 0.6600317106660547, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.14375534259676e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[7.99154559e-02 5.63837005e+01 0.00000000e+00 8.59312621e-14\n",
      " 8.12294404e+01]\n",
      "4-th iteration, loss: 0.6598182337738671, 28 gd steps\n",
      "insert gradient: -0.003208067918111697\n",
      "4-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.78943183 55.21113106 81.22944037]\n",
      "5-th iteration, loss: 0.6598181090174507, 8 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.8926260933401645e-15\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[1.79933416e+00 5.52448434e+01 0.00000000e+00 6.03961325e-14\n",
      " 8.12294404e+01]\n",
      "6-th iteration, loss: 0.6598180968911934, 7 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.5562500280027116e-07\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703541 55.23711806 81.22944037]\n",
      "7-th iteration, loss: 0.6598180968911866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.403167056127459e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703398e+00 0.00000000e+00 1.24206201e-15 5.52371185e+01\n",
      " 8.12294404e+01]\n",
      "8-th iteration, loss: 0.6598180968911798, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7542264101091106e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703259e+00 0.00000000e+00 1.46410661e-15 5.52371194e+01\n",
      " 8.12294404e+01]\n",
      "9-th iteration, loss: 0.6598180968911733, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.247278513368432e-07\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703122 55.23712013 81.22944037]\n",
      "10-th iteration, loss: 0.6598180968911675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.224398091968964e-07\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702987e+00 0.00000000e+00 2.10942375e-15 5.52371205e+01\n",
      " 8.12294404e+01]\n",
      "11-th iteration, loss: 0.6598180968911618, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.823357574441111e-07\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702855 55.2371211  81.22944037]\n",
      "12-th iteration, loss: 0.6598180968911564, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.834114019849022e-07\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702726e+00 0.00000000e+00 2.28289609e-15 5.52371214e+01\n",
      " 8.12294404e+01]\n",
      "13-th iteration, loss: 0.6598180968911513, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.507276905423268e-07\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.797026   55.23712195 81.22944037]\n",
      "14-th iteration, loss: 0.6598180968911466, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.539438997843736e-07\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702476 55.2371222  81.22944037]\n",
      "15-th iteration, loss: 0.659818096891142, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5596769411086797e-07\n",
      "15-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702356 55.23712246 81.22944037]\n",
      "16-th iteration, loss: 0.6598180968911377, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.569633775459286e-07\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702239 55.23712271 81.22944037]\n",
      "17-th iteration, loss: 0.6598180968911336, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5707476943174e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702125e+00 0.00000000e+00 1.07552856e-15 5.52371230e+01\n",
      " 8.12294404e+01]\n",
      "18-th iteration, loss: 0.6598180968911295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2670975559073653e-07\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702013 55.23712348 81.22944037]\n",
      "19-th iteration, loss: 0.6598180968911257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2899126890603915e-07\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701904 55.23712371 81.22944037]\n",
      "20-th iteration, loss: 0.6598180968911221, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3028582441794517e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701798e+00 0.00000000e+00 3.18495230e-15 5.52371239e+01\n",
      " 8.12294404e+01]\n",
      "21-th iteration, loss: 0.6598180968911187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0407831508847308e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701694 55.2371244  81.22944037]\n",
      "22-th iteration, loss: 0.6598180968911155, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.070010194276275e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701592 55.2371246  81.22944037]\n",
      "23-th iteration, loss: 0.6598180968911124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0890878117249461e-07\n",
      "23-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701493 55.23712481 81.22944037]\n",
      "24-th iteration, loss: 0.6598180968911095, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0994091056933794e-07\n",
      "24-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701397 55.23712502 81.22944037]\n",
      "25-th iteration, loss: 0.6598180968911067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1021937977336854e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701303e+00 0.00000000e+00 1.07552856e-15 5.52371252e+01\n",
      " 8.12294404e+01]\n",
      "26-th iteration, loss: 0.6598180968911039, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.855445940165025e-07\n",
      "26-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701211 55.23712565 81.22944037]\n",
      "27-th iteration, loss: 0.6598180968911014, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8754809822067574e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701121e+00 0.00000000e+00 2.35228503e-15 5.52371258e+01\n",
      " 8.12294404e+01]\n",
      "28-th iteration, loss: 0.6598180968910988, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6699345501771797e-07\n",
      "28-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701033 55.23712621 81.22944037]\n",
      "29-th iteration, loss: 0.6598180968910966, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7007203353320814e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700947 55.23712638 81.22944037]\n",
      "30-th iteration, loss: 0.6598180968910944, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7222010691256964e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700864e+00 0.00000000e+00 2.53269627e-15 5.52371265e+01\n",
      " 8.12294404e+01]\n",
      "31-th iteration, loss: 0.6598180968910923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5360165451289382e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700782e+00 0.00000000e+00 1.46410661e-15 5.52371269e+01\n",
      " 8.12294404e+01]\n",
      "32-th iteration, loss: 0.6598180968910902, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3879477723976764e-07\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700702e+00 0.00000000e+00 2.08860707e-15 5.52371272e+01\n",
      " 8.12294404e+01]\n",
      "33-th iteration, loss: 0.6598180968910883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2692404723932084e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700624e+00 0.00000000e+00 2.15105711e-16 5.52371275e+01\n",
      " 8.12294404e+01]\n",
      "34-th iteration, loss: 0.6598180968910864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1731778951375393e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700548e+00 0.00000000e+00 2.53269627e-15 5.52371277e+01\n",
      " 8.12294404e+01]\n",
      "35-th iteration, loss: 0.6598180968910846, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.094605399412967e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700473e+00 0.00000000e+00 2.10942375e-15 5.52371280e+01\n",
      " 8.12294404e+01]\n",
      "36-th iteration, loss: 0.659818096891083, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0295671605909823e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700400e+00 0.00000000e+00 1.88737914e-15 5.52371282e+01\n",
      " 8.12294404e+01]\n",
      "37-th iteration, loss: 0.6598180968910814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.750255558156067e-08\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700329 55.2371284  81.22944037]\n",
      "38-th iteration, loss: 0.65981809689108, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0435118917547732e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700259e+00 0.00000000e+00 1.46410661e-15 5.52371285e+01\n",
      " 8.12294404e+01]\n",
      "39-th iteration, loss: 0.6598180968910785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.768143019628885e-08\n",
      "39-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700191 55.23712871 81.22944037]\n",
      "40-th iteration, loss: 0.6598180968910772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0361248125253091e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700125e+00 0.00000000e+00 1.92207361e-15 5.52371288e+01\n",
      " 8.12294404e+01]\n",
      "41-th iteration, loss: 0.6598180968910758, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.626745612044189e-08\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700061e+00 0.00000000e+00 2.94902991e-15 5.52371290e+01\n",
      " 8.12294404e+01]\n",
      "42-th iteration, loss: 0.6598180968910745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.022220009921526e-08\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699998e+00 0.00000000e+00 1.67227343e-15 5.52371292e+01\n",
      " 8.12294404e+01]\n",
      "43-th iteration, loss: 0.6598180968910734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.518393253626606e-08\n",
      "43-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699936 55.23712939 81.22944037]\n",
      "44-th iteration, loss: 0.6598180968910723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.095649905957464e-08\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699876e+00 0.00000000e+00 1.68615122e-15 5.52371295e+01\n",
      " 8.12294404e+01]\n",
      "45-th iteration, loss: 0.6598180968910713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.497915564933918e-08\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699818e+00 0.00000000e+00 1.88737914e-15 5.52371297e+01\n",
      " 8.12294404e+01]\n",
      "46-th iteration, loss: 0.6598180968910702, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.002163814941832e-08\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699761e+00 0.00000000e+00 2.07472928e-15 5.52371298e+01\n",
      " 8.12294404e+01]\n",
      "47-th iteration, loss: 0.6598180968910693, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.585541719245531e-08\n",
      "47-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699705 55.23712999 81.22944037]\n",
      "48-th iteration, loss: 0.6598180968910683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.124295476411425e-08\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699651 55.23713007 81.22944037]\n",
      "49-th iteration, loss: 0.6598180968910675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.562711576522187e-08\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699598 55.23713015 81.22944037]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248375\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.19688805   0.         106.55688079]\n",
      "1-th iteration, loss: 0.7487306396172312, 11 gd steps\n",
      "insert gradient: -0.4426815814718002\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[ 42.74722856  62.33225191 106.55688079   0.        ]\n",
      "2-th iteration, loss: 0.6620460808349823, 13 gd steps\n",
      "insert gradient: -0.19515671517286143\n",
      "2-th iteration, new layer inserted. now 6 layers\n",
      "[8.67643970e+00 5.72808245e+01 1.09236238e+02 0.00000000e+00\n",
      " 7.06101844e-14 6.54319283e+00]\n",
      "3-th iteration, loss: 0.559528716994699, 18 gd steps\n",
      "insert gradient: -0.05828571445518015\n",
      "3-th iteration, new layer inserted. now 6 layers\n",
      "[2.58755478e+01 6.32132346e+01 0.00000000e+00 3.59712260e-14\n",
      " 8.66327832e+01 1.28286210e+02]\n",
      "4-th iteration, loss: 0.5121896352851273, 88 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.371603278151749e-18\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[  0.88549031  57.63878907  98.18852654 104.2947256    0.        ]\n",
      "5-th iteration, loss: 0.509132790197679, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[5.23225683e+00 5.34194613e+01 9.88913595e+01 9.92933165e+01\n",
      " 6.84992590e-17 0.00000000e+00 3.28796443e-15]\n",
      "6-th iteration, loss: 0.5091165531826751, 31 gd steps\n",
      "insert gradient: -0.00011495806574585649\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[4.61874413e+00 5.34185231e+01 9.90729953e+01 9.94105245e+01\n",
      " 1.51912838e-15]\n",
      "7-th iteration, loss: 0.5091165386165154, 20 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.423024500897606e-06\n",
      "7-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116327e+00 0.00000000e+00 1.66533454e-15 5.34267348e+01\n",
      " 9.90616161e+01 9.94166678e+01]\n",
      "8-th iteration, loss: 0.5091165386161828, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.803088718343046e-06\n",
      "8-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116392e+00 5.02126764e-06 6.41747066e-07 5.34267398e+01\n",
      " 9.90616197e+01 0.00000000e+00 3.10862447e-14 9.94166728e+01]\n",
      "9-th iteration, loss: 0.5091165386158576, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.364901857062935e-06\n",
      "9-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116421e+00 8.54060611e-06 6.19843775e-07 5.34267434e+01\n",
      " 9.90616227e+01 4.62800696e-06 3.02339803e-06 9.94166774e+01]\n",
      "10-th iteration, loss: 0.5091165386156401, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.989503425245668e-06\n",
      "10-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116425e+00 1.09648723e-05 1.24110440e-07 5.34267458e+01\n",
      " 9.90616251e+01 8.70025236e-06 5.21417490e-06 0.00000000e+00\n",
      " 6.35274710e-22 9.94166816e+01]\n",
      "11-th iteration, loss: 0.5091165386154303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5119234386950014e-06\n",
      "11-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116412e+00 5.34267601e+01 9.90616271e+01 1.22650158e-05\n",
      " 6.72016048e-06 3.82234078e-06 1.50598558e-06 9.94166854e+01]\n",
      "12-th iteration, loss: 0.509116538615293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1230061964095837e-06\n",
      "12-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116386e+00 5.34267612e+01 9.90616286e+01 1.52420050e-05\n",
      " 7.61780514e-06 7.13130225e-06 2.21480875e-06 9.94166888e+01]\n",
      "13-th iteration, loss: 0.5091165386151915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.797067661313409e-06\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116356e+00 5.34267621e+01 9.90616298e+01 1.77723018e-05\n",
      " 8.08497509e-06 1.00379144e-05 2.32969641e-06 0.00000000e+00\n",
      " 2.64697796e-22 9.94166918e+01]\n",
      "14-th iteration, loss: 0.5091165386150864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.429880744543104e-06\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116321e+00 5.34267627e+01 9.90616308e+01 1.99381168e-05\n",
      " 8.20795077e-06 1.26031227e-05 1.95680486e-06 0.00000000e+00\n",
      " 3.97046694e-22 9.94166972e+01]\n",
      "15-th iteration, loss: 0.5091165386150052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1236147243095274e-06\n",
      "15-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116285e+00 5.34267633e+01 9.90616316e+01 2.17711990e-05\n",
      " 8.04102252e-06 1.48416731e-05 1.16728960e-06 9.94167019e+01]\n",
      "16-th iteration, loss: 0.5091165386149534, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.935195330584988e-06\n",
      "16-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116247e+00 5.34267638e+01 9.90616323e+01 2.33789250e-05\n",
      " 7.65549404e-06 1.68466212e-05 4.85910346e-08 0.00000000e+00\n",
      " 2.02659250e-23 9.94167039e+01]\n",
      "17-th iteration, loss: 0.5091165386149008, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6843957131856278e-06\n",
      "17-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116210e+00 5.34267643e+01 9.90616329e+01 2.48661077e-05\n",
      " 7.12884113e-06 0.00000000e+00 2.27640105e-21 9.94167264e+01]\n",
      "18-th iteration, loss: 0.5091165386148694, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5266194695243366e-06\n",
      "18-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116172e+00 5.34267646e+01 9.90616334e+01 2.61509063e-05\n",
      " 6.40630722e-06 0.00000000e+00 5.55865372e-21 9.94167296e+01]\n",
      "19-th iteration, loss: 0.5091165386148423, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3865458720533173e-06\n",
      "19-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116134e+00 5.34267650e+01 9.90616339e+01 2.73200897e-05\n",
      " 5.55413710e-06 9.94167326e+01]\n",
      "20-th iteration, loss: 0.5091165386148251, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3083936450747006e-06\n",
      "20-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116095e+00 5.34267653e+01 9.90616343e+01 2.84066984e-05\n",
      " 4.59127683e-06 0.00000000e+00 9.26442286e-22 9.94167340e+01]\n",
      "21-th iteration, loss: 0.5091165386148024, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1912016160581643e-06\n",
      "21-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116057e+00 5.34267656e+01 9.90616346e+01 2.94556976e-05\n",
      " 3.55449715e-06 9.94167365e+01]\n",
      "22-th iteration, loss: 0.5091165386147867, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1263253618230792e-06\n",
      "22-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116020e+00 5.34267659e+01 9.90616350e+01 3.04501049e-05\n",
      " 2.42890028e-06 0.00000000e+00 3.04402465e-22 9.94167377e+01]\n",
      "23-th iteration, loss: 0.5091165386147667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0266642321720043e-06\n",
      "23-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115983e+00 5.34267662e+01 9.90616353e+01 3.14301874e-05\n",
      " 1.24875758e-06 0.00000000e+00 4.76456033e-22 9.94167399e+01]\n",
      "24-th iteration, loss: 0.5091165386147479, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.364258856559679e-07\n",
      "24-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115946 53.42676654 99.06163559 99.41677428]\n",
      "25-th iteration, loss: 0.5091165386147434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.991923982916023e-07\n",
      "25-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011591  53.42676682 99.06163587 99.4167752 ]\n",
      "26-th iteration, loss: 0.5091165386147395, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.638275184299479e-07\n",
      "26-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115873e+00 5.34267671e+01 9.90616361e+01 0.00000000e+00\n",
      " 6.70574707e-14 9.94167761e+01]\n",
      "27-th iteration, loss: 0.5091165386147329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.97970191530365e-07\n",
      "27-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115836e+00 5.34267673e+01 9.90616363e+01 8.42342867e-07\n",
      " 2.12014835e-07 0.00000000e+00 8.76811449e-23 9.94167769e+01]\n",
      "28-th iteration, loss: 0.5091165386147254, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.10910080641519e-07\n",
      "28-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115799e+00 5.34267674e+01 9.90616365e+01 1.60189596e-06\n",
      " 3.34243607e-07 7.70026519e-07 1.22228772e-07 9.94167777e+01]\n",
      "29-th iteration, loss: 0.5091165386147193, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.371321430788544e-07\n",
      "29-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115761e+00 5.34267676e+01 9.90616366e+01 2.26671788e-06\n",
      " 3.70108621e-07 1.45135992e-06 1.20054845e-07 9.94167784e+01]\n",
      "30-th iteration, loss: 0.5091165386147144, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.73882713845461e-07\n",
      "30-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115724e+00 5.34267677e+01 9.90616367e+01 2.85954812e-06\n",
      " 3.38322489e-07 2.06247335e-06 1.65722312e-08 9.94167790e+01]\n",
      "31-th iteration, loss: 0.5091165386147103, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.164636370836422e-07\n",
      "31-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115687e+00 5.34267678e+01 9.90616367e+01 3.39851612e-06\n",
      " 2.53204446e-07 0.00000000e+00 8.76811449e-23 9.94167822e+01]\n",
      "32-th iteration, loss: 0.5091165386147067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.655301415795781e-07\n",
      "32-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115651e+00 5.34267679e+01 9.90616368e+01 3.88699507e-06\n",
      " 1.18563674e-07 9.94167832e+01]\n",
      "33-th iteration, loss: 0.5091165386147045, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3510276020063646e-07\n",
      "33-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115615e+00 5.34267680e+01 9.90616368e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94167880e+01]\n",
      "34-th iteration, loss: 0.5091165386147026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.049709176162558e-07\n",
      "34-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115580e+00 5.34267681e+01 9.90616368e+01 4.25474397e-07\n",
      " 4.82479656e-09 0.00000000e+00 2.68833699e-24 9.94167884e+01]\n",
      "35-th iteration, loss: 0.5091165386147003, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6380455506712807e-07\n",
      "35-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115545 53.42676821 99.06163679 99.41679   ]\n",
      "36-th iteration, loss: 0.5091165386146992, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.514181003698071e-07\n",
      "36-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011551  53.42676831 99.06163676 99.41679036]\n",
      "37-th iteration, loss: 0.5091165386146983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.395372064381214e-07\n",
      "37-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115476e+00 5.34267684e+01 9.90616367e+01 0.00000000e+00\n",
      " 7.23865412e-14 9.94167907e+01]\n",
      "38-th iteration, loss: 0.509116538614697, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1662111216918323e-07\n",
      "38-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115442 53.42676849 99.0616367  99.41679137]\n",
      "39-th iteration, loss: 0.5091165386146961, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0614953764134813e-07\n",
      "39-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115409 53.42676858 99.06163666 99.41679169]\n",
      "40-th iteration, loss: 0.5091165386146953, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.960917428428432e-07\n",
      "40-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115376 53.42676866 99.06163661 99.41679199]\n",
      "41-th iteration, loss: 0.5091165386146945, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.864289144299447e-07\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115344 53.42676875 99.06163657 99.41679228]\n",
      "42-th iteration, loss: 0.5091165386146937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7714332816428173e-07\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115312 53.42676883 99.06163651 99.41679257]\n",
      "43-th iteration, loss: 0.5091165386146931, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6821818436630965e-07\n",
      "43-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115281e+00 5.34267689e+01 9.90616365e+01 0.00000000e+00\n",
      " 1.11466392e-13 9.94167928e+01]\n",
      "44-th iteration, loss: 0.5091165386146921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5052121375976044e-07\n",
      "44-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115250e+00 5.34267690e+01 9.90616364e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167934e+01]\n",
      "45-th iteration, loss: 0.5091165386146912, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3412202320982744e-07\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115219 53.42676909 99.06163634 99.41679386]\n",
      "46-th iteration, loss: 0.5091165386146906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2689000049931032e-07\n",
      "46-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115189e+00 5.34267692e+01 9.90616363e+01 0.00000000e+00\n",
      " 1.37667655e-14 9.94167941e+01]\n",
      "47-th iteration, loss: 0.5091165386146899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.122009734956615e-07\n",
      "47-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115159e+00 5.34267693e+01 9.90616362e+01 0.00000000e+00\n",
      " 6.70574707e-14 9.94167945e+01]\n",
      "48-th iteration, loss: 0.5091165386146892, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9858021900163887e-07\n",
      "48-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115130e+00 5.34267693e+01 9.90616361e+01 0.00000000e+00\n",
      " 4.84057239e-14 9.94167950e+01]\n",
      "49-th iteration, loss: 0.5091165386146885, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8594621959260348e-07\n",
      "49-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115101 53.42676942 99.06163605 99.41679534]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536802874516846\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         131.37979052]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6205102372394407\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 123.36882768   0.           8.01096284]\n",
      "2-th iteration, loss: 0.5091175827746208, 55 gd steps\n",
      "insert gradient: -0.002857187333261249\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.59861158 53.40007459 99.06864677 99.52958242  8.01096284]\n",
      "3-th iteration, loss: 0.5091165386152302, 33 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.065014823455803e-06\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118862 53.42673467 99.06159311 99.41683838  8.01096284]\n",
      "4-th iteration, loss: 0.5091165386151245, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.352388968971235e-06\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118890e+00 0.00000000e+00 2.08166817e-15 5.34267396e+01\n",
      " 9.90615950e+01 9.94168378e+01 8.01096284e+00]\n",
      "5-th iteration, loss: 0.5091165386149921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.232676016093853e-06\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118900e+00 4.08078894e-06 9.96726995e-08 5.34267436e+01\n",
      " 9.90615966e+01 9.94168371e+01 8.01096284e+00]\n",
      "6-th iteration, loss: 0.5091165386149171, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4101032066933867e-06\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118886 53.42675379 99.06159777 99.41683641  8.01096284]\n",
      "7-th iteration, loss: 0.5091165386148906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0799708419487044e-06\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118856 53.42675612 99.06159878 99.41683575  8.01096284]\n",
      "8-th iteration, loss: 0.50911653861487, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.797436281221257e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118820e+00 0.00000000e+00 3.30291350e-15 5.34267581e+01\n",
      " 9.90615997e+01 9.94168351e+01 8.01096284e+00]\n",
      "9-th iteration, loss: 0.5091165386148444, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3447067099779261e-06\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118777 53.42676151 99.06160043 99.41683442  8.01096284]\n",
      "10-th iteration, loss: 0.5091165386148336, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1676369019154933e-06\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118725 53.42676281 99.06160108 99.41683376  8.01096284]\n",
      "11-th iteration, loss: 0.5091165386148246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.015883006683058e-06\n",
      "11-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118671e+00 0.00000000e+00 1.41553436e-15 5.34267639e+01\n",
      " 9.90616017e+01 9.94168331e+01 8.01096284e+00]\n",
      "12-th iteration, loss: 0.509116538614814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.657251567217315e-07\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118613e+00 0.00000000e+00 4.16333634e-16 5.34267659e+01\n",
      " 9.90616022e+01 9.94168325e+01 8.01096284e+00]\n",
      "13-th iteration, loss: 0.5091165386148063, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.80252989042138e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118551e+00 0.00000000e+00 4.16333634e-16 5.34267673e+01\n",
      " 9.90616026e+01 9.94168319e+01 8.01096284e+00]\n",
      "14-th iteration, loss: 0.5091165386148004, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4264042599018157e-07\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118486 53.42676842 99.06160304 99.41683124  8.01096284]\n",
      "15-th iteration, loss: 0.509116538614796, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.938157210106524e-07\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118419 53.42676886 99.0616034  99.41683065  8.01096284]\n",
      "16-th iteration, loss: 0.5091165386147919, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5168498873078e-07\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118352e+00 0.00000000e+00 4.30211422e-15 5.34267692e+01\n",
      " 9.90616037e+01 9.94168301e+01 8.01096284e+00]\n",
      "17-th iteration, loss: 0.5091165386147877, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0942488726450083e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118285 53.42676992 99.06160408 99.4168295   8.01096284]\n",
      "18-th iteration, loss: 0.509116538614784, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0154222015168226e-07\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118217e+00 5.34267702e+01 0.00000000e+00 6.39488462e-14\n",
      " 9.90616044e+01 9.94168289e+01 8.01096284e+00]\n",
      "19-th iteration, loss: 0.50911653861478, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.748371920290345e-07\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118149e+00 5.34267704e+01 2.94622120e-07 2.31646057e-07\n",
      " 0.00000000e+00 1.24077092e-22 9.90616047e+01 9.94168284e+01\n",
      " 8.01096284e+00]\n",
      "20-th iteration, loss: 0.509116538614776, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.415199316041322e-07\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118081e+00 5.34267706e+01 5.39791589e-07 3.71499970e-07\n",
      " 2.66609000e-07 1.39853914e-07 9.90616049e+01 9.94168279e+01\n",
      " 8.01096284e+00]\n",
      "21-th iteration, loss: 0.5091165386147726, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2274551535335392e-07\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118012e+00 5.34267707e+01 7.30487452e-07 4.30529932e-07\n",
      " 4.91688291e-07 1.74208420e-07 9.90616052e+01 9.94168273e+01\n",
      " 8.01096284e+00]\n",
      "22-th iteration, loss: 0.5091165386147692, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1151892776973463e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117944e+00 5.34267708e+01 8.96251415e-07 4.46002074e-07\n",
      " 6.97299083e-07 1.44173347e-07 0.00000000e+00 8.10637000e-23\n",
      " 9.90616054e+01 9.94168268e+01 8.01096284e+00]\n",
      "23-th iteration, loss: 0.5091165386147659, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9624036688738334e-07\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117876e+00 5.34267708e+01 1.05290479e-06 4.36596735e-07\n",
      " 8.95231276e-07 7.02308973e-08 0.00000000e+00 2.48154184e-24\n",
      " 9.90616058e+01 9.94168263e+01 8.01096284e+00]\n",
      "24-th iteration, loss: 0.5091165386147626, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8313467558046534e-07\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117809e+00 5.34267709e+01 1.20379980e-06 4.07442181e-07\n",
      " 0.00000000e+00 2.24993127e-22 9.90616073e+01 9.94168258e+01\n",
      " 8.01096284e+00]\n",
      "25-th iteration, loss: 0.5091165386147595, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7655558735565587e-07\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117744e+00 5.34267710e+01 1.35025486e-06 3.58302358e-07\n",
      " 9.90616077e+01 9.94168253e+01 8.01096284e+00]\n",
      "26-th iteration, loss: 0.5091165386147567, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7755431661296364e-07\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117678e+00 5.34267710e+01 1.49560118e-06 2.95600027e-07\n",
      " 0.00000000e+00 4.96308368e-23 9.90616079e+01 9.94168248e+01\n",
      " 8.01096284e+00]\n",
      "27-th iteration, loss: 0.5091165386147538, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7121360839419658e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117614e+00 5.34267711e+01 1.64721667e-06 2.26045760e-07\n",
      " 0.00000000e+00 9.09898674e-23 9.90616082e+01 9.94168244e+01\n",
      " 8.01096284e+00]\n",
      "28-th iteration, loss: 0.509116538614751, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6560633369781565e-07\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117551e+00 5.34267712e+01 1.79935117e-06 1.42958868e-07\n",
      " 9.90616086e+01 9.94168239e+01 8.01096284e+00]\n",
      "29-th iteration, loss: 0.5091165386147486, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6690065759079695e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117489e+00 5.34267713e+01 1.95380688e-06 4.76203432e-08\n",
      " 9.90616087e+01 9.94168235e+01 8.01096284e+00]\n",
      "30-th iteration, loss: 0.5091165386147462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.652087399148199e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60117428 53.42677133 99.06161101 99.41682302  8.01096284]\n",
      "31-th iteration, loss: 0.5091165386147438, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6765007312980443e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117368e+00 5.34267714e+01 0.00000000e+00 1.90958360e-14\n",
      " 9.90616112e+01 9.94168226e+01 8.01096284e+00]\n",
      "32-th iteration, loss: 0.5091165386147415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6049020124714165e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117308e+00 5.34267715e+01 1.66622582e-07 7.79617363e-08\n",
      " 0.00000000e+00 8.27180613e-25 9.90616113e+01 9.94168222e+01\n",
      " 8.01096284e+00]\n",
      "33-th iteration, loss: 0.5091165386147392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.481815359800327e-07\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117249e+00 5.34267716e+01 3.17775016e-07 1.23534696e-07\n",
      " 1.58368026e-07 4.55729598e-08 9.90616115e+01 9.94168218e+01\n",
      " 8.01096284e+00]\n",
      "34-th iteration, loss: 0.5091165386147369, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.411428349713736e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117191e+00 5.34267716e+01 4.49906844e-07 1.38506273e-07\n",
      " 3.01933357e-07 4.58871053e-08 0.00000000e+00 2.23338765e-23\n",
      " 9.90616117e+01 9.94168214e+01 8.01096284e+00]\n",
      "35-th iteration, loss: 0.5091165386147347, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.316036532153377e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117133e+00 5.34267716e+01 5.73872857e-07 1.35482490e-07\n",
      " 4.38718537e-07 1.49184894e-08 9.90616119e+01 9.94168210e+01\n",
      " 8.01096284e+00]\n",
      "36-th iteration, loss: 0.5091165386147326, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2792405664785642e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117076e+00 5.34267717e+01 6.92834323e-07 1.19214329e-07\n",
      " 0.00000000e+00 6.20385459e-23 9.90616126e+01 9.94168206e+01\n",
      " 8.01096284e+00]\n",
      "37-th iteration, loss: 0.5091165386147307, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.241060528702231e-07\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117019e+00 5.34267717e+01 8.10667843e-07 9.29321542e-08\n",
      " 0.00000000e+00 5.54211010e-23 9.90616129e+01 9.94168202e+01\n",
      " 8.01096284e+00]\n",
      "38-th iteration, loss: 0.5091165386147288, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.207254343526074e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116964e+00 5.34267717e+01 9.27381293e-07 5.72298398e-08\n",
      " 9.90616131e+01 9.94168198e+01 8.01096284e+00]\n",
      "39-th iteration, loss: 0.5091165386147269, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2221040691575937e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116909e+00 5.34267718e+01 1.04451582e-06 1.36582818e-08\n",
      " 9.90616133e+01 9.94168195e+01 8.01096284e+00]\n",
      "40-th iteration, loss: 0.5091165386147252, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.217157069828043e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60116856 53.42677184 99.06161456 99.41681912  8.01096284]\n",
      "41-th iteration, loss: 0.5091165386147236, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2503560689220583e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116803e+00 5.34267719e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90616147e+01 9.94168188e+01 8.01096284e+00]\n",
      "42-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2114846805828598e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116750e+00 5.34267719e+01 1.24791342e-07 5.30460467e-08\n",
      " 0.00000000e+00 2.27474668e-23 9.90616148e+01 9.94168184e+01\n",
      " 8.01096284e+00]\n",
      "43-th iteration, loss: 0.5091165386147203, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1300347289066253e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[4.60116699e+00 5.34267720e+01 2.39843551e-07 8.52580876e-08\n",
      " 1.19961778e-07 3.22120410e-08 0.00000000e+00 2.89513214e-24\n",
      " 9.90616149e+01 9.94168181e+01 8.01096284e+00]\n",
      "44-th iteration, loss: 0.5091165386147187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0438194581151545e-07\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[4.60116648e+00 5.34267720e+01 3.41132163e-07 9.53757678e-08\n",
      " 2.29141279e-07 3.12269000e-08 0.00000000e+00 1.03397577e-23\n",
      " 9.90616152e+01 9.94168178e+01 8.01096284e+00]\n",
      "45-th iteration, loss: 0.5091165386147171, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.849898089296709e-08\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116597e+00 5.34267720e+01 4.34174263e-07 9.15505810e-08\n",
      " 3.31010691e-07 6.19400449e-09 9.90616154e+01 9.94168175e+01\n",
      " 8.01096284e+00]\n",
      "46-th iteration, loss: 0.5091165386147156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.632997765868812e-08\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116547e+00 5.34267721e+01 5.24856152e-07 8.02566584e-08\n",
      " 0.00000000e+00 2.39882378e-23 9.90616159e+01 9.94168172e+01\n",
      " 8.01096284e+00]\n",
      "47-th iteration, loss: 0.5091165386147143, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.414132212031372e-08\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116498e+00 5.34267721e+01 6.14680622e-07 6.22973296e-08\n",
      " 9.90616161e+01 9.94168169e+01 8.01096284e+00]\n",
      "48-th iteration, loss: 0.509116538614713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.563497077844682e-08\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116450e+00 5.34267721e+01 7.04480400e-07 3.90990997e-08\n",
      " 9.90616162e+01 9.94168166e+01 8.01096284e+00]\n",
      "49-th iteration, loss: 0.5091165386147116, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.661302737413419e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116402e+00 5.34267722e+01 7.97762796e-07 1.34802419e-08\n",
      " 0.00000000e+00 1.16839262e-23 9.90616163e+01 9.94168163e+01\n",
      " 8.01096284e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355685\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         156.6063248 ]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6184134081367765\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 124.1391599    0.          32.4671649 ]\n",
      "2-th iteration, loss: 0.5091165615223171, 66 gd steps\n",
      "insert gradient: -0.0006350363902161577\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.61191766 53.41584316 99.07248885 99.4238018  32.4671649 ]\n",
      "3-th iteration, loss: 0.5091165386236958, 19 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.114817208604025e-06\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6009542  53.42698916 99.06151165 99.41663291 32.4671649 ]\n",
      "4-th iteration, loss: 0.5091165386149656, 9 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0023778509777978e-16\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010349  53.42680407 99.06163953 99.41681291 32.4671649 ]\n",
      "5-th iteration, loss: 0.5091165386149524, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.037107531448759e-15\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103600e+00 5.34268029e+01 9.90616388e+01 9.94168124e+01\n",
      " 0.00000000e+00 1.00364161e-13 3.24671649e+01]\n",
      "6-th iteration, loss: 0.509116538614941, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.3845596693891704e-18\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60103711 53.42680187 99.06163811 99.41681191 32.4671649 ]\n",
      "7-th iteration, loss: 0.5091165386149306, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6297197084166053e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103823e+00 5.34268010e+01 9.90616375e+01 9.94168115e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "8-th iteration, loss: 0.5091165386149212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.954585203368061e-17\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60103936 53.42680017 99.06163692 99.41681103 32.4671649 ]\n",
      "9-th iteration, loss: 0.5091165386149126, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.156540759662093e-18\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104049 53.42679947 99.0616364  99.41681063 32.4671649 ]\n",
      "10-th iteration, loss: 0.5091165386149046, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.072849450698253e-17\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104163 53.42679884 99.06163593 99.41681024 32.4671649 ]\n",
      "11-th iteration, loss: 0.5091165386148969, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4897692592589256e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104276 53.42679827 99.06163549 99.41680988 32.4671649 ]\n",
      "12-th iteration, loss: 0.5091165386148899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1960588812721723e-17\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010439  53.42679776 99.06163508 99.41680954 32.4671649 ]\n",
      "13-th iteration, loss: 0.5091165386148832, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6009662303774784e-17\n",
      "13-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104502 53.4267973  99.06163471 99.41680922 32.4671649 ]\n",
      "14-th iteration, loss: 0.5091165386148768, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.120427388024693e-16\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104614 53.42679689 99.06163436 99.41680891 32.4671649 ]\n",
      "15-th iteration, loss: 0.5091165386148706, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.4864937949280603e-15\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60104725e+00 5.34267965e+01 9.90616340e+01 9.94168086e+01\n",
      " 0.00000000e+00 1.14575016e-13 3.24671649e+01]\n",
      "16-th iteration, loss: 0.5091165386148648, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.5182445022955804e-17\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104836 53.42679615 99.06163372 99.41680835 32.4671649 ]\n",
      "17-th iteration, loss: 0.5091165386148591, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.3714181369137146e-17\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104945 53.42679582 99.06163344 99.41680809 32.4671649 ]\n",
      "18-th iteration, loss: 0.5091165386148536, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.59897560283836e-15\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105053e+00 5.34267955e+01 9.90616332e+01 9.94168078e+01\n",
      " 0.00000000e+00 8.43769499e-14 3.24671649e+01]\n",
      "19-th iteration, loss: 0.5091165386148484, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.122566036257092e-15\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105160e+00 5.34267952e+01 9.90616329e+01 9.94168076e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "20-th iteration, loss: 0.5091165386148433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.08234471792363e-17\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60105266 53.42679496 99.06163267 99.41680739 32.4671649 ]\n",
      "21-th iteration, loss: 0.5091165386148384, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1455776657445783e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105371e+00 5.34267947e+01 9.90616324e+01 9.94168072e+01\n",
      " 0.00000000e+00 7.46069873e-14 3.24671649e+01]\n",
      "22-th iteration, loss: 0.5091165386148336, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7181879823072777e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105474e+00 5.34267945e+01 9.90616322e+01 9.94168070e+01\n",
      " 0.00000000e+00 5.68434189e-14 3.24671649e+01]\n",
      "23-th iteration, loss: 0.5091165386148291, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2104799596131088e-15\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105577e+00 5.34267942e+01 9.90616320e+01 9.94168068e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "24-th iteration, loss: 0.5091165386148246, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2339313827730386e-15\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105678e+00 5.34267940e+01 9.90616318e+01 9.94168066e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "25-th iteration, loss: 0.5091165386148202, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6641455666890749e-15\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105777e+00 5.34267938e+01 9.90616317e+01 9.94168064e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "26-th iteration, loss: 0.5091165386148162, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5444073563777963e-17\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60105876 53.42679358 99.06163148 99.41680628 32.4671649 ]\n",
      "27-th iteration, loss: 0.5091165386148121, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2359532904889408e-15\n",
      "27-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105973e+00 5.34267934e+01 9.90616313e+01 9.94168061e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "28-th iteration, loss: 0.5091165386148081, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.158103158173363e-15\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106069e+00 5.34267932e+01 9.90616311e+01 9.94168060e+01\n",
      " 0.00000000e+00 7.01660952e-14 3.24671649e+01]\n",
      "29-th iteration, loss: 0.5091165386148043, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.74350932681397e-18\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106164 53.42679298 99.06163099 99.41680585 32.4671649 ]\n",
      "30-th iteration, loss: 0.5091165386148007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7387975320274398e-17\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106257 53.42679279 99.06163085 99.41680571 32.4671649 ]\n",
      "31-th iteration, loss: 0.509116538614797, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.808985910564859e-17\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106349 53.4267926  99.06163071 99.41680559 32.4671649 ]\n",
      "32-th iteration, loss: 0.5091165386147936, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.627467489908381e-15\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106440e+00 5.34267924e+01 9.90616306e+01 9.94168055e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "33-th iteration, loss: 0.5091165386147902, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6419578082163515e-15\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106530e+00 5.34267922e+01 9.90616304e+01 9.94168054e+01\n",
      " 0.00000000e+00 5.68434189e-14 3.24671649e+01]\n",
      "34-th iteration, loss: 0.5091165386147869, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.141600401327268e-17\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106618 53.42679206 99.06163032 99.41680525 32.4671649 ]\n",
      "35-th iteration, loss: 0.5091165386147837, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.132113495586646e-17\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106706 53.42679188 99.0616302  99.41680515 32.4671649 ]\n",
      "36-th iteration, loss: 0.5091165386147806, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6636515059025463e-15\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106792e+00 5.34267917e+01 9.90616301e+01 9.94168051e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "37-th iteration, loss: 0.5091165386147776, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1923500900228173e-15\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106877e+00 5.34267915e+01 9.90616300e+01 9.94168050e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "38-th iteration, loss: 0.5091165386147747, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6687632442541796e-15\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106960e+00 5.34267914e+01 9.90616299e+01 9.94168049e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "39-th iteration, loss: 0.5091165386147718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.058139924201221e-15\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107043e+00 5.34267912e+01 9.90616298e+01 9.94168048e+01\n",
      " 0.00000000e+00 7.01660952e-14 3.24671649e+01]\n",
      "40-th iteration, loss: 0.509116538614769, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.953977574336948e-16\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107125e+00 5.34267910e+01 9.90616297e+01 9.94168047e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "41-th iteration, loss: 0.5091165386147664, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0053809367671108e-16\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107205 53.42679087 99.06162958 99.41680465 32.4671649 ]\n",
      "42-th iteration, loss: 0.5091165386147638, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.7626467327824937e-15\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107284e+00 5.34267907e+01 9.90616295e+01 9.94168046e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "43-th iteration, loss: 0.5091165386147614, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.876627873059228e-17\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107362 53.42679055 99.06162941 99.41680452 32.4671649 ]\n",
      "44-th iteration, loss: 0.5091165386147588, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.900319490504165e-17\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107439 53.42679039 99.06162932 99.41680445 32.4671649 ]\n",
      "45-th iteration, loss: 0.5091165386147564, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.855634020617389e-16\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107515e+00 5.34267902e+01 9.90616292e+01 9.94168044e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "46-th iteration, loss: 0.5091165386147541, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.4423105813651585e-15\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107590e+00 5.34267901e+01 9.90616292e+01 9.94168043e+01\n",
      " 0.00000000e+00 1.14575016e-13 3.24671649e+01]\n",
      "47-th iteration, loss: 0.5091165386147519, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.809470480367603e-16\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107664e+00 5.34267899e+01 9.90616291e+01 9.94168043e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "48-th iteration, loss: 0.5091165386147496, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0390663487420435e-15\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107737e+00 5.34267898e+01 9.90616290e+01 9.94168042e+01\n",
      " 0.00000000e+00 1.00364161e-13 3.24671649e+01]\n",
      "49-th iteration, loss: 0.5091165386147475, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1164799588398936e-15\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107809e+00 5.34267896e+01 9.90616290e+01 9.94168042e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368028745168436\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         181.6310468 ]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6204761741881215\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 124.04071489   0.          57.59033191]\n",
      "2-th iteration, loss: 0.5091170879368359, 50 gd steps\n",
      "insert gradient: -0.002823694092044642\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[4.58849421e+00 5.34134044e+01 9.90512501e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.93423484e+01 5.75903319e+01]\n",
      "3-th iteration, loss: 0.5091165409529652, 11 gd steps\n",
      "insert gradient: -0.00012759241660170124\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.5994115  53.42818335 99.06473892 99.41174489 57.59033191]\n",
      "4-th iteration, loss: 0.5091165386303973, 16 gd steps\n",
      "insert gradient: -1.368276059659435e-05\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103003e+00 5.34268367e+01 9.90618591e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94163118e+01 5.75903319e+01]\n",
      "5-th iteration, loss: 0.5091165386149508, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8090738994637445e-07\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109387 53.42676423 99.06170605 99.41676443 57.59033191]\n",
      "6-th iteration, loss: 0.5091165386149404, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9417920364904945e-07\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109428 53.42676394 99.06170453 99.41676472 57.59033191]\n",
      "7-th iteration, loss: 0.5091165386149308, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.058517849248523e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109471 53.42676374 99.06170307 99.41676502 57.59033191]\n",
      "8-th iteration, loss: 0.5091165386149219, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1610541136961433e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109515 53.4267636  99.06170166 99.41676533 57.59033191]\n",
      "9-th iteration, loss: 0.5091165386149136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.25096874375932e-07\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109560e+00 5.34267635e+01 9.90617003e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167656e+01 5.75903319e+01]\n",
      "10-th iteration, loss: 0.5091165386149052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.210867003320641e-07\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109606e+00 5.34267635e+01 9.90616990e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94167663e+01 5.75903319e+01]\n",
      "11-th iteration, loss: 0.5091165386148973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.167465273550666e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109651 53.42676353 99.06169772 99.41676696 57.59033191]\n",
      "12-th iteration, loss: 0.5091165386148899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.236872456014151e-07\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109697e+00 5.34267636e+01 9.90616965e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167673e+01 5.75903319e+01]\n",
      "13-th iteration, loss: 0.5091165386148826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1802563799845507e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109743e+00 5.34267636e+01 9.90616953e+01 0.00000000e+00\n",
      " 7.19424520e-14 9.94167679e+01 5.75903319e+01]\n",
      "14-th iteration, loss: 0.5091165386148755, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1230096549354134e-07\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109789 53.42676375 99.0616941  99.41676858 57.59033191]\n",
      "15-th iteration, loss: 0.509116538614869, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1786055005263166e-07\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109834e+00 5.34267639e+01 9.90616929e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167689e+01 5.75903319e+01]\n",
      "16-th iteration, loss: 0.5091165386148625, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.112552256504448e-07\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109880e+00 5.34267640e+01 9.90616918e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167695e+01 5.75903319e+01]\n",
      "17-th iteration, loss: 0.5091165386148562, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0475123630274266e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109925 53.42676415 99.06169073 99.41677016 57.59033191]\n",
      "18-th iteration, loss: 0.5091165386148504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.093585860712213e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109969 53.42676429 99.06168964 99.41677047 57.59033191]\n",
      "19-th iteration, loss: 0.5091165386148448, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.133909149858172e-07\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110013e+00 5.34267644e+01 9.90616886e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167708e+01 5.75903319e+01]\n",
      "20-th iteration, loss: 0.5091165386148392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0564476696289256e-07\n",
      "20-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110057e+00 5.34267646e+01 9.90616876e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167714e+01 5.75903319e+01]\n",
      "21-th iteration, loss: 0.5091165386148335, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9816737336798185e-07\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110101e+00 5.34267648e+01 9.90616866e+01 0.00000000e+00\n",
      " 7.01660952e-14 9.94167720e+01 5.75903319e+01]\n",
      "22-th iteration, loss: 0.5091165386148282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.909503793981057e-07\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110144e+00 5.34267650e+01 9.90616856e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.94167726e+01 5.75903319e+01]\n",
      "23-th iteration, loss: 0.509116538614823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8398490359386446e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110186 53.42676517 99.0616846  99.41677321 57.59033191]\n",
      "24-th iteration, loss: 0.5091165386148183, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8746942784587135e-07\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110227e+00 5.34267653e+01 9.90616836e+01 0.00000000e+00\n",
      " 3.90798505e-14 9.94167735e+01 5.75903319e+01]\n",
      "25-th iteration, loss: 0.5091165386148134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.80202851893275e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110269 53.42676553 99.0616827  99.41677407 57.59033191]\n",
      "26-th iteration, loss: 0.5091165386148091, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.832778055465411e-07\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110309e+00 5.34267657e+01 9.90616818e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167743e+01 5.75903319e+01]\n",
      "27-th iteration, loss: 0.5091165386148045, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7580511055766884e-07\n",
      "27-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110350e+00 5.34267659e+01 9.90616809e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.94167749e+01 5.75903319e+01]\n",
      "28-th iteration, loss: 0.5091165386148001, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6865192596144397e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110389 53.42676608 99.06168    99.41677547 57.59033191]\n",
      "29-th iteration, loss: 0.5091165386147961, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.714318299654945e-07\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110428 53.42676626 99.06167912 99.41677574 57.59033191]\n",
      "30-th iteration, loss: 0.5091165386147922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.73844412995204e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110467 53.42676643 99.06167827 99.41677601 57.59033191]\n",
      "31-th iteration, loss: 0.5091165386147886, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.759147931695573e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110505e+00 5.34267666e+01 9.90616774e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167763e+01 5.75903319e+01]\n",
      "32-th iteration, loss: 0.5091165386147847, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6782270004538956e-07\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110542e+00 5.34267668e+01 9.90616766e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167768e+01 5.75903319e+01]\n",
      "33-th iteration, loss: 0.509116538614781, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.601308858115458e-07\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110579e+00 5.34267670e+01 9.90616758e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167774e+01 5.75903319e+01]\n",
      "34-th iteration, loss: 0.5091165386147772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.528116334673536e-07\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110616e+00 5.34267671e+01 9.90616750e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167779e+01 5.75903319e+01]\n",
      "35-th iteration, loss: 0.5091165386147738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.45839448831736e-07\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110652e+00 5.34267673e+01 9.90616742e+01 0.00000000e+00\n",
      " 5.68434189e-14 9.94167784e+01 5.75903319e+01]\n",
      "36-th iteration, loss: 0.5091165386147705, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.391908957846423e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110687 53.42676749 99.06167346 99.4167789  57.59033191]\n",
      "37-th iteration, loss: 0.5091165386147674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4140863176274687e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110722 53.42676765 99.0616727  99.41677914 57.59033191]\n",
      "38-th iteration, loss: 0.5091165386147645, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.433227705529929e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110756e+00 5.34267678e+01 9.90616719e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167794e+01 5.75903319e+01]\n",
      "39-th iteration, loss: 0.5091165386147614, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3626900636914234e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110790e+00 5.34267680e+01 9.90616712e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167799e+01 5.75903319e+01]\n",
      "40-th iteration, loss: 0.5091165386147584, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.295631133678563e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110823 53.42676813 99.06167051 99.41678034 57.59033191]\n",
      "41-th iteration, loss: 0.5091165386147557, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3138647450381094e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110856e+00 5.34267683e+01 9.90616698e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167806e+01 5.75903319e+01]\n",
      "42-th iteration, loss: 0.5091165386147529, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2468196802560368e-07\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110888 53.42676844 99.0616691  99.41678103 57.59033191]\n",
      "43-th iteration, loss: 0.5091165386147504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.263337124680571e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011092  53.42676858 99.06166842 99.41678126 57.59033191]\n",
      "44-th iteration, loss: 0.5091165386147479, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2773089403654638e-07\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110951e+00 5.34267687e+01 9.90616677e+01 0.00000000e+00\n",
      " 8.52651283e-14 9.94167815e+01 5.75903319e+01]\n",
      "45-th iteration, loss: 0.5091165386147454, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2077853047784126e-07\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110982 53.42676888 99.06166709 99.41678194 57.59033191]\n",
      "46-th iteration, loss: 0.5091165386147432, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2205780266296124e-07\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111012e+00 5.34267690e+01 9.90616664e+01 0.00000000e+00\n",
      " 8.88178420e-14 9.94167822e+01 5.75903319e+01]\n",
      "47-th iteration, loss: 0.5091165386147407, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.152041394911115e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111042 53.42676916 99.06166581 99.4167826  57.59033191]\n",
      "48-th iteration, loss: 0.5091165386147386, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1637822578152962e-07\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111071e+00 5.34267693e+01 9.90616652e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167828e+01 5.75903319e+01]\n",
      "49-th iteration, loss: 0.5091165386147364, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0963483581328384e-07\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111101e+00 5.34267694e+01 9.90616646e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94167833e+01 5.75903319e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355685\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         206.85758108]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6185213411276936\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 123.61001796   0.          83.24756312]\n",
      "2-th iteration, loss: 0.50917051042473, 45 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.459896391438879e-15\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[3.61241440e+00 5.35208071e+01 9.98414342e+01 9.92299661e+01\n",
      " 0.00000000e+00 1.14575016e-13 8.32475631e+01]\n",
      "3-th iteration, loss: 0.5091166040187136, 36 gd steps\n",
      "insert gradient: -0.00046139220112038687\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6360841  53.42804166 99.03333583 99.43257542 83.24756312]\n",
      "4-th iteration, loss: 0.5091165386198226, 26 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.350232595579709e-06\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110324 53.42690786 99.06160509 99.41664891 83.24756312]\n",
      "5-th iteration, loss: 0.5091165386148037, 7 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.722181601640729e-07\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110946e+00 5.34267945e+01 9.90616510e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167877e+01 8.32475631e+01]\n",
      "6-th iteration, loss: 0.5091165386147763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.831346706410901e-07\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011092  53.42679204 99.0616499  99.4167881  83.24756312]\n",
      "7-th iteration, loss: 0.5091165386147559, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.977161597963257e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110902 53.42678996 99.06164892 99.4167883  83.24756312]\n",
      "8-th iteration, loss: 0.5091165386147408, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0913958353114511e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110891 53.42678818 99.06164805 99.41678851 83.24756312]\n",
      "9-th iteration, loss: 0.5091165386147297, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1789915555740283e-07\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110886e+00 5.34267867e+01 9.90616473e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167887e+01 8.32475631e+01]\n",
      "10-th iteration, loss: 0.5091165386147212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.163165510355324e-07\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110887e+00 5.34267854e+01 9.90616466e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167892e+01 8.32475631e+01]\n",
      "11-th iteration, loss: 0.5091165386147146, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1333044387662787e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110891 53.42678426 99.061646   99.41678964 83.24756312]\n",
      "12-th iteration, loss: 0.5091165386147097, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1704310663782193e-07\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110899 53.4267833  99.06164544 99.41678986 83.24756312]\n",
      "13-th iteration, loss: 0.509116538614706, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.193999084002067e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110910e+00 5.34267825e+01 9.90616449e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167901e+01 8.32475631e+01]\n",
      "14-th iteration, loss: 0.5091165386147029, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.127505351472637e-07\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110923e+00 5.34267818e+01 9.90616445e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167905e+01 8.32475631e+01]\n",
      "15-th iteration, loss: 0.5091165386147004, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0575695786698844e-07\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110939e+00 5.34267812e+01 9.90616441e+01 0.00000000e+00\n",
      " 7.19424520e-14 9.94167910e+01 8.32475631e+01]\n",
      "16-th iteration, loss: 0.5091165386146983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9855649144199138e-07\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110956 53.42678067 99.06164367 99.41679137 83.24756312]\n",
      "17-th iteration, loss: 0.5091165386146967, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9831104835707108e-07\n",
      "17-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110974e+00 5.34267802e+01 9.90616433e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167916e+01 8.32475631e+01]\n",
      "18-th iteration, loss: 0.5091165386146953, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9047696201753164e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110994 53.42677985 99.06164297 99.41679197 83.24756312]\n",
      "19-th iteration, loss: 0.5091165386146942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.894715866483669e-07\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111015 53.42677952 99.06164265 99.41679216 83.24756312]\n",
      "20-th iteration, loss: 0.5091165386146932, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.880615002547127e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111036 53.42677924 99.06164235 99.41679235 83.24756312]\n",
      "21-th iteration, loss: 0.5091165386146923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8632042239703927e-07\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111058 53.426779   99.06164207 99.41679254 83.24756312]\n",
      "22-th iteration, loss: 0.5091165386146916, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8431082751717994e-07\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111081 53.42677879 99.0616418  99.41679273 83.24756312]\n",
      "23-th iteration, loss: 0.5091165386146909, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8208558903666247e-07\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111104e+00 5.34267786e+01 9.90616415e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167929e+01 8.32475631e+01]\n",
      "24-th iteration, loss: 0.5091165386146901, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.733386380629188e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111127 53.42677847 99.0616413  99.41679327 83.24756312]\n",
      "25-th iteration, loss: 0.5091165386146894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7107286568332305e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011115  53.42677833 99.06164106 99.41679345 83.24756312]\n",
      "26-th iteration, loss: 0.5091165386146889, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6869239579803002e-07\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111174e+00 5.34267782e+01 9.90616408e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167936e+01 8.32475631e+01]\n",
      "27-th iteration, loss: 0.5091165386146883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6035550346314166e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111197 53.42677813 99.06164062 99.41679395 83.24756312]\n",
      "28-th iteration, loss: 0.5091165386146878, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5806779384010432e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011122  53.42677804 99.06164041 99.41679411 83.24756312]\n",
      "29-th iteration, loss: 0.5091165386146872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5572364123400979e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111244e+00 5.34267780e+01 9.90616402e+01 0.00000000e+00\n",
      " 4.17443857e-14 9.94167943e+01 8.32475631e+01]\n",
      "30-th iteration, loss: 0.5091165386146866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4792877495660778e-07\n",
      "30-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111267e+00 5.34267779e+01 9.90616400e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167946e+01 8.32475631e+01]\n",
      "31-th iteration, loss: 0.5091165386146861, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4060129741268345e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111290e+00 5.34267779e+01 9.90616398e+01 0.00000000e+00\n",
      " 7.01660952e-14 9.94167949e+01 8.32475631e+01]\n",
      "32-th iteration, loss: 0.5091165386146856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3371484378475405e-07\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111313e+00 5.34267778e+01 9.90616396e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167952e+01 8.32475631e+01]\n",
      "33-th iteration, loss: 0.5091165386146851, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2724395653976695e-07\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111336e+00 5.34267778e+01 9.90616394e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167954e+01 8.32475631e+01]\n",
      "34-th iteration, loss: 0.5091165386146846, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.211641611801704e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111359 53.42677773 99.06163923 99.41679567 83.24756312]\n",
      "35-th iteration, loss: 0.5091165386146843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1967760707687809e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111381 53.4267777  99.06163904 99.41679579 83.24756312]\n",
      "36-th iteration, loss: 0.5091165386146839, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1816458613845167e-07\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111403e+00 5.34267777e+01 9.90616389e+01 0.00000000e+00\n",
      " 4.17443857e-14 9.94167959e+01 8.32475631e+01]\n",
      "37-th iteration, loss: 0.5091165386146834, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1251589616962068e-07\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111425e+00 5.34267777e+01 9.90616387e+01 0.00000000e+00\n",
      " 1.13686838e-13 9.94167961e+01 8.32475631e+01]\n",
      "38-th iteration, loss: 0.509116538614683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0721905481193475e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111447e+00 5.34267776e+01 9.90616385e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167964e+01 8.32475631e+01]\n",
      "39-th iteration, loss: 0.5091165386146826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0225085985134943e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111468e+00 5.34267776e+01 9.90616383e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167966e+01 8.32475631e+01]\n",
      "40-th iteration, loss: 0.5091165386146823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.758957713660134e-08\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111489 53.4267776  99.06163816 99.41679678 83.24756312]\n",
      "41-th iteration, loss: 0.509116538614682, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.662778376662009e-08\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011151  53.42677759 99.06163799 99.41679688 83.24756312]\n",
      "42-th iteration, loss: 0.5091165386146818, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.564119664621636e-08\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111531e+00 5.34267776e+01 9.90616378e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167970e+01 8.32475631e+01]\n",
      "43-th iteration, loss: 0.5091165386146813, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.129277405480157e-08\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111551e+00 5.34267776e+01 9.90616377e+01 0.00000000e+00\n",
      " 8.52651283e-14 9.94167972e+01 8.32475631e+01]\n",
      "44-th iteration, loss: 0.5091165386146811, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.721614772608333e-08\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111572 53.42677756 99.06163751 99.41679735 83.24756312]\n",
      "45-th iteration, loss: 0.5091165386146806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.644616759757935e-08\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111592e+00 5.34267775e+01 9.90616373e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167974e+01 8.32475631e+01]\n",
      "46-th iteration, loss: 0.5091165386146804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.262770286286964e-08\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111611 53.42677754 99.06163719 99.41679761 83.24756312]\n",
      "47-th iteration, loss: 0.5091165386146801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.194142670805027e-08\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111631e+00 5.34267775e+01 9.90616370e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167977e+01 8.32475631e+01]\n",
      "48-th iteration, loss: 0.5091165386146799, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.836271248430662e-08\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011165  53.42677753 99.06163689 99.41679785 83.24756312]\n",
      "49-th iteration, loss: 0.5091165386146795, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.775365036994375e-08\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111669 53.42677753 99.06163674 99.41679793 83.24756312]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355756\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         231.98320922]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6292589800370129\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[ 42.89280919  62.32063131 231.98320922   0.        ]\n",
      "2-th iteration, loss: 0.6034846706022171, 13 gd steps\n",
      "insert gradient: -0.44655276234212676\n",
      "2-th iteration, new layer inserted. now 6 layers\n",
      "[  3.45157305  77.7695492  115.92076696   0.         101.8697649\n",
      "  42.36759133]\n",
      "3-th iteration, loss: 0.5141481088059235, 33 gd steps\n",
      "insert gradient: -0.05968592998493505\n",
      "3-th iteration, new layer inserted. now 6 layers\n",
      "[5.63880738e+00 0.00000000e+00 9.32587341e-15 5.43376558e+01\n",
      " 9.38145533e+01 1.09026079e+02]\n",
      "4-th iteration, loss: 0.5091167136537478, 37 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0550315235655874e-15\n",
      "4-th iteration, new layer inserted. now 6 layers\n",
      "[4.59547386e+00 5.34131460e+01 9.91260018e+01 9.94008908e+01\n",
      " 0.00000000e+00 9.90318938e-14]\n",
      "5-th iteration, loss: 0.5091165393082356, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.4351878172544115e-17\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60151615 53.4273449  99.06384517 99.41665619  0.        ]\n",
      "6-th iteration, loss: 0.509116538708158, 6 gd steps\n",
      "insert gradient: -4.219195233470164e-05\n",
      "6-th iteration, new layer inserted. now 6 layers\n",
      "[4.60114269e+00 0.00000000e+00 3.52495810e-15 5.34260552e+01\n",
      " 9.90629052e+01 9.94164825e+01]\n",
      "7-th iteration, loss: 0.509116538615995, 9 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.803100346864696e-06\n",
      "7-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112339 53.42678184 99.0617143  99.41665839]\n",
      "8-th iteration, loss: 0.50911653861589, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6998255454428097e-06\n",
      "8-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112268 53.42677852 99.06171308 99.41666218]\n",
      "9-th iteration, loss: 0.5091165386158006, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.596797178710269e-06\n",
      "9-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112209e+00 5.34267757e+01 9.90617120e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94166659e+01]\n",
      "10-th iteration, loss: 0.5091165386156761, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3715497838141046e-06\n",
      "10-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112159e+00 5.34267732e+01 9.90617110e+01 0.00000000e+00\n",
      " 1.25677246e-13 9.94166729e+01]\n",
      "11-th iteration, loss: 0.5091165386155697, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1602642761971385e-06\n",
      "11-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112119e+00 5.34267711e+01 9.90617100e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94166796e+01]\n",
      "12-th iteration, loss: 0.5091165386154783, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9622370221334585e-06\n",
      "12-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112085e+00 5.34267693e+01 9.90617090e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94166858e+01]\n",
      "13-th iteration, loss: 0.5091165386153997, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7767702659188515e-06\n",
      "13-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112058e+00 5.34267677e+01 9.90617081e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94166916e+01]\n",
      "14-th iteration, loss: 0.5091165386153316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.603178300942294e-06\n",
      "14-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112036 53.42676636 99.0617072  99.41669711]\n",
      "15-th iteration, loss: 0.5091165386152974, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.529797020545773e-06\n",
      "15-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112020e+00 5.34267652e+01 9.90617063e+01 0.00000000e+00\n",
      " 1.37667655e-14 9.94166997e+01]\n",
      "16-th iteration, loss: 0.5091165386152429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3713327661522094e-06\n",
      "16-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112007 53.42676423 99.06170543 99.41670468]\n",
      "17-th iteration, loss: 0.5091165386152157, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3043796287358524e-06\n",
      "17-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111998 53.42676341 99.06170456 99.41670704]\n",
      "18-th iteration, loss: 0.5091165386151908, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2388753366557976e-06\n",
      "18-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111993 53.42676271 99.06170373 99.41670932]\n",
      "19-th iteration, loss: 0.5091165386151677, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1749095890573726e-06\n",
      "19-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011199  53.42676213 99.06170292 99.41671154]\n",
      "20-th iteration, loss: 0.5091165386151462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.112546941724929e-06\n",
      "20-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111989 53.42676164 99.06170213 99.4167137 ]\n",
      "21-th iteration, loss: 0.5091165386151262, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0518310156137918e-06\n",
      "21-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011199  53.42676124 99.06170135 99.4167158 ]\n",
      "22-th iteration, loss: 0.5091165386151073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.992788032145511e-06\n",
      "22-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111993 53.42676091 99.06170059 99.41671783]\n",
      "23-th iteration, loss: 0.5091165386150897, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.935429784592568e-06\n",
      "23-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111998 53.42676065 99.06169984 99.41671981]\n",
      "24-th iteration, loss: 0.5091165386150729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8797562302874908e-06\n",
      "24-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112003e+00 5.34267604e+01 9.90616991e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167217e+01]\n",
      "25-th iteration, loss: 0.5091165386150442, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7615767024838682e-06\n",
      "25-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112010e+00 5.34267603e+01 9.90616984e+01 0.00000000e+00\n",
      " 9.76996262e-14 9.94167254e+01]\n",
      "26-th iteration, loss: 0.5091165386150187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6516822264768653e-06\n",
      "26-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112018 53.42676016 99.06169761 99.41672889]\n",
      "27-th iteration, loss: 0.5091165386150058, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6059467222848888e-06\n",
      "27-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112026 53.42676007 99.06169684 99.41673052]\n",
      "28-th iteration, loss: 0.5091165386149935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5615751784892932e-06\n",
      "28-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112035 53.42676001 99.06169607 99.41673212]\n",
      "29-th iteration, loss: 0.509116538614982, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5185499182882804e-06\n",
      "29-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112044 53.42675998 99.06169532 99.41673367]\n",
      "30-th iteration, loss: 0.5091165386149709, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4768487846702715e-06\n",
      "30-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112054e+00 5.34267600e+01 9.90616946e+01 0.00000000e+00\n",
      " 1.01252340e-13 9.94167352e+01]\n",
      "31-th iteration, loss: 0.5091165386149522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3859394431284462e-06\n",
      "31-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112065e+00 5.34267600e+01 9.90616938e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167381e+01]\n",
      "32-th iteration, loss: 0.5091165386149354, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3014454817503766e-06\n",
      "32-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112076 53.42676004 99.06169305 99.4167408 ]\n",
      "33-th iteration, loss: 0.5091165386149266, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.267474456704376e-06\n",
      "33-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112087e+00 5.34267601e+01 9.90616923e+01 0.00000000e+00\n",
      " 1.52766688e-13 9.94167421e+01]\n",
      "34-th iteration, loss: 0.5091165386149121, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1910995083109037e-06\n",
      "34-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112098e+00 5.34267602e+01 9.90616915e+01 0.00000000e+00\n",
      " 7.23865412e-14 9.94167446e+01]\n",
      "35-th iteration, loss: 0.5091165386148989, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1200879722497653e-06\n",
      "35-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112110e+00 5.34267602e+01 9.90616907e+01 0.00000000e+00\n",
      " 6.88338275e-14 9.94167469e+01]\n",
      "36-th iteration, loss: 0.509116538614887, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0540442509304598e-06\n",
      "36-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112121e+00 5.34267603e+01 9.90616899e+01 0.00000000e+00\n",
      " 2.44249065e-14 9.94167491e+01]\n",
      "37-th iteration, loss: 0.509116538614876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.926026961673016e-07\n",
      "37-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112133e+00 5.34267604e+01 9.90616891e+01 0.00000000e+00\n",
      " 2.44249065e-14 9.94167512e+01]\n",
      "38-th iteration, loss: 0.509116538614866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.354253358797386e-07\n",
      "38-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112145e+00 5.34267605e+01 9.90616883e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167532e+01]\n",
      "39-th iteration, loss: 0.5091165386148567, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.821996938459807e-07\n",
      "39-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112157e+00 5.34267607e+01 9.90616875e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167550e+01]\n",
      "40-th iteration, loss: 0.5091165386148482, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.32636773790055e-07\n",
      "40-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112169e+00 5.34267608e+01 9.90616867e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167568e+01]\n",
      "41-th iteration, loss: 0.5091165386148403, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.864692743465465e-07\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112181 53.42676093 99.06168592 99.4167584 ]\n",
      "42-th iteration, loss: 0.5091165386148352, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.705944920624335e-07\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112193 53.42676106 99.06168511 99.41675918]\n",
      "43-th iteration, loss: 0.5091165386148303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.55057723329563e-07\n",
      "43-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112205e+00 5.34267612e+01 9.90616843e+01 0.00000000e+00\n",
      " 6.21724894e-14 9.94167599e+01]\n",
      "44-th iteration, loss: 0.5091165386148234, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.137999286635362e-07\n",
      "44-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112217 53.42676137 99.06168353 99.41676144]\n",
      "45-th iteration, loss: 0.509116538614819, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.000210753300185e-07\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112229 53.42676152 99.06168274 99.41676214]\n",
      "46-th iteration, loss: 0.5091165386148145, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.86526911101746e-07\n",
      "46-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112241e+00 5.34267617e+01 9.90616820e+01 0.00000000e+00\n",
      " 1.35447209e-13 9.94167628e+01]\n",
      "47-th iteration, loss: 0.5091165386148085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.49596551496966e-07\n",
      "47-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112253e+00 5.34267619e+01 9.90616812e+01 0.00000000e+00\n",
      " 9.76996262e-14 9.94167642e+01]\n",
      "48-th iteration, loss: 0.5091165386148029, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.151842140115341e-07\n",
      "48-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112265e+00 5.34267620e+01 9.90616804e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167655e+01]\n",
      "49-th iteration, loss: 0.5091165386147977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.831032473721061e-07\n",
      "49-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112277e+00 5.34267622e+01 9.90616797e+01 0.00000000e+00\n",
      " 1.35447209e-13 9.94167667e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5364665204182857\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.80232488   0.         256.70521281]\n",
      "1-th iteration, loss: 0.7498498981810913, 11 gd steps\n",
      "insert gradient: -0.6347981907359495\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.18401986  62.29250037 234.79135318   0.          21.91385963]\n",
      "2-th iteration, loss: 0.6040366479862144, 13 gd steps\n",
      "insert gradient: -0.4409384390366693\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1922579   77.35207373 116.74055884   0.         103.27049435\n",
      "  41.72503259  21.91385963]\n",
      "3-th iteration, loss: 0.5094713963188506, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7512866751076424e-17\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[  2.18581045  54.60749015  97.88579313 101.16888107  21.91385963]\n",
      "4-th iteration, loss: 0.5091165406028058, 34 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2343484860321998e-15\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60091336e+00 5.34303134e+01 9.90577282e+01 9.94189383e+01\n",
      " 0.00000000e+00 3.90798505e-14 2.19138596e+01]\n",
      "5-th iteration, loss: 0.5091165386413579, 13 gd steps\n",
      "insert gradient: -1.2175552311110703e-05\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60096683e+00 5.34269713e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90609229e+01 9.94173675e+01 2.19138596e+01]\n",
      "6-th iteration, loss: 0.5091165386150943, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.210635993452795e-18\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107859 53.42680132 99.06163615 99.41686349 21.91385963]\n",
      "7-th iteration, loss: 0.509116538615048, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.7617083249047845e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107887e+00 5.34267993e+01 9.90616345e+01 9.94168613e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "8-th iteration, loss: 0.5091165386150103, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.5827423714735345e-15\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107922e+00 5.34267975e+01 9.90616330e+01 9.94168592e+01\n",
      " 0.00000000e+00 1.14575016e-13 2.19138596e+01]\n",
      "9-th iteration, loss: 0.509116538614979, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.807033786149365e-15\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107962e+00 5.34267961e+01 9.90616317e+01 9.94168572e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "10-th iteration, loss: 0.5091165386149522, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.278382616027425e-15\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108007e+00 5.34267948e+01 9.90616305e+01 9.94168553e+01\n",
      " 0.00000000e+00 7.46069873e-14 2.19138596e+01]\n",
      "11-th iteration, loss: 0.5091165386149294, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.101959332894695e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108056 53.42679378 99.06162947 99.41685343 21.91385963]\n",
      "12-th iteration, loss: 0.5091165386149095, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6476484820416026e-17\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108107 53.42679289 99.06162849 99.41685166 21.91385963]\n",
      "13-th iteration, loss: 0.509116538614892, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.696435611074485e-15\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108161e+00 5.34267921e+01 9.90616276e+01 9.94168500e+01\n",
      " 0.00000000e+00 5.68434189e-14 2.19138596e+01]\n",
      "14-th iteration, loss: 0.5091165386148767, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.944852430980597e-15\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108217e+00 5.34267915e+01 9.90616268e+01 9.94168484e+01\n",
      " 0.00000000e+00 1.27897692e-13 2.19138596e+01]\n",
      "15-th iteration, loss: 0.5091165386148631, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.093954914580368e-15\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108274e+00 5.34267910e+01 9.90616261e+01 9.94168468e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "16-th iteration, loss: 0.5091165386148506, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1160073531290597e-15\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108333e+00 5.34267905e+01 9.90616254e+01 9.94168454e+01\n",
      " 0.00000000e+00 7.10542736e-14 2.19138596e+01]\n",
      "17-th iteration, loss: 0.5091165386148395, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.945781924756327e-16\n",
      "17-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108392e+00 5.34267901e+01 9.90616248e+01 9.94168440e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "18-th iteration, loss: 0.5091165386148293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.565305451907113e-17\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108451 53.42678983 99.06162425 99.41684262 21.91385963]\n",
      "19-th iteration, loss: 0.5091165386148199, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1252790730502623e-15\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108511e+00 5.34267895e+01 9.90616237e+01 9.94168413e+01\n",
      " 0.00000000e+00 7.19424520e-14 2.19138596e+01]\n",
      "20-th iteration, loss: 0.5091165386148114, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.052582831997984e-15\n",
      "20-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108571e+00 5.34267893e+01 9.90616233e+01 9.94168401e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "21-th iteration, loss: 0.5091165386148035, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.267796493968059e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108630e+00 5.34267891e+01 9.90616228e+01 9.94168389e+01\n",
      " 0.00000000e+00 7.46069873e-14 2.19138596e+01]\n",
      "22-th iteration, loss: 0.5091165386147962, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.666112444189505e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108690e+00 5.34267889e+01 9.90616224e+01 9.94168377e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "23-th iteration, loss: 0.5091165386147894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.728685926835555e-17\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108749 53.4267888  99.06162209 99.41683663 21.91385963]\n",
      "24-th iteration, loss: 0.5091165386147832, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5131255929754893e-17\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108808 53.42678866 99.06162176 99.41683556 21.91385963]\n",
      "25-th iteration, loss: 0.5091165386147773, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.184894970686345e-17\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108866 53.42678854 99.06162145 99.41683453 21.91385963]\n",
      "26-th iteration, loss: 0.5091165386147719, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.596066655321674e-17\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108923 53.42678844 99.06162117 99.41683355 21.91385963]\n",
      "27-th iteration, loss: 0.5091165386147668, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.619083834025772e-17\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010898  53.42678834 99.06162091 99.4168326  21.91385963]\n",
      "28-th iteration, loss: 0.509116538614762, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.241419362392719e-17\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109037 53.42678826 99.06162068 99.41683168 21.91385963]\n",
      "29-th iteration, loss: 0.5091165386147577, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.3489765787752024e-16\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109092e+00 5.34267882e+01 9.90616205e+01 9.94168308e+01\n",
      " 0.00000000e+00 1.42108547e-14 2.19138596e+01]\n",
      "30-th iteration, loss: 0.5091165386147536, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.3417107790371953e-17\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109148 53.42678811 99.06162027 99.41682997 21.91385963]\n",
      "31-th iteration, loss: 0.5091165386147497, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5939436836330419e-15\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109202e+00 5.34267880e+01 9.90616201e+01 9.94168292e+01\n",
      " 0.00000000e+00 5.32907052e-14 2.19138596e+01]\n",
      "32-th iteration, loss: 0.5091165386147462, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.630312068339595e-17\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109256 53.42678797 99.06161993 99.41682838 21.91385963]\n",
      "33-th iteration, loss: 0.5091165386147427, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0511949831839097e-15\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109308e+00 5.34267879e+01 9.90616198e+01 9.94168276e+01\n",
      " 0.00000000e+00 7.01660952e-14 2.19138596e+01]\n",
      "34-th iteration, loss: 0.5091165386147396, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9728007362605147e-17\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109361 53.42678784 99.06161965 99.41682691 21.91385963]\n",
      "35-th iteration, loss: 0.5091165386147365, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.30156338141989e-17\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109412 53.42678777 99.06161953 99.41682622 21.91385963]\n",
      "36-th iteration, loss: 0.5091165386147337, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.224662698542142e-16\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109463e+00 5.34267877e+01 9.90616194e+01 9.94168255e+01\n",
      " 0.00000000e+00 1.42108547e-14 2.19138596e+01]\n",
      "37-th iteration, loss: 0.509116538614731, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7905175692206597e-15\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109513e+00 5.34267876e+01 9.90616193e+01 9.94168249e+01\n",
      " 0.00000000e+00 5.77315973e-14 2.19138596e+01]\n",
      "38-th iteration, loss: 0.5091165386147286, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.895609012908725e-17\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109562 53.42678758 99.06161925 99.41682428 21.91385963]\n",
      "39-th iteration, loss: 0.5091165386147262, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.254120149514949e-17\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010961  53.42678752 99.06161917 99.41682368 21.91385963]\n",
      "40-th iteration, loss: 0.509116538614724, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1577571209990733e-15\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109658e+00 5.34267874e+01 9.90616191e+01 9.94168231e+01\n",
      " 0.00000000e+00 7.01660952e-14 2.19138596e+01]\n",
      "41-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6967641702413162e-15\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109705e+00 5.34267874e+01 9.90616191e+01 9.94168226e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "42-th iteration, loss: 0.5091165386147198, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.163361591490623e-15\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109752e+00 5.34267873e+01 9.90616190e+01 9.94168220e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "43-th iteration, loss: 0.509116538614718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.239145151064029e-16\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109797e+00 5.34267872e+01 9.90616190e+01 9.94168215e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "44-th iteration, loss: 0.5091165386147162, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.504436255781421e-15\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109842e+00 5.34267872e+01 9.90616189e+01 9.94168210e+01\n",
      " 0.00000000e+00 1.14575016e-13 2.19138596e+01]\n",
      "45-th iteration, loss: 0.5091165386147145, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.386394193304624e-16\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109886e+00 5.34267871e+01 9.90616189e+01 9.94168205e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "46-th iteration, loss: 0.509116538614713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.059540305763039e-17\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010993  53.42678704 99.06161889 99.41682007 21.91385963]\n",
      "47-th iteration, loss: 0.5091165386147114, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6108179403182298e-15\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109973e+00 5.34267870e+01 9.90616189e+01 9.94168196e+01\n",
      " 0.00000000e+00 5.32907052e-14 2.19138596e+01]\n",
      "48-th iteration, loss: 0.50911653861471, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.500525522502344e-17\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110015 53.42678689 99.06161888 99.41681919 21.91385963]\n",
      "49-th iteration, loss: 0.5091165386147085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7368169260311371e-09\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110057 53.42678681 99.06161887 99.41681877 21.91385963]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536721336560604\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         282.03265323]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6333108339474548\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.32015942   0.          44.7124938 ]\n",
      "2-th iteration, loss: 0.6051562988423069, 13 gd steps\n",
      "insert gradient: -0.44287306577339\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.13890924  77.08377718 117.82668764   0.         104.2313006\n",
      "  40.82946936  44.7124938 ]\n",
      "3-th iteration, loss: 0.5094819386284526, 50 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1637720451077163e-15\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[2.09441160e+00 5.46242065e+01 9.80241166e+01 1.01144724e+02\n",
      " 0.00000000e+00 7.19424520e-14 4.47124938e+01]\n",
      "4-th iteration, loss: 0.5091172269371531, 17 gd steps\n",
      "insert gradient: -0.0016064205639812212\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.63972049e+00 5.34657345e+01 9.90510432e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.93712684e+01 4.47124938e+01]\n",
      "5-th iteration, loss: 0.509116542510028, 28 gd steps\n",
      "insert gradient: -2.9084962098561204e-05\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.61251497e+00 5.34248560e+01 0.00000000e+00 5.72875081e-14\n",
      " 9.90592495e+01 9.94189124e+01 4.47124938e+01]\n",
      "6-th iteration, loss: 0.5091165386436309, 19 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6007246952170097e-06\n",
      "6-th iteration, new layer inserted. now 7 layers\n",
      "[4.60099134e+00 0.00000000e+00 9.15933995e-16 5.34265308e+01\n",
      " 9.90624673e+01 9.94165833e+01 4.47124938e+01]\n",
      "7-th iteration, loss: 0.509116538615336, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7390092186916205e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111376e+00 5.34267904e+01 9.90616724e+01 9.94168642e+01\n",
      " 0.00000000e+00 5.68434189e-14 4.47124938e+01]\n",
      "8-th iteration, loss: 0.5091165386152379, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1074673196754186e-17\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011133  53.4267873  99.06166956 99.4168615  44.7124938 ]\n",
      "9-th iteration, loss: 0.509116538615159, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2776061822782934e-16\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111295 53.42678473 99.06166693 99.41685888 44.7124938 ]\n",
      "10-th iteration, loss: 0.5091165386150956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.270882550252856e-17\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111271 53.4267826  99.06166454 99.41685641 44.7124938 ]\n",
      "11-th iteration, loss: 0.5091165386150437, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.640707828780049e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111256 53.42678084 99.06166235 99.41685405 44.7124938 ]\n",
      "12-th iteration, loss: 0.5091165386150007, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.432274732709646e-15\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111248e+00 5.34267794e+01 9.90616604e+01 9.94168518e+01\n",
      " 0.00000000e+00 1.14575016e-13 4.47124938e+01]\n",
      "13-th iteration, loss: 0.5091165386149651, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.425414447078349e-15\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111246e+00 5.34267782e+01 9.90616585e+01 9.94168497e+01\n",
      " 0.00000000e+00 8.43769499e-14 4.47124938e+01]\n",
      "14-th iteration, loss: 0.5091165386149348, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4022459190324148e-17\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011125  53.42677728 99.06165684 99.41684771 44.7124938 ]\n",
      "15-th iteration, loss: 0.5091165386149085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3837180533090305e-18\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111257 53.42677651 99.06165527 99.41684578 44.7124938 ]\n",
      "16-th iteration, loss: 0.5091165386148856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.388599456144329e-17\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111268 53.4267759  99.06165382 99.41684394 44.7124938 ]\n",
      "17-th iteration, loss: 0.5091165386148656, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.184188438390914e-17\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111282 53.42677543 99.06165247 99.41684218 44.7124938 ]\n",
      "18-th iteration, loss: 0.509116538614848, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6251558541142734e-15\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111298e+00 5.34267751e+01 9.90616512e+01 9.94168405e+01\n",
      " 0.00000000e+00 5.68434189e-14 4.47124938e+01]\n",
      "19-th iteration, loss: 0.5091165386148324, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5952148412479132e-16\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111317 53.4267748  99.06165004 99.41683893 44.7124938 ]\n",
      "20-th iteration, loss: 0.5091165386148184, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7876556249292795e-17\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111336 53.42677461 99.06164894 99.41683741 44.7124938 ]\n",
      "21-th iteration, loss: 0.5091165386148058, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.12597815340521e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111357e+00 5.34267745e+01 9.90616479e+01 9.94168360e+01\n",
      " 0.00000000e+00 7.19424520e-14 4.47124938e+01]\n",
      "22-th iteration, loss: 0.5091165386147946, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.643961739420902e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111379e+00 5.34267744e+01 9.90616469e+01 9.94168346e+01\n",
      " 0.00000000e+00 8.88178420e-14 4.47124938e+01]\n",
      "23-th iteration, loss: 0.5091165386147843, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5416242987368004e-15\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111402e+00 5.34267744e+01 9.90616460e+01 9.94168333e+01\n",
      " 0.00000000e+00 5.32907052e-14 4.47124938e+01]\n",
      "24-th iteration, loss: 0.509116538614775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5477050853825678e-08\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111425 53.4267744  99.06164519 99.41683201 44.7124938 ]\n",
      "25-th iteration, loss: 0.5091165386147665, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.144702560342823e-08\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111448 53.42677443 99.06164437 99.41683079 44.7124938 ]\n",
      "26-th iteration, loss: 0.5091165386147587, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.225230270469821e-08\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111471e+00 0.00000000e+00 2.35922393e-15 5.34267745e+01\n",
      " 9.90616436e+01 9.94168296e+01 4.47124938e+01]\n",
      "27-th iteration, loss: 0.5091165386147515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.261795389816296e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111495e+00 2.07194510e-09 0.00000000e+00 7.04461334e-08\n",
      " 2.34816203e-07 5.34267746e+01 9.90616429e+01 9.94168285e+01\n",
      " 4.47124938e+01]\n",
      "28-th iteration, loss: 0.5091165386147446, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1403510720591362e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111517e+00 2.12586358e-09 0.00000000e+00 5.52724532e-08\n",
      " 2.27549351e-07 1.25772505e-07 4.57929892e-07 5.34267746e+01\n",
      " 9.90616422e+01 9.94168275e+01 4.47124938e+01]\n",
      "29-th iteration, loss: 0.509116538614738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9992819524145863e-07\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[4.60111539e+00 8.10207389e-10 0.00000000e+00 1.70143552e-08\n",
      " 2.15408257e-07 7.09711522e-08 4.39477361e-07 1.55798919e-07\n",
      " 6.61938597e-07 5.34267747e+01 9.90616415e+01 9.94168264e+01\n",
      " 4.47124938e+01]\n",
      "30-th iteration, loss: 0.509116538614732, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.851635021568765e-07\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111621e+00 1.97793667e-09 0.00000000e+00 5.14263534e-08\n",
      " 6.35799415e-07 1.65903884e-07 8.48450724e-07 5.34267747e+01\n",
      " 9.90616409e+01 9.94168255e+01 4.47124938e+01]\n",
      "31-th iteration, loss: 0.5091165386147267, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7626772222034098e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111659e+00 3.11272239e-10 0.00000000e+00 8.09307821e-09\n",
      " 8.20023925e-07 1.62915185e-07 1.02222904e-06 5.34267748e+01\n",
      " 9.90616403e+01 9.94168245e+01 4.47124938e+01]\n",
      "32-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6618772578283382e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111794e+00 4.44518352e-09 0.00000000e+00 1.51136240e-07\n",
      " 1.19012039e-06 5.34267748e+01 9.90616397e+01 9.94168237e+01\n",
      " 4.47124938e+01]\n",
      "33-th iteration, loss: 0.5091165386147175, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6073265166838715e-07\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111828e+00 3.77268818e-09 0.00000000e+00 1.28271398e-07\n",
      " 1.34878875e-06 5.34267749e+01 9.90616391e+01 9.94168228e+01\n",
      " 4.47124938e+01]\n",
      "34-th iteration, loss: 0.5091165386147136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5522958829732472e-07\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111860e+00 3.01152732e-09 0.00000000e+00 1.02391929e-07\n",
      " 1.50333829e-06 5.34267750e+01 9.90616386e+01 9.94168220e+01\n",
      " 4.47124938e+01]\n",
      "35-th iteration, loss: 0.50911653861471, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.497450383972368e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111892e+00 2.14094383e-09 0.00000000e+00 7.27920901e-08\n",
      " 1.65393561e-06 5.34267750e+01 9.90616381e+01 9.94168212e+01\n",
      " 4.47124938e+01]\n",
      "36-th iteration, loss: 0.5091165386147066, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4432313764503513e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111922e+00 1.14812295e-09 0.00000000e+00 3.90361802e-08\n",
      " 1.80085216e-06 5.34267751e+01 9.90616376e+01 9.94168205e+01\n",
      " 4.47124938e+01]\n",
      "37-th iteration, loss: 0.5091165386147036, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.389924365970731e-07\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111951e+00 2.56443845e-11 0.00000000e+00 8.71909074e-10\n",
      " 1.94442669e-06 5.34267752e+01 9.90616372e+01 9.94168198e+01\n",
      " 4.47124938e+01]\n",
      "38-th iteration, loss: 0.5091165386147007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.778287915536088e-08\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112188 53.42677526 99.06163673 99.41681908 44.7124938 ]\n",
      "39-th iteration, loss: 0.5091165386146984, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.310176877501799e-08\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112201 53.42677533 99.06163631 99.41681843 44.7124938 ]\n",
      "40-th iteration, loss: 0.5091165386146962, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.69070613509461e-08\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112215 53.42677541 99.06163592 99.41681781 44.7124938 ]\n",
      "41-th iteration, loss: 0.5091165386146942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.945235811404187e-08\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112228e+00 0.00000000e+00 4.96824804e-15 5.34267755e+01\n",
      " 9.90616355e+01 9.94168172e+01 4.47124938e+01]\n",
      "42-th iteration, loss: 0.5091165386146922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2291328367775442e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[4.60112241e+00 2.33141997e-09 0.00000000e+00 7.92682788e-08\n",
      " 1.29888291e-07 5.34267756e+01 9.90616352e+01 9.94168166e+01\n",
      " 4.47124938e+01]\n",
      "43-th iteration, loss: 0.5091165386146904, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1350991835691805e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112253e+00 2.28637120e-09 0.00000000e+00 5.94456512e-08\n",
      " 1.22927966e-07 1.38668881e-07 2.47825104e-07 5.34267756e+01\n",
      " 9.90616348e+01 9.94168161e+01 4.47124938e+01]\n",
      "44-th iteration, loss: 0.5091165386146888, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.039718926797924e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[4.60112264e+00 1.29133651e-09 0.00000000e+00 2.71180667e-08\n",
      " 1.13688596e-07 8.55686827e-08 2.32873547e-07 1.72532111e-07\n",
      " 3.49039356e-07 5.34267757e+01 9.90616345e+01 9.94168156e+01\n",
      " 4.47124938e+01]\n",
      "45-th iteration, loss: 0.5091165386146872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.573997600546809e-08\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[4.60112285e+00 9.13319653e-10 0.00000000e+00 1.91797127e-08\n",
      " 2.16629159e-07 8.57020855e-08 3.30426251e-07 1.87328472e-07\n",
      " 4.35728521e-07 5.34267757e+01 9.90616342e+01 9.94168151e+01\n",
      " 4.47124938e+01]\n",
      "46-th iteration, loss: 0.5091165386146858, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.915223086607371e-08\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112336e+00 2.62731695e-09 0.00000000e+00 6.83102408e-08\n",
      " 4.20684299e-07 1.93369338e-07 5.14191373e-07 5.34267758e+01\n",
      " 9.90616339e+01 9.94168146e+01 4.47124938e+01]\n",
      "47-th iteration, loss: 0.5091165386146845, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.49789527777623e-08\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112354e+00 1.59219715e-09 0.00000000e+00 4.13971260e-08\n",
      " 5.06652276e-07 1.94536943e-07 5.87983787e-07 5.34267758e+01\n",
      " 9.90616336e+01 9.94168141e+01 4.47124938e+01]\n",
      "48-th iteration, loss: 0.5091165386146833, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.110187742313197e-08\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112371e+00 3.26097810e-10 0.00000000e+00 8.47854307e-09\n",
      " 5.90179730e-07 1.93845979e-07 6.59262161e-07 5.34267758e+01\n",
      " 9.90616333e+01 9.94168137e+01 4.47124938e+01]\n",
      "49-th iteration, loss: 0.5091165386146823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.612395475543989e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[4.60112455e+00 5.48016420e-09 0.00000000e+00 1.86325583e-07\n",
      " 7.28803278e-07 5.34267759e+01 9.90616330e+01 9.94168133e+01\n",
      " 4.47124938e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5365154209642973\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.79326351   0.         307.96553046]\n",
      "1-th iteration, loss: 0.7479635372963356, 11 gd steps\n",
      "insert gradient: -0.6308616976652868\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.45607731  62.35062607 236.60766365   0.          71.35786681]\n",
      "2-th iteration, loss: 0.6054338475143709, 13 gd steps\n",
      "insert gradient: -0.45076273962597946\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.41006398  77.3179864  117.58506071   0.         104.0175537\n",
      "  40.90133603  71.35786681]\n",
      "3-th iteration, loss: 0.5094596098736359, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.724385749047338e-17\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[  2.22457638  54.58246062  97.89056962 101.14251937  71.35786681]\n",
      "4-th iteration, loss: 0.509116539241859, 37 gd steps\n",
      "insert gradient: -7.991871506944366e-05\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60142087e+00 5.34279174e+01 9.90593730e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94153673e+01 7.13578668e+01]\n",
      "5-th iteration, loss: 0.509116538616539, 15 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5496290395444653e-06\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109939e+00 0.00000000e+00 1.02695630e-15 5.34267944e+01\n",
      " 9.90615414e+01 9.94169858e+01 7.13578668e+01]\n",
      "6-th iteration, loss: 0.5091165386164169, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1311927254112953e-06\n",
      "6-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110022e+00 1.47625667e-06 8.35286826e-07 5.34267959e+01\n",
      " 9.90615415e+01 9.94169807e+01 7.13578668e+01]\n",
      "7-th iteration, loss: 0.5091165386163107, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.311779213142963e-07\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[4.60110094e+00 2.50895830e-06 1.46465958e-06 0.00000000e+00\n",
      " 7.80858498e-22 5.34267970e+01 9.90615416e+01 9.94169758e+01\n",
      " 7.13578668e+01]\n",
      "8-th iteration, loss: 0.5091165386162133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.416955867971397e-07\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110158e+00 1.18373502e-07 0.00000000e+00 3.07771105e-06\n",
      " 1.94004364e-06 7.79348828e-07 4.75384066e-07 5.34267978e+01\n",
      " 9.90615417e+01 9.94169710e+01 7.13578668e+01]\n",
      "9-th iteration, loss: 0.5091165386161256, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.690638434772865e-07\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110212e+00 2.01136147e-08 0.00000000e+00 4.22385908e-07\n",
      " 5.34081866e-07 3.40183707e-06 2.28033685e-06 1.22563007e-06\n",
      " 7.66605415e-07 5.34267982e+01 9.90615417e+01 9.94169664e+01\n",
      " 7.13578668e+01]\n",
      "10-th iteration, loss: 0.5091165386160452, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.174035750545395e-07\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[4.60110259e+00 3.32702188e-09 0.00000000e+00 5.65593720e-08\n",
      " 4.67396973e-07 4.62158687e-07 9.74883231e-07 3.47523842e-06\n",
      " 2.50694088e-06 1.44261328e-06 9.16037407e-07 5.34267985e+01\n",
      " 9.90615418e+01 9.94169619e+01 7.13578668e+01]\n",
      "11-th iteration, loss: 0.5091165386159701, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.772479598893315e-07\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110431e+00 1.60381894e-08 0.00000000e+00 3.36801978e-07\n",
      " 1.36271420e-06 3.42730361e-06 2.67595278e-06 1.55252851e-06\n",
      " 9.94214870e-07 5.34267987e+01 9.90615418e+01 9.94169576e+01\n",
      " 7.13578668e+01]\n",
      "12-th iteration, loss: 0.5091165386159007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.55605224464375e-07\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110508e+00 6.50543189e-09 0.00000000e+00 1.36614070e-07\n",
      " 1.72339484e-06 3.31942470e-06 2.82083257e-06 1.61314151e-06\n",
      " 1.04133938e-06 5.34267988e+01 9.90615420e+01 9.94169535e+01\n",
      " 7.13578668e+01]\n",
      "13-th iteration, loss: 0.509116538615836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.262746117250606e-07\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110787e+00 1.18159911e-07 0.00000000e+00 3.07215770e-06\n",
      " 2.96407895e-06 1.66164873e-06 1.08301397e-06 5.34267989e+01\n",
      " 9.90615421e+01 9.94169495e+01 7.13578668e+01]\n",
      "14-th iteration, loss: 0.5091165386157759, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.145097674831355e-07\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110854e+00 1.07767999e-07 0.00000000e+00 2.80196796e-06\n",
      " 3.10093917e-06 1.68586084e-06 1.11524812e-06 5.34267990e+01\n",
      " 9.90615423e+01 9.94169456e+01 7.13578668e+01]\n",
      "15-th iteration, loss: 0.5091165386157198, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.025707480501758e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110919e+00 9.73846855e-08 0.00000000e+00 2.53200182e-06\n",
      " 3.24289921e-06 1.70853068e-06 1.15105757e-06 5.34267991e+01\n",
      " 9.90615426e+01 9.94169418e+01 7.13578668e+01]\n",
      "16-th iteration, loss: 0.5091165386156673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2629044317013235e-07\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110980e+00 2.34722795e-06 3.38983696e-06 1.72794664e-06\n",
      " 1.19041732e-06 5.34267992e+01 0.00000000e+00 2.39808173e-14\n",
      " 9.90615429e+01 9.94169382e+01 7.13578668e+01]\n",
      "17-th iteration, loss: 0.5091165386156179, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.497509935368353e-07\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[4.60111010e+00 2.13836001e-06 3.53964616e-06 1.73252052e-06\n",
      " 1.23142598e-06 5.34267993e+01 3.37446526e-07 7.95287879e-08\n",
      " 0.00000000e+00 2.56425990e-23 9.90615432e+01 9.94169347e+01\n",
      " 7.13578668e+01]\n",
      "18-th iteration, loss: 0.5091165386155709, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6193858085042553e-07\n",
      "18-th iteration, new layer inserted. now 15 layers\n",
      "[4.60111039e+00 1.89087112e-06 3.68882443e-06 1.70790622e-06\n",
      " 1.27151572e-06 5.34267993e+01 6.89280752e-07 1.01219827e-07\n",
      " 3.59194849e-07 2.16910387e-08 0.00000000e+00 3.30872245e-24\n",
      " 9.90615436e+01 9.94169313e+01 7.13578668e+01]\n",
      "19-th iteration, loss: 0.5091165386155259, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.643312088061833e-07\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111066e+00 1.61033817e-06 3.84108377e-06 1.65964091e-06\n",
      " 1.31623635e-06 5.34267993e+01 1.04985738e-06 6.92208269e-08\n",
      " 9.90615451e+01 9.94169280e+01 7.13578668e+01]\n",
      "20-th iteration, loss: 0.509116538615484, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.949654581977994e-07\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111092e+00 1.30930003e-06 4.00130821e-06 1.60045745e-06\n",
      " 1.37196113e-06 5.34267994e+01 0.00000000e+00 2.70894418e-14\n",
      " 9.90615469e+01 9.94169249e+01 7.13578668e+01]\n",
      "21-th iteration, loss: 0.5091165386154446, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1636131209317743e-07\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111118e+00 1.00760710e-06 4.17580771e-06 1.55070778e-06\n",
      " 1.44568747e-06 5.34267994e+01 4.06465990e-07 3.66362031e-08\n",
      " 9.90615473e+01 9.94169218e+01 7.13578668e+01]\n",
      "22-th iteration, loss: 0.5091165386154073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3713466123012844e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111142e+00 6.89498439e-07 4.36107937e-06 1.49552979e-06\n",
      " 1.53331846e-06 5.34267994e+01 8.31014444e-07 3.48666316e-08\n",
      " 9.90615477e+01 9.94169188e+01 7.13578668e+01]\n",
      "23-th iteration, loss: 0.509116538615372, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.5650826083181043e-07\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111167e+00 3.58377247e-07 4.55904262e-06 1.43900494e-06\n",
      " 1.63711533e-06 5.34267995e+01 9.90615494e+01 9.94169159e+01\n",
      " 7.13578668e+01]\n",
      "24-th iteration, loss: 0.5091165386153392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.886569584081363e-07\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111190e+00 1.97192823e-08 4.77181356e-06 1.38740813e-06\n",
      " 1.75927900e-06 5.34267995e+01 9.90615499e+01 9.94169130e+01\n",
      " 7.13578668e+01]\n",
      "25-th iteration, loss: 0.5091165386153087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.04132519685508e-07\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111713e+00 1.34382659e-06 1.90076975e-06 5.34267996e+01\n",
      " 9.90615504e+01 9.94169103e+01 7.13578668e+01]\n",
      "26-th iteration, loss: 0.5091165386152803, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.228358291760236e-07\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111735e+00 1.25516583e-06 2.02895823e-06 5.34267996e+01\n",
      " 0.00000000e+00 3.15303339e-14 9.90615509e+01 9.94169077e+01\n",
      " 7.13578668e+01]\n",
      "27-th iteration, loss: 0.5091165386152523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.215406882397523e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111755e+00 1.13660390e-06 2.15237438e-06 5.34267996e+01\n",
      " 5.27541403e-07 9.19193487e-09 0.00000000e+00 9.30578189e-25\n",
      " 9.90615514e+01 9.94169051e+01 7.13578668e+01]\n",
      "28-th iteration, loss: 0.5091165386152248, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.032051082439788e-07\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111774e+00 9.78185293e-07 2.27056500e-06 5.34267996e+01\n",
      " 9.90615535e+01 9.94169027e+01 7.13578668e+01]\n",
      "29-th iteration, loss: 0.5091165386152007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.264471875399688e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111792e+00 7.85948174e-07 2.38726958e-06 5.34267996e+01\n",
      " 9.90615541e+01 9.94169002e+01 7.13578668e+01]\n",
      "30-th iteration, loss: 0.5091165386151778, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.472276960105089e-07\n",
      "30-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111809e+00 5.95050896e-07 2.51322994e-06 5.34267995e+01\n",
      " 9.90615546e+01 9.94168979e+01 7.13578668e+01]\n",
      "31-th iteration, loss: 0.5091165386151563, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.659116271958049e-07\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111826e+00 4.02495611e-07 2.64772922e-06 5.34267995e+01\n",
      " 0.00000000e+00 3.77475828e-14 9.90615552e+01 9.94168956e+01\n",
      " 7.13578668e+01]\n",
      "32-th iteration, loss: 0.5091165386151345, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.618173307888983e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111843e+00 1.99016853e-07 2.78889950e-06 5.34267995e+01\n",
      " 0.00000000e+00 1.64313008e-14 9.90615563e+01 9.94168934e+01\n",
      " 7.13578668e+01]\n",
      "33-th iteration, loss: 0.5091165386151139, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.575739907150986e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112152 53.4267994  99.06155743 99.41689131 71.35786681]\n",
      "34-th iteration, loss: 0.5091165386150956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.681655929531871e-07\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112167e+00 5.34267993e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90615580e+01 9.94168892e+01 7.13578668e+01]\n",
      "35-th iteration, loss: 0.5091165386150771, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.565978413136549e-07\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112181e+00 5.34267993e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90615591e+01 9.94168872e+01 7.13578668e+01]\n",
      "36-th iteration, loss: 0.5091165386150596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.46385049208558e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112195 53.42679914 99.06156026 99.41688526 71.35786681]\n",
      "37-th iteration, loss: 0.509116538615044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.579651025541763e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112207 53.426799   99.06156081 99.41688335 71.35786681]\n",
      "38-th iteration, loss: 0.5091165386150293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.683757567188375e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011222  53.42679886 99.06156137 99.4168815  71.35786681]\n",
      "39-th iteration, loss: 0.5091165386150153, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.777016165435082e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112232e+00 5.34267987e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90615619e+01 9.94168797e+01 7.13578668e+01]\n",
      "40-th iteration, loss: 0.5091165386150007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.641767373409481e-07\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112243e+00 5.34267986e+01 0.00000000e+00 4.13002965e-14\n",
      " 9.90615631e+01 9.94168780e+01 7.13578668e+01]\n",
      "41-th iteration, loss: 0.5091165386149868, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.52002853656945e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112254e+00 5.34267984e+01 0.00000000e+00 4.26325641e-14\n",
      " 9.90615642e+01 9.94168763e+01 7.13578668e+01]\n",
      "42-th iteration, loss: 0.5091165386149735, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.40937277871605e-07\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112264e+00 5.34267982e+01 0.00000000e+00 5.90638649e-14\n",
      " 9.90615654e+01 9.94168746e+01 7.13578668e+01]\n",
      "43-th iteration, loss: 0.5091165386149609, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.307791280910284e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112274 53.426798   99.06156645 99.41687302 71.35786681]\n",
      "44-th iteration, loss: 0.5091165386149499, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.416865108910824e-07\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112283e+00 5.34267978e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90615670e+01 9.94168714e+01 7.13578668e+01]\n",
      "45-th iteration, loss: 0.5091165386149382, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.30454089356228e-07\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112292 53.42679756 99.06156808 99.41686992 71.35786681]\n",
      "46-th iteration, loss: 0.5091165386149283, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.403775203393498e-07\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.601123   53.42679733 99.06156862 99.41686844 71.35786681]\n",
      "47-th iteration, loss: 0.5091165386149189, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.489059327845704e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112309 53.42679712 99.06156917 99.416867   71.35786681]\n",
      "48-th iteration, loss: 0.5091165386149098, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.56186012694776e-07\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112318 53.42679691 99.06156972 99.4168656  71.35786681]\n",
      "49-th iteration, loss: 0.5091165386149012, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.623462500946394e-07\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112326e+00 5.34267967e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90615703e+01 9.94168642e+01 7.13578668e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536648578875073\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.89416964   0.         332.99025247]\n",
      "1-th iteration, loss: 0.7481568929324234, 11 gd steps\n",
      "insert gradient: -0.6317802871014003\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.52886631  62.34663952 235.52969077   0.          97.4605617 ]\n",
      "2-th iteration, loss: 0.6049244714380901, 13 gd steps\n",
      "insert gradient: -0.4493983797547323\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.4231431   77.4251234  117.11640185   0.         103.60297087\n",
      "  41.29093827  97.4605617 ]\n",
      "3-th iteration, loss: 0.5093962299073077, 46 gd steps\n",
      "insert gradient: -0.004143449235379363\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[2.52687069e+00 5.44613615e+01 0.00000000e+00 2.97539771e-14\n",
      " 9.77951608e+01 1.01016079e+02 9.74605617e+01]\n",
      "4-th iteration, loss: 0.5091259512142529, 29 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.574344316312493e-15\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.10070158e+00 5.36175090e+01 9.89524018e+01 9.95931410e+01\n",
      " 0.00000000e+00 1.13686838e-13 9.74605617e+01]\n",
      "5-th iteration, loss: 0.5091165388925496, 33 gd steps\n",
      "insert gradient: -0.0001285211202457494\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[4.60140241e+00 0.00000000e+00 2.88657986e-15 5.34258407e+01\n",
      " 9.90611860e+01 9.94163038e+01 6.52542723e-16 1.17075077e-03\n",
      " 9.74605617e+01]\n",
      "6-th iteration, loss: 0.5091165386211892, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3625865647593567e-06\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60142489 53.42679318 99.06135333 99.41704939 97.4605617 ]\n",
      "7-th iteration, loss: 0.5091165386209902, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3567519280167423e-06\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60142075 53.42679284 99.0613567  99.41704466 97.4605617 ]\n",
      "8-th iteration, loss: 0.5091165386207978, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.347201802452666e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60141668e+00 5.34267926e+01 0.00000000e+00 3.68594044e-14\n",
      " 9.90613601e+01 9.94170401e+01 9.74605617e+01]\n",
      "9-th iteration, loss: 0.5091165386205686, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.210023867567041e-06\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60141266 53.42679225 99.06136675 99.41703555 97.4605617 ]\n",
      "10-th iteration, loss: 0.5091165386203891, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2050017457537955e-06\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60140869e+00 5.34267919e+01 0.00000000e+00 5.90638649e-14\n",
      " 9.90613700e+01 9.94170311e+01 9.74605617e+01]\n",
      "11-th iteration, loss: 0.5091165386201759, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0769139479885836e-06\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60140477 53.42679154 99.06137637 99.41702679 97.4605617 ]\n",
      "12-th iteration, loss: 0.5091165386200085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.075021237038052e-06\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60140089e+00 5.34267912e+01 0.00000000e+00 1.90958360e-14\n",
      " 9.90613795e+01 9.94170225e+01 9.74605617e+01]\n",
      "13-th iteration, loss: 0.5091165386198101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.954437865053403e-06\n",
      "13-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60139707 53.42679077 99.06138561 99.41701836 97.4605617 ]\n",
      "14-th iteration, loss: 0.5091165386196537, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9546797015563493e-06\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60139329e+00 5.34267904e+01 0.00000000e+00 3.41948692e-14\n",
      " 9.90613886e+01 9.94170143e+01 9.74605617e+01]\n",
      "15-th iteration, loss: 0.5091165386194689, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.840437186814864e-06\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60138957 53.42678996 99.06139448 99.41701026 97.4605617 ]\n",
      "16-th iteration, loss: 0.509116538619323, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8421078087843506e-06\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6013859  53.42678954 99.06139733 99.41700633 97.4605617 ]\n",
      "17-th iteration, loss: 0.509116538619182, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.839442020977499e-06\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60138228 53.42678917 99.06140019 99.41700249 97.4605617 ]\n",
      "18-th iteration, loss: 0.5091165386190458, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8330440908887572e-06\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60137873 53.42678885 99.06140304 99.41699874 97.4605617 ]\n",
      "19-th iteration, loss: 0.509116538618914, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8234342338970485e-06\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60137524e+00 5.34267886e+01 0.00000000e+00 5.72875081e-14\n",
      " 9.90614059e+01 9.94169951e+01 9.74605617e+01]\n",
      "20-th iteration, loss: 0.5091165386187559, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7060695427553277e-06\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60137179 53.42678829 99.06141152 99.41699151 97.4605617 ]\n",
      "21-th iteration, loss: 0.5091165386186328, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.700211620059498e-06\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136838 53.42678796 99.06141423 99.41698799 97.4605617 ]\n",
      "22-th iteration, loss: 0.5091165386185137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6911875862575664e-06\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136503 53.42678768 99.06141695 99.41698456 97.4605617 ]\n",
      "23-th iteration, loss: 0.5091165386183986, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.679443791445919e-06\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136173 53.42678743 99.06141965 99.4169812  97.4605617 ]\n",
      "24-th iteration, loss: 0.5091165386182871, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.66536482795754e-06\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60135848e+00 5.34267872e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90614223e+01 9.94169779e+01 9.74605617e+01]\n",
      "25-th iteration, loss: 0.5091165386181519, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5503142258882544e-06\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60135528e+00 5.34267870e+01 0.00000000e+00 3.19744231e-14\n",
      " 9.90614276e+01 9.94169747e+01 9.74605617e+01]\n",
      "26-th iteration, loss: 0.5091165386180226, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.445912689936802e-06\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6013521  53.42678668 99.06143274 99.41697155 97.4605617 ]\n",
      "27-th iteration, loss: 0.5091165386179219, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4420145449723814e-06\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134896 53.42678634 99.06143519 99.41696843 97.4605617 ]\n",
      "28-th iteration, loss: 0.5091165386178246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4349353677717023e-06\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134587 53.42678605 99.06143765 99.41696538 97.4605617 ]\n",
      "29-th iteration, loss: 0.5091165386177303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4251309104195413e-06\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134283 53.42678579 99.06144009 99.4169624  97.4605617 ]\n",
      "30-th iteration, loss: 0.5091165386176391, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.41299321607541e-06\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60133984 53.42678557 99.06144252 99.4169595  97.4605617 ]\n",
      "31-th iteration, loss: 0.5091165386175508, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.39885974435423e-06\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60133690e+00 5.34267854e+01 0.00000000e+00 2.88657986e-14\n",
      " 9.90614449e+01 9.94169567e+01 9.74605617e+01]\n",
      "32-th iteration, loss: 0.5091165386174433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.293965661698472e-06\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60133399 53.42678516 99.06144972 99.41695388 97.4605617 ]\n",
      "33-th iteration, loss: 0.5091165386173607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2842124417905107e-06\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60133112e+00 5.34267849e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90614520e+01 9.94169511e+01 9.74605617e+01]\n",
      "34-th iteration, loss: 0.5091165386172607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1873133630649984e-06\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132829e+00 5.34267847e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90614566e+01 9.94169484e+01 9.74605617e+01]\n",
      "35-th iteration, loss: 0.509116538617165, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.099121275574982e-06\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132548e+00 5.34267844e+01 0.00000000e+00 1.37667655e-14\n",
      " 9.90614609e+01 9.94169458e+01 9.74605617e+01]\n",
      "36-th iteration, loss: 0.5091165386170734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0184457622581513e-06\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132270e+00 5.34267840e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90614651e+01 9.94169432e+01 9.74605617e+01]\n",
      "37-th iteration, loss: 0.5091165386169855, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9442798551531775e-06\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60131995e+00 5.34267837e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90614692e+01 9.94169406e+01 9.74605617e+01]\n",
      "38-th iteration, loss: 0.5091165386169011, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.875770815395197e-06\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60131724 53.42678326 99.06147308 99.41693801 97.4605617 ]\n",
      "39-th iteration, loss: 0.5091165386168338, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8827055348821934e-06\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60131456 53.42678286 99.06147496 99.41693549 97.4605617 ]\n",
      "40-th iteration, loss: 0.5091165386167688, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8856641327600474e-06\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60131193e+00 5.34267825e+01 0.00000000e+00 3.41948692e-14\n",
      " 9.90614769e+01 9.94169330e+01 9.74605617e+01]\n",
      "41-th iteration, loss: 0.5091165386166921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8146985799779228e-06\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130934e+00 5.34267822e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.90614806e+01 9.94169306e+01 9.74605617e+01]\n",
      "42-th iteration, loss: 0.5091165386166188, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7493274005810112e-06\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60130679 53.42678188 99.06148426 99.41692826 97.4605617 ]\n",
      "43-th iteration, loss: 0.5091165386165601, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7544847176894599e-06\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130426e+00 5.34267815e+01 0.00000000e+00 2.88657986e-14\n",
      " 9.90614860e+01 9.94169259e+01 9.74605617e+01]\n",
      "44-th iteration, loss: 0.5091165386164915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6904511507852722e-06\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130178e+00 5.34267812e+01 0.00000000e+00 4.39648318e-14\n",
      " 9.90614895e+01 9.94169236e+01 9.74605617e+01]\n",
      "45-th iteration, loss: 0.5091165386164257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6312769010118217e-06\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60129933 53.42678091 99.06149291 99.4169214  97.4605617 ]\n",
      "46-th iteration, loss: 0.5091165386163727, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6376116545960274e-06\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129691e+00 5.34267806e+01 0.00000000e+00 1.37667655e-14\n",
      " 9.90614946e+01 9.94169192e+01 9.74605617e+01]\n",
      "47-th iteration, loss: 0.5091165386163112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5791250629422343e-06\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129454e+00 5.34267803e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90614978e+01 9.94169170e+01 9.74605617e+01]\n",
      "48-th iteration, loss: 0.5091165386162522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5249576303005705e-06\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129219e+00 5.34267800e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90615010e+01 9.94169149e+01 9.74605617e+01]\n",
      "49-th iteration, loss: 0.5091165386161955, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4745642843768111e-06\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60128987 53.42677967 99.06150405 99.41691278 97.4605617 ]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224633\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         358.41859902]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6306377752902012\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 236.03176033   0.         122.38683869]\n",
      "2-th iteration, loss: 0.6053204051370064, 13 gd steps\n",
      "insert gradient: -0.601544884542837\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.48858706  77.41183875 221.16730282  41.05484644 119.8891481\n",
      "   0.           2.49769059]\n",
      "3-th iteration, loss: 0.46612598501238417, 33 gd steps\n",
      "insert gradient: -0.18687363939791926\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         101.98170961 106.33312455   0.          79.74984341\n",
      "  57.62253391 100.73799881  58.08011102   2.49769059]\n",
      "4-th iteration, loss: 0.4565502198648592, 19 gd steps\n",
      "insert gradient: -0.2413655063829578\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         100.95705006  98.70806018  12.26706368  71.45861816\n",
      "  56.38804621 103.70246795  63.11943575   2.49769059]\n",
      "5-th iteration, loss: 0.34167297461453605, 86 gd steps\n",
      "insert gradient: -0.1281305633055248\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[2.66343621e+00 8.32691456e+01 1.27313316e+02 7.62433188e+01\n",
      " 9.80309389e+01 0.00000000e+00 3.64153152e-14 4.21058314e+01\n",
      " 6.59014028e+01 4.84630326e+01 2.49769059e+00]\n",
      "6-th iteration, loss: 0.3400102133021442, 49 gd steps\n",
      "insert gradient: -0.004279676441337002\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  1.42128433  84.88283555 124.41513058  78.430674    98.90392544\n",
      "  44.70258387  68.10902609  47.96219196   2.49769059]\n",
      "7-th iteration, loss: 0.33998323257926916, 13 gd steps\n",
      "insert gradient: -0.003836935921141341\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[3.82148275e-01 0.00000000e+00 1.35308431e-16 8.54757700e+01\n",
      " 1.23454902e+02 7.88245457e+01 9.87035499e+01 4.47194963e+01\n",
      " 6.83256745e+01 4.80029837e+01 2.49769059e+00]\n",
      "8-th iteration, loss: 0.3399751351597464, 43 gd steps\n",
      "insert gradient: -0.0001231978606845064\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.39368871  85.62953539 123.45427426  78.80163664  98.79900465\n",
      "  44.87706704  68.31930119  47.7226276    2.49769059]\n",
      "9-th iteration, loss: 0.33997510991237917, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.478298930568304e-06\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565624  85.642724   123.42742087  78.81367414  98.80671331\n",
      "  44.87255808  68.34189538  47.71588117   2.49769059]\n",
      "10-th iteration, loss: 0.3399751099112745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.05221141008726e-06\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856574   85.64273025 123.42742348  78.81367989  98.80671232\n",
      "  44.87255044  68.34189029  47.71587609   2.49769059]\n",
      "11-th iteration, loss: 0.33997510991034857, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.659745264240285e-06\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658460e-01 0.00000000e+00 2.63677968e-16 8.56427361e+01\n",
      " 1.23427426e+02 7.88136852e+01 9.88067114e+01 4.48725436e+01\n",
      " 6.83418856e+01 4.77158715e+01 2.49769059e+00]\n",
      "12-th iteration, loss: 0.3399751099094087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.147506235958417e-06\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[3.85659396e-01 5.35040525e-06 9.36028554e-07 0.00000000e+00\n",
      " 2.77932686e-22 8.56427414e+01 1.23427428e+02 7.88136901e+01\n",
      " 9.88067106e+01 4.48725373e+01 6.83418814e+01 4.77158674e+01\n",
      " 2.49769059e+00]\n",
      "13-th iteration, loss: 0.3399751099085181, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.554028582441382e-06\n",
      "13-th iteration, new layer inserted. now 15 layers\n",
      "[3.85660186e-01 1.01081629e-05 1.66829069e-06 4.76791393e-06\n",
      " 7.32262137e-07 0.00000000e+00 9.26442286e-23 8.56427462e+01\n",
      " 1.23427430e+02 7.88136945e+01 9.88067099e+01 4.48725318e+01\n",
      " 6.83418775e+01 4.77158637e+01 2.49769059e+00]\n",
      "14-th iteration, loss: 0.33997510990772883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.921027643032778e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[3.85660817e-01 1.42179756e-05 2.18984245e-06 8.89582820e-06\n",
      " 1.20208032e-06 4.13585958e-06 4.69818181e-07 8.56427503e+01\n",
      " 1.23427431e+02 7.88136985e+01 9.88067092e+01 4.48725269e+01\n",
      " 6.83418741e+01 4.77158604e+01 2.49769059e+00]\n",
      "15-th iteration, loss: 0.339975109907133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5159871352236242e-06\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661302e-01 1.77382622e-05 2.52064205e-06 1.24398753e-05\n",
      " 1.43635700e-06 7.69294973e-06 6.59219303e-07 8.56427539e+01\n",
      " 1.23427433e+02 7.88137021e+01 9.88067087e+01 4.48725227e+01\n",
      " 6.83418710e+01 4.77158575e+01 2.49769059e+00]\n",
      "16-th iteration, loss: 0.3399751099066804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1988539668523456e-06\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[3.85661663e-01 2.07579131e-05 2.68932641e-06 1.54868761e-05\n",
      " 1.47006427e-06 1.07555354e-05 6.09455328e-07 8.56427570e+01\n",
      " 1.23427434e+02 0.00000000e+00 3.73034936e-14 7.88137053e+01\n",
      " 9.88067081e+01 4.48725190e+01 6.83418682e+01 4.77158549e+01\n",
      " 2.49769059e+00]\n",
      "17-th iteration, loss: 0.3399751099062848, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.829621786232623e-06\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[3.85661909e-01 2.33063069e-05 2.71005929e-06 1.80644500e-05\n",
      " 1.32275904e-06 1.33490601e-05 3.45448698e-07 8.56427596e+01\n",
      " 1.23427434e+02 2.93213258e-06 8.05024692e-07 0.00000000e+00\n",
      " 9.26442286e-23 7.88137083e+01 9.88067077e+01 4.48725158e+01\n",
      " 6.83418658e+01 4.77158527e+01 2.49769059e+00]\n",
      "18-th iteration, loss: 0.3399751099059534, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.435837008479167e-06\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[3.85662053e-01 2.54340781e-05 2.60098863e-06 2.02216264e-05\n",
      " 1.01768269e-06 8.56427773e+01 1.23427435e+02 5.47752869e-06\n",
      " 1.32812106e-06 2.55824904e-06 5.23096365e-07 0.00000000e+00\n",
      " 9.26442286e-23 7.88137108e+01 9.88067072e+01 4.48725130e+01\n",
      " 6.83418636e+01 4.77158507e+01 2.49769059e+00]\n",
      "19-th iteration, loss: 0.33997510990570573, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0590183642946555e-06\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[3.85662116e-01 2.72289832e-05 2.38784062e-06 2.20447531e-05\n",
      " 5.85122874e-07 8.56427791e+01 1.23427435e+02 7.64496677e-06\n",
      " 1.59712641e-06 4.74689175e-06 7.51257000e-07 2.19699441e-06\n",
      " 2.28160636e-07 7.88137130e+01 9.88067068e+01 4.48725107e+01\n",
      " 6.83418617e+01 4.77158490e+01 2.49769059e+00]\n",
      "20-th iteration, loss: 0.3399751099055249, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7494307570503479e-06\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[3.85662118e-01 2.87726385e-05 2.09445958e-06 2.36143173e-05\n",
      " 5.25484765e-08 8.56427807e+01 1.23427436e+02 9.46708222e-06\n",
      " 1.65211730e-06 6.59450674e-06 7.30459552e-07 4.05660388e-06\n",
      " 1.72286223e-07 0.00000000e+00 2.31610572e-23 7.88137149e+01\n",
      " 9.88067064e+01 4.48725087e+01 6.83418600e+01 4.77158475e+01\n",
      " 2.49769059e+00]\n",
      "21-th iteration, loss: 0.339975109905379, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4495358920104744e-06\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[3.85662067e-01 3.00951267e-05 1.73142461e-06 8.56428070e+01\n",
      " 1.23427436e+02 1.09923756e-05 1.52680841e-06 8.14617764e-06\n",
      " 4.99863495e-07 0.00000000e+00 9.26442286e-23 7.88137236e+01\n",
      " 9.88067060e+01 4.48725070e+01 6.83418585e+01 4.77158463e+01\n",
      " 2.49769059e+00]\n",
      "22-th iteration, loss: 0.33997510990528906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2480211281593639e-06\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661980e-01 3.12708337e-05 1.31815183e-06 8.56428082e+01\n",
      " 1.23427436e+02 1.22942152e-05 1.26395372e-06 9.47239406e-06\n",
      " 1.06947832e-07 7.88137263e+01 9.88067057e+01 4.48725056e+01\n",
      " 6.83418572e+01 4.77158451e+01 2.49769059e+00]\n",
      "23-th iteration, loss: 0.3399751099052265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1048106772158143e-06\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661868e-01 3.23401325e-05 8.66436482e-07 8.56428093e+01\n",
      " 1.23427435e+02 1.34393222e-05 9.04147016e-07 0.00000000e+00\n",
      " 1.98523347e-22 7.88137381e+01 9.88067053e+01 4.48725044e+01\n",
      " 6.83418560e+01 4.77158442e+01 2.49769059e+00]\n",
      "24-th iteration, loss: 0.3399751099051757, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.790521654412725e-07\n",
      "24-th iteration, new layer inserted. now 13 layers\n",
      "[3.85661732e-01 3.33113865e-05 3.79725602e-07 8.56428102e+01\n",
      " 1.23427435e+02 1.44590259e-05 4.64706979e-07 7.88137402e+01\n",
      " 9.88067050e+01 4.48725033e+01 6.83418549e+01 4.77158433e+01\n",
      " 2.49769059e+00]\n",
      "25-th iteration, loss: 0.3399751099051389, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.947111755461576e-07\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566158  85.64284535 123.42743487  78.81375648  98.80670466\n",
      "  44.87250243  68.34185391  47.71584257   2.49769059]\n",
      "26-th iteration, loss: 0.3399751099051182, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.450319062288227e-07\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[3.85661413e-01 8.56428462e+01 1.23427435e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88137574e+01 9.88067044e+01 4.48725017e+01\n",
      " 6.83418530e+01 4.77158419e+01 2.49769059e+00]\n",
      "27-th iteration, loss: 0.3399751099050965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.761740169721811e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566123  85.642847   123.42743425  78.81375898  98.80670408\n",
      "  44.87250099  68.34185212  47.71584129   2.49769059]\n",
      "28-th iteration, loss: 0.33997510990508106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.35255318631416e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85661047e-01 8.56428477e+01 1.23427434e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88137597e+01 9.88067038e+01 4.48725004e+01\n",
      " 6.83418513e+01 4.77158407e+01 2.49769059e+00]\n",
      "29-th iteration, loss: 0.33997510990506463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.912575837882989e-07\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85660848e-01 0.00000000e+00 6.59194921e-17 8.56428485e+01\n",
      " 1.23427434e+02 7.88137612e+01 9.88067035e+01 4.48724999e+01\n",
      " 6.83418505e+01 4.77158403e+01 2.49769059e+00]\n",
      "30-th iteration, loss: 0.33997510990505037, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.408814090595308e-07\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566064  85.64284978 123.42743314  78.8137618   98.8067033\n",
      "  44.87249949  68.34184983  47.71583981   2.49769059]\n",
      "31-th iteration, loss: 0.33997510990503993, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.118317999486597e-07\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566042  85.6428504  123.42743273  78.81376242  98.80670306\n",
      "  44.87249913  68.34184915  47.71583942   2.49769059]\n",
      "32-th iteration, loss: 0.33997510990503055, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.851445957107139e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856602   85.642851   123.4274323   78.81376302  98.80670284\n",
      "  44.87249882  68.34184851  47.71583907   2.49769059]\n",
      "33-th iteration, loss: 0.339975109905022, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.606158290485923e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85659968e-01 0.00000000e+00 1.76941795e-16 8.56428516e+01\n",
      " 1.23427432e+02 7.88137636e+01 9.88067026e+01 4.48724986e+01\n",
      " 6.83418479e+01 4.77158388e+01 2.49769059e+00]\n",
      "34-th iteration, loss: 0.3399751099050125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.240215705109872e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85659731e-01 8.56428526e+01 1.23427431e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88137641e+01 9.88067024e+01 4.48724984e+01\n",
      " 6.83418473e+01 4.77158385e+01 2.49769059e+00]\n",
      "35-th iteration, loss: 0.33997510990500385, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.981981023572174e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565949  85.64285316 123.42743097  78.81376513  98.8067022\n",
      "  44.8724982   68.34184677  47.71583823   2.49769059]\n",
      "36-th iteration, loss: 0.3399751099049973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.801732333341555e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565924  85.64285364 123.4274305   78.81376561  98.80670201\n",
      "  44.87249808  68.34184624  47.71583801   2.49769059]\n",
      "37-th iteration, loss: 0.33997510990499114, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.63580125568065e-07\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658988e-01 0.00000000e+00 1.00613962e-16 8.56428541e+01\n",
      " 1.23427430e+02 7.88137661e+01 9.88067018e+01 4.48724980e+01\n",
      " 6.83418457e+01 4.77158378e+01 2.49769059e+00]\n",
      "38-th iteration, loss: 0.33997510990498414, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3621018246521483e-07\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565873  85.642855   123.42742954  78.8137665   98.80670163\n",
      "  44.87249793  68.34184526  47.71583764   2.49769059]\n",
      "39-th iteration, loss: 0.33997510990497865, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.226604200445897e-07\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658470e-01 0.00000000e+00 1.83880688e-16 8.56428554e+01\n",
      " 1.23427429e+02 7.88137669e+01 9.88067014e+01 4.48724979e+01\n",
      " 6.83418448e+01 4.77158375e+01 2.49769059e+00]\n",
      "40-th iteration, loss: 0.33997510990497243, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.991299020569328e-07\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565821  85.64285625 123.42742855  78.81376733  98.80670127\n",
      "  44.8724979   68.34184435  47.71583736   2.49769059]\n",
      "41-th iteration, loss: 0.3399751099049674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8810023860934307e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657938e-01 0.00000000e+00 5.20417043e-17 8.56428566e+01\n",
      " 1.23427428e+02 7.88137677e+01 9.88067011e+01 4.48724979e+01\n",
      " 6.83418439e+01 4.77158372e+01 2.49769059e+00]\n",
      "42-th iteration, loss: 0.33997510990496166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.715028868387082e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565767  85.64285739 123.42742755  78.81376809  98.80670093\n",
      "  44.87249796  68.3418435   47.71583715   2.49769059]\n",
      "43-th iteration, loss: 0.339975109904957, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6142497058909145e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657393e-01 8.56428577e+01 1.23427427e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88137685e+01 9.88067008e+01 4.48724980e+01\n",
      " 6.83418431e+01 4.77158371e+01 2.49769059e+00]\n",
      "44-th iteration, loss: 0.3399751099049517, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4687588871315934e-07\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657118e-01 0.00000000e+00 1.14491749e-16 8.56428581e+01\n",
      " 1.23427427e+02 7.88137692e+01 9.88067006e+01 4.48724981e+01\n",
      " 6.83418427e+01 4.77158370e+01 2.49769059e+00]\n",
      "45-th iteration, loss: 0.33997510990494667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3046961225744226e-07\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[3.85656839e-01 0.00000000e+00 1.87350135e-16 8.56428588e+01\n",
      " 1.23427426e+02 7.88137695e+01 9.88067004e+01 4.48724982e+01\n",
      " 6.83418423e+01 4.77158369e+01 2.49769059e+00]\n",
      "46-th iteration, loss: 0.33997510990494173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.197347338099567e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85656558e-01 8.56428594e+01 1.23427425e+02 0.00000000e+00\n",
      " 6.57252031e-14 7.88137698e+01 9.88067003e+01 4.48724983e+01\n",
      " 6.83418420e+01 4.77158369e+01 2.49769059e+00]\n",
      "47-th iteration, loss: 0.33997510990493696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.06644139655494e-07\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565628  85.64285972 123.42742495  78.81377044  98.80670013\n",
      "  44.8724984   68.34184161  47.71583686   2.49769059]\n",
      "48-th iteration, loss: 0.3399751099049328, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.018641331500698e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[3.85655991e-01 0.00000000e+00 2.63677968e-16 8.56428600e+01\n",
      " 1.23427424e+02 7.88137707e+01 9.88067000e+01 4.48724985e+01\n",
      " 6.83418413e+01 4.77158368e+01 2.49769059e+00]\n",
      "49-th iteration, loss: 0.33997510990492824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9014443731187036e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85655706e-01 8.56428606e+01 1.23427424e+02 0.00000000e+00\n",
      " 8.52651283e-14 7.88137710e+01 9.88066998e+01 4.48724987e+01\n",
      " 6.83418409e+01 4.77158368e+01 2.49769059e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5367213365606\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         382.53516579]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6324887568650404\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.91821287   0.         144.61695292]\n",
      "2-th iteration, loss: 0.6054314507011219, 13 gd steps\n",
      "insert gradient: -0.5912024952973778\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.11499841  77.01458289 222.53525413  40.59979633 118.05465545\n",
      "   0.          26.56229748]\n",
      "3-th iteration, loss: 0.46422438221632906, 53 gd steps\n",
      "insert gradient: -0.12778897775100037\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         107.33114882 103.69264498   0.          77.76948373\n",
      "  55.99704096 101.16078815  59.66216168  26.56229748]\n",
      "4-th iteration, loss: 0.3960516218135036, 45 gd steps\n",
      "insert gradient: -0.086173202733914\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[4.47306884e-01 8.06086017e+01 1.16281904e+02 5.22139810e+01\n",
      " 8.54730510e+01 0.00000000e+00 1.33226763e-14 1.98697011e+01\n",
      " 8.94420670e+01 6.82666463e+01 2.65622975e+01]\n",
      "5-th iteration, loss: 0.3570384548149841, 40 gd steps\n",
      "insert gradient: -0.25870729499796946\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[3.46600140e+00 0.00000000e+00 1.55431223e-15 7.24394783e+01\n",
      " 1.24679083e+02 7.60600853e+01 9.91818489e+01 3.39862477e+01\n",
      " 6.93803725e+01 5.63475370e+01 2.65622975e+01]\n",
      "6-th iteration, loss: 0.3427465394651828, 14 gd steps\n",
      "insert gradient: -0.09274209872129408\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[2.50740684e+00 0.00000000e+00 3.05311332e-16 8.05433149e+01\n",
      " 1.27038338e+02 7.67678287e+01 9.83862084e+01 4.25273753e+01\n",
      " 6.68475247e+01 5.22870887e+01 2.65622975e+01]\n",
      "7-th iteration, loss: 0.34003661491030635, 20 gd steps\n",
      "insert gradient: -0.006998928604309705\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[1.51187247e+00 8.47446097e+01 1.24716517e+02 7.80560328e+01\n",
      " 9.87638570e+01 0.00000000e+00 3.46389584e-14 4.47426155e+01\n",
      " 6.77189453e+01 4.78002585e+01 2.65622975e+01]\n",
      "8-th iteration, loss: 0.3399826139335977, 12 gd steps\n",
      "insert gradient: -0.006645889822940358\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.60457205  85.31129962 123.50710905  78.74536952  98.79325356\n",
      "  44.72793053  68.49087624  47.64867592  26.56229748]\n",
      "9-th iteration, loss: 0.3399751099112865, 76 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.585685420388472e-06\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688039e-01 0.00000000e+00 1.83880688e-16 8.56426574e+01\n",
      " 1.23427540e+02 7.88136547e+01 9.88066652e+01 4.48724863e+01\n",
      " 6.83417463e+01 4.77160011e+01 2.65622975e+01]\n",
      "10-th iteration, loss: 0.33997510991054064, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.127897264138434e-06\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688835e-01 5.28230224e-06 7.95938859e-07 8.56426627e+01\n",
      " 1.23427541e+02 7.88136591e+01 9.88066652e+01 4.48724829e+01\n",
      " 6.83417433e+01 4.77159958e+01 2.65622975e+01]\n",
      "11-th iteration, loss: 0.339975109909922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.71299644812659e-06\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[3.85689522e-01 1.01235714e-05 1.42513467e-06 0.00000000e+00\n",
      " 5.55865372e-22 8.56426675e+01 1.23427542e+02 7.88136633e+01\n",
      " 9.88066652e+01 4.48724800e+01 6.83417407e+01 4.77159909e+01\n",
      " 2.65622975e+01]\n",
      "12-th iteration, loss: 0.33997510990930246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2184122800441454e-06\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[3.85690089e-01 1.44769463e-05 1.88319592e-06 4.36883811e-06\n",
      " 4.58061252e-07 8.56426719e+01 1.23427543e+02 7.88136670e+01\n",
      " 9.88066653e+01 4.48724775e+01 6.83417384e+01 4.77159864e+01\n",
      " 2.65622975e+01]\n",
      "13-th iteration, loss: 0.3399751099088052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7810024546542e-06\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[3.85690542e-01 1.83628483e-05 2.17887940e-06 8.27517339e-06\n",
      " 7.06341294e-07 8.56426758e+01 1.23427544e+02 7.88136704e+01\n",
      " 9.88066654e+01 4.48724755e+01 6.83417363e+01 4.77159821e+01\n",
      " 2.65622975e+01]\n",
      "14-th iteration, loss: 0.3399751099084033, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.394001513913673e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[3.85690894e-01 2.18381646e-05 2.33152250e-06 1.17741313e-05\n",
      " 7.69195842e-07 0.00000000e+00 2.77932686e-22 8.56426793e+01\n",
      " 1.23427545e+02 7.88136736e+01 9.88066655e+01 4.48724739e+01\n",
      " 6.83417346e+01 4.77159782e+01 2.65622975e+01]\n",
      "15-th iteration, loss: 0.3399751099080252, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.971429332384482e-06\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691144e-01 2.48931916e-05 2.34481708e-06 1.48544562e-05\n",
      " 6.54736940e-07 8.56426855e+01 1.23427545e+02 7.88136764e+01\n",
      " 9.88066657e+01 4.48724727e+01 6.83417330e+01 4.77159746e+01\n",
      " 2.65622975e+01]\n",
      "16-th iteration, loss: 0.3399751099077657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.675854853358395e-06\n",
      "16-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691313e-01 2.76176770e-05 2.24315237e-06 1.76043837e-05\n",
      " 3.91896153e-07 8.56426882e+01 1.23427545e+02 7.88136791e+01\n",
      " 9.88066659e+01 4.48724718e+01 6.83417317e+01 4.77159712e+01\n",
      " 2.65622975e+01]\n",
      "17-th iteration, loss: 0.3399751099075498, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.429306277275514e-06\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691416e-01 3.00725376e-05 2.04637095e-06 2.00835832e-05\n",
      " 4.10098592e-09 8.56426907e+01 1.23427545e+02 7.88136815e+01\n",
      " 9.88066661e+01 4.48724712e+01 6.83417306e+01 4.77159680e+01\n",
      " 2.65622975e+01]\n",
      "18-th iteration, loss: 0.3399751099073699, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2578518272123705e-06\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691461e-01 3.22912951e-05 1.76579160e-06 8.56427153e+01\n",
      " 1.23427545e+02 7.88136838e+01 9.88066664e+01 4.48724708e+01\n",
      " 6.83417296e+01 4.77159650e+01 2.65622975e+01]\n",
      "19-th iteration, loss: 0.33997510990724056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1236012104821445e-06\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691464e-01 3.43329873e-05 1.41770591e-06 8.56427174e+01\n",
      " 1.23427545e+02 7.88136860e+01 9.88066666e+01 4.48724706e+01\n",
      " 6.83417288e+01 4.77159622e+01 2.65622975e+01]\n",
      "20-th iteration, loss: 0.3399751099071265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.000834088412041e-06\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691432e-01 3.62315099e-05 1.01321179e-06 8.56427193e+01\n",
      " 1.23427544e+02 0.00000000e+00 3.37507799e-14 7.88136880e+01\n",
      " 9.88066669e+01 4.48724705e+01 6.83417281e+01 4.77159596e+01\n",
      " 2.65622975e+01]\n",
      "21-th iteration, loss: 0.33997510990700525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.835096472164758e-06\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691364e-01 3.79770436e-05 5.52181251e-07 8.56427210e+01\n",
      " 1.23427544e+02 7.88136918e+01 9.88066672e+01 4.48724706e+01\n",
      " 6.83417274e+01 4.77159570e+01 2.65622975e+01]\n",
      "22-th iteration, loss: 0.3399751099069164, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.734778558598726e-06\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691270e-01 3.96136940e-05 4.62790132e-08 8.56427227e+01\n",
      " 1.23427544e+02 7.88136935e+01 9.88066675e+01 4.48724708e+01\n",
      " 6.83417269e+01 4.77159546e+01 2.65622975e+01]\n",
      "23-th iteration, loss: 0.33997510990683755, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6416345192010994e-06\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569115  85.64276535 123.4275432   78.81369519  98.80666777\n",
      "  44.87247112  68.34172649  47.71595234  26.56229748]\n",
      "24-th iteration, loss: 0.33997510990677815, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.570254114205771e-06\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691016e-01 8.56427668e+01 1.23427543e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.88136968e+01 9.88066681e+01 4.48724715e+01\n",
      " 6.83417261e+01 4.77159502e+01 2.65622975e+01]\n",
      "25-th iteration, loss: 0.3399751099067101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4608511032204291e-06\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690864e-01 8.56427682e+01 1.23427542e+02 0.00000000e+00\n",
      " 5.68434189e-14 7.88136998e+01 9.88066683e+01 4.48724720e+01\n",
      " 6.83417258e+01 4.77159481e+01 2.65622975e+01]\n",
      "26-th iteration, loss: 0.3399751099066483, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3607825471022833e-06\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856907   85.64276951 123.4275415   78.81370256  98.80666862\n",
      "  44.87247253  68.34172554  47.71594606  26.56229748]\n",
      "27-th iteration, loss: 0.3399751099066014, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3064004339490099e-06\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569052  85.64277079 123.42754086  78.81370388  98.80666889\n",
      "  44.87247313  68.34172534  47.71594414  26.56229748]\n",
      "28-th iteration, loss: 0.3399751099065571, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2555666840790078e-06\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690331e-01 8.56427720e+01 1.23427540e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88137051e+01 9.88066692e+01 4.48724738e+01\n",
      " 6.83417252e+01 4.77159423e+01 2.65622975e+01]\n",
      "29-th iteration, loss: 0.3399751099065069, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1775890558089533e-06\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690133e-01 0.00000000e+00 2.25514052e-16 8.56427732e+01\n",
      " 1.23427539e+02 7.88137075e+01 9.88066694e+01 4.48724744e+01\n",
      " 6.83417251e+01 4.77159405e+01 2.65622975e+01]\n",
      "30-th iteration, loss: 0.33997510990646035, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1196619013097735e-06\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568992  85.64277543 123.42753876  78.81370867  98.80666966\n",
      "  44.87247513  68.34172497  47.71593878  26.56229748]\n",
      "31-th iteration, loss: 0.3399751099064232, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0804503873067462e-06\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689701e-01 8.56427765e+01 1.23427538e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88137098e+01 9.88066699e+01 4.48724759e+01\n",
      " 6.83417249e+01 4.77159371e+01 2.65622975e+01]\n",
      "32-th iteration, loss: 0.3399751099063814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0398718714998658e-06\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568947  85.64277754 123.42753723  78.81371182  98.80667015\n",
      "  44.87247661  68.3417249   47.71593551  26.56229748]\n",
      "33-th iteration, loss: 0.33997510990634766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0131305147304315e-06\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689237e-01 0.00000000e+00 1.35308431e-16 8.56427786e+01\n",
      " 1.23427536e+02 7.88137128e+01 9.88066704e+01 4.48724774e+01\n",
      " 6.83417249e+01 4.77159340e+01 2.65622975e+01]\n",
      "34-th iteration, loss: 0.3399751099063098, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.62028429376246e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688993e-01 0.00000000e+00 1.83880688e-16 8.56427805e+01\n",
      " 1.23427536e+02 7.88137137e+01 9.88066706e+01 4.48724782e+01\n",
      " 6.83417249e+01 4.77159325e+01 2.65622975e+01]\n",
      "35-th iteration, loss: 0.3399751099062739, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.149751211891346e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688738e-01 0.00000000e+00 1.83880688e-16 8.56427823e+01\n",
      " 1.23427535e+02 7.88137146e+01 9.88066708e+01 4.48724790e+01\n",
      " 6.83417250e+01 4.77159310e+01 2.65622975e+01]\n",
      "36-th iteration, loss: 0.33997510990623964, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.716134693416218e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568847  85.64278406 123.42753393  78.8137155   98.80667104\n",
      "  44.87247979  68.34172507  47.71592959  26.56229748]\n",
      "37-th iteration, loss: 0.339975109906211, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.54102741758964e-07\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688204e-01 0.00000000e+00 1.76941795e-16 8.56427849e+01\n",
      " 1.23427533e+02 7.88137163e+01 9.88066713e+01 4.48724806e+01\n",
      " 6.83417252e+01 4.77159282e+01 2.65622975e+01]\n",
      "38-th iteration, loss: 0.33997510990617935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.358867334999364e-07\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568793  85.64278653 123.42753218  78.81371716  98.80667145\n",
      "  44.87248143  68.34172528  47.71592688  26.56229748]\n",
      "39-th iteration, loss: 0.3399751099061526, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.299469323035286e-07\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568765  85.64278732 123.42753129  78.81371795  98.80667165\n",
      "  44.87248226  68.34172541  47.71592558  26.56229748]\n",
      "40-th iteration, loss: 0.3399751099061265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.231320367565178e-07\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568737  85.6427881  123.4275304   78.81371872  98.80667184\n",
      "  44.87248308  68.34172555  47.71592431  26.56229748]\n",
      "41-th iteration, loss: 0.3399751099061013, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.155521919166896e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85687081e-01 8.56427889e+01 1.23427529e+02 7.88137195e+01\n",
      " 9.88066720e+01 0.00000000e+00 2.93098879e-14 4.48724839e+01\n",
      " 6.83417257e+01 4.77159231e+01 2.65622975e+01]\n",
      "42-th iteration, loss: 0.3399751099060729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.714050938375107e-07\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[3.85686795e-01 8.56427896e+01 1.23427529e+02 7.88137202e+01\n",
      " 9.88066722e+01 7.96324494e-07 1.74916625e-07 4.48724847e+01\n",
      " 6.83417258e+01 4.77159219e+01 2.65622975e+01]\n",
      "43-th iteration, loss: 0.3399751099060458, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.67245522401178e-07\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[3.85686509e-01 0.00000000e+00 1.17961196e-16 8.56427904e+01\n",
      " 1.23427528e+02 7.88137209e+01 9.88066724e+01 1.53840161e-06\n",
      " 3.10246438e-07 4.48724854e+01 6.83417260e+01 4.77159207e+01\n",
      " 2.65622975e+01]\n",
      "44-th iteration, loss: 0.3399751099060167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.424525045460171e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[3.85686221e-01 0.00000000e+00 9.02056208e-17 8.56427918e+01\n",
      " 1.23427527e+02 7.88137216e+01 9.88066725e+01 2.23130532e-06\n",
      " 4.06331869e-07 4.48724861e+01 6.83417261e+01 4.77159195e+01\n",
      " 2.65622975e+01]\n",
      "45-th iteration, loss: 0.33997510990598884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.186234881709821e-07\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685929e-01 8.56427932e+01 1.23427526e+02 7.88137223e+01\n",
      " 9.88066726e+01 2.89091196e-06 4.69785484e-07 4.48724868e+01\n",
      " 6.83417262e+01 4.77159183e+01 2.65622975e+01]\n",
      "46-th iteration, loss: 0.3399751099059649, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.143716085953062e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685636e-01 8.56427939e+01 1.23427525e+02 7.88137230e+01\n",
      " 9.88066727e+01 3.52235335e-06 5.05712791e-07 4.48724875e+01\n",
      " 6.83417264e+01 4.77159171e+01 2.65622975e+01]\n",
      "47-th iteration, loss: 0.33997510990594176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.09600836479954e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685343e-01 8.56427946e+01 1.23427524e+02 7.88137237e+01\n",
      " 9.88066729e+01 4.11941048e-06 5.12777101e-07 4.48724881e+01\n",
      " 6.83417265e+01 4.77159160e+01 2.65622975e+01]\n",
      "48-th iteration, loss: 0.3399751099059194, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.044306737139008e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685050e-01 8.56427953e+01 1.23427523e+02 7.88137243e+01\n",
      " 9.88066729e+01 4.68635911e-06 4.93186729e-07 4.48724886e+01\n",
      " 6.83417266e+01 4.77159149e+01 2.65622975e+01]\n",
      "49-th iteration, loss: 0.33997510990589774, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.98961298616846e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85684758e-01 8.56427960e+01 1.23427522e+02 7.88137250e+01\n",
      " 9.88066730e+01 5.22693123e-06 4.48889545e-07 4.48724892e+01\n",
      " 6.83417268e+01 4.77159138e+01 2.65622975e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224646\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         408.6698553 ]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6306424795698111\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 234.23759999   0.         174.43225531]\n",
      "2-th iteration, loss: 0.6046347771589909, 13 gd steps\n",
      "insert gradient: -0.6047968613702339\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.55936244  77.6127003  219.71573098  41.6177556  121.03462613\n",
      "   0.          53.39762918]\n",
      "3-th iteration, loss: 0.4641745801326174, 101 gd steps\n",
      "insert gradient: -0.11599961014627878\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         107.59132309 104.57611445   0.          78.43208583\n",
      "  55.82003969 101.3862897   59.81730964  53.39762918]\n",
      "4-th iteration, loss: 0.42960286513562307, 30 gd steps\n",
      "insert gradient: -0.1663587609723339\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 8.26300994e+01 1.11746698e+02 6.07924261e+01\n",
      " 0.00000000e+00 4.26325641e-14 2.99583558e+01 4.14721431e+01\n",
      " 1.06617681e+02 6.91121606e+01 5.33976292e+01]\n",
      "5-th iteration, loss: 0.39788442796506857, 47 gd steps\n",
      "insert gradient: -0.028390431432818296\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[1.45469950e-01 8.20284868e+01 1.10972422e+02 5.46640035e+01\n",
      " 0.00000000e+00 2.48689958e-14 6.36965116e+01 2.93694521e+01\n",
      " 9.63943499e+01 6.88460208e+01 5.33976292e+01]\n",
      "6-th iteration, loss: 0.39367329259726036, 37 gd steps\n",
      "insert gradient: -0.01492457965988628\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.54261255  80.02779027 115.07527118  52.68106507  77.48170054\n",
      "  24.45377573  90.40837291  70.5584737   53.39762918]\n",
      "7-th iteration, loss: 0.37765360163966555, 29 gd steps\n",
      "insert gradient: -0.14914068713594344\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[4.32573256e+00 0.00000000e+00 2.77555756e-15 6.21256752e+01\n",
      " 1.39508118e+02 6.21142348e+01 8.91582993e+01 2.64054620e+01\n",
      " 7.50954096e+01 5.20885748e+01 5.33976292e+01]\n",
      "8-th iteration, loss: 0.35211207346759993, 17 gd steps\n",
      "insert gradient: -0.1748016845167724\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  3.83059645  74.59924128 127.53205084  71.36325961  93.57710219\n",
      "  34.47682007  73.15053961  49.84728529  53.39762918]\n",
      "9-th iteration, loss: 0.3400029902668376, 44 gd steps\n",
      "insert gradient: -0.009237863594801977\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.8819494   85.10048644 124.08526566  78.40492723  98.84088284\n",
      "  45.00232383  68.18741687  47.81760837  53.39762918]\n",
      "10-th iteration, loss: 0.33997595640438105, 32 gd steps\n",
      "insert gradient: -0.0018821012544554014\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.4023804   85.58151547 123.45637032  78.77704247  98.81270191\n",
      "  44.85396296  68.36016903  47.77429015  53.39762918]\n",
      "11-th iteration, loss: 0.3399751710014429, 13 gd steps\n",
      "insert gradient: -0.0006914426293477419\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[4.04253284e-01 8.56249553e+01 1.23455047e+02 7.87981072e+01\n",
      " 9.88072339e+01 0.00000000e+00 3.46389584e-14 4.48636952e+01\n",
      " 6.83303418e+01 4.77050215e+01 5.33976292e+01]\n",
      "12-th iteration, loss: 0.3399751195863385, 16 gd steps\n",
      "insert gradient: -0.0002115773416453164\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.39966262  85.6326284  123.44358721  78.80712139  98.80702959\n",
      "  44.8683359   68.33761796  47.71541097  53.39762918]\n",
      "13-th iteration, loss: 0.3399751099049552, 66 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1615063783296065e-07\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[3.85279735e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 6.57252031e-14 7.88138014e+01 9.88066723e+01 4.48725295e+01\n",
      " 6.83417790e+01 4.77158560e+01 5.33976292e+01]\n",
      "14-th iteration, loss: 0.3399751099049517, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0521604732217834e-07\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528042  85.64299069 123.42732493  78.8138018   98.80667241\n",
      "  44.87252964  68.34177902  47.71585615  53.39762918]\n",
      "15-th iteration, loss: 0.33997510990494856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0058697988684302e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[3.85281098e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138020e+01 9.88066725e+01 4.48725298e+01\n",
      " 6.83417791e+01 4.77158563e+01 5.33976292e+01]\n",
      "16-th iteration, loss: 0.3399751099049452, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.905816583876583e-07\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[3.85281775e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138024e+01 9.88066725e+01 4.48725299e+01\n",
      " 6.83417791e+01 4.77158564e+01 5.33976292e+01]\n",
      "17-th iteration, loss: 0.339975109904942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8121060930840753e-07\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[3.85282449e-01 8.56429906e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138028e+01 9.88066726e+01 4.48725300e+01\n",
      " 6.83417791e+01 4.77158565e+01 5.33976292e+01]\n",
      "18-th iteration, loss: 0.33997510990493884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.724293049069663e-07\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528312  85.64299061 123.42732468  78.8138031   98.80667262\n",
      "  44.87253005  68.34177915  47.7158566   53.39762918]\n",
      "19-th iteration, loss: 0.33997510990493585, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6893305140221473e-07\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528379  85.64299058 123.42732461  78.81380327  98.80667267\n",
      "  44.87253014  68.34177918  47.7158567   53.39762918]\n",
      "20-th iteration, loss: 0.3399751099049329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.655090475057553e-07\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[3.85284455e-01 8.56429905e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.79616347e-14 7.88138034e+01 9.88066727e+01 4.48725302e+01\n",
      " 6.83417792e+01 4.77158568e+01 5.33976292e+01]\n",
      "21-th iteration, loss: 0.33997510990492985, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.576060419535387e-07\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528512  85.64299051 123.42732446  78.81380375  98.80667275\n",
      "  44.87253031  68.34177924  47.7158569   53.39762918]\n",
      "22-th iteration, loss: 0.339975109904927, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5454250579840903e-07\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528578  85.64299047 123.42732438  78.81380391  98.80667279\n",
      "  44.87253039  68.34177926  47.71585699  53.39762918]\n",
      "23-th iteration, loss: 0.3399751099049242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.515432737811418e-07\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528644  85.64299043 123.42732429  78.81380406  98.80667282\n",
      "  44.87253047  68.34177929  47.71585708  53.39762918]\n",
      "24-th iteration, loss: 0.3399751099049214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4861005561573754e-07\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[3.85287096e-01 8.56429904e+01 1.23427324e+02 0.00000000e+00\n",
      " 9.23705556e-14 7.88138042e+01 9.88066729e+01 4.48725305e+01\n",
      " 6.83417793e+01 4.77158572e+01 5.33976292e+01]\n",
      "25-th iteration, loss: 0.33997510990491847, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4164022439351884e-07\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85287750e-01 8.56429903e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138045e+01 9.88066729e+01 4.48725306e+01\n",
      " 6.83417793e+01 4.77158572e+01 5.33976292e+01]\n",
      "26-th iteration, loss: 0.33997510990491564, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.351143953397422e-07\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[3.85288402e-01 8.56429903e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88138048e+01 9.88066729e+01 4.48725307e+01\n",
      " 6.83417794e+01 4.77158573e+01 5.33976292e+01]\n",
      "27-th iteration, loss: 0.3399751099049128, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.290009072447194e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[3.85289051e-01 8.56429902e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.26325641e-14 7.88138050e+01 9.88066729e+01 4.48725307e+01\n",
      " 6.83417794e+01 4.77158574e+01 5.33976292e+01]\n",
      "28-th iteration, loss: 0.3399751099049101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2327058399599382e-07\n",
      "28-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3852897   85.64299018 123.42732384  78.81380528  98.80667296\n",
      "  44.87253081  68.3417794   47.71585748  53.39762918]\n",
      "29-th iteration, loss: 0.33997510990490737, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2132174263158974e-07\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85290342e-01 8.56429901e+01 1.23427324e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.88138054e+01 9.88066730e+01 4.48725309e+01\n",
      " 6.83417794e+01 4.77158575e+01 5.33976292e+01]\n",
      "30-th iteration, loss: 0.3399751099049047, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1602870229608971e-07\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529098  85.64299006 123.42732365  78.81380563  98.806673\n",
      "  44.87253093  68.34177944  47.71585762  53.39762918]\n",
      "31-th iteration, loss: 0.33997510990490204, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1429663907879238e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85291624e-01 8.56429900e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138057e+01 9.88066730e+01 4.48725310e+01\n",
      " 6.83417795e+01 4.77158577e+01 5.33976292e+01]\n",
      "32-th iteration, loss: 0.3399751099048994, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0940278327810435e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529226  85.64298993 123.42732345  78.81380597  98.80667304\n",
      "  44.87253104  68.34177948  47.71585775  53.39762918]\n",
      "33-th iteration, loss: 0.3399751099048968, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0786803485281753e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85292898e-01 8.56429899e+01 1.23427323e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138061e+01 9.88066731e+01 4.48725311e+01\n",
      " 6.83417795e+01 4.77158578e+01 5.33976292e+01]\n",
      "34-th iteration, loss: 0.3399751099048942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0333850837161342e-07\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529353  85.6429898  123.42732324  78.81380628  98.80667306\n",
      "  44.87253114  68.34177951  47.71585788  53.39762918]\n",
      "35-th iteration, loss: 0.33997510990489166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0198322928288804e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85294163e-01 8.56429897e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138064e+01 9.88066731e+01 4.48725312e+01\n",
      " 6.83417795e+01 4.77158579e+01 5.33976292e+01]\n",
      "36-th iteration, loss: 0.3399751099048891, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.778617213402768e-08\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[3.85294792e-01 8.56429897e+01 1.23427323e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.88138066e+01 9.88066731e+01 4.48725312e+01\n",
      " 6.83417795e+01 4.77158580e+01 5.33976292e+01]\n",
      "37-th iteration, loss: 0.33997510990488655, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.38503713245958e-08\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529542  85.64298958 123.42732292  78.81380677  98.80667309\n",
      "  44.87253128  68.34177956  47.71585805  53.39762918]\n",
      "38-th iteration, loss: 0.33997510990488405, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.279530106498568e-08\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[3.85296045e-01 8.56429895e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138069e+01 9.88066731e+01 4.48725313e+01\n",
      " 6.83417796e+01 4.77158581e+01 5.33976292e+01]\n",
      "39-th iteration, loss: 0.33997510990488156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.913504028397377e-08\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[3.85296668e-01 8.56429894e+01 1.23427323e+02 0.00000000e+00\n",
      " 7.99360578e-14 7.88138070e+01 9.88066731e+01 4.48725314e+01\n",
      " 6.83417796e+01 4.77158582e+01 5.33976292e+01]\n",
      "40-th iteration, loss: 0.33997510990487906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.570077275871551e-08\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[3.85297289e-01 8.56429894e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138072e+01 9.88066731e+01 4.48725314e+01\n",
      " 6.83417796e+01 4.77158582e+01 5.33976292e+01]\n",
      "41-th iteration, loss: 0.3399751099048766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.247655434307828e-08\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529791  85.64298928 123.42732247  78.81380739  98.80667312\n",
      "  44.87253145  68.34177962  47.71585825  53.39762918]\n",
      "42-th iteration, loss: 0.3399751099048742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.17802011750019e-08\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529853  85.6429892  123.42732236  78.81380747  98.80667312\n",
      "  44.87253149  68.34177963  47.7158583   53.39762918]\n",
      "43-th iteration, loss: 0.3399751099048718, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.107635479108649e-08\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[3.85299141e-01 8.56429891e+01 1.23427322e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138075e+01 9.88066731e+01 4.48725315e+01\n",
      " 6.83417796e+01 4.77158583e+01 5.33976292e+01]\n",
      "44-th iteration, loss: 0.33997510990486934, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.807262289162491e-08\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529975  85.64298904 123.42732213  78.81380771  98.80667312\n",
      "  44.87253156  68.34177965  47.71585839  53.39762918]\n",
      "45-th iteration, loss: 0.339975109904867, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.746775780742047e-08\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530037  85.64298895 123.42732202  78.81380778  98.80667312\n",
      "  44.8725316   68.34177967  47.71585843  53.39762918]\n",
      "46-th iteration, loss: 0.3399751099048646, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.685610549632e-08\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530098  85.64298887 123.42732191  78.81380786  98.80667312\n",
      "  44.87253163  68.34177968  47.71585847  53.39762918]\n",
      "47-th iteration, loss: 0.3399751099048623, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.624000848287457e-08\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[3.85301583e-01 8.56429888e+01 1.23427322e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88138079e+01 9.88066731e+01 4.48725317e+01\n",
      " 6.83417797e+01 4.77158585e+01 5.33976292e+01]\n",
      "48-th iteration, loss: 0.33997510990485996, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.34563925894494e-08\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530219  85.6429887  123.42732168  78.81380808  98.80667312\n",
      "  44.8725317   68.3417797   47.71585855  53.39762918]\n",
      "49-th iteration, loss: 0.33997510990485763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.293739394443388e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530279  85.64298862 123.42732156  78.81380816  98.80667312\n",
      "  44.87253173  68.34177971  47.71585859  53.39762918]\n",
      "0-th iteration, loss: 0.9545036783633585, 7 gd steps\n",
      "insert gradient: -2.536822723274191\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.09598192   0.         433.29095276]\n",
      "1-th iteration, loss: 0.7485404418159318, 11 gd steps\n",
      "insert gradient: -0.6305151656698674\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.67444096  62.33745299 237.78162041   0.         195.50933234]\n",
      "2-th iteration, loss: 0.6057575328448086, 13 gd steps\n",
      "insert gradient: -0.5951526425486448\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.27757689  77.13458233 222.50187546  40.5340884  119.69959123\n",
      "   0.          75.80974111]\n",
      "3-th iteration, loss: 0.46430423849049596, 48 gd steps\n",
      "insert gradient: -0.15281013657602754\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         106.25721829 104.87726602   0.          78.65794951\n",
      "  55.76883993 101.6580866   59.85799745  75.80974111]\n",
      "4-th iteration, loss: 0.4116732561472047, 32 gd steps\n",
      "insert gradient: -0.22400001064941757\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          83.29301204 104.29835637  51.68607023  54.21545136\n",
      "  40.38776126  94.74159533  63.68652576  75.80974111]\n",
      "5-th iteration, loss: 0.35220868775281466, 51 gd steps\n",
      "insert gradient: -0.23585599632111348\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[1.05812659e+00 0.00000000e+00 6.24500451e-16 7.20396528e+01\n",
      " 1.31672766e+02 7.78711810e+01 9.88292216e+01 3.82086133e+01\n",
      " 6.80745709e+01 5.40575652e+01 7.58097411e+01]\n",
      "6-th iteration, loss: 0.3402160645326017, 20 gd steps\n",
      "insert gradient: -0.03318410160211695\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[1.44306237e+00 8.48449328e+01 1.25766874e+02 7.72866400e+01\n",
      " 9.86402323e+01 0.00000000e+00 2.57571742e-14 4.44894919e+01\n",
      " 6.74055565e+01 4.73379230e+01 7.58097411e+01]\n",
      "7-th iteration, loss: 0.33997584723409785, 51 gd steps\n",
      "insert gradient: -0.0008101660701794044\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[2.69405326e-01 8.57052745e+01 0.00000000e+00 2.57571742e-14\n",
      " 1.23260684e+02 7.88744611e+01 9.88610041e+01 4.48778666e+01\n",
      " 6.82841603e+01 4.77514991e+01 7.58097411e+01]\n",
      "8-th iteration, loss: 0.33997517409833095, 34 gd steps\n",
      "insert gradient: -4.2438665068458045e-05\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[3.20603925e-01 8.56804177e+01 1.23401352e+02 7.88233475e+01\n",
      " 9.88052025e+01 4.48823924e+01 0.00000000e+00 1.11022302e-14\n",
      " 6.83337990e+01 4.77183799e+01 7.58097411e+01]\n",
      "9-th iteration, loss: 0.3399751105950451, 43 gd steps\n",
      "insert gradient: -1.630004361220139e-05\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[3.86615300e-01 8.56444427e+01 1.23425178e+02 7.88147556e+01\n",
      " 9.88087742e+01 4.48713793e+01 6.83467222e+01 0.00000000e+00\n",
      " 8.88178420e-16 4.77138526e+01 7.58097411e+01]\n",
      "10-th iteration, loss: 0.33997510992359103, 25 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.392202179138017e-06\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572875  85.64327062 123.42660141  78.81422375  98.80667497\n",
      "  44.87250315  68.34239977  47.71570064  75.80974111]\n",
      "11-th iteration, loss: 0.33997510992335117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4173491076947585e-06\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[3.85727557e-01 8.56432676e+01 0.00000000e+00 1.68753900e-14\n",
      " 1.23426604e+02 7.88142208e+01 9.88066747e+01 4.48725020e+01\n",
      " 6.83423960e+01 4.77157009e+01 7.58097411e+01]\n",
      "12-th iteration, loss: 0.3399751099230829, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.405593778336409e-06\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[3.85726387e-01 8.56432647e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426609e+02 7.88142180e+01 9.88066744e+01 4.48725010e+01\n",
      " 6.83423923e+01 4.77157013e+01 7.58097411e+01]\n",
      "13-th iteration, loss: 0.3399751099228196, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.394099130685104e-06\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[3.85725225e-01 8.56432618e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426614e+02 7.88142152e+01 9.88066742e+01 4.48725000e+01\n",
      " 6.83423886e+01 4.77157016e+01 7.58097411e+01]\n",
      "14-th iteration, loss: 0.3399751099225611, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3828095086768742e-06\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[3.85724070e-01 8.56432589e+01 0.00000000e+00 6.03961325e-14\n",
      " 1.23426619e+02 7.88142123e+01 9.88066739e+01 4.48724991e+01\n",
      " 6.83423850e+01 4.77157020e+01 7.58097411e+01]\n",
      "15-th iteration, loss: 0.3399751099223072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3716763809144864e-06\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[3.85722923e-01 8.56432561e+01 0.00000000e+00 2.22044605e-14\n",
      " 1.23426623e+02 7.88142095e+01 9.88066737e+01 4.48724983e+01\n",
      " 6.83423815e+01 4.77157025e+01 7.58097411e+01]\n",
      "16-th iteration, loss: 0.33997510992205754, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3606575845949157e-06\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572178  85.64325323 123.42662828  78.81420676  98.80667355\n",
      "  44.87249749  68.34237794  47.71570295  75.80974111]\n",
      "17-th iteration, loss: 0.33997510992184476, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3844586544641448e-06\n",
      "17-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572065  85.64325038 123.42663068  78.81420397  98.80667337\n",
      "  44.87249676  68.34237446  47.71570345  75.80974111]\n",
      "18-th iteration, loss: 0.33997510992163643, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4068350479358843e-06\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571952  85.64324759 123.4266331   78.81420122  98.80667322\n",
      "  44.87249607  68.342371    47.71570397  75.80974111]\n",
      "19-th iteration, loss: 0.33997510992143226, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.427836328028074e-06\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[3.85718412e-01 8.56432449e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426636e+02 7.88141985e+01 9.88066731e+01 4.48724954e+01\n",
      " 6.83423676e+01 4.77157045e+01 7.58097411e+01]\n",
      "20-th iteration, loss: 0.3399751099211976, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.411999303451084e-06\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[3.85717324e-01 8.56432422e+01 0.00000000e+00 1.15463195e-14\n",
      " 1.23426640e+02 7.88141959e+01 9.88066730e+01 4.48724948e+01\n",
      " 6.83423642e+01 4.77157051e+01 7.58097411e+01]\n",
      "21-th iteration, loss: 0.3399751099209666, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3964431960537777e-06\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571624  85.64323957 123.4266454   78.81419328  98.8066729\n",
      "  44.87249424  68.3423608   47.71570564  75.80974111]\n",
      "22-th iteration, loss: 0.3399751099207729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.416177311621318e-06\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571516  85.64323692 123.42664783  78.81419066  98.80667281\n",
      "  44.87249371  68.34235747  47.71570624  75.80974111]\n",
      "23-th iteration, loss: 0.3399751099205828, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.434586979419704e-06\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[3.85714096e-01 8.56432343e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426650e+02 7.88141881e+01 9.88066727e+01 4.48724932e+01\n",
      " 6.83423542e+01 4.77157068e+01 7.58097411e+01]\n",
      "24-th iteration, loss: 0.33997510992036134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4162497740306433e-06\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571305  85.6432318  123.42665522  78.81418557  98.8066727\n",
      "  44.87249275  68.34235088  47.71570747  75.80974111]\n",
      "25-th iteration, loss: 0.33997510992017754, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.433489723878338e-06\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85712003e-01 8.56432293e+01 0.00000000e+00 3.46389584e-14\n",
      " 1.23426658e+02 7.88141830e+01 9.88066727e+01 4.48724923e+01\n",
      " 6.83423476e+01 4.77157081e+01 7.58097411e+01]\n",
      "26-th iteration, loss: 0.3399751099199622, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.414086692768499e-06\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571098  85.64322682 123.42666261  78.81418058  98.80667264\n",
      "  44.87249191  68.3423444   47.71570875  75.80974111]\n",
      "27-th iteration, loss: 0.3399751099197843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4302130531728945e-06\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[3.85709950e-01 8.56432243e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23426665e+02 7.88141781e+01 9.88066726e+01 4.48724915e+01\n",
      " 6.83423412e+01 4.77157094e+01 7.58097411e+01]\n",
      "28-th iteration, loss: 0.33997510991957475, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.409829626916722e-06\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85708942e-01 8.56432220e+01 0.00000000e+00 4.70734562e-14\n",
      " 1.23426670e+02 7.88141757e+01 9.88066726e+01 4.48724912e+01\n",
      " 6.83423380e+01 4.77157101e+01 7.58097411e+01]\n",
      "29-th iteration, loss: 0.3399751099193683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3898944726769827e-06\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570794  85.64321957 123.42667488  78.81417329  98.80667262\n",
      "  44.87249085  68.34233486  47.71571074  75.80974111]\n",
      "30-th iteration, loss: 0.3399751099191983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.40509140145545e-06\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[3.85706938e-01 8.56432172e+01 0.00000000e+00 2.39808173e-14\n",
      " 1.23426677e+02 7.88141709e+01 9.88066726e+01 4.48724905e+01\n",
      " 6.83423317e+01 4.77157114e+01 7.58097411e+01]\n",
      "31-th iteration, loss: 0.3399751099189973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3842057124902796e-06\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85705955e-01 8.56432149e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426682e+02 7.88141685e+01 9.88066727e+01 4.48724903e+01\n",
      " 6.83423286e+01 4.77157121e+01 7.58097411e+01]\n",
      "32-th iteration, loss: 0.33997510991879915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3637760996028113e-06\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[3.85704978e-01 8.56432126e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426687e+02 7.88141662e+01 9.88066727e+01 4.48724900e+01\n",
      " 6.83423256e+01 4.77157128e+01 7.58097411e+01]\n",
      "33-th iteration, loss: 0.33997510991860386, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.343768363204252e-06\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85704006e-01 8.56432103e+01 0.00000000e+00 4.70734562e-14\n",
      " 1.23426692e+02 7.88141638e+01 9.88066727e+01 4.48724898e+01\n",
      " 6.83423225e+01 4.77157135e+01 7.58097411e+01]\n",
      "34-th iteration, loss: 0.33997510991841134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.324151545093056e-06\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85703040e-01 8.56432080e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23426697e+02 7.88141615e+01 9.88066727e+01 4.48724895e+01\n",
      " 6.83423195e+01 4.77157142e+01 7.58097411e+01]\n",
      "35-th iteration, loss: 0.33997510991822166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3048975916686194e-06\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85702080e-01 8.56432057e+01 0.00000000e+00 1.68753900e-14\n",
      " 1.23426701e+02 7.88141592e+01 9.88066728e+01 4.48724893e+01\n",
      " 6.83423165e+01 4.77157149e+01 7.58097411e+01]\n",
      "36-th iteration, loss: 0.3399751099180346, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2859810965273435e-06\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570113  85.64320349 123.42670596  78.81415688  98.80667282\n",
      "  44.87248915  68.34231348  47.71571565  75.80974111]\n",
      "37-th iteration, loss: 0.3399751099178809, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.300557981356601e-06\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570017  85.64320124 123.42670827  78.81415457  98.80667286\n",
      "  44.87248898  68.34231052  47.71571637  75.80974111]\n",
      "38-th iteration, loss: 0.3399751099177296, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3139238091585636e-06\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[3.85699232e-01 8.56431990e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23426711e+02 7.88141523e+01 9.88066729e+01 4.48724888e+01\n",
      " 6.83423076e+01 4.77157171e+01 7.58097411e+01]\n",
      "39-th iteration, loss: 0.33997510991754937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2926585147736343e-06\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569831  85.64319689 123.42671529  78.81415008  98.80667296\n",
      "  44.87248866  68.34230464  47.7157178   75.80974111]\n",
      "40-th iteration, loss: 0.3399751099174026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3050287486013405e-06\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569739  85.64319474 123.4267176   78.81414785  98.80667302\n",
      "  44.87248853  68.34230173  47.71571852  75.80974111]\n",
      "41-th iteration, loss: 0.33997510991725804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.31629468680708e-06\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85696474e-01 8.56431926e+01 0.00000000e+00 3.46389584e-14\n",
      " 1.23426720e+02 7.88141457e+01 9.88066731e+01 4.48724884e+01\n",
      " 6.83422988e+01 4.77157192e+01 7.58097411e+01]\n",
      "42-th iteration, loss: 0.3399751099170842, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.293083548089638e-06\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[3.85695579e-01 8.56431906e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23426725e+02 7.88141435e+01 9.88066732e+01 4.48724883e+01\n",
      " 6.83422959e+01 4.77157200e+01 7.58097411e+01]\n",
      "43-th iteration, loss: 0.3399751099169129, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2704463922677438e-06\n",
      "43-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569469  85.64318853 123.42672926  78.81414139  98.80667324\n",
      "  44.87248816  68.34229308  47.71572068  75.80974111]\n",
      "44-th iteration, loss: 0.3399751099167742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2811325630950974e-06\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[3.85693797e-01 8.56431865e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426732e+02 7.88141392e+01 9.88066733e+01 4.48724881e+01\n",
      " 6.83422902e+01 4.77157214e+01 7.58097411e+01]\n",
      "45-th iteration, loss: 0.33997510991660707, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2578966028649445e-06\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569292  85.64318449 123.42673618  78.81413715  98.8066734\n",
      "  44.87248797  68.34228741  47.71572212  75.80974111]\n",
      "46-th iteration, loss: 0.3399751099164723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2678073679578747e-06\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85692046e-01 8.56431825e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23426738e+02 7.88141351e+01 9.88066735e+01 4.48724879e+01\n",
      " 6.83422846e+01 4.77157228e+01 7.58097411e+01]\n",
      "47-th iteration, loss: 0.3399751099163093, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2440574350480163e-06\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569119  85.64318054 123.42674304  78.814133    98.80667357\n",
      "  44.87248782  68.34228181  47.71572355  75.80974111]\n",
      "48-th iteration, loss: 0.3399751099161782, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2532577272540796e-06\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569033  85.64317858 123.42674531  78.81413094  98.80667366\n",
      "  44.87248776  68.34227904  47.71572427  75.80974111]\n",
      "49-th iteration, loss: 0.33997510991604896, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.261504371371068e-06\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689475e-01 8.56431767e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426748e+02 7.88141289e+01 9.88066738e+01 4.48724877e+01\n",
      " 6.83422763e+01 4.77157250e+01 7.58097411e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355716\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         458.11386249]\n",
      "1-th iteration, loss: 0.7491078933829632, 11 gd steps\n",
      "insert gradient: -0.6332402534573919\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 234.64368566   0.         223.47017682]\n",
      "2-th iteration, loss: 0.6042697166674827, 13 gd steps\n",
      "insert gradient: -0.6019316850733866\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.31802897  77.4470074  219.9411972   41.67877133 118.57601219\n",
      "   0.         104.89416463]\n",
      "3-th iteration, loss: 0.4719680401577259, 21 gd steps\n",
      "insert gradient: -0.35512655994492237\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  1.87963554  96.63098772 192.52710059  48.95569079  97.59005985\n",
      "  58.46126112 104.89416463]\n",
      "4-th iteration, loss: 0.466673400776488, 12 gd steps\n",
      "insert gradient: -0.20699914579543405\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[  0.6389503  102.03439013 188.01922596  51.86324402  99.17266572\n",
      "  58.71586533 104.89416463]\n",
      "5-th iteration, loss: 0.46433767087633304, 24 gd steps\n",
      "insert gradient: -0.2053967455015048\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[0.00000000e+00 1.05973132e+02 1.84307424e+02 5.59292891e+01\n",
      " 1.01431941e+02 5.99757831e+01 1.04894165e+02 0.00000000e+00\n",
      " 4.70734562e-14]\n",
      "6-th iteration, loss: 0.3788398171858579, 33 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.82195774e+01 1.49620322e+02 7.43267769e+01\n",
      " 1.27052692e+02 5.15032708e+01 8.27675886e+01 3.99241296e+01\n",
      " 2.46870861e-15 0.00000000e+00 6.41864240e-14]\n",
      "7-th iteration, loss: 0.3758974718572075, 21 gd steps\n",
      "insert gradient: -0.0485685471983215\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.73047546e+01 1.45086744e+02 7.59220977e+01\n",
      " 1.25018289e+02 5.15241049e+01 7.99185473e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.23959088e+01 6.36517476e-14]\n",
      "8-th iteration, loss: 0.3735295973747247, 22 gd steps\n",
      "insert gradient: -0.0068695295057222515\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.79838847e+01 1.37311122e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.88182323e+01 1.18424300e+02 5.25155408e+01\n",
      " 7.96394203e+01 4.49157593e+01 6.07974760e-14]\n",
      "9-th iteration, loss: 0.3555454583293367, 231 gd steps\n",
      "insert gradient: -0.03340917834320511\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[6.86088664e-01 1.02637254e+02 1.08338029e+02 1.00135499e+02\n",
      " 1.04091270e+02 6.17951022e+01 8.39994482e+01 4.82610465e+01\n",
      " 3.52787221e-14]\n",
      "10-th iteration, loss: 0.3525947778472343, 24 gd steps\n",
      "insert gradient: -0.039372576770335\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[5.86174132e-01 1.06764491e+02 1.01699481e+02 1.04777738e+02\n",
      " 1.04864946e+02 0.00000000e+00 4.70734562e-14 6.25050955e+01\n",
      " 9.23133275e+01 4.86652743e+01 6.93639314e-15]\n",
      "11-th iteration, loss: 0.35213322071364755, 14 gd steps\n",
      "insert gradient: -0.019770866809445068\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[8.41360622e-01 5.99287439e+01 0.00000000e+00 4.79429951e+01\n",
      " 9.92052982e+01 1.05992209e+02 1.05830599e+02 6.30872859e+01\n",
      " 9.41917595e+01 4.83510808e+01 2.07571509e-15]\n",
      "12-th iteration, loss: 0.35180395185915386, 24 gd steps\n",
      "insert gradient: -0.012715300995478265\n",
      "12-th iteration, new layer inserted. now 10 layers\n",
      "[  0.88796347  59.14609861   2.42670544  47.70437855  98.98154609\n",
      " 106.43084817 106.67497053  63.86771827  94.89856091  48.03702842]\n",
      "13-th iteration, loss: 0.35161349155452737, 21 gd steps\n",
      "insert gradient: -0.013658366159006325\n",
      "13-th iteration, new layer inserted. now 12 layers\n",
      "[1.16402532e+00 5.79111422e+01 4.89796091e+00 4.73151003e+01\n",
      " 9.91955467e+01 1.06755902e+02 1.07539943e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.39778567e+01 9.58803178e+01 4.85588845e+01]\n",
      "14-th iteration, loss: 0.3514228580603139, 18 gd steps\n",
      "insert gradient: -0.013653897845594032\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[  1.33397661  57.62708058   7.22337958  46.21390541  99.08066778\n",
      " 107.1718039  108.81731239  64.516082    96.53934022  48.54554029]\n",
      "15-th iteration, loss: 0.3297155248738684, 62 gd steps\n",
      "insert gradient: -0.16341739847415246\n",
      "15-th iteration, new layer inserted. now 12 layers\n",
      "[7.86737758e-01 5.06571230e+01 0.00000000e+00 3.55271368e-14\n",
      " 2.69507368e+01 4.51860485e+01 9.83485984e+01 1.22583667e+02\n",
      " 1.18495649e+02 6.88568153e+01 1.13242577e+02 6.04486918e+01]\n",
      "16-th iteration, loss: 0.3152264823366797, 59 gd steps\n",
      "insert gradient: -0.0510729079717163\n",
      "16-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          39.47757729  50.53724698  47.79214502  95.26731738\n",
      "  51.84668674   0.          86.41114457 116.44525169  69.59067652\n",
      " 117.29321124  69.94711959]\n",
      "17-th iteration, loss: 0.30122614189842994, 34 gd steps\n",
      "insert gradient: -0.061608421047495254\n",
      "17-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          28.37405713  61.90738415  55.3021177   95.45136044\n",
      "  36.83504761  46.78862344  79.55611741 116.20615924  74.02696109\n",
      " 123.26385974  78.29322101]\n",
      "18-th iteration, loss: 0.2963558113875898, 15 gd steps\n",
      "insert gradient: -0.08886453257292203\n",
      "18-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 2.05892671e+01 7.78507692e+01 0.00000000e+00\n",
      " 2.13162821e-14 5.29889567e+01 9.62546603e+01 3.07541332e+01\n",
      " 7.22229612e+01 7.76859439e+01 1.19744445e+02 7.33850536e+01\n",
      " 1.25846904e+02 8.29425584e+01]\n",
      "19-th iteration, loss: 0.2939537916449943, 51 gd steps\n",
      "insert gradient: -0.009482311930317176\n",
      "19-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          19.51813893  86.60812471  52.64634078  92.28676594\n",
      "  32.4634488   76.99073252  76.93536147 118.6857536   77.0061749\n",
      " 126.14138106  82.78325197]\n",
      "20-th iteration, loss: 0.2935888668763615, 14 gd steps\n",
      "insert gradient: -0.02597026904472926\n",
      "20-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          16.90796888  92.94205405  51.94088746  88.91832246\n",
      "  32.58733624  85.58843516  75.72638591 119.11560707  78.07627593\n",
      " 126.50765937  83.66868388]\n",
      "21-th iteration, loss: 0.29345430911080456, 54 gd steps\n",
      "insert gradient: -0.000615239444112239\n",
      "21-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71983221e+01 0.00000000e+00 3.10862447e-15\n",
      " 9.34934922e+01 5.22199139e+01 8.90065374e+01 3.22012667e+01\n",
      " 8.47302622e+01 7.63683833e+01 1.18291442e+02 7.79000961e+01\n",
      " 1.26284936e+02 8.41467854e+01]\n",
      "22-th iteration, loss: 0.2934542145808781, 28 gd steps\n",
      "insert gradient: -0.0005870658972997845\n",
      "22-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71962192e+01 0.00000000e+00 1.11022302e-15\n",
      " 9.34996603e+01 5.22197030e+01 8.90074848e+01 3.22022464e+01\n",
      " 8.47249423e+01 7.63708190e+01 1.18288707e+02 7.79011573e+01\n",
      " 1.26283504e+02 8.41471838e+01]\n",
      "23-th iteration, loss: 0.29345412615822924, 27 gd steps\n",
      "insert gradient: -0.0005725642029702486\n",
      "23-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.19414884  93.50539753  52.21930364  89.00830047\n",
      "  32.20313227  84.71986346  76.37317621 118.28614086  77.90222422\n",
      " 126.28215838  84.14757531]\n",
      "24-th iteration, loss: 0.2934540627486072, 23 gd steps\n",
      "insert gradient: -0.0005896946216967619\n",
      "24-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71924786e+01 0.00000000e+00 3.77475828e-15\n",
      " 9.35078189e+01 5.22190083e+01 8.90089994e+01 3.22039349e+01\n",
      " 8.47156293e+01 7.63751396e+01 1.18284010e+02 7.79031055e+01\n",
      " 1.26281041e+02 8.41479043e+01]\n",
      "25-th iteration, loss: 0.29345398205217577, 26 gd steps\n",
      "insert gradient: -0.0005683658284092156\n",
      "25-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.190672    93.51328016  52.21868684  89.00977777\n",
      "  32.20486412  84.71089102  76.37731554 118.2816381   77.90408223\n",
      " 126.27980183  84.1482778 ]\n",
      "26-th iteration, loss: 0.29345392397058795, 22 gd steps\n",
      "insert gradient: -0.0005823035586364738\n",
      "26-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71891736e+01 0.00000000e+00 3.10862447e-15\n",
      " 9.35155806e+01 5.22184008e+01 8.90104234e+01 3.22056667e+01\n",
      " 8.47069056e+01 7.63791492e+01 1.18279659e+02 7.79049111e+01\n",
      " 1.26278768e+02 8.41485963e+01]\n",
      "27-th iteration, loss: 0.29345384993675944, 25 gd steps\n",
      "insert gradient: -0.000560922505428517\n",
      "27-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71875425e+01 0.00000000e+00 4.88498131e-15\n",
      " 9.35207400e+01 5.22180757e+01 8.90111336e+01 3.22065786e+01\n",
      " 8.47024548e+01 7.63811806e+01 1.18277464e+02 7.79058326e+01\n",
      " 1.26277624e+02 8.41489566e+01]\n",
      "28-th iteration, loss: 0.2934537796156989, 24 gd steps\n",
      "insert gradient: -0.0005484799363679335\n",
      "28-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71859151e+01 0.00000000e+00 1.77635684e-15\n",
      " 9.35256176e+01 5.22176393e+01 8.90117635e+01 3.22074284e+01\n",
      " 8.46981581e+01 7.63831676e+01 1.18275377e+02 7.79067584e+01\n",
      " 1.26276535e+02 8.41493097e+01]\n",
      "29-th iteration, loss: 0.29345371245815177, 23 gd steps\n",
      "insert gradient: -0.0005394332136835595\n",
      "29-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18432239  93.530292    52.21716755  89.01235216\n",
      "  32.20826222  84.69400348  76.38510736 118.27337998  77.90766977\n",
      " 126.27548841  84.14965307]\n",
      "30-th iteration, loss: 0.29345366269489037, 20 gd steps\n",
      "insert gradient: -0.0005552596937265622\n",
      "30-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71829959e+01 0.00000000e+00 5.55111512e-15\n",
      " 9.35323332e+01 5.22168163e+01 8.90128835e+01 3.22090359e+01\n",
      " 8.46904283e+01 7.63867739e+01 1.18271666e+02 7.79084419e+01\n",
      " 1.26274590e+02 8.41499482e+01]\n",
      "31-th iteration, loss: 0.29345360007724725, 23 gd steps\n",
      "insert gradient: -0.0005380155746782537\n",
      "31-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18158346  93.53687733  52.21646101  89.0134752\n",
      "  32.20991679  84.68648862  76.38859079 118.26978185  77.9092786\n",
      " 126.27360509  84.1502753 ]\n",
      "32-th iteration, loss: 0.2934535537051507, 19 gd steps\n",
      "insert gradient: -0.000550278514499966\n",
      "32-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18037331  93.53884684  52.21614771  89.01398105\n",
      "  32.21069147  84.68307853  76.39016494 118.26816371  77.9100068\n",
      " 126.27275942  84.15056073]\n",
      "33-th iteration, loss: 0.29345350886557764, 19 gd steps\n",
      "insert gradient: -0.0005557456513626195\n",
      "33-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17926868  93.54082193  52.21592538  89.01450501\n",
      "  32.21149645  84.67974362  76.39167659 118.2665781   77.91069232\n",
      " 126.27193463  84.15084035]\n",
      "34-th iteration, loss: 0.29345346533891153, 19 gd steps\n",
      "insert gradient: -0.0005580098563727028\n",
      "34-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71782360e+01 0.00000000e+00 1.99840144e-15\n",
      " 9.35427876e+01 5.22157416e+01 8.90150243e+01 3.22123014e+01\n",
      " 8.46764698e+01 7.63931380e+01 1.18265026e+02 7.79113522e+01\n",
      " 1.26271131e+02 8.41511172e+01]\n",
      "35-th iteration, loss: 0.29345340973242456, 21 gd steps\n",
      "insert gradient: -0.0005349497812573328\n",
      "35-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17707597  93.54708987  52.21546557  89.01555616\n",
      "  32.21315195  84.67284365  76.39475175 118.2633272   77.91209782\n",
      " 126.27025339  84.15142795]\n",
      "36-th iteration, loss: 0.29345336864370297, 18 gd steps\n",
      "insert gradient: -0.0005435916939108208\n",
      "36-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17604783  93.54894893  52.21518071  89.01599689\n",
      "  32.21388184  84.66968299  76.39616909 118.2618639   77.91276242\n",
      " 126.26949591  84.1517006 ]\n",
      "37-th iteration, loss: 0.29345332872272273, 18 gd steps\n",
      "insert gradient: -0.0005472509179544911\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17509544  93.55080899  52.21496391  89.01645115\n",
      "  32.2146349   84.6665832   76.39754054 118.26042822  77.91339412\n",
      " 126.26875452  84.15196704]\n",
      "38-th iteration, loss: 0.2934532898387362, 18 gd steps\n",
      "insert gradient: -0.0005484915860287965\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17419565  93.5526591   52.21477759  89.01690226\n",
      "  32.21538876  84.66353473  76.3988741  118.25902096  77.91400498\n",
      " 126.26802978  84.15222958]\n",
      "39-th iteration, loss: 0.293453251924034, 18 gd steps\n",
      "insert gradient: -0.0005484968949674981\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17333627  93.55449398  52.21460485  89.01734341\n",
      "  32.21613433  84.66053323  76.40017462 118.25764235  77.91460059\n",
      " 126.26732151  84.15248891]\n",
      "40-th iteration, loss: 0.29345321492809834, 18 gd steps\n",
      "insert gradient: -0.0005478286798773133\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17251014  93.55631121  52.21443802  89.01777211\n",
      "  32.21686836  84.65757625  76.40144554 118.25629212  77.91518369\n",
      " 126.26662917  84.1527451 ]\n",
      "41-th iteration, loss: 0.29345317882182415, 17 gd steps\n",
      "insert gradient: -0.0005467631250306316\n",
      "41-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17171291  93.55810902  52.2142737   89.01818762\n",
      "  32.21758981  84.65466329  76.40268902 118.2549703   77.9157555\n",
      " 126.26595234  84.15299789]\n",
      "42-th iteration, loss: 0.2934531435502156, 17 gd steps\n",
      "insert gradient: -0.0005454447453401262\n",
      "42-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17094089  93.5598878   52.21411021  89.01859035\n",
      "  32.21829926  84.65179174  76.40390773 118.25367565  77.91631714\n",
      " 126.26529002  84.1532472 ]\n",
      "43-th iteration, loss: 0.293453109077604, 17 gd steps\n",
      "insert gradient: -0.0005439524391861601\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17019149  93.56164752  52.21394688  89.01898082\n",
      "  32.21899737  84.64896044  76.40510343 118.25240748  77.91686916\n",
      " 126.26464154  84.15349283]\n",
      "44-th iteration, loss: 0.29345307537105075, 17 gd steps\n",
      "insert gradient: -0.0005423334675802315\n",
      "44-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71694626e+01 0.00000000e+00 1.11022302e-15\n",
      " 9.35633883e+01 5.22137834e+01 8.90193596e+01 3.22196849e+01\n",
      " 8.46461683e+01 7.64062776e+01 1.18251165e+02 7.79174120e+01\n",
      " 1.26264006e+02 8.41537346e+01]\n",
      "45-th iteration, loss: 0.29345303195296113, 19 gd steps\n",
      "insert gradient: -0.0005192183639500634\n",
      "45-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71686241e+01 0.00000000e+00 3.77475828e-15\n",
      " 9.35671490e+01 5.22135318e+01 8.90197388e+01 3.22204043e+01\n",
      " 8.46431031e+01 7.64075743e+01 1.18249819e+02 7.79180240e+01\n",
      " 1.26263316e+02 8.41540002e+01]\n",
      "46-th iteration, loss: 0.29345299016929965, 19 gd steps\n",
      "insert gradient: -0.0005050512464151216\n",
      "46-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.16772722  93.57072002  52.2131742   89.02006163\n",
      "  32.22106693  84.64010543  76.40887381 118.24852882  77.91865515\n",
      " 126.26264903  84.15425963]\n",
      "47-th iteration, loss: 0.29345295871297955, 16 gd steps\n",
      "insert gradient: -0.0005141253129220337\n",
      "47-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.16692337  93.57231235  52.2128612   89.02035439\n",
      "  32.22167924  84.63743695  76.41004327 118.24738731  77.9192181\n",
      " 126.26205498  84.15448747]\n",
      "48-th iteration, loss: 0.29345292803011175, 16 gd steps\n",
      "insert gradient: -0.0005181103187465647\n",
      "48-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71661785e+01 0.00000000e+00 1.99840144e-15\n",
      " 9.35739123e+01 5.22126225e+01 8.90206711e+01 3.22223259e+01\n",
      " 8.46348155e+01 7.64111807e+01 1.18246262e+02 7.79197501e+01\n",
      " 1.26261469e+02 8.41547078e+01]\n",
      "49-th iteration, loss: 0.2934528890124966, 18 gd steps\n",
      "insert gradient: -0.0004998587051557277\n",
      "49-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71653596e+01 0.00000000e+00 5.55111512e-15\n",
      " 9.35773681e+01 5.22123407e+01 8.90210011e+01 3.22230201e+01\n",
      " 8.46319700e+01 7.64124150e+01 1.18245051e+02 7.79203300e+01\n",
      " 1.26260837e+02 8.41549461e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368028745168503\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         483.13858449]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6338943430935724\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 235.67735829   0.         247.4612262 ]\n",
      "2-th iteration, loss: 0.6045644704212222, 13 gd steps\n",
      "insert gradient: -0.6805507104959237\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.24069056  77.30193988 220.75829298  41.38070239 242.4109971\n",
      "   0.           5.05022911]\n",
      "3-th iteration, loss: 0.46851108920036433, 20 gd steps\n",
      "insert gradient: -0.4296688299100961\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.6303661  116.47503403   0.         110.00419881\n",
      "  54.87991758 163.36276674  51.36288895   5.05022911]\n",
      "4-th iteration, loss: 0.3783689467939926, 30 gd steps\n",
      "insert gradient: -0.19633619877402855\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[1.52614328e+00 5.05294567e+01 6.45915769e+01 0.00000000e+00\n",
      " 4.44089210e-14 3.13629838e+01 1.04615486e+02 7.32626193e+01\n",
      " 1.21601817e+02 7.67153597e+01 5.05022911e+00]\n",
      "5-th iteration, loss: 0.3634809333146551, 14 gd steps\n",
      "insert gradient: -0.01038262492074788\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.39602121e+01 5.45561220e+01 4.95591278e+01\n",
      " 1.08433560e+02 7.62961852e+01 1.24070625e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.91907133e+01 5.05022911e+00]\n",
      "6-th iteration, loss: 0.3632811427912788, 32 gd steps\n",
      "insert gradient: -0.0007205060379669465\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37764057  55.31898834  48.88350137 106.88120874\n",
      "  76.74975691 123.29501113  80.12932721   5.05022911]\n",
      "7-th iteration, loss: 0.36328100493128834, 59 gd steps\n",
      "insert gradient: -0.0005192819345988708\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37087424  55.32226269  48.88386141 106.88376234\n",
      "  76.76062247 123.28082203  80.12171978   5.05022911]\n",
      "8-th iteration, loss: 0.36328096720883873, 33 gd steps\n",
      "insert gradient: -0.0004637030737957822\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43688764e+01 5.53236957e+01 4.88840715e+01\n",
      " 1.06884536e+02 0.00000000e+00 4.35207426e-14 7.67641280e+01\n",
      " 1.23275740e+02 8.01197143e+01 5.05022911e+00]\n",
      "9-th iteration, loss: 0.3632809318984339, 30 gd steps\n",
      "insert gradient: -0.0003275944205994616\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43672549e+01 5.53251403e+01 4.88842395e+01\n",
      " 1.06885062e+02 2.67818019e-03 3.42675863e-04 7.67668374e+01\n",
      " 1.23271063e+02 8.01181655e+01 5.05022911e+00]\n",
      "10-th iteration, loss: 0.36328091150025127, 24 gd steps\n",
      "insert gradient: -0.00027074351297176445\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43662489e+01 5.53262414e+01 4.88843516e+01\n",
      " 1.06885301e+02 4.12979579e-03 1.93509308e-04 7.67683264e+01\n",
      " 1.23267772e+02 8.01172349e+01 5.05022911e+00]\n",
      "11-th iteration, loss: 0.36328089609032865, 20 gd steps\n",
      "insert gradient: -0.00023526625267307905\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43654570e+01 5.53271922e+01 4.88844371e+01\n",
      " 1.06885458e+02 0.00000000e+00 4.35207426e-14 7.67746209e+01\n",
      " 1.23265026e+02 8.01165718e+01 5.05022911e+00]\n",
      "12-th iteration, loss: 0.36328088376496376, 18 gd steps\n",
      "insert gradient: -0.0002146431993933982\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43647872e+01 0.00000000e+00 2.13162821e-14\n",
      " 5.53280340e+01 4.88844926e+01 1.06885566e+02 8.52706978e-04\n",
      " 8.21782816e-05 7.67754767e+01 1.23262634e+02 8.01160819e+01\n",
      " 5.05022911e+00]\n",
      "13-th iteration, loss: 0.3632808719973363, 18 gd steps\n",
      "insert gradient: -0.00019605072956433787\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43641356e+01 5.53295950e+01 4.88844988e+01\n",
      " 1.06885641e+02 1.60270296e-03 5.76254312e-05 0.00000000e+00\n",
      " 1.77876919e-20 7.67762340e+01 1.23260377e+02 8.01157038e+01\n",
      " 5.05022911e+00]\n",
      "14-th iteration, loss: 0.3632808622054664, 16 gd steps\n",
      "insert gradient: -0.00020047929633290127\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36354432  55.33029017  48.88447808 106.88568839\n",
      "  76.77970162 123.25835123  80.11543093   5.05022911]\n",
      "15-th iteration, loss: 0.36328085511424824, 14 gd steps\n",
      "insert gradient: -0.0001910677969942886\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36305558  55.33090712  48.88446774 106.88571951\n",
      "  76.78019143 123.25660096  80.11524121   5.05022911]\n",
      "16-th iteration, loss: 0.36328084870713284, 14 gd steps\n",
      "insert gradient: -0.00018370657181601525\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36259218  55.33147867  48.88444995 106.88575854\n",
      "  76.78068774 123.25497169  80.11511848   5.05022911]\n",
      "17-th iteration, loss: 0.3632808428502493, 13 gd steps\n",
      "insert gradient: -0.00017776122150228257\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43621535e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53320148e+01 4.88844285e+01 1.06885803e+02 7.67811845e+01\n",
      " 1.23253442e+02 8.01150492e+01 5.05022911e+00]\n",
      "18-th iteration, loss: 0.3632808367978078, 14 gd steps\n",
      "insert gradient: -0.0001824797972871759\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36170966  55.33303797  48.88439044 106.88585177\n",
      "  76.78169959 123.2519515   80.11502713   5.05022911]\n",
      "19-th iteration, loss: 0.36328083180198784, 13 gd steps\n",
      "insert gradient: -0.00018187185610436048\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36128907  55.33349694  48.88434224 106.88590213\n",
      "  76.78220085 123.25058841  80.11504821   5.05022911]\n",
      "20-th iteration, loss: 0.36328082712695065, 13 gd steps\n",
      "insert gradient: -0.00018072138711598034\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36089564  55.33394225  48.8843019  106.88595491\n",
      "  76.78269122 123.24928417  80.11509671   5.05022911]\n",
      "21-th iteration, loss: 0.3632808227364164, 12 gd steps\n",
      "insert gradient: -0.00017914200952425888\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43605261e+01 5.53343746e+01 4.88842675e+01\n",
      " 1.06886009e+02 0.00000000e+00 4.52970994e-14 7.67831701e+01\n",
      " 1.23248033e+02 8.01151688e+01 5.05022911e+00]\n",
      "22-th iteration, loss: 0.3632808180294299, 13 gd steps\n",
      "insert gradient: -0.00015964777911818342\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43601687e+01 5.53348127e+01 4.88842358e+01\n",
      " 1.06886060e+02 4.64789759e-04 4.30869022e-05 0.00000000e+00\n",
      " 1.43995601e-20 7.67836358e+01 1.23246784e+02 8.01152598e+01\n",
      " 5.05022911e+00]\n",
      "23-th iteration, loss: 0.36328081329769063, 13 gd steps\n",
      "insert gradient: -0.00016449867906003933\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43598372e+01 5.53352609e+01 4.88842114e+01\n",
      " 1.06886095e+02 8.65027601e-04 4.01954748e-05 7.67844423e+01\n",
      " 1.23245544e+02 8.01153571e+01 5.05022911e+00]\n",
      "24-th iteration, loss: 0.36328080929836504, 12 gd steps\n",
      "insert gradient: -0.00016295474651101477\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43595422e+01 5.53356993e+01 4.88841963e+01\n",
      " 1.06886116e+02 1.19421951e-03 9.65219770e-07 7.67847735e+01\n",
      " 1.23244366e+02 8.01154581e+01 5.05022911e+00]\n",
      "25-th iteration, loss: 0.3632808055320263, 12 gd steps\n",
      "insert gradient: -0.00016100227308573354\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43592625e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.53361276e+01 4.88841818e+01 1.06886133e+02 7.67865808e+01\n",
      " 1.23243226e+02 8.01155728e+01 5.05022911e+00]\n",
      "26-th iteration, loss: 0.3632808017759372, 12 gd steps\n",
      "insert gradient: -0.00014736404443637822\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35898005  55.33694883  48.88415536 106.88614893\n",
      "  76.78688147 123.24211404  80.11570686   5.05022911]\n",
      "27-th iteration, loss: 0.36328079863135654, 11 gd steps\n",
      "insert gradient: -0.00014452921294667487\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43587007e+01 0.00000000e+00 2.17603713e-14\n",
      " 5.53373234e+01 4.88841178e+01 1.06886168e+02 7.67871879e+01\n",
      " 1.23241075e+02 8.01158593e+01 5.05022911e+00]\n",
      "28-th iteration, loss: 0.363280795297109, 11 gd steps\n",
      "insert gradient: -0.00013451740112199119\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43584186e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53380505e+01 4.88840737e+01 1.06886190e+02 7.67875046e+01\n",
      " 1.23240052e+02 8.01160301e+01 5.05022911e+00]\n",
      "29-th iteration, loss: 0.36328079214862685, 11 gd steps\n",
      "insert gradient: -0.00013404393524031418\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35813118  55.33872547  48.88401689 106.8862144\n",
      "  76.78782925 123.23906922  80.11621776   5.05022911]\n",
      "30-th iteration, loss: 0.3632807894163954, 11 gd steps\n",
      "insert gradient: -0.00013272399803940837\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43578542e+01 5.53390439e+01 4.88839604e+01\n",
      " 1.06886241e+02 0.00000000e+00 5.41788836e-14 7.67881511e+01\n",
      " 1.23238134e+02 8.01164137e+01 5.05022911e+00]\n",
      "31-th iteration, loss: 0.363280786540881, 11 gd steps\n",
      "insert gradient: -0.00012981689638658347\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43575910e+01 0.00000000e+00 7.99360578e-15\n",
      " 5.53393668e+01 4.88839119e+01 1.06886267e+02 3.11070510e-04\n",
      " 2.21343571e-05 7.67884625e+01 1.23237205e+02 8.01166153e+01\n",
      " 5.05022911e+00]\n",
      "32-th iteration, loss: 0.36328078354983756, 11 gd steps\n",
      "insert gradient: -0.00012492825934199477\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43573371e+01 0.00000000e+00 1.50990331e-14\n",
      " 5.53400155e+01 4.88838653e+01 1.06886287e+02 5.95679133e-04\n",
      " 1.95056967e-05 7.67887485e+01 1.23236280e+02 8.01168203e+01\n",
      " 5.05022911e+00]\n",
      "33-th iteration, loss: 0.3632807807101571, 11 gd steps\n",
      "insert gradient: -0.00012058444338132232\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35708474  55.34063653  48.88381221 106.88630273\n",
      "  76.78987834 123.23538004  80.1170306    5.05022911]\n",
      "34-th iteration, loss: 0.36328077840566536, 11 gd steps\n",
      "insert gradient: -0.00011991257648972015\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35684643  55.34093178  48.8837625  106.88631763\n",
      "  76.79012904 123.23452544  80.11724073   5.05022911]\n",
      "35-th iteration, loss: 0.3632807761914422, 10 gd steps\n",
      "insert gradient: -0.0001190217677062339\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35662054  55.34122318  48.88371983 106.88633556\n",
      "  76.79037894 123.23369294  80.11745496   5.05022911]\n",
      "36-th iteration, loss: 0.3632807740589924, 10 gd steps\n",
      "insert gradient: -0.00011797385192530675\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35640534  55.34151073  48.88368267 106.8863559\n",
      "  76.79062782 123.23288054  80.11767294   5.05022911]\n",
      "37-th iteration, loss: 0.3632807720029435, 10 gd steps\n",
      "insert gradient: -0.00011681438804260915\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43561996e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53417943e+01 4.88836499e+01 1.06886378e+02 7.67908753e+01\n",
      " 1.23232087e+02 8.01178941e+01 5.05022911e+00]\n",
      "38-th iteration, loss: 0.363280769822111, 10 gd steps\n",
      "insert gradient: -0.00010998843017129755\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43559938e+01 5.53423485e+01 4.88836140e+01\n",
      " 1.06886401e+02 0.00000000e+00 3.64153152e-14 7.67911259e+01\n",
      " 1.23231303e+02 8.01181224e+01 5.05022911e+00]\n",
      "39-th iteration, loss: 0.36328076774772916, 10 gd steps\n",
      "insert gradient: -0.00011237730284447946\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43557903e+01 5.53426161e+01 4.88835748e+01\n",
      " 1.06886423e+02 2.45261409e-04 1.92625535e-05 7.67913714e+01\n",
      " 1.23230539e+02 8.01183533e+01 5.05022911e+00]\n",
      "40-th iteration, loss: 0.36328076577080876, 10 gd steps\n",
      "insert gradient: -0.00011371430811655429\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43556015e+01 5.53428867e+01 4.88835430e+01\n",
      " 1.06886441e+02 4.65776511e-04 2.04954007e-05 7.67915931e+01\n",
      " 1.23229786e+02 8.01185798e+01 5.05022911e+00]\n",
      "41-th iteration, loss: 0.36328076387526786, 10 gd steps\n",
      "insert gradient: -0.00011424563277366837\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43554243e+01 5.53431583e+01 4.88835166e+01\n",
      " 1.06886456e+02 6.67657192e-04 6.51261528e-06 7.67917960e+01\n",
      " 1.23229044e+02 8.01188034e+01 5.05022911e+00]\n",
      "42-th iteration, loss: 0.3632807620528317, 10 gd steps\n",
      "insert gradient: -0.0001141683408697548\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43552565e+01 0.00000000e+00 1.90958360e-14\n",
      " 5.53434293e+01 4.88834941e+01 1.06886468e+02 7.67928390e+01\n",
      " 1.23228314e+02 8.01190254e+01 5.05022911e+00]\n",
      "43-th iteration, loss: 0.36328076020566685, 10 gd steps\n",
      "insert gradient: -0.00010630256051144528\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43550885e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53439576e+01 4.88834677e+01 1.06886479e+02 7.67930215e+01\n",
      " 1.23227595e+02 8.01192505e+01 5.05022911e+00]\n",
      "44-th iteration, loss: 0.36328075844037255, 10 gd steps\n",
      "insert gradient: -0.00010493100366740441\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43549124e+01 5.53444511e+01 4.88834305e+01\n",
      " 1.06886491e+02 7.67932129e+01 1.23226898e+02 0.00000000e+00\n",
      " 8.52651283e-14 8.01194823e+01 5.05022911e+00]\n",
      "45-th iteration, loss: 0.36328075674213667, 10 gd steps\n",
      "insert gradient: -0.00010146641478336992\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43547376e+01 0.00000000e+00 1.28785871e-14\n",
      " 5.53446892e+01 4.88833925e+01 1.06886506e+02 7.67934084e+01\n",
      " 1.23226219e+02 8.01199462e+01 5.05022911e+00]\n",
      "46-th iteration, loss: 0.36328075509517604, 10 gd steps\n",
      "insert gradient: -0.00010198602649129783\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43545676e+01 5.53451582e+01 4.88833560e+01\n",
      " 1.06886521e+02 7.67936003e+01 1.23225549e+02 0.00000000e+00\n",
      " 4.08562073e-14 8.01201700e+01 5.05022911e+00]\n",
      "47-th iteration, loss: 0.3632807535049975, 10 gd steps\n",
      "insert gradient: -9.722862993347218e-05\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3543991   55.34538501  48.88331898 106.8865377\n",
      "  76.79379536 123.22489513  80.12061757   5.05022911]\n",
      "48-th iteration, loss: 0.36328075209334354, 10 gd steps\n",
      "insert gradient: -9.715369650583903e-05\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35424175  55.34561133  48.8832887  106.8865547\n",
      "  76.79398325 123.22425433  80.12083021   5.05022911]\n",
      "49-th iteration, loss: 0.3632807507255397, 10 gd steps\n",
      "insert gradient: -9.766242522435842e-05\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43540905e+01 5.53458350e+01 4.88832614e+01\n",
      " 1.06886573e+02 7.67941705e+01 1.23223626e+02 0.00000000e+00\n",
      " 4.97379915e-14 8.01210435e+01 5.05022911e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235571\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         508.36511877]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6335178126710227\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 235.58383553   0.         272.78128324]\n",
      "2-th iteration, loss: 0.6046018105977554, 13 gd steps\n",
      "insert gradient: -0.6750448388891083\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.27488554  77.33265715 220.69625589  41.38811104 239.37949346\n",
      "   0.          33.40178978]\n",
      "3-th iteration, loss: 0.469464367429951, 20 gd steps\n",
      "insert gradient: -0.40056111866799093\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.98728814 116.07442618   0.         109.62584695\n",
      "  56.16811469 163.82440226  48.76198286  33.40178978]\n",
      "4-th iteration, loss: 0.3633640786604565, 49 gd steps\n",
      "insert gradient: -0.023210391172775693\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.30462367  54.10872534  49.50299064 106.36302326\n",
      "  76.20978909 122.81082842  79.97055351  33.40178978]\n",
      "5-th iteration, loss: 0.3632917438234116, 12 gd steps\n",
      "insert gradient: -0.002551324147334875\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.77041751  54.560786    49.00136925 106.64648411\n",
      "  76.77975805 122.98106822  80.03220491  33.40178978]\n",
      "6-th iteration, loss: 0.36328076903538614, 26 gd steps\n",
      "insert gradient: -0.00021594096083199152\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43725718e+01 0.00000000e+00 1.90958360e-14\n",
      " 5.52990742e+01 4.88901147e+01 1.06893953e+02 7.68021605e+01\n",
      " 1.23147445e+02 8.01430454e+01 3.34017898e+01]\n",
      "7-th iteration, loss: 0.3632807666778427, 11 gd steps\n",
      "insert gradient: -0.00021381336063476666\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43722807e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53001044e+01 4.88899264e+01 1.06893738e+02 7.68024841e+01\n",
      " 1.23147873e+02 8.01432125e+01 3.34017898e+01]\n",
      "8-th iteration, loss: 0.36328076444221574, 10 gd steps\n",
      "insert gradient: -0.0002112605722009454\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37199366  55.30111547  48.88974663 106.89352694\n",
      "  76.80278471 123.14828354  80.14336387  33.40178978]\n",
      "9-th iteration, loss: 0.3632807629646177, 10 gd steps\n",
      "insert gradient: -0.00021823015321411097\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37173189  55.30160106  48.88959179 106.89332932\n",
      "  76.80305146 123.14866113  80.14349364  33.40178978]\n",
      "10-th iteration, loss: 0.3632807615843888, 10 gd steps\n",
      "insert gradient: -0.00022314532251840638\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43714936e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53020968e+01 4.88894622e+01 1.06893136e+02 7.68032911e+01\n",
      " 1.23149020e+02 8.01436047e+01 3.34017898e+01]\n",
      "11-th iteration, loss: 0.36328075958900075, 10 gd steps\n",
      "insert gradient: -0.00021619155271480743\n",
      "11-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37125051  55.30312571  48.88933451 106.89293548\n",
      "  76.80352103 123.14937988  80.14370713  33.40178978]\n",
      "12-th iteration, loss: 0.3632807583286205, 10 gd steps\n",
      "insert gradient: -0.00022006588398676684\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37102047  55.30361216  48.88921831 106.89274804\n",
      "  76.8037304  123.14971524  80.14379725  33.40178978]\n",
      "13-th iteration, loss: 0.3632807571265141, 10 gd steps\n",
      "insert gradient: -0.00022261416856180668\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37080554  55.30410382  48.88911849 106.89256424\n",
      "  76.80392126 123.15003707  80.14387453  33.40178978]\n",
      "14-th iteration, loss: 0.3632807559717493, 10 gd steps\n",
      "insert gradient: -0.00022413325161744474\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37060213  55.30459844  48.88903131 106.89238376\n",
      "  76.8040965  123.15034717  80.14394099  33.40178978]\n",
      "15-th iteration, loss: 0.36328075485698885, 10 gd steps\n",
      "insert gradient: -0.00022485222501985732\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37040752  55.30509423  48.88895389 106.89220637\n",
      "  76.80425843 123.15064702  80.14399825  33.40178978]\n",
      "16-th iteration, loss: 0.36328075377715696, 10 gd steps\n",
      "insert gradient: -0.00022494892873947125\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43702197e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53055898e+01 4.88888840e+01 1.06892032e+02 7.68044090e+01\n",
      " 1.23150938e+02 8.01440477e+01 3.34017898e+01]\n",
      "17-th iteration, loss: 0.36328075207245136, 10 gd steps\n",
      "insert gradient: -0.0002143152273664303\n",
      "17-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37001629  55.30660029  48.88880493 106.89185091\n",
      "  76.80455871 123.15123519  80.1440948   33.40178978]\n",
      "18-th iteration, loss: 0.3632807510542596, 9 gd steps\n",
      "insert gradient: -0.00021528879437769546\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36981554  55.30707187  48.88872559 106.89168183\n",
      "  76.80470161 123.15151676  80.14413917  33.40178978]\n",
      "19-th iteration, loss: 0.363280750066939, 9 gd steps\n",
      "insert gradient: -0.0002156101659537288\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36962259  55.30754392  48.88865479 106.89151617\n",
      "  76.80483464 123.15179031  80.14417672  33.40178978]\n",
      "20-th iteration, loss: 0.36328074910716646, 9 gd steps\n",
      "insert gradient: -0.00021542346628551946\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36943573  55.30801528  48.88859067 106.89135375\n",
      "  76.80495919 123.1520567   80.14420845  33.40178978]\n",
      "21-th iteration, loss: 0.3632807481725466, 9 gd steps\n",
      "insert gradient: -0.0002148415465887287\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43692537e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53084851e+01 4.88885318e+01 1.06891194e+02 7.68050764e+01\n",
      " 1.23152317e+02 8.01442352e+01 3.34017898e+01]\n",
      "22-th iteration, loss: 0.3632807466767522, 10 gd steps\n",
      "insert gradient: -0.00020425988667995852\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36905647  55.3094377   48.88846348 106.89103009\n",
      "  76.80519433 123.15258312  80.14426096  33.40178978]\n",
      "23-th iteration, loss: 0.3632807457896679, 9 gd steps\n",
      "insert gradient: -0.00020476605203184888\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43688604e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.53098835e+01 4.88883932e+01 1.06890876e+02 7.68053098e+01\n",
      " 1.23152838e+02 8.01442869e+01 3.34017898e+01]\n",
      "24-th iteration, loss: 0.3632807443958091, 10 gd steps\n",
      "insert gradient: -0.0001955534551930872\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43686533e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53107887e+01 4.88883173e+01 1.06890719e+02 7.68054247e+01\n",
      " 1.23153098e+02 8.01443110e+01 3.34017898e+01]\n",
      "25-th iteration, loss: 0.3632807430662715, 10 gd steps\n",
      "insert gradient: -0.0001879084886012101\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43684328e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53116535e+01 4.88882297e+01 1.06890565e+02 7.68055428e+01\n",
      " 1.23153357e+02 8.01443377e+01 3.34017898e+01]\n",
      "26-th iteration, loss: 0.3632807417915767, 10 gd steps\n",
      "insert gradient: -0.00018145067331735428\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36820278  55.31248452  48.88813441 106.89041423\n",
      "  76.80566257 123.1536149   80.14436582  33.40178978]\n",
      "27-th iteration, loss: 0.3632807409889863, 9 gd steps\n",
      "insert gradient: -0.00018402597558445053\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43679815e+01 0.00000000e+00 6.66133815e-15\n",
      " 5.53128818e+01 4.88880457e+01 1.06890273e+02 7.68057772e+01\n",
      " 1.23153862e+02 8.01443917e+01 3.34017898e+01]\n",
      "28-th iteration, loss: 0.36328073978510456, 10 gd steps\n",
      "insert gradient: -0.00017746227613958201\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43677573e+01 0.00000000e+00 2.17603713e-14\n",
      " 5.53136919e+01 4.88879583e+01 1.06890130e+02 7.68058882e+01\n",
      " 1.23154111e+02 8.01444142e+01 3.34017898e+01]\n",
      "29-th iteration, loss: 0.36328073862736476, 10 gd steps\n",
      "insert gradient: -0.00017184873784831833\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43675260e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53144733e+01 4.88878654e+01 1.06889991e+02 7.68060002e+01\n",
      " 1.23154358e+02 8.01444376e+01 3.34017898e+01]\n",
      "30-th iteration, loss: 0.3632807375115516, 10 gd steps\n",
      "insert gradient: -0.0001669642714047948\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43672901e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53152301e+01 4.88877692e+01 1.06889856e+02 7.68061123e+01\n",
      " 1.23154604e+02 8.01444614e+01 3.34017898e+01]\n",
      "31-th iteration, loss: 0.36328073643465597, 10 gd steps\n",
      "insert gradient: -0.00016264304452092622\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36705113  55.31596514  48.88767127 106.88972436\n",
      "  76.80622364 123.15484675  80.14448501  33.40178978]\n",
      "32-th iteration, loss: 0.36328073573340497, 9 gd steps\n",
      "insert gradient: -0.0001660252410611977\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36682336  55.31632115  48.8875824  106.88960042\n",
      "  76.80632942 123.15508044  80.14450588  33.40178978]\n",
      "33-th iteration, loss: 0.3632807350607044, 9 gd steps\n",
      "insert gradient: -0.00016836512409810488\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43666100e+01 0.00000000e+00 6.66133815e-15\n",
      " 5.53166825e+01 4.88875077e+01 1.06889480e+02 7.68064260e+01\n",
      " 1.23155307e+02 8.01445206e+01 3.34017898e+01]\n",
      "34-th iteration, loss: 0.3632807340586268, 9 gd steps\n",
      "insert gradient: -0.00016242131945610225\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36639547  55.3174155   48.88743458 106.88935997\n",
      "  76.80651895 123.15553431  80.14453235  33.40178978]\n",
      "35-th iteration, loss: 0.36328073342268846, 9 gd steps\n",
      "insert gradient: -0.0001645395820149867\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43661871e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53177682e+01 4.88873651e+01 1.06889246e+02 7.68066094e+01\n",
      " 1.23155754e+02 8.01445436e+01 3.34017898e+01]\n",
      "36-th iteration, loss: 0.36328073247326237, 9 gd steps\n",
      "insert gradient: -0.0001586183182808521\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36597735  55.31848238  48.88729689 106.88913195\n",
      "  76.80669658 123.15597485  80.14455208  33.40178978]\n",
      "37-th iteration, loss: 0.3632807318709317, 9 gd steps\n",
      "insert gradient: -0.00016058720819878854\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36577314  55.31882619  48.88723158 106.88902393\n",
      "  76.80678181 123.15618877  80.14456042  33.40178978]\n",
      "38-th iteration, loss: 0.36328073128775473, 9 gd steps\n",
      "insert gradient: -0.0001618388116074239\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43655793e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53191729e+01 4.88871759e+01 1.06888919e+02 7.68068604e+01\n",
      " 1.23156397e+02 8.01445645e+01 3.34017898e+01]\n",
      "39-th iteration, loss: 0.36328073040200975, 9 gd steps\n",
      "insert gradient: -0.0001553953947526078\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36538229  55.31987138  48.88711905 106.88881364\n",
      "  76.80693683 123.15660692  80.1445669   33.40178978]\n",
      "40-th iteration, loss: 0.3632807298464106, 9 gd steps\n",
      "insert gradient: -0.00015681895127494554\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43651887e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.53202069e+01 4.88870630e+01 1.06888714e+02 7.68070127e+01\n",
      " 1.23156811e+02 8.01445701e+01 3.34017898e+01]\n",
      "41-th iteration, loss: 0.3632807290079443, 9 gd steps\n",
      "insert gradient: -0.0001507509553434828\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36499282  55.32088271  48.88700648 106.88861357\n",
      "  76.80708623 123.1570159   80.14457136  33.40178978]\n",
      "42-th iteration, loss: 0.36328072847891435, 9 gd steps\n",
      "insert gradient: -0.00015226300777102178\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43648006e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53212081e+01 4.88869510e+01 1.06888519e+02 7.68071592e+01\n",
      " 1.23157216e+02 8.01445734e+01 3.34017898e+01]\n",
      "43-th iteration, loss: 0.3632807276842549, 9 gd steps\n",
      "insert gradient: -0.00014648278944985748\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43646068e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53218633e+01 4.88868956e+01 1.06888424e+02 7.68072296e+01\n",
      " 1.23157416e+02 8.01445736e+01 3.34017898e+01]\n",
      "44-th iteration, loss: 0.3632807269185948, 9 gd steps\n",
      "insert gradient: -0.00014158010992284162\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43644067e+01 0.00000000e+00 9.32587341e-15\n",
      " 5.53224950e+01 4.88868339e+01 1.06888331e+02 7.68073024e+01\n",
      " 1.23157616e+02 8.01445759e+01 3.34017898e+01]\n",
      "45-th iteration, loss: 0.3632807261788969, 9 gd steps\n",
      "insert gradient: -0.00013735424531462545\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43642023e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.53231064e+01 4.88867678e+01 1.06888240e+02 7.68073765e+01\n",
      " 1.23157815e+02 8.01445797e+01 3.34017898e+01]\n",
      "46-th iteration, loss: 0.36328072546311496, 9 gd steps\n",
      "insert gradient: -0.00013365299946282208\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43639953e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.53237004e+01 4.88866991e+01 1.06888152e+02 7.68074513e+01\n",
      " 1.23158014e+02 8.01445845e+01 3.34017898e+01]\n",
      "47-th iteration, loss: 0.3632807247697769, 9 gd steps\n",
      "insert gradient: -0.00013036067141335543\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43637867e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.53242788e+01 4.88866288e+01 1.06888067e+02 7.68075260e+01\n",
      " 1.23158211e+02 8.01445897e+01 3.34017898e+01]\n",
      "48-th iteration, loss: 0.36328072409774714, 9 gd steps\n",
      "insert gradient: -0.00012738911903423234\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36357757  55.32484336  48.8865578  106.88798388\n",
      "  76.80760038 123.15840635  80.1445952   33.40178978]\n",
      "49-th iteration, loss: 0.36328072364766073, 9 gd steps\n",
      "insert gradient: -0.00013032127304911056\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36337662  55.32512031  48.886493   106.88790527\n",
      "  76.80767169 123.15859661  80.14459934  33.40178978]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536822723274187\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.09598192   0.         533.79346532]\n",
      "1-th iteration, loss: 0.7485404418159318, 11 gd steps\n",
      "insert gradient: -0.6320778488893469\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.67444096  62.33745299 234.34835063   0.         299.44511469]\n",
      "2-th iteration, loss: 0.6043688060073337, 13 gd steps\n",
      "insert gradient: -0.6651681320057647\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.41754116  77.5322004  219.74073292  41.69768811 244.44499158\n",
      "   0.          55.00012311]\n",
      "3-th iteration, loss: 0.4744786338070808, 20 gd steps\n",
      "insert gradient: -0.38358074035489387\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  0.          58.91200117 227.21459818  54.34099352 162.55212847\n",
      "  39.79964427  55.00012311]\n",
      "4-th iteration, loss: 0.4327268984789332, 27 gd steps\n",
      "insert gradient: -0.42694671369981374\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  2.16084314  45.38787432 103.0800309    0.         109.14356213\n",
      "  58.65432342 134.10306458  58.60436403  55.00012311]\n",
      "5-th iteration, loss: 0.3636004250564142, 44 gd steps\n",
      "insert gradient: -0.017160766387702326\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          45.8918658   52.60982744  49.13185397 107.38323304\n",
      "  77.73819609 122.77476021  79.37330352  55.00012311]\n",
      "6-th iteration, loss: 0.36328118383301444, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.30572675  55.33093059  48.88471244 106.93338137\n",
      "  76.79115984 123.15566191  80.0881066   55.00012311]\n",
      "7-th iteration, loss: 0.3632807594935392, 25 gd steps\n",
      "insert gradient: -0.00035353534534524555\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.33948658  55.34351604  48.88420855 106.92079118\n",
      "  76.79820106 123.17280434  80.12516806  55.00012311]\n",
      "8-th iteration, loss: 0.363280754566274, 13 gd steps\n",
      "insert gradient: -0.00033068395036597035\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.34019634  55.34396033  48.88385126 106.92001463\n",
      "  76.79834135 123.17318158  80.12612734  55.00012311]\n",
      "9-th iteration, loss: 0.3632807503049763, 12 gd steps\n",
      "insert gradient: -0.0003102383718605218\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.34083232  55.34437373  48.88352348 106.91928551\n",
      "  76.79847112 123.17351771  80.12699074  55.00012311]\n",
      "10-th iteration, loss: 0.3632807465720773, 12 gd steps\n",
      "insert gradient: -0.00029175406906093706\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43414082e+01 5.53447621e+01 4.88832210e+01\n",
      " 1.06918596e+02 7.67985931e+01 1.23173820e+02 0.00000000e+00\n",
      " 6.57252031e-14 8.01277759e+01 5.50001231e+01]\n",
      "11-th iteration, loss: 0.36328074178519826, 12 gd steps\n",
      "insert gradient: -0.00025163872991262756\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43419851e+01 5.53451698e+01 4.88829197e+01\n",
      " 1.06917877e+02 7.67987077e+01 1.23174108e+02 7.58395520e-04\n",
      " 2.77442043e-04 8.01285380e+01 5.50001231e+01]\n",
      "12-th iteration, loss: 0.3632807380844695, 12 gd steps\n",
      "insert gradient: -0.00022090460170673672\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43425014e+01 5.53455574e+01 4.88826628e+01\n",
      " 1.06917213e+02 7.67987911e+01 1.23174331e+02 1.36355668e-03\n",
      " 4.57131178e-04 8.01291587e+01 5.50001231e+01]\n",
      "13-th iteration, loss: 0.3632807350720144, 11 gd steps\n",
      "insert gradient: -0.00019637373554209822\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43429696e+01 5.53459268e+01 4.88824345e+01\n",
      " 1.06916590e+02 7.67988588e+01 1.23174513e+02 1.86513644e-03\n",
      " 5.70269618e-04 8.01296828e+01 5.50001231e+01]\n",
      "14-th iteration, loss: 0.3632807325337406, 11 gd steps\n",
      "insert gradient: -0.00017622728708258385\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43433975e+01 5.53462798e+01 4.88822261e+01\n",
      " 1.06915998e+02 7.67989192e+01 1.23174666e+02 2.29199060e-03\n",
      " 6.34822357e-04 0.00000000e+00 1.55854062e-19 8.01301360e+01\n",
      " 5.50001231e+01]\n",
      "15-th iteration, loss: 0.36328072996538613, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43438019e+01 5.53466286e+01 4.88820299e+01\n",
      " 1.06915416e+02 7.67989746e+01 1.23174797e+02 2.66254235e-03\n",
      " 6.58604534e-04 3.99505687e-04 1.98525926e-05 8.01305358e+01\n",
      " 5.50001231e+01]\n",
      "16-th iteration, loss: 0.3632807278474141, 10 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43441736e+01 5.53469644e+01 4.88818532e+01\n",
      " 1.06914863e+02 7.67990233e+01 1.23174901e+02 2.96304333e-03\n",
      " 6.43941823e-04 8.01315945e+01 5.50001231e+01]\n",
      "17-th iteration, loss: 0.36328072623289775, 10 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43445108e+01 5.53472814e+01 4.88816909e+01\n",
      " 1.06914341e+02 7.67990729e+01 1.23174990e+02 3.21636175e-03\n",
      " 6.05045568e-04 8.01318747e+01 5.50001231e+01]\n",
      "18-th iteration, loss: 0.36328072477029816, 10 gd steps\n",
      "insert gradient: -0.00012852327025058823\n",
      "18-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43448213e+01 0.00000000e+00 7.99360578e-15\n",
      " 5.53475838e+01 4.88815349e+01 1.06913836e+02 7.67991298e+01\n",
      " 1.23175073e+02 3.44747100e-03 5.51165389e-04 8.01321307e+01\n",
      " 5.50001231e+01]\n",
      "19-th iteration, loss: 0.3632807230718496, 10 gd steps\n",
      "insert gradient: -0.0001119943018418353\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43450951e+01 2.79046693e-04 2.71851541e-04\n",
      " 0.00000000e+00 7.11507676e-20 5.53478648e+01 4.88813643e+01\n",
      " 1.06913332e+02 7.67991982e+01 1.23175154e+02 3.66785002e-03\n",
      " 4.84358044e-04 8.01323740e+01 5.50001231e+01]\n",
      "20-th iteration, loss: 0.363280721439458, 10 gd steps\n",
      "insert gradient: -0.00010188992624850231\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43453004e+01 5.04006954e-04 4.65869531e-04\n",
      " 2.35830548e-04 1.92334619e-04 5.53481020e+01 4.88811639e+01\n",
      " 1.06912839e+02 7.67992862e+01 1.23175239e+02 3.88093877e-03\n",
      " 4.12145796e-04 8.01326070e+01 5.50001231e+01]\n",
      "21-th iteration, loss: 0.3632807201098867, 10 gd steps\n",
      "insert gradient: -9.668604792076754e-05\n",
      "21-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43454456e+01 6.78705904e-04 5.92752057e-04\n",
      " 4.27259765e-04 3.09906785e-04 5.53483009e+01 4.88809481e+01\n",
      " 1.06912365e+02 7.67993915e+01 1.23175326e+02 4.08427806e-03\n",
      " 3.36482142e-04 8.01328268e+01 5.50001231e+01]\n",
      "22-th iteration, loss: 0.3632807189486653, 10 gd steps\n",
      "insert gradient: -9.066542620647246e-05\n",
      "22-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43455563e+01 8.24275703e-04 6.79703997e-04\n",
      " 5.93337886e-04 3.81422890e-04 5.53484779e+01 4.88807326e+01\n",
      " 1.06911908e+02 7.67995067e+01 1.23175412e+02 4.27644914e-03\n",
      " 2.53687213e-04 0.00000000e+00 4.74338450e-20 8.01330321e+01\n",
      " 5.50001231e+01]\n",
      "23-th iteration, loss: 0.36328071781295657, 10 gd steps\n",
      "insert gradient: -7.984227716176272e-05\n",
      "23-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43456473e+01 9.53514101e-04 7.42192894e-04\n",
      " 7.45791497e-04 4.23038651e-04 5.53486434e+01 4.88805246e+01\n",
      " 1.06911464e+02 7.67996271e+01 1.23175495e+02 4.45541197e-03\n",
      " 1.60602984e-04 0.00000000e+00 3.38813179e-21 8.01334094e+01\n",
      " 5.50001231e+01]\n",
      "24-th iteration, loss: 0.36328071680044843, 9 gd steps\n",
      "insert gradient: -7.051683303874378e-05\n",
      "24-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43457262e+01 1.07292546e-03 7.88699097e-04\n",
      " 8.90162487e-04 4.43876485e-04 5.53488020e+01 4.88803310e+01\n",
      " 1.06911035e+02 7.67997476e+01 1.23175571e+02 4.61433406e-03\n",
      " 5.56015092e-05 8.01337387e+01 5.50001231e+01]\n",
      "25-th iteration, loss: 0.3632807159551344, 9 gd steps\n",
      "insert gradient: -6.540098052394057e-05\n",
      "25-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43457960e+01 1.18413617e-03 8.22551269e-04\n",
      " 1.02748230e-03 4.47683904e-04 5.53489539e+01 4.88801524e+01\n",
      " 1.06910623e+02 7.67998689e+01 1.23175643e+02 8.01386431e+01\n",
      " 5.50001231e+01]\n",
      "26-th iteration, loss: 0.363280715239094, 9 gd steps\n",
      "insert gradient: -6.331802648027066e-05\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43458584e+01 1.28827158e-03 8.45853555e-04\n",
      " 1.15852038e-03 4.36802552e-04 5.53490995e+01 4.88799872e+01\n",
      " 1.06910225e+02 7.67999916e+01 1.23175712e+02 8.01387786e+01\n",
      " 5.50001231e+01]\n",
      "27-th iteration, loss: 0.36328071456801236, 9 gd steps\n",
      "insert gradient: -6.110273731478598e-05\n",
      "27-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43459155e+01 1.38706140e-03 8.60694298e-04\n",
      " 1.28482831e-03 4.13358564e-04 5.53492398e+01 4.88798339e+01\n",
      " 1.06909840e+02 7.68001167e+01 1.23175781e+02 0.00000000e+00\n",
      " 1.42108547e-14 8.01389091e+01 5.50001231e+01]\n",
      "28-th iteration, loss: 0.36328071388579486, 9 gd steps\n",
      "insert gradient: -6.06470261932821e-05\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43459701e+01 1.48349450e-03 8.69739811e-04\n",
      " 1.40924540e-03 3.80008816e-04 0.00000000e+00 1.35525272e-19\n",
      " 5.53493775e+01 4.88796927e+01 1.06909464e+02 7.68002421e+01\n",
      " 1.23175847e+02 1.22995011e-04 6.51678773e-05 8.01390326e+01\n",
      " 5.50001231e+01]\n",
      "29-th iteration, loss: 0.3632807132084558, 9 gd steps\n",
      "insert gradient: -5.953918858412758e-05\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43460211e+01 1.57735289e-03 8.72098104e-04\n",
      " 1.53133696e-03 3.35964997e-04 5.53496459e+01 4.88795617e+01\n",
      " 1.06909098e+02 0.00000000e+00 2.66453526e-14 7.68003654e+01\n",
      " 1.23175908e+02 2.31972454e-04 1.20499227e-04 8.01391445e+01\n",
      " 5.50001231e+01]\n",
      "30-th iteration, loss: 0.3632807125805538, 9 gd steps\n",
      "insert gradient: -5.879494489921981e-05\n",
      "30-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43460706e+01 1.67054616e-03 8.70113299e-04\n",
      " 1.65270177e-03 2.83824220e-04 5.53497778e+01 4.88794428e+01\n",
      " 1.06908741e+02 7.68006068e+01 1.23175964e+02 3.28522218e-04\n",
      " 1.67156230e-04 8.01392462e+01 5.50001231e+01]\n",
      "31-th iteration, loss: 0.36328071202785583, 9 gd steps\n",
      "insert gradient: -5.787716904038953e-05\n",
      "31-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43461211e+01 1.76558081e-03 8.66405822e-04\n",
      " 1.77568897e-03 2.26318263e-04 5.53499096e+01 4.88793359e+01\n",
      " 1.06908394e+02 7.68007182e+01 1.23176014e+02 4.12045165e-04\n",
      " 2.03665859e-04 8.01393365e+01 5.50001231e+01]\n",
      "32-th iteration, loss: 0.36328071150498886, 9 gd steps\n",
      "insert gradient: -5.6834749195474314e-05\n",
      "32-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43461712e+01 1.86045910e-03 8.59293741e-04\n",
      " 1.89832118e-03 1.61586935e-04 0.00000000e+00 3.38813179e-21\n",
      " 5.53500392e+01 4.88792393e+01 1.06908055e+02 7.68008293e+01\n",
      " 1.23176062e+02 4.86674288e-04 2.34209180e-04 8.01394193e+01\n",
      " 5.50001231e+01]\n",
      "33-th iteration, loss: 0.3632807109685633, 9 gd steps\n",
      "insert gradient: -5.520326844665313e-05\n",
      "33-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43462185e+01 1.95329138e-03 8.46392867e-04\n",
      " 2.01867803e-03 8.70858332e-05 5.53502895e+01 4.88791494e+01\n",
      " 1.06907725e+02 0.00000000e+00 2.66453526e-14 7.68009411e+01\n",
      " 1.23176107e+02 5.54383955e-04 2.60159968e-04 8.01394964e+01\n",
      " 5.50001231e+01]\n",
      "34-th iteration, loss: 0.363280710460357, 9 gd steps\n",
      "insert gradient: -5.406105533289147e-05\n",
      "34-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43462648e+01 2.04576466e-03 8.29626089e-04\n",
      " 2.13817416e-03 5.01382859e-06 5.53504113e+01 4.88790676e+01\n",
      " 1.06907403e+02 7.68011640e+01 1.23176151e+02 6.15769315e-04\n",
      " 2.81887644e-04 8.01395681e+01 5.50001231e+01]\n",
      "35-th iteration, loss: 0.36328071002923984, 9 gd steps\n",
      "insert gradient: -5.179546087929864e-05\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43463110e+01 2.13900919e-03 8.10192627e-04\n",
      " 5.53527888e+01 4.88789936e+01 1.06907089e+02 0.00000000e+00\n",
      " 1.24344979e-14 7.68012685e+01 1.23176191e+02 6.69499512e-04\n",
      " 2.97448738e-04 8.01396328e+01 5.50001231e+01]\n",
      "36-th iteration, loss: 0.36328070962688075, 9 gd steps\n",
      "insert gradient: -5.186917874100472e-05\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43463545e+01 2.23019818e-03 7.85206223e-04\n",
      " 0.00000000e+00 2.16840434e-19 5.53529056e+01 4.88789264e+01\n",
      " 1.06906782e+02 7.68014760e+01 1.23176229e+02 7.18085516e-04\n",
      " 3.09389974e-04 8.01396930e+01 5.50001231e+01]\n",
      "37-th iteration, loss: 0.3632807092396791, 9 gd steps\n",
      "insert gradient: -4.8834526280265624e-05\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43463955e+01 2.31982335e-03 7.54910047e-04\n",
      " 5.53531344e+01 4.88788641e+01 1.06906481e+02 7.68015737e+01\n",
      " 1.23176264e+02 7.60740712e-04 3.16173384e-04 8.01397477e+01\n",
      " 5.50001231e+01]\n",
      "38-th iteration, loss: 0.36328070890153824, 9 gd steps\n",
      "insert gradient: -4.86495924196919e-05\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43464336e+01 2.40684707e-03 7.19192924e-04\n",
      " 5.53532451e+01 4.88788073e+01 1.06906188e+02 7.68016731e+01\n",
      " 1.23176298e+02 8.00407856e-04 3.21215239e-04 8.01397996e+01\n",
      " 5.50001231e+01]\n",
      "39-th iteration, loss: 0.363280708578623, 9 gd steps\n",
      "insert gradient: -4.854650420931303e-05\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43464720e+01 2.49396837e-03 6.81007541e-04\n",
      " 5.53533548e+01 4.88787579e+01 1.06905902e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68017721e+01 1.23176332e+02 8.36143651e-04\n",
      " 3.23478736e-04 8.01398477e+01 5.50001231e+01]\n",
      "40-th iteration, loss: 0.3632807082441444, 9 gd steps\n",
      "insert gradient: -4.819329709240638e-05\n",
      "40-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465111e+01 2.58197904e-03 6.40776548e-04\n",
      " 0.00000000e+00 5.42101086e-20 5.53534641e+01 4.88787159e+01\n",
      " 1.06905622e+02 7.68019661e+01 1.23176363e+02 8.68039728e-04\n",
      " 3.22710649e-04 8.01398921e+01 5.50001231e+01]\n",
      "41-th iteration, loss: 0.3632807079196444, 9 gd steps\n",
      "insert gradient: -4.6407149921869005e-05\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465478e+01 2.66847380e-03 5.95442080e-04\n",
      " 5.53536772e+01 4.88786767e+01 1.06905346e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68020582e+01 1.23176393e+02 8.96560686e-04\n",
      " 3.18632939e-04 8.01399330e+01 5.50001231e+01]\n",
      "42-th iteration, loss: 0.36328070761140924, 9 gd steps\n",
      "insert gradient: -4.5466067103646514e-05\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465826e+01 2.75346859e-03 5.45596083e-04\n",
      " 0.00000000e+00 4.74338450e-20 5.53537807e+01 4.88786418e+01\n",
      " 1.06905077e+02 7.68022434e+01 1.23176422e+02 9.23624228e-04\n",
      " 3.13532571e-04 8.01399723e+01 5.50001231e+01]\n",
      "43-th iteration, loss: 0.363280707311927, 9 gd steps\n",
      "insert gradient: -4.44671530807825e-05\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43466160e+01 2.83768504e-03 4.91715929e-04\n",
      " 5.53539830e+01 4.88786100e+01 1.06904813e+02 7.68023314e+01\n",
      " 1.23176450e+02 9.47979913e-04 3.05619433e-04 8.01400087e+01\n",
      " 5.50001231e+01]\n",
      "44-th iteration, loss: 0.36328070704712434, 9 gd steps\n",
      "insert gradient: -4.451023231870737e-05\n",
      "44-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43466473e+01 2.91992891e-03 4.33258952e-04\n",
      " 5.53540804e+01 4.88785813e+01 1.06904555e+02 0.00000000e+00\n",
      " 3.55271368e-14 7.68024215e+01 1.23176479e+02 9.71922958e-04\n",
      " 2.97656737e-04 8.01400443e+01 5.50001231e+01]\n",
      "45-th iteration, loss: 0.36328070677029495, 9 gd steps\n",
      "insert gradient: -4.241424665442912e-05\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43466802e+01 3.00381243e-03 3.73844813e-04\n",
      " 5.53541776e+01 4.88785587e+01 1.06904302e+02 7.68025988e+01\n",
      " 1.23176506e+02 9.93735459e-04 2.87823485e-04 8.01400774e+01\n",
      " 5.50001231e+01]\n",
      "46-th iteration, loss: 0.3632807065228354, 9 gd steps\n",
      "insert gradient: -4.17196026720187e-05\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43467140e+01 3.08890473e-03 3.12782847e-04\n",
      " 5.53542742e+01 4.88785404e+01 1.06904054e+02 7.68026826e+01\n",
      " 1.23176532e+02 1.01321662e-03 2.75425052e-04 8.01401079e+01\n",
      " 5.50001231e+01]\n",
      "47-th iteration, loss: 0.3632807062829413, 9 gd steps\n",
      "insert gradient: -4.2075211262081945e-05\n",
      "47-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43467476e+01 3.17391762e-03 2.48948849e-04\n",
      " 5.53543687e+01 4.88785257e+01 1.06903812e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68027671e+01 1.23176558e+02 1.03210454e-03\n",
      " 2.62532350e-04 8.01401372e+01 5.50001231e+01]\n",
      "48-th iteration, loss: 0.3632807060310395, 9 gd steps\n",
      "insert gradient: -4.043766130770622e-05\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43467819e+01 3.25985961e-03 1.83061311e-04\n",
      " 5.53544621e+01 4.88785152e+01 1.06903574e+02 7.68029346e+01\n",
      " 1.23176584e+02 1.04997596e-03 2.48537504e-04 8.01401651e+01\n",
      " 5.50001231e+01]\n",
      "49-th iteration, loss: 0.3632807058044003, 9 gd steps\n",
      "insert gradient: -3.979847784464311e-05\n",
      "49-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43468164e+01 3.34663333e-03 1.14868433e-04\n",
      " 5.53545543e+01 4.88785074e+01 1.06903341e+02 0.00000000e+00\n",
      " 3.73034936e-14 7.68030141e+01 1.23176608e+02 1.06643503e-03\n",
      " 2.32623493e-04 8.01401909e+01 5.50001231e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536156841022466\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         559.42362414]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6275936673118951\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 231.95613684   0.         327.4674873 ]\n",
      "2-th iteration, loss: 0.603911243450414, 13 gd steps\n",
      "insert gradient: -0.6477050526781667\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.66084701  77.86244622 217.86303052  42.20336436 240.58835802\n",
      "   0.          86.87912928]\n",
      "3-th iteration, loss: 0.4421809060638743, 40 gd steps\n",
      "insert gradient: -0.3189308620588181\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.34781212  44.15352399 102.77448888   0.         108.82004705\n",
      "  66.83844686 131.81653144  48.32776688  86.87912928]\n",
      "4-th iteration, loss: 0.36856492606656277, 35 gd steps\n",
      "insert gradient: -0.24451322888032895\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.83525779e+01 4.87161030e+01 5.01821371e+01\n",
      " 1.03820541e+02 0.00000000e+00 3.46389584e-14 7.18575569e+01\n",
      " 1.19813475e+02 7.88336367e+01 8.68791293e+01]\n",
      "5-th iteration, loss: 0.36434708536548227, 9 gd steps\n",
      "insert gradient: -0.047786446270797665\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.74520434e+01 0.00000000e+00 6.21724894e-15\n",
      " 4.81949361e+01 4.95128836e+01 1.04489400e+02 2.35286478e+00\n",
      " 6.13019658e-01 7.42247603e+01 1.20926606e+02 7.96985737e+01\n",
      " 8.68791293e+01]\n",
      "6-th iteration, loss: 0.3633293463627884, 30 gd steps\n",
      "insert gradient: -0.002811968722227069\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.51358182e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.38492351e+01 4.93389830e+01 1.06115864e+02 7.70990222e+01\n",
      " 1.22385819e+02 8.02946170e+01 8.68791293e+01]\n",
      "7-th iteration, loss: 0.36328131040019135, 27 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.44345776827212e-16\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43384875e+01 5.55028241e+01 4.88273666e+01\n",
      " 1.06943274e+02 7.68411778e+01 1.23155766e+02 8.02015908e+01\n",
      " 0.00000000e+00 1.86517468e-14 8.68791293e+01]\n",
      "8-th iteration, loss: 0.36328103950629637, 59 gd steps\n",
      "insert gradient: -0.0003359099819763673\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43229448e+01 5.54862200e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88314066e+01 1.06934212e+02 7.68283652e+01\n",
      " 1.23154182e+02 8.01843375e+01 8.68791293e+01]\n",
      "9-th iteration, loss: 0.36328099884917386, 32 gd steps\n",
      "insert gradient: -0.0002532482454171819\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32057437  55.48205155  48.83579025 106.93254259\n",
      "  76.82626662 123.15464357  80.18082396  86.87912928]\n",
      "10-th iteration, loss: 0.36328097728964837, 24 gd steps\n",
      "insert gradient: -0.00028256754447840687\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31919188  55.47920685  48.83719492 106.93146264\n",
      "  76.82507279 123.15511724  80.17867405  86.87912928]\n",
      "11-th iteration, loss: 0.36328096109120184, 21 gd steps\n",
      "insert gradient: -0.000292519314638274\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43182561e+01 5.54768776e+01 0.00000000e+00\n",
      " 1.33226763e-14 4.88384863e+01 1.06930616e+02 7.68241682e+01\n",
      " 1.23155585e+02 8.01769767e+01 8.68791293e+01]\n",
      "12-th iteration, loss: 0.3632809452097667, 20 gd steps\n",
      "insert gradient: -0.00023596956171920605\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43174636e+01 5.54746539e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88408338e+01 1.06929819e+02 7.68233880e+01\n",
      " 1.23156088e+02 8.01754621e+01 8.68791293e+01]\n",
      "13-th iteration, loss: 0.3632809324276927, 18 gd steps\n",
      "insert gradient: -0.00020390670549601242\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31679073  55.47265098  48.84261881 106.9290962\n",
      "  76.82276554 123.15657891  80.1742091   86.87912928]\n",
      "14-th iteration, loss: 0.36328092268900075, 16 gd steps\n",
      "insert gradient: -0.00022050747282701498\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43162642e+01 5.54708791e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88433833e+01 1.06928459e+02 7.68222512e+01\n",
      " 1.23157043e+02 8.01731473e+01 8.68791293e+01]\n",
      "15-th iteration, loss: 0.3632809129020029, 16 gd steps\n",
      "insert gradient: -0.0001905551635727278\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43158323e+01 5.54691750e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88448826e+01 1.06927847e+02 7.68217732e+01\n",
      " 1.23157512e+02 8.01721515e+01 8.68791293e+01]\n",
      "16-th iteration, loss: 0.36328090427873916, 15 gd steps\n",
      "insert gradient: -0.00017046702602091515\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43154551e+01 5.54675668e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88461479e+01 1.06927263e+02 7.68213529e+01\n",
      " 1.23157970e+02 8.01712597e+01 8.68791293e+01]\n",
      "17-th iteration, loss: 0.3632808965260592, 15 gd steps\n",
      "insert gradient: -0.0001559253758885402\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43151324e+01 5.54660401e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88472573e+01 1.06926703e+02 7.68209741e+01\n",
      " 1.23158418e+02 8.01704459e+01 8.68791293e+01]\n",
      "18-th iteration, loss: 0.36328088947477316, 14 gd steps\n",
      "insert gradient: -0.00014603112327446558\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43148612e+01 5.54645861e+01 4.88482528e+01\n",
      " 1.06926165e+02 7.68206276e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23158856e+02 8.01696952e+01 8.68791293e+01]\n",
      "19-th iteration, loss: 0.3632808829831059, 14 gd steps\n",
      "insert gradient: -0.0001611621120849146\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3146465   55.46319136  48.8487349  106.92564602\n",
      "  76.82029844 123.15970983  80.168983    86.87912928]\n",
      "20-th iteration, loss: 0.36328087744844206, 13 gd steps\n",
      "insert gradient: -0.00016964194126953096\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43145116e+01 5.54619125e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88492307e+01 1.06925168e+02 7.68199842e+01\n",
      " 1.23160103e+02 8.01683172e+01 8.68791293e+01]\n",
      "21-th iteration, loss: 0.3632808717316926, 13 gd steps\n",
      "insert gradient: -0.00014845715852859726\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31441526  55.46065799  48.85021379 106.92469835\n",
      "  76.81968261 123.16049733  80.16767567  86.87912928]\n",
      "22-th iteration, loss: 0.36328086685378946, 13 gd steps\n",
      "insert gradient: -0.00015631309812795958\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31433975  55.45946148  48.85065655 106.92424969\n",
      "  76.81940945 123.16088036  80.16708458  86.87912928]\n",
      "23-th iteration, loss: 0.36328086226341977, 13 gd steps\n",
      "insert gradient: -0.00016092268960109148\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43143075e+01 5.54583144e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88511093e+01 1.06923821e+02 7.68191496e+01\n",
      " 1.23161255e+02 8.01665207e+01 8.68791293e+01]\n",
      "24-th iteration, loss: 0.36328085745999283, 13 gd steps\n",
      "insert gradient: -0.00013954800671630324\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43142995e+01 5.54571803e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88519981e+01 1.06923396e+02 7.68188978e+01\n",
      " 1.23161631e+02 8.01659736e+01 8.68791293e+01]\n",
      "25-th iteration, loss: 0.3632808529719748, 12 gd steps\n",
      "insert gradient: -0.00013945748221981023\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43142917e+01 5.54560694e+01 4.88527727e+01\n",
      " 1.06922975e+02 7.68186646e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23162001e+02 8.01654598e+01 8.68791293e+01]\n",
      "26-th iteration, loss: 0.36328084866666666, 12 gd steps\n",
      "insert gradient: -0.00013314414990381757\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31429825  55.45497783  48.85313725 106.92255856\n",
      "  76.81843966 123.16272898  80.16496395  86.87912928]\n",
      "27-th iteration, loss: 0.36328084491301615, 12 gd steps\n",
      "insert gradient: -0.00013734674927798582\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43143399e+01 5.54539506e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88535081e+01 1.06922164e+02 7.68182189e+01\n",
      " 1.23163069e+02 8.01644890e+01 8.68791293e+01]\n",
      "28-th iteration, loss: 0.36328084101485253, 12 gd steps\n",
      "insert gradient: -0.00013203559606121473\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43144000e+01 5.54529364e+01 4.88542400e+01\n",
      " 1.06921773e+02 7.68180048e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23163408e+02 8.01640272e+01 8.68791293e+01]\n",
      "29-th iteration, loss: 0.3632808372721075, 12 gd steps\n",
      "insert gradient: -0.00012606855344675114\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43144640e+01 5.54519347e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88545780e+01 1.06921385e+02 7.68178001e+01\n",
      " 1.23164076e+02 8.01635822e+01 8.68791293e+01]\n",
      "30-th iteration, loss: 0.3632808337198407, 12 gd steps\n",
      "insert gradient: -0.00012624227376866192\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43145473e+01 5.54509692e+01 4.88552423e+01\n",
      " 1.06921007e+02 7.68175968e+01 0.00000000e+00 2.57571742e-14\n",
      " 1.23164394e+02 8.01631503e+01 8.68791293e+01]\n",
      "31-th iteration, loss: 0.3632808302818741, 12 gd steps\n",
      "insert gradient: -0.00012061063502990328\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31463316  55.45001353  48.8555504  106.92063091\n",
      "  76.81740238 123.1650223   80.16273321  86.87912928]\n",
      "32-th iteration, loss: 0.3632808272397968, 11 gd steps\n",
      "insert gradient: -0.00011985729418248936\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43147398e+01 5.54491035e+01 4.88558624e+01\n",
      " 1.06920271e+02 7.68172117e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23165319e+02 8.01623310e+01 8.68791293e+01]\n",
      "33-th iteration, loss: 0.3632808240808627, 11 gd steps\n",
      "insert gradient: -0.00012155212906336102\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43148639e+01 5.54482044e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88561821e+01 1.06919917e+02 7.68170243e+01\n",
      " 1.23165906e+02 8.01619349e+01 8.68791293e+01]\n",
      "34-th iteration, loss: 0.3632808210356492, 11 gd steps\n",
      "insert gradient: -0.00011557247981400184\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43149966e+01 5.54473281e+01 4.88567999e+01\n",
      " 1.06919568e+02 7.68168396e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.23166187e+02 8.01615497e+01 8.68791293e+01]\n",
      "35-th iteration, loss: 0.3632808180974856, 11 gd steps\n",
      "insert gradient: -0.00011074783500830144\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43151243e+01 5.54464573e+01 4.88570818e+01\n",
      " 1.06919220e+02 7.68166645e+01 0.00000000e+00 2.22044605e-14\n",
      " 1.23166746e+02 8.01611788e+01 8.68791293e+01]\n",
      "36-th iteration, loss: 0.3632808152762058, 11 gd steps\n",
      "insert gradient: -0.00011084780381019345\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31526753  55.4456119   48.8573689  106.91888092\n",
      "  76.8164887  123.16727742  80.16081317  86.87912928]\n",
      "37-th iteration, loss: 0.3632808127337167, 11 gd steps\n",
      "insert gradient: -0.00011106254103097924\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43154202e+01 5.54447961e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88576545e+01 1.06918553e+02 7.68163175e+01\n",
      " 1.23167531e+02 8.01604596e+01 8.68791293e+01]\n",
      "38-th iteration, loss: 0.3632808100931959, 11 gd steps\n",
      "insert gradient: -0.00010666803409737156\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43155770e+01 5.54439850e+01 4.88582085e+01\n",
      " 1.06918227e+02 7.68161535e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23167785e+02 8.01601161e+01 8.68791293e+01]\n",
      "39-th iteration, loss: 0.36328080753572206, 11 gd steps\n",
      "insert gradient: -0.00010227269653319789\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43157270e+01 5.54431774e+01 4.88584606e+01\n",
      " 1.06917902e+02 7.68159981e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23168288e+02 8.01597849e+01 8.68791293e+01]\n",
      "40-th iteration, loss: 0.3632808050714135, 11 gd steps\n",
      "insert gradient: -0.000100572997621823\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31588861  55.44239084  48.85871709 106.91758465\n",
      "  76.81584188 123.1687688   80.1594579   86.87912928]\n",
      "41-th iteration, loss: 0.3632808028342358, 11 gd steps\n",
      "insert gradient: -0.00010068984572209167\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43160570e+01 5.54416287e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88589726e+01 1.06917276e+02 7.68156893e+01\n",
      " 1.23168999e+02 8.01591404e+01 8.68791293e+01]\n",
      "42-th iteration, loss: 0.3632808005196922, 11 gd steps\n",
      "insert gradient: -9.838514779509691e-05\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31622821  55.44087134  48.85946885 106.91697026\n",
      "  76.8155431  123.16922822  80.15883184  86.87912928]\n",
      "43-th iteration, loss: 0.36328079840998445, 10 gd steps\n",
      "insert gradient: -9.750694866231722e-05\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43163897e+01 5.54401226e+01 4.88596921e+01\n",
      " 1.06916669e+02 7.68154077e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23169456e+02 8.01585385e+01 8.68791293e+01]\n",
      "44-th iteration, loss: 0.36328079622687326, 10 gd steps\n",
      "insert gradient: -9.350715353499429e-05\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43165606e+01 5.54393824e+01 4.88599205e+01\n",
      " 1.06916372e+02 7.68152744e+01 0.00000000e+00 9.76996262e-15\n",
      " 1.23169905e+02 8.01582493e+01 8.68791293e+01]\n",
      "45-th iteration, loss: 0.36328079411576086, 10 gd steps\n",
      "insert gradient: -9.187327418637054e-05\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31673925  55.43865899  48.86015135 106.91607933\n",
      "  76.81514017 123.17033349  80.15796303  86.87912928]\n",
      "46-th iteration, loss: 0.3632807921850481, 10 gd steps\n",
      "insert gradient: -9.158407037620192e-05\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3169217   55.43795488  48.86038086 106.91579453\n",
      "  76.81500848 123.17053923  80.15768404  86.87912928]\n",
      "47-th iteration, loss: 0.3632807903064654, 10 gd steps\n",
      "insert gradient: -9.107837941036803e-05\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43171067e+01 5.54372607e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88606086e+01 1.06915516e+02 7.68148831e+01\n",
      " 1.23170743e+02 8.01574138e+01 8.68791293e+01]\n",
      "48-th iteration, loss: 0.3632807883647237, 10 gd steps\n",
      "insert gradient: -8.911686293079565e-05\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31729232  55.43657047  48.86104932 106.91523828\n",
      "  76.81476243 123.17094635  80.15715069  86.87912928]\n",
      "49-th iteration, loss: 0.3632807865815689, 10 gd steps\n",
      "insert gradient: -8.814876160511429e-05\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31746731  55.43588528  48.86124728 106.91496379\n",
      "  76.81465014 123.17114785  80.15689937  86.87912928]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010547\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.4077617    0.         582.73294182]\n",
      "1-th iteration, loss: 0.7509319290812664, 11 gd steps\n",
      "insert gradient: -0.6370669541234621\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 234.51447659   0.         348.21846523]\n",
      "2-th iteration, loss: 0.6035005154945375, 13 gd steps\n",
      "insert gradient: -0.6775126096940864\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.02206677  77.25687273 219.71899216  41.94158814 241.62097588\n",
      "   0.         106.59748936]\n",
      "3-th iteration, loss: 0.4681790661645014, 20 gd steps\n",
      "insert gradient: -0.44409795988461637\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.65848109 116.21597545   0.         109.75953237\n",
      "  54.25635415 164.46527626  50.93482395 106.59748936]\n",
      "4-th iteration, loss: 0.37942240653188947, 31 gd steps\n",
      "insert gradient: -0.24890456913373143\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  2.13705981  50.44139368  65.25563306  29.79026524 106.15113396\n",
      "  75.05760259 122.6094457   75.0391451  106.59748936]\n",
      "5-th iteration, loss: 0.36328092285526026, 59 gd steps\n",
      "insert gradient: -0.0003611358955531168\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.38212133  55.25753827  48.90214603 106.91508801\n",
      "  76.78386622 123.13573449  80.15092318 106.59748936]\n",
      "6-th iteration, loss: 0.363280909689953, 18 gd steps\n",
      "insert gradient: -0.00038154703327109165\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43814448e+01 0.00000000e+00 1.28785871e-14\n",
      " 5.52589664e+01 4.89005062e+01 1.06913783e+02 7.67852167e+01\n",
      " 1.23136544e+02 8.01506446e+01 1.06597489e+02]\n",
      "7-th iteration, loss: 0.36328089429655797, 21 gd steps\n",
      "insert gradient: -0.0003787511737662588\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.38088835  55.26223083  48.89906118 106.91246978\n",
      "  76.78652871 123.13735957  80.15030699 106.59748936]\n",
      "8-th iteration, loss: 0.36328088628398597, 15 gd steps\n",
      "insert gradient: -0.00039661279396421476\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43805375e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.52634851e+01 4.88981380e+01 1.06911543e+02 7.67874554e+01\n",
      " 1.23137954e+02 8.01500523e+01 1.06597489e+02]\n",
      "9-th iteration, loss: 0.3632808746659955, 18 gd steps\n",
      "insert gradient: -0.00038013491610884565\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43801661e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.52664028e+01 4.88971882e+01 1.06910512e+02 7.67884622e+01\n",
      " 1.23138621e+02 8.01497546e+01 1.06597489e+02]\n",
      "10-th iteration, loss: 0.36328086453067665, 17 gd steps\n",
      "insert gradient: -0.0003660814884395469\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37978939  55.26904526  48.89635136 106.90958784\n",
      "  76.78938332 123.13924898  80.14949982 106.59748936]\n",
      "11-th iteration, loss: 0.363280858956111, 13 gd steps\n",
      "insert gradient: -0.00037585113975702097\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43795026e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.52701095e+01 4.88957467e+01 1.06908875e+02 7.67900979e+01\n",
      " 1.23139749e+02 8.01493016e+01 1.06597489e+02]\n",
      "12-th iteration, loss: 0.3632808505199352, 16 gd steps\n",
      "insert gradient: -0.00035842888732787203\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43791805e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.52725273e+01 4.88951092e+01 1.06908082e+02 7.67908738e+01\n",
      " 1.23140308e+02 8.01490757e+01 1.06597489e+02]\n",
      "13-th iteration, loss: 0.3632808429302424, 15 gd steps\n",
      "insert gradient: -0.0003441943999034031\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43788384e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.52747528e+01 4.88945159e+01 1.06907349e+02 7.67916045e+01\n",
      " 1.23140848e+02 8.01488798e+01 1.06597489e+02]\n",
      "14-th iteration, loss: 0.3632808360316779, 14 gd steps\n",
      "insert gradient: -0.00033210001403580913\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37848524  55.27682536  48.89396465 106.90666525\n",
      "  76.79229343 123.14136777  80.14870648 106.59748936]\n",
      "15-th iteration, loss: 0.36328083200846994, 12 gd steps\n",
      "insert gradient: -0.00033984396838173547\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37819364  55.27771063  48.89353514 106.90610064\n",
      "  76.79286342 123.14180792  80.14856238 106.59748936]\n",
      "16-th iteration, loss: 0.36328082827404673, 12 gd steps\n",
      "insert gradient: -0.0003443096435642445\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43779396e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52785971e+01 4.88931686e+01 1.06905563e+02 7.67933879e+01\n",
      " 1.23142225e+02 8.01484137e+01 1.06597489e+02]\n",
      "17-th iteration, loss: 0.36328082249542887, 13 gd steps\n",
      "insert gradient: -0.00032747517560017994\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43776502e+01 0.00000000e+00 9.32587341e-15\n",
      " 5.52805437e+01 4.88927787e+01 1.06904975e+02 7.67939479e+01\n",
      " 1.23142683e+02 8.01482507e+01 1.06597489e+02]\n",
      "18-th iteration, loss: 0.36328081717290844, 13 gd steps\n",
      "insert gradient: -0.00031386097879327425\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37733617  55.28235929  48.89239496 106.90441827\n",
      "  76.7944888  123.14313249  80.14810836 106.59748936]\n",
      "19-th iteration, loss: 0.36328081403880863, 11 gd steps\n",
      "insert gradient: -0.0003189867354448534\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37706144  55.28315064  48.8920803  106.90394315\n",
      "  76.79495339 123.14352579  80.147989   106.59748936]\n",
      "20-th iteration, loss: 0.36328081108375054, 11 gd steps\n",
      "insert gradient: -0.0003218245690864562\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43768134e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52839433e+01 4.88918081e+01 1.06903486e+02 7.67953855e+01\n",
      " 1.23143903e+02 8.01478660e+01 1.06597489e+02]\n",
      "21-th iteration, loss: 0.36328080648965116, 12 gd steps\n",
      "insert gradient: -0.00030611156990458037\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43765337e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.52856522e+01 4.88915185e+01 1.06902994e+02 7.67958411e+01\n",
      " 1.23144310e+02 8.01477339e+01 1.06597489e+02]\n",
      "22-th iteration, loss: 0.36328080221674086, 12 gd steps\n",
      "insert gradient: -0.0002933623397028238\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3762308   55.28725594  48.89122519 106.90252244\n",
      "  76.79628639 123.14471368  80.14761845 106.59748936]\n",
      "23-th iteration, loss: 0.36328079966008814, 11 gd steps\n",
      "insert gradient: -0.0002975626444224255\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43759582e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.52879698e+01 4.88909767e+01 1.06902110e+02 7.67966792e+01\n",
      " 1.23145075e+02 8.01475198e+01 1.06597489e+02]\n",
      "24-th iteration, loss: 0.3632807957837121, 12 gd steps\n",
      "insert gradient: -0.0002848478982583319\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37566401  55.2894941   48.89072124 106.90167304\n",
      "  76.79708472 123.14545887  80.14741323 106.59748936]\n",
      "25-th iteration, loss: 0.3632807934578036, 11 gd steps\n",
      "insert gradient: -0.0002884826077077707\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37539576  55.29017712  48.89050133 106.90128717\n",
      "  76.79744648 123.14580603  80.14732187 106.59748936]\n",
      "26-th iteration, loss: 0.3632807912426675, 10 gd steps\n",
      "insert gradient: -0.0002904581434127024\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37514803  55.29086146  48.89031047 106.90091409\n",
      "  76.79778499 123.14614084  80.14722736 106.59748936]\n",
      "27-th iteration, loss: 0.3632807891184137, 10 gd steps\n",
      "insert gradient: -0.00029119011602484905\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37491535  55.29154507  48.89014225 106.90055206\n",
      "  76.79810407 123.14646556  80.14713119 106.59748936]\n",
      "28-th iteration, loss: 0.36328078707336914, 10 gd steps\n",
      "insert gradient: -0.00029098616293491954\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43746939e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52922259e+01 4.88899920e+01 1.06900200e+02 7.67984064e+01\n",
      " 1.23146782e+02 8.01470345e+01 1.06597489e+02]\n",
      "29-th iteration, loss: 0.3632807838264558, 11 gd steps\n",
      "insert gradient: -0.0002756922099598996\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37444316  55.29365526  48.88982589 106.89982629\n",
      "  76.79872252 123.14711924  80.14693478 106.59748936]\n",
      "30-th iteration, loss: 0.36328078192711766, 10 gd steps\n",
      "insert gradient: -0.0002767943287334709\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43742039e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.52942958e+01 4.88896733e+01 1.06899492e+02 7.67990126e+01\n",
      " 1.23147430e+02 8.01468514e+01 1.06597489e+02]\n",
      "31-th iteration, loss: 0.3632807789647823, 11 gd steps\n",
      "insert gradient: -0.0002634787127179372\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37394172  55.29563683  48.88951046 106.89914065\n",
      "  76.79931201 123.14775924  80.14676457 106.59748936]\n",
      "32-th iteration, loss: 0.36328077719859614, 10 gd steps\n",
      "insert gradient: -0.0002652637292893304\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37369335  55.29624504  48.88936213 106.89882387\n",
      "  76.7995879  123.14806404  80.1466907  106.59748936]\n",
      "33-th iteration, loss: 0.3632807754978782, 10 gd steps\n",
      "insert gradient: -0.0002659853890456593\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37345871  55.29685352  48.88923199 106.89851586\n",
      "  76.79984848 123.14836048  80.14661488 106.59748936]\n",
      "34-th iteration, loss: 0.3632807738545636, 10 gd steps\n",
      "insert gradient: -0.00026589665916621147\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43732346e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.52974605e+01 4.88891163e+01 1.06898216e+02 7.68000958e+01\n",
      " 1.23148650e+02 8.01465381e+01 1.06597489e+02]\n",
      "35-th iteration, loss: 0.363280771261845, 11 gd steps\n",
      "insert gradient: -0.0002524726552690741\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37298755  55.29871946  48.88898914 106.89790155\n",
      "  76.80035114 123.14895439  80.14645971 106.59748936]\n",
      "36-th iteration, loss: 0.363280769723204, 10 gd steps\n",
      "insert gradient: -0.00025350690493325395\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43727497e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.52992934e+01 4.88888692e+01 1.06897616e+02 7.68005903e+01\n",
      " 1.23149240e+02 8.01463933e+01 1.06597489e+02]\n",
      "37-th iteration, loss: 0.3632807673360172, 11 gd steps\n",
      "insert gradient: -0.00024172312946656768\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43724941e+01 0.00000000e+00 3.19744231e-14\n",
      " 5.53004822e+01 4.88887423e+01 1.06897318e+02 7.68008342e+01\n",
      " 1.23149538e+02 8.01463248e+01 1.06597489e+02]\n",
      "38-th iteration, loss: 0.36328076507553203, 11 gd steps\n",
      "insert gradient: -0.000232014774693723\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37222218  55.30161166  48.88860396 106.89702966\n",
      "  76.80107898 123.14983624  80.14626573 106.59748936]\n",
      "39-th iteration, loss: 0.36328076367824264, 10 gd steps\n",
      "insert gradient: -0.0002346788512861433\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37196519  55.30213694  48.88847836 106.89676446\n",
      "  76.80130768 123.15011697  80.14621361 106.59748936]\n",
      "40-th iteration, loss: 0.3632807623336028, 10 gd steps\n",
      "insert gradient: -0.00023620760557765188\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43717236e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53026650e+01 4.88883704e+01 1.06896507e+02 7.68015227e+01\n",
      " 1.23150390e+02 8.01461583e+01 1.06597489e+02]\n",
      "41-th iteration, loss: 0.3632807602750606, 10 gd steps\n",
      "insert gradient: -0.00022587857943892368\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37146917  55.30375253  48.88825857 106.89624089\n",
      "  76.8017395  123.15067169  80.14610105 106.59748936]\n",
      "42-th iteration, loss: 0.36328075901266044, 10 gd steps\n",
      "insert gradient: -0.00022773975013899757\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43712251e+01 0.00000000e+00 2.84217094e-14\n",
      " 5.53042589e+01 4.88881539e+01 1.06895995e+02 7.68019446e+01\n",
      " 1.23150939e+02 8.01460511e+01 1.06597489e+02]\n",
      "43-th iteration, loss: 0.3632807570951755, 10 gd steps\n",
      "insert gradient: -0.00021816394868032977\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43709705e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53052999e+01 4.88880470e+01 1.06895743e+02 7.68021501e+01\n",
      " 1.23151214e+02 8.01459990e+01 1.06597489e+02]\n",
      "44-th iteration, loss: 0.36328075526727543, 10 gd steps\n",
      "insert gradient: -0.0002101545428290628\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37070426  55.30629531  48.8879314  106.89549808\n",
      "  76.80235626 123.15148891  80.14595377 106.59748936]\n",
      "45-th iteration, loss: 0.3632807541148529, 10 gd steps\n",
      "insert gradient: -0.00021296461277742416\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43704512e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53067649e+01 4.88878258e+01 1.06895270e+02 7.68025509e+01\n",
      " 1.23151750e+02 8.01459131e+01 1.06597489e+02]\n",
      "46-th iteration, loss: 0.36328075240167473, 10 gd steps\n",
      "insert gradient: -0.00020494588090819865\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37019245  55.30772958  48.88772163 106.89503757\n",
      "  76.8027435  123.15201698  80.14586962 106.59748936]\n",
      "47-th iteration, loss: 0.3632807513221036, 10 gd steps\n",
      "insert gradient: -0.00020749039323518658\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36994528  55.30818537  48.8876255  106.89482087\n",
      "  76.80292627 123.15227138  80.14583058 106.59748936]\n",
      "48-th iteration, loss: 0.36328075028217216, 9 gd steps\n",
      "insert gradient: -0.00020903221353455102\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36971249  55.30864398  48.88754409 106.89461018\n",
      "  76.80309768 123.15251872  80.14578838 106.59748936]\n",
      "49-th iteration, loss: 0.36328074927504095, 9 gd steps\n",
      "insert gradient: -0.00020979681612795566\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36949094  55.30910407  48.88747415 106.89440476\n",
      "  76.80325952 123.15276018  80.14574394 106.59748936]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536721336560601\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         608.66581906]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6330534235696021\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.52812451   0.         371.13769455]\n",
      "2-th iteration, loss: 0.6052498022827035, 13 gd steps\n",
      "insert gradient: -0.6944425690737154\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1304576   77.05949885 222.22394736  40.75088166 242.37563725\n",
      "   0.         128.76205729]\n",
      "3-th iteration, loss: 0.46911332129562944, 22 gd steps\n",
      "insert gradient: -0.3357625049874109\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.55369955 119.14539481   0.         112.52620621\n",
      "  53.57306873 164.48102793  50.29191915 128.76205729]\n",
      "4-th iteration, loss: 0.3632811676756671, 79 gd steps\n",
      "insert gradient: -0.001045869753638507\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31821539  55.44412022  48.8250639  106.98668368\n",
      "  76.7936252  123.251158    80.14677908 128.76205729]\n",
      "5-th iteration, loss: 0.36328095895853124, 32 gd steps\n",
      "insert gradient: -0.000260819867540851\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32235387  55.44523473  48.83623859 106.97327039\n",
      "  76.78348298 123.23764499  80.13662481 128.76205729]\n",
      "6-th iteration, loss: 0.3632809463812268, 18 gd steps\n",
      "insert gradient: -0.00024329956744818855\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43221806e+01 5.54444674e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88372524e+01 1.06971214e+02 7.67831938e+01\n",
      " 1.23236105e+02 8.01358787e+01 1.28762057e+02]\n",
      "7-th iteration, loss: 0.3632809347537218, 18 gd steps\n",
      "insert gradient: -0.0001904324579898063\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43219562e+01 5.54436357e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88389517e+01 1.06969319e+02 7.67830998e+01\n",
      " 1.23234759e+02 8.01353135e+01 1.28762057e+02]\n",
      "8-th iteration, loss: 0.36328092532183903, 16 gd steps\n",
      "insert gradient: -0.00016221863891061636\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32170219  55.44280468  48.84022957 106.9676378\n",
      "  76.78314232 123.23362637  80.13490878 128.76205729]\n",
      "9-th iteration, loss: 0.3632809178031234, 15 gd steps\n",
      "insert gradient: -0.00017247843400800033\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43214794e+01 5.54420213e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88407809e+01 1.06966142e+02 7.67832614e+01\n",
      " 1.23232649e+02 8.01346064e+01 1.28762057e+02]\n",
      "10-th iteration, loss: 0.3632809102873282, 15 gd steps\n",
      "insert gradient: -0.0001515829327609019\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43212827e+01 5.54412351e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88418640e+01 1.06964700e+02 7.67834341e+01\n",
      " 1.23231723e+02 8.01343570e+01 1.28762057e+02]\n",
      "11-th iteration, loss: 0.36328090346148617, 14 gd steps\n",
      "insert gradient: -0.00013803204654763254\n",
      "11-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32109372  55.44045757  48.84280636 106.96333682\n",
      "  76.78364873 123.23087356  80.13416441 128.76205729]\n",
      "12-th iteration, loss: 0.36328089761553506, 14 gd steps\n",
      "insert gradient: -0.00015061696157679438\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32093682  55.43971763  48.84324619 106.9620712\n",
      "  76.78388701 123.23009644  80.13401516 128.76205729]\n",
      "13-th iteration, loss: 0.36328089217927384, 13 gd steps\n",
      "insert gradient: -0.00015916718994664732\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32082623  55.43901183  48.84370647 106.96087238\n",
      "  76.78414059 123.22936166  80.13389248 128.76205729]\n",
      "14-th iteration, loss: 0.36328088707002976, 13 gd steps\n",
      "insert gradient: -0.000164830667210141\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43207512e+01 5.54383321e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88441786e+01 1.06959728e+02 7.67844068e+01\n",
      " 1.23228663e+02 8.01337929e+01 1.28762057e+02]\n",
      "15-th iteration, loss: 0.3632808816967494, 13 gd steps\n",
      "insert gradient: -0.00014420688143190212\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43206885e+01 5.54376444e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88451084e+01 1.06958595e+02 7.67846909e+01\n",
      " 1.23227977e+02 8.01337138e+01 1.28762057e+02]\n",
      "16-th iteration, loss: 0.3632808766598895, 13 gd steps\n",
      "insert gradient: -0.00013008256049442792\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32062097  55.43695808  48.84592456 106.95749665\n",
      "  76.78498642 123.22732782  80.13365987 128.76205729]\n",
      "17-th iteration, loss: 0.36328087224522615, 12 gd steps\n",
      "insert gradient: -0.00013930792944375072\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205681e+01 5.54362973e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88463049e+01 1.06956454e+02 7.67852832e+01\n",
      " 1.23226717e+02 8.01336242e+01 1.28762057e+02]\n",
      "18-th iteration, loss: 0.36328086766942763, 12 gd steps\n",
      "insert gradient: -0.0001254473331996197\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32053522  55.43564073  48.84707422 106.95542584\n",
      "  76.78558563 123.22611656  80.1335997  128.76205729]\n",
      "19-th iteration, loss: 0.3632808636283633, 12 gd steps\n",
      "insert gradient: -0.00013378495738768513\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205128e+01 5.54350056e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88474331e+01 1.06954444e+02 7.67858866e+01\n",
      " 1.23225548e+02 8.01335889e+01 1.28762057e+02]\n",
      "20-th iteration, loss: 0.3632808594323253, 12 gd steps\n",
      "insert gradient: -0.00012043183187371358\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205078e+01 5.54343746e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88481580e+01 1.06953473e+02 7.67861905e+01\n",
      " 1.23224986e+02 8.01335865e+01 1.28762057e+02]\n",
      "21-th iteration, loss: 0.3632808554285574, 12 gd steps\n",
      "insert gradient: -0.0001179083630951493\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3205022   55.43374743  48.84881422 106.95252447\n",
      "  76.78649539 123.22444384  80.13359584 128.76205729]\n",
      "22-th iteration, loss: 0.3632808518284468, 12 gd steps\n",
      "insert gradient: -0.00011960280595445942\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32050747  55.43313948  48.84912687 106.95161219\n",
      "  76.78679585 123.22392458  80.13361317 128.76205729]\n",
      "23-th iteration, loss: 0.36328084837676683, 12 gd steps\n",
      "insert gradient: -0.00012579327539290758\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32053774  55.43255279  48.8494559  106.95072847\n",
      "  76.78709126 123.22341768  80.13363382 128.76205729]\n",
      "24-th iteration, loss: 0.3632808450568925, 11 gd steps\n",
      "insert gradient: -0.00013001479570521387\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32058733  55.43198381  48.84979527 106.94987089\n",
      "  76.78738231 123.22292302  80.13365816 128.76205729]\n",
      "25-th iteration, loss: 0.3632808418521493, 11 gd steps\n",
      "insert gradient: -0.00013276291086184255\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32065206  55.43142903  48.85014104 106.94903621\n",
      "  76.78766983 123.22243963  80.13368638 128.76205729]\n",
      "26-th iteration, loss: 0.3632808387526886, 11 gd steps\n",
      "insert gradient: -0.00013439970906708253\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32072873  55.43088628  48.85049007 106.94822252\n",
      "  76.7879541  123.22196701  80.13371843 128.76205729]\n",
      "27-th iteration, loss: 0.3632808357508914, 11 gd steps\n",
      "insert gradient: -0.00013519664143120624\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43208149e+01 5.54303539e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88508400e+01 1.06947428e+02 7.67882352e+01\n",
      " 1.23221505e+02 8.01337542e+01 1.28762057e+02]\n",
      "28-th iteration, loss: 0.3632808325532538, 11 gd steps\n",
      "insert gradient: -0.00011735774199630009\n",
      "28-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32090163  55.42981544  48.85151851 106.94663486\n",
      "  76.78851774 123.22104483  80.13379531 128.76205729]\n",
      "29-th iteration, loss: 0.3632808297247703, 11 gd steps\n",
      "insert gradient: -0.00012055919874978542\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43209806e+01 5.54292854e+01 0.00000000e+00\n",
      " 1.33226763e-14 4.88518259e+01 1.06945866e+02 7.67887959e+01\n",
      " 1.23220604e+02 8.01338423e+01 1.28762057e+02]\n",
      "30-th iteration, loss: 0.3632808267525687, 11 gd steps\n",
      "insert gradient: -0.00011341507986484873\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43210647e+01 5.54287548e+01 4.88524317e+01\n",
      " 1.06945102e+02 0.00000000e+00 1.33226763e-14 7.67890727e+01\n",
      " 1.23220166e+02 8.01338923e+01 1.28762057e+02]\n",
      "31-th iteration, loss: 0.3632808239041918, 11 gd steps\n",
      "insert gradient: -0.00011180327091534093\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32114847  55.42823137  48.85271608 106.94435182\n",
      "  76.78960752 123.21973613  80.13394467 128.76205729]\n",
      "32-th iteration, loss: 0.3632808213212364, 11 gd steps\n",
      "insert gradient: -0.00011422258538294065\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32124541  55.42772864  48.85300453 106.94362076\n",
      "  76.78985112 123.2193125   80.13399339 128.76205729]\n",
      "33-th iteration, loss: 0.3632808188137057, 11 gd steps\n",
      "insert gradient: -0.00011576323700582436\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43213504e+01 5.54272351e+01 0.00000000e+00\n",
      " 3.55271368e-14 4.88532964e+01 1.06942907e+02 7.67900928e+01\n",
      " 1.23218897e+02 8.01340441e+01 1.28762057e+02]\n",
      "34-th iteration, loss: 0.36328081617400776, 11 gd steps\n",
      "insert gradient: -0.0001019553006720576\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43214564e+01 5.54267388e+01 4.88538658e+01\n",
      " 1.06942196e+02 0.00000000e+00 4.52970994e-14 7.67903352e+01\n",
      " 1.23218485e+02 8.01340982e+01 1.28762057e+02]\n",
      "35-th iteration, loss: 0.36328081365691484, 11 gd steps\n",
      "insert gradient: -0.00010556562686781038\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43215584e+01 5.54262470e+01 0.00000000e+00\n",
      " 2.04281037e-14 4.88541310e+01 1.06941499e+02 7.67908077e+01\n",
      " 1.23218081e+02 8.01341549e+01 1.28762057e+02]\n",
      "36-th iteration, loss: 0.3632808111814494, 11 gd steps\n",
      "insert gradient: -9.349786469307454e-05\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43216654e+01 5.54257625e+01 4.88546508e+01\n",
      " 1.06940807e+02 0.00000000e+00 4.52970994e-14 7.67910270e+01\n",
      " 1.23217679e+02 8.01342100e+01 1.28762057e+02]\n",
      "37-th iteration, loss: 0.3632808088140244, 11 gd steps\n",
      "insert gradient: -9.796895350658334e-05\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43217696e+01 5.54252825e+01 0.00000000e+00\n",
      " 3.55271368e-14 4.88548952e+01 1.06940129e+02 7.67914576e+01\n",
      " 1.23217286e+02 8.01342676e+01 1.28762057e+02]\n",
      "38-th iteration, loss: 0.3632808064830229, 11 gd steps\n",
      "insert gradient: -8.729224960743663e-05\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43218790e+01 5.54248096e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88553775e+01 1.06939457e+02 7.67916584e+01\n",
      " 1.23216894e+02 8.01343237e+01 1.28762057e+02]\n",
      "39-th iteration, loss: 0.36328080423041953, 11 gd steps\n",
      "insert gradient: -8.767848673032619e-05\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43219800e+01 5.54243346e+01 4.88558135e+01\n",
      " 1.06938795e+02 0.00000000e+00 4.79616347e-14 7.67918618e+01\n",
      " 1.23216512e+02 8.01343845e+01 1.28762057e+02]\n",
      "40-th iteration, loss: 0.36328080204913255, 10 gd steps\n",
      "insert gradient: -8.536298828140609e-05\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43220822e+01 5.54238658e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88560238e+01 1.06938146e+02 7.67922609e+01\n",
      " 1.23216138e+02 8.01344465e+01 1.28762057e+02]\n",
      "41-th iteration, loss: 0.3632807999178219, 10 gd steps\n",
      "insert gradient: -8.128298075343249e-05\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32219176  55.42340604  48.8564465  106.93750535\n",
      "  76.79244686 123.21576422  80.13450621 128.76205729]\n",
      "42-th iteration, loss: 0.3632807979459629, 10 gd steps\n",
      "insert gradient: -8.197389465354624e-05\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3222984   55.42295064  48.85664643 106.93687983\n",
      "  76.7926343  123.21540137  80.1345687  128.76205729]\n",
      "43-th iteration, loss: 0.3632807960302251, 10 gd steps\n",
      "insert gradient: -8.539779231516946e-05\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43224137e+01 5.54225045e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88568549e+01 1.06936269e+02 7.67928211e+01\n",
      " 1.23215045e+02 8.01346312e+01 1.28762057e+02]\n",
      "44-th iteration, loss: 0.36328079405751973, 10 gd steps\n",
      "insert gradient: -8.237339918080535e-05\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32253242  55.42206051  48.85727138 106.93566586\n",
      "  76.79300828 123.21469119  80.134695   128.76205729]\n",
      "45-th iteration, loss: 0.3632807922345006, 10 gd steps\n",
      "insert gradient: -8.196244517301648e-05\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43226467e+01 5.54216199e+01 4.88574668e+01\n",
      " 1.06935076e+02 0.00000000e+00 5.24025268e-14 7.67931960e+01\n",
      " 1.23214347e+02 8.01347609e+01 1.28762057e+02]\n",
      "46-th iteration, loss: 0.36328079037413125, 10 gd steps\n",
      "insert gradient: -8.404286042156858e-05\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43227707e+01 5.54211877e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88576723e+01 1.06934496e+02 7.67935616e+01\n",
      " 1.23214005e+02 8.01348253e+01 1.28762057e+02]\n",
      "47-th iteration, loss: 0.3632807885433814, 10 gd steps\n",
      "insert gradient: -7.621429659945855e-05\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43228975e+01 5.54207613e+01 4.88580775e+01\n",
      " 1.06933921e+02 0.00000000e+00 1.33226763e-14 7.67937324e+01\n",
      " 1.23213664e+02 8.01348878e+01 1.28762057e+02]\n",
      "48-th iteration, loss: 0.3632807867781529, 10 gd steps\n",
      "insert gradient: -7.866242112844741e-05\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43230207e+01 5.54203371e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88582691e+01 1.06933355e+02 7.67940710e+01\n",
      " 1.23213329e+02 8.01349517e+01 1.28762057e+02]\n",
      "49-th iteration, loss: 0.3632807850402543, 10 gd steps\n",
      "insert gradient: -7.145492730691954e-05\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43231470e+01 5.54199188e+01 4.88586494e+01\n",
      " 1.06932796e+02 0.00000000e+00 4.35207426e-14 7.67942298e+01\n",
      " 1.23212996e+02 8.01350138e+01 1.28762057e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536156841022463\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         634.80050857]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6281630689044189\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 232.2440885    0.         402.55642007]\n",
      "2-th iteration, loss: 0.6039950911279921, 13 gd steps\n",
      "insert gradient: -0.643099815252181\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.64739753  77.83171265 218.09742056  42.13716135 238.24767718\n",
      "   0.         164.30874288]\n",
      "3-th iteration, loss: 0.4300495220948163, 56 gd steps\n",
      "insert gradient: -0.3721084040102081\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.68777522  47.67161656 101.99671632   0.         107.99652316\n",
      "  57.94522617 129.13425111  58.84309205 164.30874288]\n",
      "4-th iteration, loss: 0.36336746337906706, 45 gd steps\n",
      "insert gradient: -0.35195103147027973\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          45.26953421  53.43851059  49.45655522 106.45919614\n",
      "  76.3168162  122.68222116  80.4464554  164.30874288   0.        ]\n",
      "5-th iteration, loss: 0.24140514220562592, 114 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.643787690430947e-07\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265141  47.86022265  75.4396542   41.95009194  94.54811515\n",
      "  79.28610187 118.50566582  82.28923353 128.25309466  82.83300291]\n",
      "6-th iteration, loss: 0.24140514220559056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.481194078507138e-07\n",
      "6-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265156  47.86022312  75.43965398  41.95009075  94.54811458\n",
      "  79.28610112 118.50566587  82.28923432 128.25309562  82.83300327]\n",
      "7-th iteration, loss: 0.24140514220555756, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.323334960957369e-07\n",
      "7-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265170e+00 4.78602236e+01 7.54396538e+01 4.19500896e+01\n",
      " 9.45481140e+01 7.92861004e+01 1.18505666e+02 8.22892351e+01\n",
      " 0.00000000e+00 1.50990331e-14 1.28253097e+02 8.28330036e+01]\n",
      "8-th iteration, loss: 0.24140514220551568, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.029501820142558e-07\n",
      "8-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265185e+00 4.78602241e+01 7.54396536e+01 4.19500886e+01\n",
      " 9.45481135e+01 7.92860996e+01 1.18505666e+02 8.22892358e+01\n",
      " 9.24954289e-07 7.23337676e-07 0.00000000e+00 1.19114008e-22\n",
      " 1.28253097e+02 8.28330039e+01]\n",
      "9-th iteration, loss: 0.24140514220546794, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.625210311605204e-07\n",
      "9-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265201e+00 4.78602247e+01 7.54396535e+01 4.19500876e+01\n",
      " 9.45481130e+01 7.92860989e+01 1.18505666e+02 8.22892365e+01\n",
      " 1.81026718e-06 1.37372542e-06 8.92114064e-07 6.50387748e-07\n",
      " 1.28253098e+02 8.28330042e+01]\n",
      "10-th iteration, loss: 0.24140514220542575, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.260563636148081e-07\n",
      "10-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265216e+00 4.78602252e+01 7.54396534e+01 4.19500866e+01\n",
      " 9.45481125e+01 7.92860982e+01 1.18505666e+02 8.22892371e+01\n",
      " 2.64410938e-06 1.94061849e-06 1.73887270e-06 1.20889272e-06\n",
      " 0.00000000e+00 1.85288457e-22 1.28253099e+02 8.28330045e+01]\n",
      "11-th iteration, loss: 0.24140514220538137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.824879740558872e-07\n",
      "11-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265232e+00 4.78602257e+01 7.54396533e+01 4.19500858e+01\n",
      " 9.45481120e+01 7.92860974e+01 1.18505666e+02 8.22892376e+01\n",
      " 3.42873470e-06 2.42703806e-06 2.54174467e-06 1.67896256e-06\n",
      " 8.14238570e-07 4.70069845e-07 1.28253100e+02 8.28330047e+01]\n",
      "12-th iteration, loss: 0.24140514220534273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.443636748688129e-07\n",
      "12-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265247e+00 4.78602263e+01 7.54396532e+01 4.19500849e+01\n",
      " 9.45481116e+01 7.92860966e+01 1.18505666e+02 8.22892380e+01\n",
      " 4.15805352e-06 2.82958228e-06 3.29388369e-06 2.05760805e-06\n",
      " 1.58216402e-06 8.41059465e-07 0.00000000e+00 2.77932686e-22\n",
      " 1.28253101e+02 8.28330049e+01]\n",
      "13-th iteration, loss: 0.24140514220530387, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.027232868463308e-07\n",
      "13-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265263e+00 4.78602268e+01 7.54396532e+01 4.19500842e+01\n",
      " 9.45481111e+01 7.92860959e+01 1.18505666e+02 8.22892384e+01\n",
      " 4.83741513e-06 3.15598257e-06 3.99985041e-06 2.35303763e-06\n",
      " 2.30747739e-06 1.12161276e-06 7.33221421e-07 2.80553294e-07\n",
      " 1.28253102e+02 8.28330050e+01]\n",
      "14-th iteration, loss: 0.24140514220526985, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.672233987280434e-07\n",
      "14-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265279e+00 4.78602274e+01 7.54396532e+01 4.19500834e+01\n",
      " 9.45481107e+01 7.92860951e+01 1.18505666e+02 8.22892387e+01\n",
      " 5.46592185e-06 3.40886214e-06 4.65803124e-06 2.56830865e-06\n",
      " 2.98778262e-06 1.31518774e-06 1.42407261e-06 4.67234168e-07\n",
      " 0.00000000e+00 1.05879118e-22 1.28253102e+02 8.28330051e+01]\n",
      "15-th iteration, loss: 0.24140514220523587, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.308491934666855e-07\n",
      "15-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265294e+00 4.78602279e+01 7.54396531e+01 4.19500827e+01\n",
      " 9.45481103e+01 7.92860943e+01 1.18505666e+02 8.22892389e+01\n",
      " 6.05081709e-06 3.59876227e-06 5.27497827e-06 2.71441167e-06\n",
      " 3.62887816e-06 1.43319816e-06 2.07753419e-06 5.71854760e-07\n",
      " 6.57854743e-07 1.04620592e-07 1.28253103e+02 8.28330052e+01]\n",
      "16-th iteration, loss: 0.24140514220520531, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.00415189967178e-07\n",
      "16-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265310e+00 4.78602284e+01 7.54396531e+01 4.19500821e+01\n",
      " 9.45481099e+01 7.92860936e+01 1.18505665e+02 8.22892391e+01\n",
      " 6.59495370e-06 3.73221522e-06 5.85295222e-06 2.79826669e-06\n",
      " 4.23237434e-06 1.48293268e-06 2.69450601e-06 6.02055275e-07\n",
      " 1.28020342e-06 1.28635633e-07 1.28253104e+02 8.28330052e+01]\n",
      "17-th iteration, loss: 0.24140514220517711, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.747152263028717e-07\n",
      "17-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265325e+00 4.78602289e+01 7.54396532e+01 4.19500815e+01\n",
      " 9.45481094e+01 7.92860928e+01 1.18505665e+02 8.22892392e+01\n",
      " 7.10710387e-06 3.82189502e-06 6.40019450e-06 2.83291415e-06\n",
      " 4.80592731e-06 1.47778527e-06 3.28200224e-06 5.71572795e-07\n",
      " 1.87336047e-06 8.61160489e-08 1.28253104e+02 8.28330052e+01]\n",
      "18-th iteration, loss: 0.24140514220515072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.52675904445892e-07\n",
      "18-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265340e+00 4.78602294e+01 7.54396532e+01 4.19500809e+01\n",
      " 9.45481091e+01 7.92860920e+01 1.18505665e+02 8.22892394e+01\n",
      " 7.59428306e-06 3.87788224e-06 6.92330904e-06 2.82872358e-06\n",
      " 5.35567830e-06 1.42840701e-06 3.84564809e-06 4.91335528e-07\n",
      " 1.28253107e+02 8.28330051e+01]\n",
      "19-th iteration, loss: 0.2414051422051283, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.366423787456366e-07\n",
      "19-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265355e+00 4.78602299e+01 7.54396532e+01 4.19500803e+01\n",
      " 9.45481087e+01 7.92860913e+01 1.18505665e+02 8.22892395e+01\n",
      " 8.06259418e-06 3.90853250e-06 7.42808191e-06 2.79427748e-06\n",
      " 5.88704823e-06 1.34360418e-06 4.39044861e-06 3.70374010e-07\n",
      " 0.00000000e+00 9.26442286e-23 1.28253108e+02 8.28330051e+01]\n",
      "20-th iteration, loss: 0.24140514220510453, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.182131704005934e-07\n",
      "20-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265369e+00 4.78602304e+01 7.54396532e+01 4.19500798e+01\n",
      " 9.45481083e+01 7.92860905e+01 1.18505665e+02 8.22892396e+01\n",
      " 8.51672169e-06 3.92040529e-06 7.91895937e-06 2.73630778e-06\n",
      " 6.40419886e-06 1.23028154e-06 4.92023247e-06 2.15770212e-07\n",
      " 1.28253109e+02 8.28330050e+01]\n",
      "21-th iteration, loss: 0.241405142205084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.058142918355533e-07\n",
      "21-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265384e+00 4.78602309e+01 7.54396533e+01 4.19500792e+01\n",
      " 9.45481079e+01 7.92860898e+01 1.18505665e+02 8.22892396e+01\n",
      " 8.95657352e-06 3.91350577e-06 8.39567279e-06 2.65495030e-06\n",
      " 6.90664039e-06 1.08870862e-06 5.43424172e-06 2.79348338e-08\n",
      " 0.00000000e+00 2.48154184e-24 1.28253109e+02 8.28330049e+01]\n",
      "22-th iteration, loss: 0.24140514220506246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.888168179085556e-07\n",
      "22-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265398e+00 4.78602314e+01 7.54396533e+01 4.19500787e+01\n",
      " 9.45481076e+01 7.92860891e+01 1.18505664e+02 8.22892397e+01\n",
      " 9.38788208e-06 3.89608718e-06 8.86377805e-06 2.55859145e-06\n",
      " 7.39970881e-06 9.27410132e-07 1.28253116e+02 8.28330048e+01]\n",
      "23-th iteration, loss: 0.24140514220504586, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.807670668211514e-07\n",
      "23-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265412e+00 4.78602319e+01 7.54396533e+01 4.19500782e+01\n",
      " 9.45481072e+01 7.92860884e+01 1.18505664e+02 8.22892398e+01\n",
      " 9.80621654e-06 3.86109337e-06 9.31874545e-06 2.44025603e-06\n",
      " 7.87873335e-06 7.39498991e-07 1.28253117e+02 8.28330047e+01]\n",
      "24-th iteration, loss: 0.24140514220502976, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.7319335917500296e-07\n",
      "24-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265426e+00 4.78602323e+01 7.54396534e+01 4.19500778e+01\n",
      " 9.45481069e+01 7.92860878e+01 1.18505664e+02 8.22892398e+01\n",
      " 1.02198271e-05 3.81987485e-06 9.76865995e-06 2.31141807e-06\n",
      " 8.35159234e-06 5.36581288e-07 1.28253117e+02 8.28330046e+01]\n",
      "25-th iteration, loss: 0.2414051422050142, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.660321088761262e-07\n",
      "25-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265440e+00 4.78602328e+01 7.54396534e+01 4.19500773e+01\n",
      " 9.45481066e+01 7.92860871e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.06294698e-05 3.77346431e-06 1.02142189e-05 2.17315777e-06\n",
      " 8.81888442e-06 3.19795191e-07 0.00000000e+00 1.32348898e-22\n",
      " 1.28253118e+02 8.28330045e+01]\n",
      "26-th iteration, loss: 0.24140514220499731, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.555694264649048e-07\n",
      "26-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265454e+00 4.78602332e+01 7.54396535e+01 4.19500769e+01\n",
      " 9.45481063e+01 7.92860865e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.10355492e-05 3.72302037e-06 1.06557783e-05 2.02667470e-06\n",
      " 9.28087687e-06 9.03926143e-08 0.00000000e+00 3.63959470e-23\n",
      " 1.28253119e+02 8.28330044e+01]\n",
      "27-th iteration, loss: 0.24140514220498133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.443243254839823e-07\n",
      "27-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265467e+00 4.78602337e+01 7.54396535e+01 4.19500765e+01\n",
      " 9.45481059e+01 7.92860858e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.14354220e-05 3.66488945e-06 1.10906568e-05 1.86835289e-06\n",
      " 1.28253129e+02 8.28330042e+01]\n",
      "28-th iteration, loss: 0.24140514220496936, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3993007105846506e-07\n",
      "28-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265480e+00 4.78602341e+01 7.54396536e+01 4.19500760e+01\n",
      " 9.45481056e+01 7.92860852e+01 1.18505663e+02 8.22892400e+01\n",
      " 1.18274534e-05 3.59541794e-06 1.15171473e-05 1.69460155e-06\n",
      " 0.00000000e+00 3.44107135e-22 1.28253130e+02 8.28330041e+01]\n",
      "29-th iteration, loss: 0.2414051422049562, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3218841867217145e-07\n",
      "29-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265493e+00 4.78602345e+01 7.54396537e+01 4.19500756e+01\n",
      " 9.45481054e+01 7.92860846e+01 1.18505663e+02 8.22892400e+01\n",
      " 1.22170211e-05 3.52283694e-06 1.19405209e-05 1.51373064e-06\n",
      " 1.28253131e+02 8.28330039e+01]\n",
      "30-th iteration, loss: 0.24140514220494505, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.282298392433078e-07\n",
      "30-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265506e+00 4.78602349e+01 7.54396537e+01 4.19500753e+01\n",
      " 9.45481051e+01 7.92860841e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.26016703e-05 3.44332429e-06 1.23582936e-05 1.32194733e-06\n",
      " 1.28253131e+02 8.28330038e+01]\n",
      "31-th iteration, loss: 0.2414051422049342, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.243283531025289e-07\n",
      "31-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265519e+00 4.78602353e+01 7.54396538e+01 4.19500749e+01\n",
      " 9.45481048e+01 7.92860835e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.29849047e-05 3.36162807e-06 1.27739038e-05 1.12405232e-06\n",
      " 0.00000000e+00 7.94093388e-23 1.28253131e+02 8.28330036e+01]\n",
      "32-th iteration, loss: 0.2414051422049222, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.171481021235514e-07\n",
      "32-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265531e+00 4.78602357e+01 7.54396538e+01 4.19500745e+01\n",
      " 9.45481045e+01 7.92860830e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.33666057e-05 3.27807145e-06 1.31872124e-05 9.20389164e-07\n",
      " 1.28253132e+02 8.28330034e+01]\n",
      "33-th iteration, loss: 0.24140514220491205, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1359538788817783e-07\n",
      "33-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265544e+00 4.78602361e+01 7.54396539e+01 4.19500742e+01\n",
      " 9.45481043e+01 7.92860824e+01 1.18505663e+02 8.22892402e+01\n",
      " 1.37441511e-05 3.18839415e-06 1.35955799e-05 7.06719176e-07\n",
      " 1.28253133e+02 8.28330033e+01]\n",
      "34-th iteration, loss: 0.24140514220490214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1006705933774046e-07\n",
      "34-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265556e+00 4.78602365e+01 7.54396540e+01 4.19500738e+01\n",
      " 9.45481040e+01 7.92860819e+01 1.18505663e+02 8.22892402e+01\n",
      " 1.41209817e-05 3.09733406e-06 1.40023893e-05 4.87826736e-07\n",
      " 0.00000000e+00 9.92616735e-23 1.28253133e+02 8.28330031e+01]\n",
      "35-th iteration, loss: 0.2414051422048911, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.033321354614368e-07\n",
      "35-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265568e+00 4.78602369e+01 7.54396540e+01 4.19500735e+01\n",
      " 9.45481038e+01 7.92860814e+01 1.18505662e+02 8.22892402e+01\n",
      " 1.44970248e-05 3.00532612e-06 1.44075552e-05 2.64161409e-07\n",
      " 0.00000000e+00 7.94093388e-23 1.28253134e+02 8.28330029e+01]\n",
      "36-th iteration, loss: 0.24140514220488044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.9690597923283686e-07\n",
      "36-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265579e+00 4.78602373e+01 7.54396541e+01 4.19500732e+01\n",
      " 9.45481035e+01 7.92860809e+01 1.18505662e+02 8.22892403e+01\n",
      " 1.48694060e-05 2.90824436e-06 1.48081939e-05 3.16127007e-08\n",
      " 0.00000000e+00 1.19941189e-23 1.28253135e+02 8.28330027e+01]\n",
      "37-th iteration, loss: 0.24140514220487044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8873990431614435e-07\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265591e+00 4.78602376e+01 7.54396541e+01 4.19500729e+01\n",
      " 9.45481033e+01 7.92860804e+01 1.18505662e+02 8.22892403e+01\n",
      " 1.52385546e-05 2.80663841e-06 1.28253151e+02 8.28330025e+01]\n",
      "38-th iteration, loss: 0.2414051422048634, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.868262197696537e-07\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265602e+00 4.78602380e+01 7.54396542e+01 4.19500726e+01\n",
      " 9.45481031e+01 7.92860799e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.56010153e-05 2.69437952e-06 1.28253151e+02 8.28330023e+01]\n",
      "39-th iteration, loss: 0.24140514220485657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8487581251324134e-07\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265613e+00 4.78602384e+01 7.54396543e+01 4.19500723e+01\n",
      " 9.45481029e+01 7.92860795e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.59625875e-05 2.57994458e-06 1.28253151e+02 8.28330021e+01]\n",
      "40-th iteration, loss: 0.24140514220484993, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8289460506604537e-07\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265624e+00 4.78602387e+01 7.54396543e+01 4.19500720e+01\n",
      " 9.45481026e+01 7.92860790e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.63232577e-05 2.46325306e-06 1.28253152e+02 8.28330019e+01]\n",
      "41-th iteration, loss: 0.2414051422048435, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8088794964743793e-07\n",
      "41-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265635e+00 4.78602390e+01 7.54396544e+01 4.19500717e+01\n",
      " 9.45481024e+01 7.92860786e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.66830187e-05 2.34423366e-06 0.00000000e+00 3.44107135e-22\n",
      " 1.28253152e+02 8.28330017e+01]\n",
      "42-th iteration, loss: 0.24140514220483608, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7588474695580946e-07\n",
      "42-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265646e+00 4.78602394e+01 7.54396544e+01 4.19500714e+01\n",
      " 9.45481022e+01 7.92860782e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.70414453e-05 2.22254099e-06 0.00000000e+00 5.82335151e-22\n",
      " 1.28253153e+02 8.28330016e+01]\n",
      "43-th iteration, loss: 0.24140514220482884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.710619959686969e-07\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265656e+00 4.78602397e+01 7.54396545e+01 4.19500711e+01\n",
      " 9.45481020e+01 7.92860778e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.73960726e-05 2.09492647e-06 1.28253154e+02 8.28330013e+01]\n",
      "44-th iteration, loss: 0.24140514220482298, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6931189836639376e-07\n",
      "44-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265666e+00 4.78602400e+01 7.54396546e+01 4.19500709e+01\n",
      " 9.45481018e+01 7.92860774e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.77474986e-05 1.96187281e-06 0.00000000e+00 5.82335151e-22\n",
      " 1.28253154e+02 8.28330011e+01]\n",
      "45-th iteration, loss: 0.24140514220481615, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.646388267456944e-07\n",
      "45-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265676e+00 4.78602403e+01 7.54396546e+01 4.19500706e+01\n",
      " 9.45481017e+01 7.92860770e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.80980444e-05 1.82677036e-06 1.28253155e+02 8.28330009e+01]\n",
      "46-th iteration, loss: 0.2414051422048106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.629846782254775e-07\n",
      "46-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265686e+00 4.78602406e+01 7.54396547e+01 4.19500704e+01\n",
      " 9.45481015e+01 7.92860766e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.84455704e-05 1.68637606e-06 0.00000000e+00 3.44107135e-22\n",
      " 1.28253155e+02 8.28330007e+01]\n",
      "47-th iteration, loss: 0.2414051422048041, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.584520034405542e-07\n",
      "47-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265696e+00 4.78602409e+01 7.54396547e+01 4.19500701e+01\n",
      " 9.45481013e+01 7.92860762e+01 1.18505661e+02 8.22892406e+01\n",
      " 1.87924178e-05 1.54416477e-06 1.28253156e+02 8.28330005e+01]\n",
      "48-th iteration, loss: 0.2414051422047989, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.568866942756451e-07\n",
      "48-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265706e+00 4.78602412e+01 7.54396548e+01 4.19500699e+01\n",
      " 9.45481011e+01 7.92860758e+01 1.18505661e+02 8.22892407e+01\n",
      " 1.91364159e-05 1.39678858e-06 0.00000000e+00 2.38228016e-22\n",
      " 1.28253156e+02 8.28330003e+01]\n",
      "49-th iteration, loss: 0.24140514220479273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5248590878714235e-07\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265715e+00 4.78602415e+01 7.54396549e+01 4.19500697e+01\n",
      " 9.45481009e+01 7.92860755e+01 1.18505661e+02 8.22892407e+01\n",
      " 1.94799248e-05 1.24780355e-06 1.28253157e+02 8.28330001e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535590565167533\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.20594943   0.         658.31163851]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6336159057824838\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.47534951  62.25781836 232.81753069   0.         425.49410782]\n",
      "2-th iteration, loss: 0.6031807630040061, 13 gd steps\n",
      "insert gradient: -0.6605279978497849\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1782722   77.52938313 218.37705095  42.34732975 243.13949018\n",
      "   0.         182.35461764]\n",
      "3-th iteration, loss: 0.46750685418285004, 20 gd steps\n",
      "insert gradient: -0.3581768149881661\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.49835606 115.67193958   0.         109.24572071\n",
      "  57.03947824 153.54515506  56.04003998 182.35461764]\n",
      "4-th iteration, loss: 0.36328095042507136, 81 gd steps\n",
      "insert gradient: -0.4484179934141403\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          44.40235846  55.23312461  48.91041178 106.85336532\n",
      "  76.81106462 123.13084914  80.14348952 182.35461764   0.        ]\n",
      "5-th iteration, loss: 0.24140518966994437, 73 gd steps\n",
      "insert gradient: -5.672060925629881e-05\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  1.92043892  47.85877217  75.41559678  41.95698206  94.56811181\n",
      "  79.27990746 118.49589996  82.28769316 128.27790383  82.81928341]\n",
      "6-th iteration, loss: 0.24140514249435457, 17 gd steps\n",
      "insert gradient: -1.1207861363264359e-05\n",
      "6-th iteration, new layer inserted. now 12 layers\n",
      "[1.88324536e+00 4.78599804e+01 0.00000000e+00 1.28785871e-14\n",
      " 7.54379846e+01 4.19501747e+01 9.45504300e+01 7.92854380e+01\n",
      " 1.18505066e+02 8.22887833e+01 1.28256866e+02 8.28313008e+01]\n",
      "7-th iteration, loss: 0.24140514221382287, 24 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1031860941848654e-06\n",
      "7-th iteration, new layer inserted. now 10 layers\n",
      "[  1.8826506   47.86011337  75.43998103  41.95006292  94.54779764\n",
      "  79.28608987 118.50581113  82.28903504 128.25414863  82.83251491]\n",
      "8-th iteration, loss: 0.2414051422137758, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1047587663157694e-06\n",
      "8-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265076  47.86011366  75.43997995  41.9500627   94.54779875\n",
      "  79.28608962 118.50581059  82.28903516 128.25414712  82.83251561]\n",
      "9-th iteration, loss: 0.2414051422137291, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.105786567377441e-06\n",
      "9-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265092e+00 4.78601140e+01 7.54399789e+01 4.19500625e+01\n",
      " 0.00000000e+00 1.77635684e-14 9.45477999e+01 7.92860894e+01\n",
      " 1.18505810e+02 8.22890353e+01 1.28254146e+02 8.28325163e+01]\n",
      "10-th iteration, loss: 0.24140514221367265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0934054673671704e-06\n",
      "10-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265108e+00 4.78601143e+01 7.54399778e+01 4.19500623e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478021e+01 7.92860892e+01\n",
      " 1.18505810e+02 8.22890354e+01 1.28254144e+02 8.28325170e+01]\n",
      "11-th iteration, loss: 0.24140514221361675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0815026771459908e-06\n",
      "11-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265125  47.86011457  75.43997677  41.95006208  94.54780425\n",
      "  79.28608893 118.505809    82.28903556 128.25414262  82.83251772]\n",
      "12-th iteration, loss: 0.24140514221357104, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0826851813332346e-06\n",
      "12-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265141  47.86011487  75.43997571  41.95006186  94.54780534\n",
      "  79.28608869 118.50580848  82.2890357  128.25414113  82.83251842]\n",
      "13-th iteration, loss: 0.2414051422135257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0833684657964817e-06\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265157  47.86011517  75.43997466  41.95006166  94.54780642\n",
      "  79.28608847 118.50580796  82.28903585 128.25413964  82.83251913]\n",
      "14-th iteration, loss: 0.2414051422134806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0836015674685633e-06\n",
      "14-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265173e+00 4.78601155e+01 7.54399736e+01 4.19500615e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478075e+01 7.92860882e+01\n",
      " 1.18505807e+02 8.22890360e+01 1.28254138e+02 8.28325198e+01]\n",
      "15-th iteration, loss: 0.24140514221342618, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.070783019799729e-06\n",
      "15-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265189e+00 4.78601158e+01 7.54399726e+01 4.19500613e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478097e+01 7.92860880e+01\n",
      " 1.18505807e+02 8.22890362e+01 1.28254137e+02 8.28325205e+01]\n",
      "16-th iteration, loss: 0.24140514221337228, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0585073422822943e-06\n",
      "16-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265205e+00 4.78601161e+01 7.54399716e+01 4.19500611e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478118e+01 7.92860878e+01\n",
      " 1.18505806e+02 8.22890363e+01 1.28254135e+02 8.28325213e+01]\n",
      "17-th iteration, loss: 0.2414051422133189, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.046725369659041e-06\n",
      "17-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265221e+00 4.78601164e+01 7.54399705e+01 4.19500609e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478139e+01 7.92860876e+01\n",
      " 1.18505806e+02 8.22890365e+01 1.28254134e+02 8.28325220e+01]\n",
      "18-th iteration, loss: 0.24140514221326603, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0353930238674971e-06\n",
      "18-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265237  47.86011672  75.43996948  41.95006068  94.54781603\n",
      "  79.28608736 118.50580544  82.28903667 128.25413229  82.83252267]\n",
      "19-th iteration, loss: 0.24140514221322248, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.036585315963505e-06\n",
      "19-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265253  47.86011702  75.43996845  41.95006047  94.54781707\n",
      "  79.28608713 118.50580494  82.28903684 128.25413083  82.83252338]\n",
      "20-th iteration, loss: 0.24140514221317924, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.037293370171485e-06\n",
      "20-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265268e+00 4.78601173e+01 7.54399674e+01 4.19500603e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478181e+01 7.92860869e+01\n",
      " 1.18505804e+02 8.22890370e+01 1.28254129e+02 8.28325241e+01]\n",
      "21-th iteration, loss: 0.24140514221312742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.025448152224915e-06\n",
      "21-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265284e+00 4.78601176e+01 7.54399664e+01 4.19500601e+01\n",
      " 0.00000000e+00 5.77315973e-15 9.45478202e+01 7.92860867e+01\n",
      " 1.18505804e+02 8.22890372e+01 1.28254128e+02 8.28325248e+01]\n",
      "22-th iteration, loss: 0.24140514221307607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0140769800985037e-06\n",
      "22-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265300e+00 4.78601179e+01 7.54399654e+01 4.19500599e+01\n",
      " 0.00000000e+00 1.28785871e-14 9.45478222e+01 7.92860865e+01\n",
      " 1.18505803e+02 8.22890374e+01 1.28254126e+02 8.28325255e+01]\n",
      "23-th iteration, loss: 0.24140514221302523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0031377159724911e-06\n",
      "23-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265315e+00 4.78601182e+01 7.54399644e+01 4.19500597e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478243e+01 7.92860863e+01\n",
      " 1.18505803e+02 8.22890376e+01 1.28254125e+02 8.28325262e+01]\n",
      "24-th iteration, loss: 0.2414051422129748, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.925925674159987e-07\n",
      "24-th iteration, new layer inserted. now 10 layers\n",
      "[  1.8826533   47.86011854  75.43996336  41.95005947  94.54782627\n",
      "  79.28608603 118.5058025   82.28903778 128.2541236   82.83252692]\n",
      "25-th iteration, loss: 0.24140514221293302, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.940289280117475e-07\n",
      "25-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265346e+00 4.78601188e+01 7.54399624e+01 4.19500593e+01\n",
      " 0.00000000e+00 1.33226763e-15 9.45478273e+01 7.92860858e+01\n",
      " 1.18505802e+02 8.22890380e+01 1.28254122e+02 8.28325276e+01]\n",
      "26-th iteration, loss: 0.24140514221288334, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.833462814648996e-07\n",
      "26-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265361e+00 4.78601191e+01 7.54399613e+01 4.19500591e+01\n",
      " 0.00000000e+00 1.46549439e-14 9.45478293e+01 7.92860856e+01\n",
      " 1.18505802e+02 8.22890382e+01 1.28254121e+02 8.28325283e+01]\n",
      "27-th iteration, loss: 0.24140514221283407, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.73046910537481e-07\n",
      "27-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265376  47.86011943  75.43996035  41.95005885  94.54783122\n",
      "  79.28608536 118.50580106  82.28903838 128.25411931  82.83252905]\n",
      "28-th iteration, loss: 0.24140514221279313, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.744913041360582e-07\n",
      "28-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265391  47.86011973  75.43995935  41.95005864  94.5478322\n",
      "  79.28608513 118.50580058  82.28903858 128.25411788  82.83252975]\n",
      "29-th iteration, loss: 0.24140514221275242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.754514055103986e-07\n",
      "29-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265406e+00 4.78601200e+01 7.54399584e+01 4.19500584e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478332e+01 7.92860849e+01\n",
      " 1.18505800e+02 8.22890388e+01 1.28254116e+02 8.28325305e+01]\n",
      "30-th iteration, loss: 0.24140514221270415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.645740756343162e-07\n",
      "30-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265421e+00 4.78601203e+01 7.54399574e+01 4.19500583e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478351e+01 7.92860847e+01\n",
      " 1.18505800e+02 8.22890390e+01 1.28254115e+02 8.28325312e+01]\n",
      "31-th iteration, loss: 0.24140514221265633, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.541175145759823e-07\n",
      "31-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265437  47.86012064  75.43995639  41.95005806  94.54783706\n",
      "  79.28608448 118.50579918  82.28903922 128.25411364  82.83253187]\n",
      "32-th iteration, loss: 0.2414051422126164, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.552092664087812e-07\n",
      "32-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265451  47.86012094  75.43995541  41.95005787  94.54783801\n",
      "  79.28608426 118.50579871  82.28903944 128.25411223  82.83253258]\n",
      "33-th iteration, loss: 0.2414051422125768, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.558598130445614e-07\n",
      "33-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265466  47.86012125  75.43995443  41.95005768  94.54783897\n",
      "  79.28608405 118.50579825  82.28903966 128.25411082  82.83253328]\n",
      "34-th iteration, loss: 0.24140514221253742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.561128746069293e-07\n",
      "34-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265482e+00 4.78601216e+01 7.54399535e+01 4.19500575e+01\n",
      " 0.00000000e+00 1.33226763e-15 9.45478399e+01 7.92860838e+01\n",
      " 1.18505798e+02 8.22890399e+01 1.28254109e+02 8.28325340e+01]\n",
      "35-th iteration, loss: 0.24140514221249076, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.448467179546584e-07\n",
      "35-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265497  47.86012187  75.43995251  41.95005734  94.54784184\n",
      "  79.28608363 118.50579735  82.2890401  128.25410803  82.83253469]\n",
      "36-th iteration, loss: 0.2414051422124519, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.450976204234664e-07\n",
      "36-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265512e+00 4.78601222e+01 7.54399516e+01 4.19500572e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478428e+01 7.92860834e+01\n",
      " 1.18505797e+02 8.22890403e+01 1.28254107e+02 8.28325354e+01]\n",
      "37-th iteration, loss: 0.24140514221240592, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.339623652931234e-07\n",
      "37-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265527  47.8601225   75.4399506   41.950057    94.54784468\n",
      "  79.28608322 118.50579646  82.28904056 128.25410524  82.8325361 ]\n",
      "38-th iteration, loss: 0.24140514221236753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.342114845919566e-07\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265541e+00 4.78601228e+01 7.54399497e+01 4.19500568e+01\n",
      " 0.00000000e+00 1.46549439e-14 9.45478456e+01 7.92860830e+01\n",
      " 1.18505796e+02 8.22890408e+01 1.28254104e+02 8.28325368e+01]\n",
      "39-th iteration, loss: 0.2414051422123222, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.232060103726748e-07\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265556e+00 4.78601231e+01 7.54399487e+01 4.19500567e+01\n",
      " 0.00000000e+00 7.54951657e-15 9.45478475e+01 7.92860828e+01\n",
      " 1.18505796e+02 8.22890410e+01 1.28254102e+02 8.28325375e+01]\n",
      "40-th iteration, loss: 0.24140514221227724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.126695967874049e-07\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265571e+00 4.78601234e+01 7.54399478e+01 4.19500565e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478493e+01 7.92860826e+01\n",
      " 1.18505795e+02 8.22890413e+01 1.28254101e+02 8.28325382e+01]\n",
      "41-th iteration, loss: 0.24140514221223272, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.025599927835758e-07\n",
      "41-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265585  47.86012373  75.43994682  41.95005632  94.54785115\n",
      "  79.28608239 118.50579469  82.2890415  128.25409971  82.83253891]\n",
      "42-th iteration, loss: 0.24140514221219525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.033949965036798e-07\n",
      "42-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265600e+00 4.78601240e+01 7.54399459e+01 4.19500561e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478521e+01 7.92860822e+01\n",
      " 1.18505794e+02 8.22890417e+01 1.28254098e+02 8.28325396e+01]\n",
      "43-th iteration, loss: 0.24140514221215134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.932779025061263e-07\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265614e+00 4.78601243e+01 7.54399449e+01 4.19500560e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478539e+01 7.92860820e+01\n",
      " 1.18505794e+02 8.22890420e+01 1.28254097e+02 8.28325403e+01]\n",
      "44-th iteration, loss: 0.2414051422121078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.835585394604122e-07\n",
      "44-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265629e+00 4.78601246e+01 7.54399440e+01 4.19500558e+01\n",
      " 0.00000000e+00 1.77635684e-14 9.45478557e+01 7.92860818e+01\n",
      " 1.18505793e+02 8.22890422e+01 1.28254096e+02 8.28325410e+01]\n",
      "45-th iteration, loss: 0.24140514221206463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.74201891284129e-07\n",
      "45-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265643  47.86012493  75.43994307  41.95005562  94.54785742\n",
      "  79.28608155 118.50579294  82.28904246 128.25409423  82.83254169]\n",
      "46-th iteration, loss: 0.24140514221202813, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.754084717768302e-07\n",
      "46-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265657  47.86012522  75.43994214  41.95005544  94.5478583\n",
      "  79.28608134 118.50579251  82.2890427  128.25409286  82.83254238]\n",
      "47-th iteration, loss: 0.24140514221199186, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.761942037699671e-07\n",
      "47-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265671  47.86012552  75.43994122  41.95005527  94.54785918\n",
      "  79.28608113 118.50579208  82.28904295 128.2540915   82.83254308]\n",
      "48-th iteration, loss: 0.24140514221195583, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.766008381182126e-07\n",
      "48-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265685  47.86012582  75.4399403   41.95005511  94.54786005\n",
      "  79.28608093 118.50579165  82.28904319 128.25409014  82.83254377]\n",
      "49-th iteration, loss: 0.24140514221192005, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.766659806181383e-07\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265699e+00 4.78601261e+01 7.54399394e+01 4.19500550e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478609e+01 7.92860807e+01\n",
      " 1.18505791e+02 8.22890434e+01 1.28254089e+02 8.28325445e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5366485788750763\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.89416964   0.         684.74904644]\n",
      "1-th iteration, loss: 0.7481568929324234, 11 gd steps\n",
      "insert gradient: -0.6310653355680163\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.52886631  62.34663952 233.81674756   0.         450.93229887]\n",
      "2-th iteration, loss: 0.6043164652064028, 13 gd steps\n",
      "insert gradient: -0.659365031891441\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.49693009  77.62285849 219.33640905  41.80284804 239.2701994\n",
      "   0.         211.66209947]\n",
      "3-th iteration, loss: 0.46878047382198124, 20 gd steps\n",
      "insert gradient: -0.5568536542324254\n",
      "3-th iteration, new layer inserted. now 8 layers\n",
      "[  0.          59.8620548  225.09722016  55.80320243 163.84414506\n",
      "  49.23316264 211.66209947   0.        ]\n",
      "4-th iteration, loss: 0.3597929758806964, 26 gd steps\n",
      "insert gradient: -0.7683586951214818\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.66761927  50.51309446 123.84820524   0.         123.84820524\n",
      "  49.86983697 155.619805    57.10989663 159.16918478  54.6692517 ]\n",
      "5-th iteration, loss: 0.29029072086957297, 44 gd steps\n",
      "insert gradient: -0.028145158874571283\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          69.34583452  79.07569712  36.12030387  85.87215004\n",
      "  52.69203691 161.56273294  59.67488701 155.60686627  64.09215298]\n",
      "6-th iteration, loss: 0.29007742387168195, 14 gd steps\n",
      "insert gradient: -0.03291539413738879\n",
      "6-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          70.38408758  76.07630773  36.97525194  85.98465784\n",
      "  52.45888038 162.16818581  59.71670699 155.02021876  65.12840444]\n",
      "7-th iteration, loss: 0.28991751824293, 91 gd steps\n",
      "insert gradient: -0.000979156291795891\n",
      "7-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.41376887  74.41261033  37.23538252  85.97519144\n",
      "  53.20137863 162.67559588  59.67509375 154.48535557  65.58153995]\n",
      "8-th iteration, loss: 0.28991730437844515, 43 gd steps\n",
      "insert gradient: -0.0009629694405827478\n",
      "8-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.41895155  74.40244837  37.24028478  85.9749576\n",
      "  53.20055986 162.67998358  59.67520036 154.48399461  65.58855849]\n",
      "9-th iteration, loss: 0.2899171067387849, 40 gd steps\n",
      "insert gradient: -0.0009487504016090227\n",
      "9-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14237982e+01 7.43928992e+01 3.72448160e+01\n",
      " 8.59747040e+01 5.31997359e+01 1.62684091e+02 5.96752532e+01\n",
      " 1.54482677e+02 0.00000000e+00 6.75015599e-14 6.55951135e+01]\n",
      "10-th iteration, loss: 0.2899167940439816, 54 gd steps\n",
      "insert gradient: -0.0008461240581427001\n",
      "10-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.43076885  74.38014542  37.25099986  85.9742859\n",
      "  53.1983459  162.68956042  59.67497608 154.48067068  65.61185958]\n",
      "11-th iteration, loss: 0.2899166275583249, 36 gd steps\n",
      "insert gradient: -0.0008539716515982768\n",
      "11-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14354118e+01 7.43718909e+01 3.72550556e+01\n",
      " 8.59740111e+01 5.31974558e+01 1.62693171e+02 5.96747764e+01\n",
      " 1.54479276e+02 0.00000000e+00 9.94759830e-14 6.56170891e+01]\n",
      "12-th iteration, loss: 0.289916385279708, 46 gd steps\n",
      "insert gradient: -0.0007857104582283194\n",
      "12-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14412652e+01 7.43614867e+01 3.72601307e+01\n",
      " 8.59736401e+01 5.31963245e+01 1.62697748e+02 5.96744987e+01\n",
      " 1.54477434e+02 0.00000000e+00 1.95399252e-14 6.56299117e+01]\n",
      "13-th iteration, loss: 0.2899161730768097, 42 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.44678255  74.35212675  37.26479033  85.97328125\n",
      "  53.19523824 162.70193075  59.67414784 154.47562779  65.64083741]\n",
      "14-th iteration, loss: 0.2899160384369114, 31 gd steps\n",
      "insert gradient: -0.0007534645247587004\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45080894  74.34511922  37.26823894  85.97302267\n",
      "  53.19451749 162.7051176   59.67398052 154.47426476  65.64486254]\n",
      "15-th iteration, loss: 0.2899159114761373, 30 gd steps\n",
      "insert gradient: -0.000758744703003412\n",
      "15-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45445156  74.33835413  37.27145523  85.97278173\n",
      "  53.19392454 162.70820712  59.67394297 154.47299126  65.64879398]\n",
      "16-th iteration, loss: 0.2899157906203835, 29 gd steps\n",
      "insert gradient: -0.0007597046059790246\n",
      "16-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45784316  74.33180541  37.2745015   85.97254915\n",
      "  53.19338893 162.71119423  59.67396116 154.47177703  65.65262675]\n",
      "17-th iteration, loss: 0.2899156750565602, 28 gd steps\n",
      "insert gradient: -0.0007583931429034879\n",
      "17-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.46105102  74.32545833  37.27741237  85.97232238\n",
      "  53.19288148 162.71407952  59.67400196 154.47060698  65.65635962]\n",
      "18-th iteration, loss: 0.28991556420668974, 28 gd steps\n",
      "insert gradient: -0.0007558439554638061\n",
      "18-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14641144e+01 7.43192987e+01 3.72802106e+01\n",
      " 8.59721010e+01 5.31923899e+01 1.62716868e+02 5.96740503e+01\n",
      " 1.54469472e+02 0.00000000e+00 6.75015599e-14 6.56599956e+01]\n",
      "19-th iteration, loss: 0.28991541354236106, 33 gd steps\n",
      "insert gradient: -0.0007062920674571224\n",
      "19-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.46784985  74.31204718  37.28354317  85.97181861\n",
      "  53.191714   162.72012701  59.67400216 154.46807147  65.66837078]\n",
      "20-th iteration, loss: 0.28991531168828566, 26 gd steps\n",
      "insert gradient: -0.0007118509839249001\n",
      "20-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.47088603  74.30631274  37.28622368  85.97159727\n",
      "  53.19116377 162.72271975  59.67394048 154.46692575  65.67162199]\n",
      "21-th iteration, loss: 0.2899152137830325, 26 gd steps\n",
      "insert gradient: -0.0007136867119771247\n",
      "21-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14737416e+01 7.43007195e+01 3.72887935e+01\n",
      " 8.59713885e+01 5.31906745e+01 1.62725251e+02 5.96739350e+01\n",
      " 1.54465825e+02 0.00000000e+00 2.66453526e-14 6.56748191e+01]\n",
      "22-th iteration, loss: 0.28991508458237675, 30 gd steps\n",
      "insert gradient: -0.0006733700961770494\n",
      "22-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14770880e+01 7.42942730e+01 3.72917661e+01\n",
      " 8.59711347e+01 5.31900617e+01 1.62728155e+02 5.96738769e+01\n",
      " 1.54464519e+02 0.00000000e+00 9.94759830e-14 6.56820649e+01]\n",
      "23-th iteration, loss: 0.2899149636731344, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48043214  74.28815861  37.29465877  85.97088299\n",
      "  53.18941444 162.73091999  59.67374173 154.46320776  65.6886904 ]\n",
      "24-th iteration, loss: 0.28991487611257305, 24 gd steps\n",
      "insert gradient: -0.0006528994457853435\n",
      "24-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14832132e+01 7.42830495e+01 3.72970797e+01\n",
      " 8.59706805e+01 5.31889060e+01 1.62733252e+02 5.96736606e+01\n",
      " 1.54462104e+02 0.00000000e+00 1.77635684e-15 6.56914320e+01]\n",
      "25-th iteration, loss: 0.28991476562231194, 28 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "25-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48629208  74.27731249  37.29976968  85.97044852\n",
      "  53.18834318 162.73586947  59.67358203 154.46085648  65.69753414]\n",
      "26-th iteration, loss: 0.2899146840115599, 23 gd steps\n",
      "insert gradient: -0.0006327152517430554\n",
      "26-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48890621  74.27244676  37.30205886  85.97025617\n",
      "  53.18788098 162.73810389  59.67353194 154.45978859  65.70009739]\n",
      "27-th iteration, loss: 0.2899146051249046, 23 gd steps\n",
      "insert gradient: -0.0006373191958709217\n",
      "27-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.49136725  74.26767107  37.3042597   85.97007299\n",
      "  53.18747128 162.74029824  59.67353722 154.45876262  65.70264262]\n",
      "28-th iteration, loss: 0.28991452864874, 22 gd steps\n",
      "insert gradient: -0.000639630778912058\n",
      "28-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14937156e+01 7.42629849e+01 3.73063884e+01\n",
      " 8.59698956e+01 5.31870901e+01 1.62742447e+02 5.96735723e+01\n",
      " 1.54457769e+02 0.00000000e+00 5.15143483e-14 6.57051620e+01]\n",
      "29-th iteration, loss: 0.28991443240454823, 25 gd steps\n",
      "insert gradient: -0.0006095841641735134\n",
      "29-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.49635898  74.25777814  37.30875694  85.96968764\n",
      "  53.18662646 162.74482015  59.67357304 154.45664314  65.71069096]\n",
      "30-th iteration, loss: 0.289914360218427, 22 gd steps\n",
      "insert gradient: -0.0006150501113203594\n",
      "30-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14986756e+01 7.42533006e+01 3.73108185e+01\n",
      " 8.59695105e+01 5.31862186e+01 1.62746867e+02 5.96735633e+01\n",
      " 1.54455657e+02 0.00000000e+00 9.94759830e-14 6.57130414e+01]\n",
      "31-th iteration, loss: 0.289914270819755, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "31-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50122286  74.24837057  37.31307907  85.96930973\n",
      "  53.18575646 162.7491139   59.67354156 154.45456017  65.71817076]\n",
      "32-th iteration, loss: 0.2899142025819352, 21 gd steps\n",
      "insert gradient: -0.000595914234683489\n",
      "32-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50346571  74.24407373  37.31506557  85.96913807\n",
      "  53.18535618 162.75108025  59.67352454 154.45359247  65.72038596]\n",
      "33-th iteration, loss: 0.2899141362368082, 21 gd steps\n",
      "insert gradient: -0.0005997026492926028\n",
      "33-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15056013e+01 7.42398424e+01 3.73169906e+01\n",
      " 8.59689737e+01 5.31849931e+01 1.62753016e+02 5.96735459e+01\n",
      " 1.54452656e+02 0.00000000e+00 6.75015599e-14 6.57225910e+01]\n",
      "34-th iteration, loss: 0.28991405463183817, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50794696  74.23521846  37.31909088  85.96878714\n",
      "  53.18457391 162.75512131  59.67354883 154.45161903  65.7273653 ]\n",
      "35-th iteration, loss: 0.28991399160897574, 20 gd steps\n",
      "insert gradient: -0.0005805571051419331\n",
      "35-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15100423e+01 7.42311485e+01 3.73209585e+01\n",
      " 8.59686253e+01 5.31842014e+01 1.62756980e+02 5.96735470e+01\n",
      " 1.54450694e+02 0.00000000e+00 1.95399252e-14 6.57294473e+01]\n",
      "36-th iteration, loss: 0.2899139149670645, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "36-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.51230951  74.2267291   37.3229762   85.96844502\n",
      "  53.18378849 162.75899216  59.67353895 154.44968272  65.73393666]\n",
      "37-th iteration, loss: 0.2899138549949614, 20 gd steps\n",
      "insert gradient: -0.0005648798401899938\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15143438e+01 7.42228004e+01 3.73247837e+01\n",
      " 8.59682880e+01 5.31834242e+01 1.62760787e+02 5.96735342e+01\n",
      " 1.54448775e+02 0.00000000e+00 5.68434189e-14 6.57359171e+01]\n",
      "38-th iteration, loss: 0.28991378269340795, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.51652588  74.21855776  37.32672296  85.96811455\n",
      "  53.18302651 162.76272056  59.67352751 154.44778999  65.74016942]\n",
      "39-th iteration, loss: 0.28991372554644945, 19 gd steps\n",
      "insert gradient: -0.0005511453554406479\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15184941e+01 7.42147583e+01 3.73284711e+01\n",
      " 8.59679624e+01 5.31826746e+01 1.62764458e+02 5.96735258e+01\n",
      " 1.54446901e+02 0.00000000e+00 5.68434189e-14 6.57420608e+01]\n",
      "40-th iteration, loss: 0.2899136571387945, 21 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52059277  74.2106747   37.33033646  85.96779544\n",
      "  53.18229341 162.76632037  59.67352409 154.44594255  65.74610602]\n",
      "41-th iteration, loss: 0.28991360256849386, 19 gd steps\n",
      "insert gradient: -0.0005387161956462358\n",
      "41-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15224972e+01 7.42069924e+01 3.73320287e+01\n",
      " 8.59676479e+01 5.31819539e+01 1.62768005e+02 5.96735271e+01\n",
      " 1.54445072e+02 0.00000000e+00 5.86197757e-14 6.57479189e+01]\n",
      "42-th iteration, loss: 0.28991353766751554, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52451741  74.20305287  37.33382577  85.96748668\n",
      "  53.18158814 162.7698028   59.67353084 154.44413908  65.75178047]\n",
      "43-th iteration, loss: 0.28991348548892004, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52636059  74.19948004  37.33546501  85.96734336\n",
      "  53.18126032 162.77143819  59.67353859 154.44328735  65.75352181]\n",
      "44-th iteration, loss: 0.2899134344890953, 18 gd steps\n",
      "insert gradient: -0.0005313977774290897\n",
      "44-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15281274e+01 7.41959471e+01 3.73370619e+01\n",
      " 8.59672051e+01 5.31809593e+01 1.62773054e+02 5.96735751e+01\n",
      " 1.54442459e+02 0.00000000e+00 9.94759830e-14 6.57552641e+01]\n",
      "45-th iteration, loss: 0.28991337387168153, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53001242  74.19218071  37.33875831  85.96705287\n",
      "  53.18062452 162.77476862  59.67360254 154.44157008  65.7589565 ]\n",
      "46-th iteration, loss: 0.28991332492329974, 18 gd steps\n",
      "insert gradient: -0.0005183337349436476\n",
      "46-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15317509e+01 7.41887507e+01 3.73403170e+01\n",
      " 8.59669162e+01 5.31803169e+01 1.62776333e+02 5.96736246e+01\n",
      " 1.54440753e+02 0.00000000e+00 6.75015599e-14 6.57606268e+01]\n",
      "47-th iteration, loss: 0.2899132671519165, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53358862  74.18510825  37.34196311  85.96676763\n",
      "  53.17998378 162.7779887   59.67364383 154.43988192  65.76415823]\n",
      "48-th iteration, loss: 0.2899132201350472, 18 gd steps\n",
      "insert gradient: -0.000507242573518087\n",
      "48-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53528719  74.18177175  37.34348206  85.96663408\n",
      "  53.17967984 162.77950908  59.67366243 154.43907833  65.7657667 ]\n",
      "49-th iteration, loss: 0.28991317410656886, 17 gd steps\n",
      "insert gradient: -0.000510973106950792\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15369215e+01 7.41784703e+01 3.73449653e+01\n",
      " 8.59665050e+01 5.31793985e+01 1.62781012e+02 5.96737047e+01\n",
      " 1.54438295e+02 0.00000000e+00 3.37507799e-14 6.57673764e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010627\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.4077617    0.         708.36108252]\n",
      "1-th iteration, loss: 0.7509319290812663, 11 gd steps\n",
      "insert gradient: -0.6352343139406681\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 233.24084424   0.         475.12023828]\n",
      "2-th iteration, loss: 0.6031459279500627, 13 gd steps\n",
      "insert gradient: -0.6664488763860885\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.09379575  77.43082016 218.69660091  42.28660271 242.40828483\n",
      "   0.         232.71195344]\n",
      "3-th iteration, loss: 0.46727127496117205, 20 gd steps\n",
      "insert gradient: -0.572051658172892\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.24241481 225.52287286  54.91248034 160.82793181\n",
      "  53.04839126 212.76521458   0.          19.94673887]\n",
      "4-th iteration, loss: 0.35822815703124083, 37 gd steps\n",
      "insert gradient: -0.683665323532189\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.1875882   52.47210031 127.65090025   0.         118.53297881\n",
      "  53.08820655 154.52721102  55.29760568 160.94566795  53.20751706\n",
      "  19.94673887]\n",
      "5-th iteration, loss: 0.2899159316695374, 121 gd steps\n",
      "insert gradient: -0.00113836250367192\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14640629e+01 7.42092404e+01 3.72926482e+01\n",
      " 8.59428423e+01 5.32237253e+01 1.62606793e+02 5.97056863e+01\n",
      " 1.54453956e+02 0.00000000e+00 4.79616347e-14 6.55717193e+01\n",
      " 1.99467389e+01]\n",
      "6-th iteration, loss: 0.28991558963596376, 57 gd steps\n",
      "insert gradient: -0.0010061164957668308\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47127299  74.20110682  37.29922314  85.94355674\n",
      "  53.22131295 162.61669893  59.70439993 154.45379677  65.59306045\n",
      "  19.94673887]\n",
      "7-th iteration, loss: 0.2899154498443298, 32 gd steps\n",
      "insert gradient: -0.0010144997349880683\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14753248e+01 7.41966033e+01 3.73027449e+01\n",
      " 8.59438789e+01 5.32199235e+01 1.62622123e+02 5.97036212e+01\n",
      " 1.54453574e+02 0.00000000e+00 1.77635684e-15 6.55986490e+01\n",
      " 1.99467389e+01]\n",
      "8-th iteration, loss: 0.2899152086593886, 45 gd steps\n",
      "insert gradient: -0.0009619649648282179\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.48094406  74.19019993  37.30753246  85.94425787\n",
      "  53.21794116 162.62967988  59.7024731  154.45311374  65.61384348\n",
      "  19.94673887]\n",
      "9-th iteration, loss: 0.28991509102036983, 29 gd steps\n",
      "insert gradient: -0.0009607747828860331\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14844957e+01 7.41861029e+01 3.73105309e+01\n",
      " 8.59444839e+01 5.32167230e+01 0.00000000e+00 1.15463195e-14\n",
      " 1.62634480e+02 5.97017665e+01 1.54452747e+02 6.56185244e+01\n",
      " 1.99467389e+01]\n",
      "10-th iteration, loss: 0.28991490103814493, 38 gd steps\n",
      "insert gradient: -0.0009383143065006878\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14890056e+01 7.41806028e+01 3.73143961e+01\n",
      " 8.59447661e+01 5.32150485e+01 1.62646726e+02 5.97007781e+01\n",
      " 1.54452201e+02 0.00000000e+00 1.77635684e-15 6.56247562e+01\n",
      " 1.99467389e+01]\n",
      "11-th iteration, loss: 0.28991472623401005, 37 gd steps\n",
      "insert gradient: -0.0008835416739513316\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14933437e+01 7.41754372e+01 3.73180945e+01\n",
      " 8.59450326e+01 5.32133279e+01 1.62652247e+02 5.96996752e+01\n",
      " 1.54451570e+02 0.00000000e+00 7.28306304e-14 6.56362547e+01\n",
      " 1.99467389e+01]\n",
      "12-th iteration, loss: 0.28991457176741847, 34 gd steps\n",
      "insert gradient: -0.0008740087490279489\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49741009  74.17068148  37.32148744  85.94526496\n",
      "  53.21179893 162.65734376  59.6986716  154.45088082  65.64632969\n",
      "  19.94673887]\n",
      "13-th iteration, loss: 0.2899144830946842, 24 gd steps\n",
      "insert gradient: -0.0008767872270387542\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15002908e+01 7.41671852e+01 3.73239178e+01\n",
      " 8.59454407e+01 5.32107865e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.62661086e+02 5.96980359e+01 1.54450360e+02 6.56499475e+01\n",
      " 1.99467389e+01]\n",
      "14-th iteration, loss: 0.2899143485440195, 31 gd steps\n",
      "insert gradient: -0.0008527564265786557\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15037237e+01 7.41627639e+01 3.73268667e+01\n",
      " 8.59456537e+01 5.32095306e+01 1.62670190e+02 5.96972655e+01\n",
      " 1.54449691e+02 0.00000000e+00 1.59872116e-14 6.56545047e+01\n",
      " 1.99467389e+01]\n",
      "15-th iteration, loss: 0.2899142226529973, 30 gd steps\n",
      "insert gradient: -0.0008158320440366761\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50705427  74.15854605  37.32972063  85.94585897\n",
      "  53.20824603 162.67439302  59.69643877 154.44899327  65.66310229\n",
      "  19.94673887]\n",
      "16-th iteration, loss: 0.2899141470589896, 22 gd steps\n",
      "insert gradient: -0.000822713885717546\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50955072  74.15532099  37.33186914  85.94602127\n",
      "  53.2073455  162.67761424  59.69588017 154.44844656  65.6663252\n",
      "  19.94673887]\n",
      "17-th iteration, loss: 0.28991407454186113, 22 gd steps\n",
      "insert gradient: -0.0008265187087684173\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15118514e+01 7.41521304e+01 3.73339004e+01\n",
      " 8.59461832e+01 5.32065512e+01 1.62680771e+02 5.96954294e+01\n",
      " 1.54447928e+02 0.00000000e+00 5.32907052e-15 6.56695075e+01\n",
      " 1.99467389e+01]\n",
      "18-th iteration, loss: 0.28991396664366065, 27 gd steps\n",
      "insert gradient: -0.0008046477874752342\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51464617  74.1482377   37.33633539  85.94636232\n",
      "  53.20557681 162.68458569  59.69487032 154.44726106  65.67710922\n",
      "  19.94673887]\n",
      "19-th iteration, loss: 0.2899138992420893, 21 gd steps\n",
      "insert gradient: -0.0008029476641587885\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51687032  74.14517766  37.33825413  85.94650295\n",
      "  53.20482593 162.68758623  59.6944413  154.44671238  65.68004398\n",
      "  19.94673887]\n",
      "20-th iteration, loss: 0.289913834062906, 21 gd steps\n",
      "insert gradient: -0.0007981589463985573\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51896028  74.14214839  37.34009094  85.94664197\n",
      "  53.20413941 162.69052842  59.69407864 154.44618225  65.68294649\n",
      "  19.94673887]\n",
      "21-th iteration, loss: 0.2899137708305173, 20 gd steps\n",
      "insert gradient: -0.0007915959216129823\n",
      "21-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15209519e+01 7.41391538e+01 3.73418611e+01\n",
      " 8.59467773e+01 5.32034937e+01 0.00000000e+00 6.21724894e-15\n",
      " 1.62693409e+02 5.96937575e+01 1.54445664e+02 6.56858114e+01\n",
      " 1.99467389e+01]\n",
      "22-th iteration, loss: 0.28991367967189263, 24 gd steps\n",
      "insert gradient: -0.0007847460787396109\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52330442  74.13560478  37.34393666  85.9469259\n",
      "  53.20267204 162.7000344   59.69332515 154.44502325  65.68919468\n",
      "  19.94673887]\n",
      "23-th iteration, loss: 0.2899136205550636, 20 gd steps\n",
      "insert gradient: -0.000786985562284491\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52523571  74.13273244  37.34565357  85.94705666\n",
      "  53.20196539 162.70264546  59.69293635 154.44449143  65.69195707\n",
      "  19.94673887]\n",
      "24-th iteration, loss: 0.2899135630781503, 19 gd steps\n",
      "insert gradient: -0.0007867501936979235\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52707584  74.12988892  37.34731243  85.94718829\n",
      "  53.2013151  162.70521538  59.69260076 154.44397279  65.69468848\n",
      "  19.94673887]\n",
      "25-th iteration, loss: 0.2899135070536776, 19 gd steps\n",
      "insert gradient: -0.0007849541962407556\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52884896  74.1270754   37.34892309  85.94731861\n",
      "  53.20070276 162.70774215  59.69230038 154.44346252  65.69738655\n",
      "  19.94673887]\n",
      "26-th iteration, loss: 0.2899134523763635, 19 gd steps\n",
      "insert gradient: -0.0007821695321155699\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15305699e+01 7.41242933e+01 3.73504921e+01\n",
      " 8.59474465e+01 5.32001174e+01 1.62710224e+02 5.96920241e+01\n",
      " 1.54442958e+02 0.00000000e+00 4.79616347e-14 6.57000493e+01\n",
      " 1.99467389e+01]\n",
      "27-th iteration, loss: 0.28991337270531475, 23 gd steps\n",
      "insert gradient: -0.0007433691394065065\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53264725  74.12103013  37.35233693  85.94758336\n",
      "  53.19939293 162.71311493  59.69165604 154.44233024  65.70618327\n",
      "  19.94673887]\n",
      "28-th iteration, loss: 0.28991332084672894, 18 gd steps\n",
      "insert gradient: -0.0007451400455752477\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.5344146   74.11835618  37.35387785  85.94769713\n",
      "  53.19879337 162.71549107  59.69134208 154.44179057  65.70865858\n",
      "  19.94673887]\n",
      "29-th iteration, loss: 0.2899132702287764, 18 gd steps\n",
      "insert gradient: -0.0007450124076006166\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53610996  74.11570469  37.35537486  85.94781117\n",
      "  53.19823329 162.7178332   59.69106707 154.44126193  65.71111224\n",
      "  19.94673887]\n",
      "30-th iteration, loss: 0.28991322074758274, 18 gd steps\n",
      "insert gradient: -0.0007436427744253974\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53775023  74.11307772  37.35683442  85.94792399\n",
      "  53.1977003  162.72013899  59.69081846 154.44074099  65.71354124\n",
      "  19.94673887]\n",
      "31-th iteration, loss: 0.289913172333216, 18 gd steps\n",
      "insert gradient: -0.0007414558693200992\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15393465e+01 7.41104766e+01 3.73582612e+01\n",
      " 8.59480348e+01 5.31971867e+01 1.62722407e+02 5.96905883e+01\n",
      " 1.54440226e+02 0.00000000e+00 3.01980663e-14 6.57159438e+01\n",
      " 1.99467389e+01]\n",
      "32-th iteration, loss: 0.2899131036970446, 21 gd steps\n",
      "insert gradient: -0.0007073901759010153\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54122468  74.10749157  37.35990281  85.94815084\n",
      "  53.19656322 162.72499126  59.6902864  154.43960561  65.72136831\n",
      "  19.94673887]\n",
      "33-th iteration, loss: 0.2899130575012752, 17 gd steps\n",
      "insert gradient: -0.0007091106827890051\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54286188  74.10498049  37.3613113   85.94825003\n",
      "  53.19603116 162.72717078  59.69002024 154.43906228  65.72361898\n",
      "  19.94673887]\n",
      "34-th iteration, loss: 0.28991301230257505, 17 gd steps\n",
      "insert gradient: -0.0007092210426579731\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15444380e+01 7.41024880e+01 3.73626843e+01\n",
      " 8.59483496e+01 5.31955307e+01 1.62729322e+02 5.96897861e+01\n",
      " 1.54438529e+02 0.00000000e+00 1.59872116e-14 6.57258539e+01\n",
      " 1.99467389e+01]\n",
      "35-th iteration, loss: 0.2899129496502923, 20 gd steps\n",
      "insert gradient: -0.0006795687709264483\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54624424  74.09966404  37.36423574  85.9484536\n",
      "  53.19494444 162.73174164  59.68949967 154.43790479  65.73084399\n",
      "  19.94673887]\n",
      "36-th iteration, loss: 0.28991290638150147, 17 gd steps\n",
      "insert gradient: -0.0006821759487287941\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54783326  74.09724436  37.3655862   85.94854469\n",
      "  53.19444063 162.73382015  59.68924957 154.43735281  65.73295556\n",
      "  19.94673887]\n",
      "37-th iteration, loss: 0.2899128640183478, 17 gd steps\n",
      "insert gradient: -0.0006830752432603486\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54936045  74.09484003  37.36690257  85.94863641\n",
      "  53.19396792 162.73587377  59.68903209 154.436812    65.73505617\n",
      "  19.94673887]\n",
      "38-th iteration, loss: 0.28991282248827366, 17 gd steps\n",
      "insert gradient: -0.0006828102986294323\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15508397e+01 7.40924531e+01 3.73681898e+01\n",
      " 8.59487275e+01 5.31935164e+01 1.62737900e+02 5.96888370e+01\n",
      " 1.54436280e+02 0.00000000e+00 1.42108547e-14 6.57371428e+01\n",
      " 1.99467389e+01]\n",
      "39-th iteration, loss: 0.28991276584175235, 19 gd steps\n",
      "insert gradient: -0.0006552215679299371\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55251651  74.0897841   37.36962734  85.94882069\n",
      "  53.19298909 162.74014961  59.68859566 154.43566568  65.74174497\n",
      "  19.94673887]\n",
      "40-th iteration, loss: 0.28991272596650036, 16 gd steps\n",
      "insert gradient: -0.0006575227891569441\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55401515  74.08746451  37.37089799  85.94890323\n",
      "  53.19252597 162.74210849  59.68837858 154.43511618  65.74372156\n",
      "  19.94673887]\n",
      "41-th iteration, loss: 0.28991268686588767, 16 gd steps\n",
      "insert gradient: -0.000658362507267293\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55546026  74.08515875  37.37213942  85.94898623\n",
      "  53.19208854 162.74404482  59.68818885 154.43457685  65.74568941\n",
      "  19.94673887]\n",
      "42-th iteration, loss: 0.2899126484849987, 16 gd steps\n",
      "insert gradient: -0.0006581897179609029\n",
      "42-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15568633e+01 7.40828687e+01 3.73733557e+01\n",
      " 8.59490687e+01 5.31916688e+01 1.62745957e+02 5.96880181e+01\n",
      " 1.54434045e+02 0.00000000e+00 7.28306304e-14 6.57476458e+01\n",
      " 1.99467389e+01]\n",
      "43-th iteration, loss: 0.2899125968649508, 18 gd steps\n",
      "insert gradient: -0.0006326527306614531\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15584360e+01 7.40803351e+01 3.73747000e+01\n",
      " 8.59491520e+01 5.31911832e+01 1.62748056e+02 5.96878076e+01\n",
      " 1.54433441e+02 0.00000000e+00 4.79616347e-14 6.57519188e+01\n",
      " 1.99467389e+01]\n",
      "44-th iteration, loss: 0.28991254716896214, 18 gd steps\n",
      "insert gradient: -0.0006245925297911742\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56005242  74.07786907  37.37604311  85.949228\n",
      "  53.19067565 162.75010168  59.68756242 154.43281957  65.75597767\n",
      "  19.94673887]\n",
      "45-th iteration, loss: 0.2899125108929783, 16 gd steps\n",
      "insert gradient: -0.0006234986598519709\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56150527  74.07567338  37.37724932  85.94929868\n",
      "  53.19023324 162.75192689  59.68735224 154.4322579   65.75776786\n",
      "  19.94673887]\n",
      "46-th iteration, loss: 0.2899124753187014, 16 gd steps\n",
      "insert gradient: -0.0006208236994948888\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56289801  74.07348683  37.37842575  85.94937074\n",
      "  53.18982078 162.75373518  59.68717477 154.43170879  65.75955621\n",
      "  19.94673887]\n",
      "47-th iteration, loss: 0.28991244038061376, 16 gd steps\n",
      "insert gradient: -0.0006194395228302146\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.5642443   74.07131172  37.37957698  85.94944285\n",
      "  53.18942856 162.75552387  59.68701993 154.43116935  65.7613393\n",
      "  19.94673887]\n",
      "48-th iteration, loss: 0.28991240605078056, 15 gd steps\n",
      "insert gradient: -0.0006196660282405624\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15655531e+01 7.40691506e+01 3.73807060e+01\n",
      " 8.59495143e+01 5.31890504e+01 1.62757291e+02 5.96868810e+01\n",
      " 1.54430638e+02 0.00000000e+00 7.28306304e-14 6.57631140e+01\n",
      " 1.99467389e+01]\n",
      "49-th iteration, loss: 0.2899123608965793, 17 gd steps\n",
      "insert gradient: -0.0006044731180738825\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56699565  74.0667945   37.38193493  85.94958501\n",
      "  53.18861895 162.75920255  59.68671085 154.43004551  65.76693852\n",
      "  19.94673887]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5365154209643013\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.79326351   0.         735.10120886]\n",
      "1-th iteration, loss: 0.7479635372963357, 11 gd steps\n",
      "insert gradient: -0.629935558820551\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.45607731  62.35062607 233.0808711    0.         502.02033775]\n",
      "2-th iteration, loss: 0.6041408570093462, 13 gd steps\n",
      "insert gradient: -0.6488079377536331\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.55728014  77.7190542  218.75271497  41.97575678 245.88751237\n",
      "   0.         256.13282539]\n",
      "3-th iteration, loss: 0.4533194909358523, 20 gd steps\n",
      "insert gradient: -0.47971318921656037\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          50.34666105 227.57940403  55.93163505 143.23116118\n",
      "  48.73260395 190.27009886   0.          65.86272653]\n",
      "4-th iteration, loss: 0.3679040355946287, 21 gd steps\n",
      "insert gradient: -0.6945722474153982\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.79507152  51.21420814 111.33479806   0.         119.89901329\n",
      "  62.65974693 133.56601204  58.82764426 157.17153119  49.014536\n",
      "  65.86272653]\n",
      "5-th iteration, loss: 0.2414051425610868, 82 gd steps\n",
      "insert gradient: -1.373488081248325e-05\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[1.88380728e+00 4.78618775e+01 7.54371478e+01 4.19509101e+01\n",
      " 9.45463203e+01 7.92869192e+01 1.18505120e+02 0.00000000e+00\n",
      " 3.73034936e-14 8.22885996e+01 1.28252212e+02 8.28340307e+01\n",
      " 6.58627265e+01]\n",
      "6-th iteration, loss: 0.2414051422211473, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.264054523245517e-06\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259297  47.86035475  75.43960072  41.95000832  94.54812601\n",
      "  79.28597337 118.50565336  82.28962274 128.25280265  82.83328733\n",
      "  65.86272653]\n",
      "7-th iteration, loss: 0.24140514222014317, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.037815172144931e-06\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259307  47.86035393  75.43960215  41.95001154  94.54812622\n",
      "  79.28597217 118.5056501   82.2896144  128.25280016  82.83328269\n",
      "  65.86272653]\n",
      "8-th iteration, loss: 0.24140514221923257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8299276514707934e-06\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259315e+00 4.78603531e+01 7.54396035e+01 0.00000000e+00\n",
      " 2.39808173e-14 4.19500145e+01 9.45481264e+01 7.92859711e+01\n",
      " 1.18505647e+02 8.22896064e+01 1.28252798e+02 8.28332782e+01\n",
      " 6.58627265e+01]\n",
      "9-th iteration, loss: 0.24140514221833842, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4609811677014146e-06\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259321e+00 4.78603521e+01 7.54396047e+01 2.73631659e-06\n",
      " 1.18242317e-06 4.19500173e+01 9.45481266e+01 7.92859703e+01\n",
      " 1.18505644e+02 8.22895988e+01 1.28252796e+02 8.28332739e+01\n",
      " 6.58627265e+01]\n",
      "10-th iteration, loss: 0.24140514221754575, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.149114522272456e-06\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259323e+00 4.78603510e+01 7.54396057e+01 5.10382014e-06\n",
      " 2.14812083e-06 4.19500197e+01 9.45481267e+01 7.92859695e+01\n",
      " 1.18505641e+02 8.22895915e+01 1.28252794e+02 8.28332698e+01\n",
      " 6.58627265e+01]\n",
      "11-th iteration, loss: 0.24140514221683826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8850007849657438e-06\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259323e+00 4.78603498e+01 7.54396065e+01 7.15974066e-06\n",
      " 2.93110493e-06 0.00000000e+00 1.11173074e-21 4.19500217e+01\n",
      " 9.45481268e+01 7.92859689e+01 1.18505639e+02 8.22895845e+01\n",
      " 1.28252792e+02 8.28332659e+01 6.58627265e+01]\n",
      "12-th iteration, loss: 0.2414051422161775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.550565290489033e-06\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259321e+00 4.78603485e+01 7.54396072e+01 8.91699602e-06\n",
      " 3.54005067e-06 1.79953094e-06 6.08945747e-07 4.19500235e+01\n",
      " 9.45481268e+01 7.92859684e+01 1.18505636e+02 8.22895778e+01\n",
      " 1.28252790e+02 8.28332621e+01 6.58627265e+01]\n",
      "13-th iteration, loss: 0.24140514221559362, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2891401681792642e-06\n",
      "13-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259316e+00 4.78603471e+01 7.54396078e+01 1.03447445e-05\n",
      " 3.97194784e-06 3.27833791e-06 1.01488813e-06 4.19500250e+01\n",
      " 9.45481268e+01 7.92859681e+01 1.18505634e+02 8.22895714e+01\n",
      " 1.28252788e+02 8.28332584e+01 6.58627265e+01]\n",
      "14-th iteration, loss: 0.24140514221507126, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.083972506648409e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259309e+00 4.78603456e+01 7.54396082e+01 1.15160885e-05\n",
      " 4.26681928e-06 4.50696960e-06 1.26247585e-06 4.19500263e+01\n",
      " 9.45481267e+01 7.92859678e+01 1.18505632e+02 8.22895653e+01\n",
      " 1.28252787e+02 8.28332549e+01 6.58627265e+01]\n",
      "15-th iteration, loss: 0.2414051422145999, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.221788860705804e-07\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259302e+00 4.78603440e+01 7.54396086e+01 1.24874135e-05\n",
      " 4.45551704e-06 5.53983522e-06 1.38616925e-06 4.19500273e+01\n",
      " 9.45481266e+01 7.92859677e+01 1.18505630e+02 8.22895594e+01\n",
      " 1.28252785e+02 8.28332515e+01 6.58627265e+01]\n",
      "16-th iteration, loss: 0.2414051422141719, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.938506348008796e-07\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259293e+00 4.78603425e+01 7.54396089e+01 1.33022455e-05\n",
      " 4.56183708e-06 6.41892949e-06 1.41258785e-06 0.00000000e+00\n",
      " 1.85288457e-22 4.19500282e+01 9.45481266e+01 7.92859676e+01\n",
      " 1.18505628e+02 8.22895538e+01 1.28252784e+02 8.28332483e+01\n",
      " 6.58627265e+01]\n",
      "17-th iteration, loss: 0.24140514221377724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.514032215275355e-07\n",
      "17-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259285e+00 4.78603410e+01 7.54396091e+01 1.39824485e-05\n",
      " 4.59761576e-06 7.16492810e-06 1.35578589e-06 4.19500298e+01\n",
      " 9.45481265e+01 7.92859677e+01 1.18505626e+02 8.22895484e+01\n",
      " 1.28252783e+02 8.28332451e+01 6.58627265e+01]\n",
      "18-th iteration, loss: 0.2414051422134207, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.77352108480204e-07\n",
      "18-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259277e+00 4.78603394e+01 7.54396093e+01 1.45405374e-05\n",
      " 4.57342884e-06 7.78932877e-06 1.22825876e-06 4.19500304e+01\n",
      " 9.45481264e+01 7.92859678e+01 1.18505624e+02 8.22895432e+01\n",
      " 1.28252782e+02 8.28332421e+01 6.58627265e+01]\n",
      "19-th iteration, loss: 0.24140514221309314, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.168338680697711e-07\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259269e+00 4.78603379e+01 7.54396094e+01 1.50302437e-05\n",
      " 4.51493520e-06 8.34499785e-06 1.05741918e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.19500310e+01 9.45481263e+01 7.92859680e+01\n",
      " 1.18505623e+02 8.22895382e+01 1.28252781e+02 8.28332392e+01\n",
      " 6.58627265e+01]\n",
      "20-th iteration, loss: 0.24140514221278972, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.398974254666863e-07\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259261e+00 4.78603365e+01 7.54396096e+01 1.54589961e-05\n",
      " 4.42616771e-06 8.83886939e-06 8.48291327e-07 4.19500320e+01\n",
      " 9.45481262e+01 7.92859683e+01 1.18505621e+02 8.22895334e+01\n",
      " 1.28252780e+02 8.28332364e+01 6.58627265e+01]\n",
      "21-th iteration, loss: 0.24140514221251197, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.0376852838059454e-07\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259255e+00 4.78603350e+01 7.54396097e+01 1.58250756e-05\n",
      " 4.30841438e-06 9.26878778e-06 6.03054533e-07 0.00000000e+00\n",
      " 1.32348898e-23 4.19500324e+01 9.45481261e+01 7.92859687e+01\n",
      " 1.18505620e+02 8.22895288e+01 1.28252779e+02 8.28332337e+01\n",
      " 6.58627265e+01]\n",
      "22-th iteration, loss: 0.24140514221225404, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.615960606120244e-07\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259249e+00 4.78603336e+01 7.54396098e+01 1.61570625e-05\n",
      " 4.17497390e-06 9.66291511e-06 3.35929899e-07 4.19500332e+01\n",
      " 9.45481261e+01 7.92859691e+01 1.18505619e+02 8.22895244e+01\n",
      " 1.28252779e+02 8.28332311e+01 6.58627265e+01]\n",
      "23-th iteration, loss: 0.24140514221201662, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.185170929336474e-07\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259243e+00 4.78603323e+01 7.54396098e+01 1.64483515e-05\n",
      " 4.02384598e-06 1.00144199e-05 4.54333310e-08 4.19500336e+01\n",
      " 9.45481260e+01 7.92859696e+01 1.18505618e+02 8.22895201e+01\n",
      " 1.28252778e+02 8.28332286e+01 6.58627265e+01]\n",
      "24-th iteration, loss: 0.24140514221179715, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.709020333684209e-07\n",
      "24-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259239e+00 4.78603310e+01 7.54396099e+01 1.67235069e-05\n",
      " 3.86657798e-06 4.19500443e+01 9.45481260e+01 7.92859701e+01\n",
      " 1.18505617e+02 8.22895160e+01 1.28252777e+02 8.28332261e+01\n",
      " 6.58627265e+01]\n",
      "25-th iteration, loss: 0.24140514221159434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.198102721098876e-07\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259235e+00 4.78603297e+01 7.54396100e+01 1.69772400e-05\n",
      " 3.70107695e-06 4.19500446e+01 9.45481260e+01 0.00000000e+00\n",
      " 1.24344979e-14 7.92859707e+01 1.18505615e+02 8.22895120e+01\n",
      " 1.28252777e+02 8.28332238e+01 6.58627265e+01]\n",
      "26-th iteration, loss: 0.2414051422114022, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.495740140253613e-07\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259233e+00 4.78603285e+01 7.54396101e+01 1.72224015e-05\n",
      " 3.53356696e-06 4.19500449e+01 9.45481260e+01 6.32795722e-07\n",
      " 3.29715765e-10 7.92859713e+01 1.18505615e+02 8.22895082e+01\n",
      " 1.28252776e+02 8.28332215e+01 6.58627265e+01]\n",
      "27-th iteration, loss: 0.24140514221122233, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.753092500541655e-07\n",
      "27-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603274e+01 7.54396102e+01 1.74590538e-05\n",
      " 3.36465011e-06 4.19500452e+01 9.45481260e+01 1.29399215e-06\n",
      " 2.47072027e-09 7.92859720e+01 1.18505614e+02 8.22895045e+01\n",
      " 1.28252776e+02 8.28332193e+01 6.58627265e+01]\n",
      "28-th iteration, loss: 0.2414051422110536, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.97367724503152e-07\n",
      "28-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603263e+01 7.54396102e+01 1.76868870e-05\n",
      " 3.19415012e-06 4.19500454e+01 9.45481260e+01 1.97965379e-06\n",
      " 4.53413430e-09 7.92859727e+01 1.18505613e+02 8.22895009e+01\n",
      " 1.28252776e+02 8.28332172e+01 6.58627265e+01]\n",
      "29-th iteration, loss: 0.2414051422108952, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.160717212651321e-07\n",
      "29-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603252e+01 7.54396103e+01 1.79057692e-05\n",
      " 3.02197945e-06 4.19500457e+01 9.45481261e+01 2.68621242e-06\n",
      " 4.86148000e-09 7.92859734e+01 1.18505612e+02 8.22894974e+01\n",
      " 1.28252776e+02 8.28332151e+01 6.58627265e+01]\n",
      "30-th iteration, loss: 0.24140514221074624, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.317166744588034e-07\n",
      "30-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603242e+01 7.54396104e+01 1.81157081e-05\n",
      " 2.84811842e-06 4.19500460e+01 9.45481261e+01 3.41043555e-06\n",
      " 1.99774262e-09 7.92859741e+01 1.18505611e+02 8.22894941e+01\n",
      " 1.28252775e+02 8.28332132e+01 6.58627265e+01]\n",
      "31-th iteration, loss: 0.24140514221060594, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.445339955114505e-07\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259233e+00 4.78603232e+01 7.54396105e+01 1.83168198e-05\n",
      " 2.67259830e-06 4.19500462e+01 9.45481261e+01 7.92859790e+01\n",
      " 1.18505611e+02 8.22894908e+01 1.28252775e+02 8.28332112e+01\n",
      " 6.58627265e+01]\n",
      "32-th iteration, loss: 0.2414051422104785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.725297704408164e-07\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259235e+00 4.78603222e+01 7.54396106e+01 1.85114946e-05\n",
      " 2.49626514e-06 4.19500464e+01 9.45481262e+01 7.92859797e+01\n",
      " 1.18505610e+02 8.22894876e+01 1.28252775e+02 8.28332094e+01\n",
      " 6.58627265e+01]\n",
      "33-th iteration, loss: 0.24140514221035883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.977318732636531e-07\n",
      "33-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259238e+00 4.78603213e+01 7.54396107e+01 1.86989560e-05\n",
      " 2.31797023e-06 4.19500467e+01 9.45481262e+01 7.92859805e+01\n",
      " 1.18505609e+02 8.22894846e+01 1.28252775e+02 8.28332076e+01\n",
      " 6.58627265e+01]\n",
      "34-th iteration, loss: 0.2414051422102463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.203188588626633e-07\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259242e+00 4.78603204e+01 7.54396108e+01 1.88791837e-05\n",
      " 2.13767898e-06 4.19500469e+01 9.45481263e+01 7.92859813e+01\n",
      " 1.18505609e+02 8.22894816e+01 1.28252775e+02 8.28332058e+01\n",
      " 6.58627265e+01]\n",
      "35-th iteration, loss: 0.2414051422101403, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.404585563749972e-07\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259246e+00 4.78603196e+01 7.54396108e+01 1.90522299e-05\n",
      " 1.95539528e-06 4.19500471e+01 9.45481264e+01 0.00000000e+00\n",
      " 3.55271368e-14 7.92859822e+01 1.18505608e+02 8.22894787e+01\n",
      " 1.28252775e+02 8.28332041e+01 6.58627265e+01]\n",
      "36-th iteration, loss: 0.24140514221003434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.378341641290647e-07\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259251e+00 4.78603188e+01 7.54396109e+01 1.92151056e-05\n",
      " 1.76995987e-06 4.19500473e+01 9.45481264e+01 8.42529911e-07\n",
      " 7.45849818e-08 7.92859830e+01 1.18505608e+02 8.22894759e+01\n",
      " 1.28252775e+02 8.28332024e+01 6.58627265e+01]\n",
      "37-th iteration, loss: 0.24140514220993367, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.340421285901981e-07\n",
      "37-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259256e+00 4.78603180e+01 7.54396110e+01 1.93684731e-05\n",
      " 1.58256061e-06 4.19500475e+01 9.45481265e+01 1.68099089e-06\n",
      " 1.36212682e-07 7.92859838e+01 1.18505608e+02 8.22894732e+01\n",
      " 1.28252775e+02 8.28332008e+01 6.58627265e+01]\n",
      "38-th iteration, loss: 0.24140514220983805, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.291965498671701e-07\n",
      "38-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259262e+00 4.78603172e+01 7.54396111e+01 1.95132244e-05\n",
      " 1.39359948e-06 4.19500476e+01 9.45481266e+01 2.51442879e-06\n",
      " 1.84800351e-07 7.92859847e+01 1.18505607e+02 8.22894706e+01\n",
      " 1.28252775e+02 8.28331993e+01 6.58627265e+01]\n",
      "39-th iteration, loss: 0.24140514220974713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.234027710764139e-07\n",
      "39-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259268e+00 4.78603165e+01 7.54396112e+01 1.96501586e-05\n",
      " 1.20342140e-06 4.19500478e+01 9.45481267e+01 3.34200254e-06\n",
      " 2.20289732e-07 0.00000000e+00 4.30133919e-23 7.92859855e+01\n",
      " 1.18505607e+02 8.22894680e+01 1.28252776e+02 8.28331978e+01\n",
      " 6.58627265e+01]\n",
      "40-th iteration, loss: 0.2414051422096551, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.974026542471645e-07\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259275e+00 4.78603158e+01 7.54396113e+01 1.97773349e-05\n",
      " 1.01133913e-06 4.19500479e+01 9.45481267e+01 4.15728766e-06\n",
      " 2.40297098e-07 8.18349804e-07 2.00073662e-08 7.92859863e+01\n",
      " 1.18505607e+02 8.22894655e+01 1.28252776e+02 8.28331963e+01\n",
      " 6.58627265e+01]\n",
      "41-th iteration, loss: 0.24140514220956757, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.726934523263347e-07\n",
      "41-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259282e+00 4.78603151e+01 7.54396114e+01 1.98966813e-05\n",
      " 8.19031379e-07 4.19500481e+01 9.45481268e+01 4.94637728e-06\n",
      " 2.41581537e-07 1.61078245e-06 9.90687729e-09 7.92859871e+01\n",
      " 1.18505606e+02 8.22894631e+01 1.28252776e+02 8.28331948e+01\n",
      " 6.58627265e+01]\n",
      "42-th iteration, loss: 0.2414051422094843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.489346163995599e-07\n",
      "42-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259289e+00 4.78603145e+01 7.54396115e+01 2.00097232e-05\n",
      " 6.27076391e-07 4.19500482e+01 9.45481268e+01 5.71119421e-06\n",
      " 2.25447134e-07 0.00000000e+00 2.31610572e-23 7.92859903e+01\n",
      " 1.18505606e+02 8.22894607e+01 1.28252776e+02 8.28331934e+01\n",
      " 6.58627265e+01]\n",
      "43-th iteration, loss: 0.24140514220940498, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.260059834946482e-07\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259297e+00 4.78603138e+01 7.54396116e+01 2.01170449e-05\n",
      " 4.35591644e-07 4.19500483e+01 9.45481269e+01 6.45281346e-06\n",
      " 1.92626607e-07 0.00000000e+00 7.27918939e-23 7.92859918e+01\n",
      " 1.18505606e+02 8.22894584e+01 1.28252777e+02 8.28331921e+01\n",
      " 6.58627265e+01]\n",
      "44-th iteration, loss: 0.2414051422093294, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.039339342378633e-07\n",
      "44-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259306e+00 4.78603132e+01 7.54396117e+01 2.02193310e-05\n",
      " 2.44756171e-07 4.19500484e+01 9.45481269e+01 7.17220719e-06\n",
      " 1.43848418e-07 7.92859932e+01 1.18505605e+02 8.22894561e+01\n",
      " 1.28252777e+02 8.28331907e+01 6.58627265e+01]\n",
      "45-th iteration, loss: 0.24140514220926138, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.991226630206466e-07\n",
      "45-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259315e+00 4.78603127e+01 7.54396118e+01 2.03189851e-05\n",
      " 5.52995122e-08 4.19500485e+01 9.45481270e+01 7.87507469e-06\n",
      " 8.16139864e-08 0.00000000e+00 2.15066959e-23 7.92859939e+01\n",
      " 1.18505605e+02 8.22894539e+01 1.28252777e+02 8.28331894e+01\n",
      " 6.58627265e+01]\n",
      "46-th iteration, loss: 0.24140514220919262, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.772199115005901e-07\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259324e+00 4.78603121e+01 7.54396119e+01 4.19500690e+01\n",
      " 9.45481270e+01 8.56912208e-06 7.72061969e-09 7.92859953e+01\n",
      " 1.18505605e+02 8.22894517e+01 1.28252778e+02 8.28331881e+01\n",
      " 6.58627265e+01]\n",
      "47-th iteration, loss: 0.24140514220913087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.716673465306043e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259334  47.86031155  75.43961199  41.95006911  94.54812702\n",
      "  79.28600522 118.50560476  82.28944959 128.25277811  82.83318688\n",
      "  65.86272653]\n",
      "48-th iteration, loss: 0.24140514220907555, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.81273381878295e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259344  47.86031103  75.4396121   41.95006919  94.54812706\n",
      "  79.2860059  118.50560457  82.28944752 128.25277853  82.83318567\n",
      "  65.86272653]\n",
      "49-th iteration, loss: 0.24140514220902273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.895065718779149e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259354  47.86031052  75.4396122   41.95006927  94.54812709\n",
      "  79.28600658 118.5056044   82.28944551 128.25277897  82.83318449\n",
      "  65.86272653]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535590565167538\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.20594943   0.         758.81415108]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6298257046951171\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.47534951  62.25781836 231.34577777   0.         527.46837331]\n",
      "2-th iteration, loss: 0.6028357090092895, 13 gd steps\n",
      "insert gradient: -0.640428788151959\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.26457513  77.72398141 217.18307533  42.67576272 236.82253496\n",
      "   0.         290.64583835]\n",
      "3-th iteration, loss: 0.4680449105790495, 20 gd steps\n",
      "insert gradient: -0.5340526710477517\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          58.98204479 223.57435155  56.64604459 162.34065052\n",
      "  48.42104135 207.60417025   0.          83.0416681 ]\n",
      "4-th iteration, loss: 0.3637895354020658, 18 gd steps\n",
      "insert gradient: -0.7955197611880737\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.7281548   50.80577965 123.05263203   0.         114.26315831\n",
      "  60.7481694  141.170654    57.7880987  162.7244079   50.24138847\n",
      "  83.0416681 ]\n",
      "5-th iteration, loss: 0.30548107477298586, 15 gd steps\n",
      "insert gradient: -0.18879097559312943\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  2.25223407  68.0327811   87.34479274  27.92415681  81.59003782\n",
      "  61.42249859 161.07959783  50.49366898 148.33443595  67.22317974\n",
      "  83.0416681 ]\n",
      "6-th iteration, loss: 0.29146927491542046, 16 gd steps\n",
      "insert gradient: -0.03201606158015947\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  0.71241323  67.66859156  82.77964067  33.56202263  83.68691251\n",
      "  55.09552583 159.87988901  59.90477305 150.84290069  63.2906816\n",
      "  83.0416681 ]\n",
      "7-th iteration, loss: 0.2905474449430984, 14 gd steps\n",
      "insert gradient: -0.01527595102285939\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 6.98699132e+01 7.76802943e+01 0.00000000e+00\n",
      " 7.99360578e-15 3.48750371e+01 8.54454574e+01 5.51435256e+01\n",
      " 1.60686190e+02 5.92378827e+01 1.53303942e+02 6.34323660e+01\n",
      " 8.30416681e+01]\n",
      "8-th iteration, loss: 0.2899473158174556, 58 gd steps\n",
      "insert gradient: -0.0021037439461637987\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          70.99180622  75.34360543  36.92132222  85.77370893\n",
      "  53.32604567 162.19987325  59.75455541 154.53873511  65.0733081\n",
      "  83.0416681 ]\n",
      "9-th iteration, loss: 0.2899162529362811, 77 gd steps\n",
      "insert gradient: -0.0008946947704866726\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14448165e+01 7.43428006e+01 3.72719331e+01\n",
      " 8.59389093e+01 5.32074753e+01 1.62681003e+02 5.96822982e+01\n",
      " 1.54458874e+02 0.00000000e+00 5.32907052e-15 6.56242248e+01\n",
      " 8.30416681e+01]\n",
      "10-th iteration, loss: 0.2899160267531113, 44 gd steps\n",
      "insert gradient: -0.0008136837581369243\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.45009695  74.3334231   37.27659057  85.93973387\n",
      "  53.20620391 162.68586498  59.68217487 154.45776828  65.63695619\n",
      "  83.0416681 ]\n",
      "11-th iteration, loss: 0.2899158961675684, 31 gd steps\n",
      "insert gradient: -0.0008189830161628703\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14539221e+01 7.43268473e+01 3.72798542e+01\n",
      " 8.59402663e+01 5.32052493e+01 1.62689295e+02 5.96820174e+01\n",
      " 1.54456923e+02 0.00000000e+00 3.01980663e-14 6.56412796e+01\n",
      " 8.30416681e+01]\n",
      "12-th iteration, loss: 0.28991570977744796, 38 gd steps\n",
      "insert gradient: -0.0007626610963282614\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.45863454  74.31874124  37.28381025  85.94086714\n",
      "  53.20403228 162.69351358  59.68176819 154.45581601  65.65166857\n",
      "  83.0416681 ]\n",
      "13-th iteration, loss: 0.28991559319944266, 29 gd steps\n",
      "insert gradient: -0.0007708487123257304\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14621759e+01 7.43126779e+01 3.72867622e+01\n",
      " 8.59412998e+01 5.32031404e+01 1.62696697e+02 5.96815862e+01\n",
      " 1.54454949e+02 0.00000000e+00 4.08562073e-14 6.56554681e+01\n",
      " 8.30416681e+01]\n",
      "14-th iteration, loss: 0.28991543306952317, 35 gd steps\n",
      "insert gradient: -0.000724896272019905\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.46636886  74.30541935  37.29024095  85.94178403\n",
      "  53.2020714  162.70050187  59.68135408 154.45387108  65.6643877\n",
      "  83.0416681 ]\n",
      "15-th iteration, loss: 0.2899153275839849, 27 gd steps\n",
      "insert gradient: -0.0007334302026060037\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14696377e+01 7.42997642e+01 3.72929497e+01\n",
      " 8.59421526e+01 5.32012627e+01 1.62703488e+02 5.96811862e+01\n",
      " 1.54453003e+02 0.00000000e+00 4.79616347e-14 6.56678007e+01\n",
      " 8.30416681e+01]\n",
      "16-th iteration, loss: 0.289915186647475, 32 gd steps\n",
      "insert gradient: -0.00069390446043002\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47341839  74.29314026  37.29607854  85.94255962\n",
      "  53.20031776 162.70697825  59.68098344 154.45195689  65.67566752\n",
      "  83.0416681 ]\n",
      "17-th iteration, loss: 0.2899150901126489, 26 gd steps\n",
      "insert gradient: -0.0007023116613443414\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47645603  74.28782069  37.29859406  85.94288127\n",
      "  53.19957954 162.70979777  59.68083303 154.45109448  65.6787791\n",
      "  83.0416681 ]\n",
      "18-th iteration, loss: 0.2899149974707947, 25 gd steps\n",
      "insert gradient: -0.0007059960307738558\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.4792854   74.2826204   37.30098849  85.94319118\n",
      "  53.19891861 162.71255079  59.68075074 154.45027101  65.68184722\n",
      "  83.0416681 ]\n",
      "19-th iteration, loss: 0.28991490816232, 24 gd steps\n",
      "insert gradient: -0.0007069689192529621\n",
      "19-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14819657e+01 7.42775345e+01 3.73032876e+01\n",
      " 8.59434877e+01 5.31983013e+01 1.62715234e+02 5.96807022e+01\n",
      " 1.54449474e+02 0.00000000e+00 4.08562073e-14 6.56848654e+01\n",
      " 8.30416681e+01]\n",
      "20-th iteration, loss: 0.2899147903621449, 29 gd steps\n",
      "insert gradient: -0.0006684142022091772\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14850947e+01 7.42716929e+01 3.73059333e+01\n",
      " 8.59438068e+01 5.31975396e+01 1.62718294e+02 5.96805860e+01\n",
      " 1.54448520e+02 0.00000000e+00 4.08562073e-14 6.56916790e+01\n",
      " 8.30416681e+01]\n",
      "21-th iteration, loss: 0.28991467972108953, 28 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.48823003  74.26613058  37.30851618  85.94409536\n",
      "  53.19674878 162.72121286  59.68039179 154.44754241  65.69793873\n",
      "  83.0416681 ]\n",
      "22-th iteration, loss: 0.28991459928119845, 23 gd steps\n",
      "insert gradient: -0.0006481394727698176\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14908593e+01 7.42614477e+01 3.73106944e+01\n",
      " 8.59443395e+01 5.31961114e+01 1.62723684e+02 5.96802511e+01\n",
      " 1.54446708e+02 0.00000000e+00 3.01980663e-14 6.57005472e+01\n",
      " 8.30416681e+01]\n",
      "23-th iteration, loss: 0.28991449769703276, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49376151  74.25620114  37.31310674  85.94460129\n",
      "  53.19540599 162.72644557  59.68010031 154.44576249  65.70633654\n",
      "  83.0416681 ]\n",
      "24-th iteration, loss: 0.28991442243174265, 22 gd steps\n",
      "insert gradient: -0.0006285144375772933\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49624152  74.25172131  37.31517361  85.9448246\n",
      "  53.19481939 162.7288131   59.67998404 154.44494328  65.70878427\n",
      "  83.0416681 ]\n",
      "25-th iteration, loss: 0.2899143495491773, 22 gd steps\n",
      "insert gradient: -0.000632815180395195\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49858143  74.24731521  37.31716339  85.94504319\n",
      "  53.19428603 162.73113779  59.67991849 154.4441548   65.71121678\n",
      "  83.0416681 ]\n",
      "26-th iteration, loss: 0.28991427878794507, 21 gd steps\n",
      "insert gradient: -0.0006349685672988097\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50081698  74.24298359  37.31908999  85.94525517\n",
      "  53.1937842  162.73341509  59.67988099 154.4433895   65.71362679\n",
      "  83.0416681 ]\n",
      "27-th iteration, loss: 0.2899142099454693, 21 gd steps\n",
      "insert gradient: -0.0006357531722864722\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50297152  74.23872427  37.32096384  85.94546012\n",
      "  53.19330188 162.73564436  59.67985862 154.44264231  65.71601128\n",
      "  83.0416681 ]\n",
      "28-th iteration, loss: 0.2899141428941931, 21 gd steps\n",
      "insert gradient: -0.0006356440688609367\n",
      "28-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15050597e+01 7.42345362e+01 3.73227918e+01\n",
      " 8.59456582e+01 5.31928326e+01 1.62737826e+02 5.96798440e+01\n",
      " 1.54441910e+02 0.00000000e+00 3.01980663e-14 6.57183681e+01\n",
      " 8.30416681e+01]\n",
      "29-th iteration, loss: 0.2899140581659976, 24 gd steps\n",
      "insert gradient: -0.000605837168624865\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15074190e+01 7.42299022e+01 3.73248258e+01\n",
      " 8.59458642e+01 5.31922703e+01 1.62740221e+02 5.96797807e+01\n",
      " 1.54441073e+02 0.00000000e+00 2.30926389e-14 6.57235117e+01\n",
      " 8.30416681e+01]\n",
      "30-th iteration, loss: 0.2899139772170939, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50981814  74.22543169  37.32684065  85.94605415\n",
      "  53.19167521 162.74253186  59.67965592 154.44021956  65.72832731\n",
      "  83.0416681 ]\n",
      "31-th iteration, loss: 0.2899139151901269, 20 gd steps\n",
      "insert gradient: -0.0005893594411983714\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51193568  74.22150459  37.32862284  85.9462234\n",
      "  53.19116626 162.74457025  59.67955624 154.43945999  65.73042534\n",
      "  83.0416681 ]\n",
      "32-th iteration, loss: 0.28991385480065235, 20 gd steps\n",
      "insert gradient: -0.000593215997597732\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51395125  74.2176295   37.33035108  85.9463914\n",
      "  53.19070031 162.74657956  59.67949762 154.43872462  65.73251672\n",
      "  83.0416681 ]\n",
      "33-th iteration, loss: 0.2899137958825479, 20 gd steps\n",
      "insert gradient: -0.0005953462508193789\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51588988  74.21380705  37.33203445  85.94655626\n",
      "  53.19026103 162.74855616  59.67946345 154.43800802  65.73459608\n",
      "  83.0416681 ]\n",
      "34-th iteration, loss: 0.2899137383490367, 19 gd steps\n",
      "insert gradient: -0.0005963336092520295\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15177670e+01 7.42100388e+01 3.73336787e+01\n",
      " 8.59467171e+01 5.31898391e+01 1.62750498e+02 5.96794437e+01\n",
      " 1.54437307e+02 0.00000000e+00 2.30926389e-14 6.57366592e+01\n",
      " 8.30416681e+01]\n",
      "35-th iteration, loss: 0.2899136671724674, 21 gd steps\n",
      "insert gradient: -0.0005713352866261891\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51984132  74.2059365   37.33547393  85.94688214\n",
      "  53.18934986 162.75259726  59.67939047 154.43652556  65.74110043\n",
      "  83.0416681 ]\n",
      "36-th iteration, loss: 0.2899136121952745, 19 gd steps\n",
      "insert gradient: -0.0005751557226816843\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52172156  74.20230104  37.33708857  85.94702866\n",
      "  53.18890839 162.7544615   59.67933176 154.43581799  65.74304795\n",
      "  83.0416681 ]\n",
      "37-th iteration, loss: 0.2899135584611548, 19 gd steps\n",
      "insert gradient: -0.0005773452562294527\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52353321  74.19871017  37.33866465  85.94717356\n",
      "  53.18849343 162.75629903  59.67929736 154.43512767  65.74498709\n",
      "  83.0416681 ]\n",
      "38-th iteration, loss: 0.28991350588434533, 19 gd steps\n",
      "insert gradient: -0.0005784403372581874\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52529066  74.19516447  37.34020738  85.94731582\n",
      "  53.18809579 162.7581078   59.67927779 154.43445148  65.74691446\n",
      "  83.0416681 ]\n",
      "39-th iteration, loss: 0.2899134544199846, 18 gd steps\n",
      "insert gradient: -0.0005787887951359445\n",
      "39-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15270030e+01 7.41916653e+01 3.73417201e+01\n",
      " 8.59474549e+01 5.31877101e+01 1.62759886e+02 5.96792672e+01\n",
      " 1.54433788e+02 0.00000000e+00 3.01980663e-14 6.57488273e+01\n",
      " 8.30416681e+01]\n",
      "40-th iteration, loss: 0.2899133911483969, 20 gd steps\n",
      "insert gradient: -0.0005552915579641917\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52888495  74.18788544  37.34336096  85.94759649\n",
      "  53.18726547 162.76179504  59.67922569 154.43305362  65.75291406\n",
      "  83.0416681 ]\n",
      "41-th iteration, loss: 0.28991334177065603, 18 gd steps\n",
      "insert gradient: -0.0005584638304820247\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53061281  74.18450099  37.34485407  85.94772388\n",
      "  53.18685901 162.76350747  59.67917656 154.43238224  65.75472516\n",
      "  83.0416681 ]\n",
      "42-th iteration, loss: 0.2899132934088341, 18 gd steps\n",
      "insert gradient: -0.0005602821918182003\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53228434  74.18115412  37.34631552  85.94785028\n",
      "  53.18647521 162.7651978   59.67914807 154.43172558  65.7565296\n",
      "  83.0416681 ]\n",
      "43-th iteration, loss: 0.2899132460015321, 18 gd steps\n",
      "insert gradient: -0.0005611755803928601\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15339109e+01 7.41778455e+01 3.73477493e+01\n",
      " 8.59479748e+01 5.31861066e+01 1.62766864e+02 5.96791326e+01\n",
      " 1.54431081e+02 0.00000000e+00 1.42108547e-14 6.57583245e+01\n",
      " 8.30416681e+01]\n",
      "44-th iteration, loss: 0.28991318817547373, 19 gd steps\n",
      "insert gradient: -0.0005396296857333447\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15356805e+01 7.41742947e+01 3.73492920e+01\n",
      " 8.59481009e+01 5.31856891e+01 1.62768642e+02 5.96790925e+01\n",
      " 1.54430376e+02 0.00000000e+00 3.01980663e-14 6.57621388e+01\n",
      " 8.30416681e+01]\n",
      "45-th iteration, loss: 0.2899131321879292, 19 gd steps\n",
      "insert gradient: -0.0005222507315353482\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53748744  74.17083096  37.35083141  85.94821871\n",
      "  53.18524892 162.77037671  59.67901435 154.42965944  65.76576925\n",
      "  83.0416681 ]\n",
      "46-th iteration, loss: 0.28991308745317146, 17 gd steps\n",
      "insert gradient: -0.0005273053264491935\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53914627  74.16767554  37.35224511  85.94832836\n",
      "  53.18485642 162.77196325  59.67894936 154.42900033  65.76741221\n",
      "  83.0416681 ]\n",
      "47-th iteration, loss: 0.2899130436210481, 17 gd steps\n",
      "insert gradient: -0.000530568694680669\n",
      "47-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15407413e+01 7.41645490e+01 3.73536259e+01\n",
      " 8.59484382e+01 5.31844926e+01 1.62773535e+02 5.96789129e+01\n",
      " 1.54428358e+02 0.00000000e+00 1.77635684e-15 6.57690557e+01\n",
      " 8.30416681e+01]\n",
      "48-th iteration, loss: 0.28991299105208357, 18 gd steps\n",
      "insert gradient: -0.0005128064281306106\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54244042  74.16122079  37.35509077  85.94854913\n",
      "  53.18409661 162.77520014  59.67886574 154.42766737  65.77253026\n",
      "  83.0416681 ]\n",
      "49-th iteration, loss: 0.2899129488151653, 17 gd steps\n",
      "insert gradient: -0.0005169769284391468\n",
      "49-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15440170e+01 7.41581748e+01 3.73564451e+01\n",
      " 8.59486517e+01 5.31837345e+01 1.62776729e+02 5.96788214e+01\n",
      " 1.54427028e+02 0.00000000e+00 2.30926389e-14 6.57741086e+01\n",
      " 8.30416681e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5353880465751923\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.28873282   0.         785.85699582]\n",
      "1-th iteration, loss: 0.7469808567750643, 11 gd steps\n",
      "insert gradient: -0.6228908420477111\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.09207687  62.36451321 239.59054751   0.         546.26644832]\n",
      "2-th iteration, loss: 0.6073428866251602, 13 gd steps\n",
      "insert gradient: -0.7003321457777142\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.46951943  77.07216638 224.12065477  39.56862861 245.262487\n",
      "   0.         301.00396132]\n",
      "3-th iteration, loss: 0.45896331676869556, 18 gd steps\n",
      "insert gradient: -0.5174613944140584\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          35.30152593 230.8995637   63.40011918 137.39758525\n",
      "  51.1576633  172.00226361   0.         129.00169771]\n",
      "4-th iteration, loss: 0.38329758343224213, 14 gd steps\n",
      "insert gradient: -0.6453517420931537\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          43.85258274 117.55103545   0.         109.15453292\n",
      "  70.42379358 132.86630111  47.88472485 148.32266905  57.51868436\n",
      " 129.00169771]\n",
      "5-th iteration, loss: 0.241405142221727, 94 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.917490668006998e-06\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88266746  47.85989307  75.43979642  41.95022222  94.54812524\n",
      "  79.28616465 118.50575615  82.28921178 128.25339733  82.83259567\n",
      " 129.00169771]\n",
      "6-th iteration, loss: 0.2414051422210046, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.667039411447256e-06\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88266893  47.85989994  75.43979711  41.95021985  94.54812416\n",
      "  79.28616185 118.50575597  82.28921359 128.25339956  82.83259958\n",
      " 129.00169771]\n",
      "7-th iteration, loss: 0.24140514222032075, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.432972199702665e-06\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[1.88267035e+00 0.00000000e+00 9.15933995e-16 4.78599066e+01\n",
      " 7.54397977e+01 4.19502174e+01 9.45481230e+01 7.92861591e+01\n",
      " 1.18505756e+02 8.22892153e+01 1.28253402e+02 8.28326034e+01\n",
      " 1.29001698e+02]\n",
      "8-th iteration, loss: 0.24140514221934206, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.9354788511280365e-06\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267170e+00 6.32966921e-06 1.35100398e-06 0.00000000e+00\n",
      " 5.02925812e-22 4.78599129e+01 7.54397982e+01 4.19502148e+01\n",
      " 9.45481219e+01 7.92861563e+01 1.18505755e+02 8.22892170e+01\n",
      " 1.28253404e+02 8.28326071e+01 1.29001698e+02]\n",
      "9-th iteration, loss: 0.24140514221820467, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.245596458467507e-06\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267293e+00 1.21026107e-05 2.49659034e-06 5.79044493e-06\n",
      " 1.14558636e-06 4.78599187e+01 7.54397984e+01 4.19502120e+01\n",
      " 9.45481206e+01 7.92861536e+01 1.18505755e+02 8.22892185e+01\n",
      " 1.28253406e+02 8.28326107e+01 1.29001698e+02]\n",
      "10-th iteration, loss: 0.24140514221724657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.669298204339548e-06\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267400e+00 1.71841978e-05 3.41605092e-06 1.09043776e-05\n",
      " 1.99002645e-06 4.78599238e+01 7.54397983e+01 4.19502087e+01\n",
      " 9.45481193e+01 7.92861509e+01 1.18505755e+02 8.22892200e+01\n",
      " 1.28253408e+02 8.28326142e+01 1.29001698e+02]\n",
      "11-th iteration, loss: 0.24140514221642018, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.186423430468257e-06\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267495e+00 2.16896959e-05 4.14345894e-06 1.54541338e-05\n",
      " 2.57615829e-06 0.00000000e+00 1.58818678e-22 4.78599284e+01\n",
      " 7.54397981e+01 4.19502053e+01 9.45481178e+01 7.92861483e+01\n",
      " 1.18505754e+02 8.22892214e+01 1.28253409e+02 8.28326176e+01\n",
      " 1.29001698e+02]\n",
      "12-th iteration, loss: 0.24140514221556972, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6093684020535283e-06\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267579e+00 2.56755442e-05 4.69822590e-06 1.94936644e-05\n",
      " 2.93070282e-06 4.07290705e-06 3.54544531e-07 0.00000000e+00\n",
      " 7.27918939e-23 4.78599325e+01 7.54397976e+01 4.19502015e+01\n",
      " 9.45481162e+01 7.92861457e+01 1.18505754e+02 8.22892227e+01\n",
      " 1.28253411e+02 8.28326208e+01 1.29001698e+02]\n",
      "13-th iteration, loss: 0.24140514221475942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.097966093563517e-06\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[1.88267649e+00 2.90640279e-05 5.07083753e-06 2.29430180e-05\n",
      " 3.05075620e-06 7.56023055e-06 4.21829665e-07 3.49191695e-06\n",
      " 6.72851342e-08 4.78599359e+01 7.54397968e+01 4.19501976e+01\n",
      " 9.45481146e+01 7.92861431e+01 1.18505753e+02 8.22892240e+01\n",
      " 1.28253413e+02 0.00000000e+00 2.48689958e-14 8.28326240e+01\n",
      " 1.29001698e+02]\n",
      "14-th iteration, loss: 0.241405142213987, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9526783533959085e-06\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267707e+00 3.18714003e-05 5.27356638e-06 2.58160877e-05\n",
      " 2.95623727e-06 1.04728256e-05 2.29361011e-07 4.78599453e+01\n",
      " 7.54397959e+01 4.19501933e+01 9.45481129e+01 7.92861405e+01\n",
      " 1.18505752e+02 8.22892251e+01 1.28253414e+02 3.05795360e-06\n",
      " 1.53796719e-06 8.28326271e+01 1.29001698e+02]\n",
      "15-th iteration, loss: 0.24140514221336137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8287151981032474e-06\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267756e+00 3.42589315e-05 5.34973022e-06 2.82719427e-05\n",
      " 2.69793011e-06 4.78599607e+01 7.54397948e+01 4.19501890e+01\n",
      " 9.45481111e+01 7.92861380e+01 1.18505752e+02 8.22892262e+01\n",
      " 1.28253416e+02 5.97059765e-06 2.95593145e-06 8.28326300e+01\n",
      " 1.29001698e+02]\n",
      "16-th iteration, loss: 0.2414051422128309, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7203413368490455e-06\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267800e+00 3.64219528e-05 5.34670221e-06 3.05042747e-05\n",
      " 2.32861325e-06 4.78599630e+01 7.54397936e+01 4.19501847e+01\n",
      " 9.45481094e+01 7.92861355e+01 1.18505751e+02 8.22892272e+01\n",
      " 1.28253417e+02 8.75758990e-06 4.27191406e-06 8.28326328e+01\n",
      " 1.29001698e+02]\n",
      "17-th iteration, loss: 0.2414051422123368, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6182176346775023e-06\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267842e+00 3.84695348e-05 5.29053903e-06 3.26211280e-05\n",
      " 1.87723933e-06 4.78599652e+01 7.54397925e+01 4.19501805e+01\n",
      " 9.45481077e+01 7.92861331e+01 1.18505750e+02 8.22892280e+01\n",
      " 1.28253418e+02 1.14314760e-05 5.49715777e-06 0.00000000e+00\n",
      " 1.16467030e-21 8.28326355e+01 1.29001698e+02]\n",
      "18-th iteration, loss: 0.24140514221181295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.478289738731962e-06\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267881e+00 4.04126891e-05 5.18526268e-06 3.46328261e-05\n",
      " 1.34932648e-06 4.78599672e+01 7.54397914e+01 4.19501764e+01\n",
      " 9.45481060e+01 7.92861307e+01 1.18505749e+02 8.22892288e+01\n",
      " 1.28253420e+02 1.39868883e-05 6.62968924e-06 2.58261069e-06\n",
      " 1.13253147e-06 8.28326381e+01 1.29001698e+02]\n",
      "19-th iteration, loss: 0.24140514221133166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.34881703176118e-06\n",
      "19-th iteration, new layer inserted. now 21 layers\n",
      "[1.88267918e+00 4.22586680e-05 5.03426495e-06 3.65459847e-05\n",
      " 7.49628882e-07 4.78599691e+01 7.54397903e+01 4.19501724e+01\n",
      " 9.45481044e+01 7.92861283e+01 1.18505748e+02 8.22892295e+01\n",
      " 1.28253421e+02 1.63940771e-05 7.65443649e-06 5.02260140e-06\n",
      " 2.14450068e-06 0.00000000e+00 6.88214270e-22 8.28326405e+01\n",
      " 1.29001698e+02]\n",
      "20-th iteration, loss: 0.24140514221084072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1910362338360028e-06\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[1.88267954e+00 4.40172822e-05 4.84096731e-06 3.83698225e-05\n",
      " 8.28446689e-08 4.78599710e+01 7.54397892e+01 4.19501686e+01\n",
      " 9.45481028e+01 7.92861260e+01 1.18505748e+02 8.22892300e+01\n",
      " 1.28253422e+02 1.86539882e-05 8.57359800e-06 7.32038457e-06\n",
      " 3.03881175e-06 2.30839357e-06 8.94311067e-07 0.00000000e+00\n",
      " 7.94093388e-23 8.28326428e+01 1.29001698e+02]\n",
      "21-th iteration, loss: 0.24140514221036072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0131501670817546e-06\n",
      "21-th iteration, new layer inserted. now 23 layers\n",
      "[1.88267988e+00 4.56919543e-05 4.60768941e-06 4.78600128e+01\n",
      " 7.54397881e+01 4.19501648e+01 9.45481012e+01 7.92861237e+01\n",
      " 1.18505747e+02 8.22892304e+01 1.28253423e+02 2.07373101e-05\n",
      " 9.37228010e-06 9.44612625e-06 3.80127460e-06 4.44917044e-06\n",
      " 1.64535263e-06 2.14520168e-06 7.51041563e-07 0.00000000e+00\n",
      " 7.94093388e-23 8.28326450e+01 1.29001698e+02]\n",
      "22-th iteration, loss: 0.24140514220992496, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8291693921538931e-06\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268020e+00 4.72876110e-05 4.33697397e-06 4.78600145e+01\n",
      " 7.54397871e+01 4.19501612e+01 9.45480997e+01 7.92861214e+01\n",
      " 1.18505746e+02 8.22892307e+01 1.28253423e+02 2.26263084e-05\n",
      " 1.00433011e-05 1.13814961e-05 4.42555881e-06 6.40334791e-06\n",
      " 2.24762357e-06 4.10751989e-06 1.34269864e-06 1.96603416e-06\n",
      " 5.91657073e-07 8.28326469e+01 1.29001698e+02]\n",
      "23-th iteration, loss: 0.24140514220954254, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6682993531828372e-06\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268051e+00 4.88636148e-05 4.04246653e-06 4.78600161e+01\n",
      " 7.54397861e+01 4.19501577e+01 9.45480982e+01 7.92861191e+01\n",
      " 1.18505745e+02 8.22892308e+01 1.28253424e+02 2.43222977e-05\n",
      " 1.05913434e-05 1.31271768e-05 4.91728850e-06 8.17092510e-06\n",
      " 2.70767126e-06 5.88621771e-06 1.78242345e-06 3.75137527e-06\n",
      " 1.02165449e-06 8.28326487e+01 1.29001698e+02]\n",
      "24-th iteration, loss: 0.241405142209203, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5792395732948866e-06\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268083e+00 5.04214415e-05 3.72440193e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.78600177e+01 7.54397852e+01 4.19501544e+01\n",
      " 9.45480967e+01 7.92861169e+01 1.18505744e+02 8.22892308e+01\n",
      " 1.28253425e+02 2.58510396e-05 1.10335046e-05 1.47083219e-05\n",
      " 5.29450002e-06 9.77639957e-06 3.04445527e-06 7.50508899e-06\n",
      " 2.09008408e-06 5.37906550e-06 1.31075435e-06 8.28326504e+01\n",
      " 1.29001698e+02]\n",
      "25-th iteration, loss: 0.2414051422088783, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4925993876837029e-06\n",
      "25-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268114e+00 5.19480450e-05 3.37975803e-06 4.78600208e+01\n",
      " 7.54397843e+01 4.19501512e+01 9.45480954e+01 7.92861147e+01\n",
      " 1.18505743e+02 8.22892308e+01 1.28253425e+02 2.72331151e-05\n",
      " 1.13828233e-05 1.61449881e-05 5.57104596e-06 1.12392616e-05\n",
      " 3.27263026e-06 8.98301413e-06 2.28112594e-06 6.86733180e-06\n",
      " 1.47518208e-06 8.28326518e+01 1.29001698e+02]\n",
      "26-th iteration, loss: 0.2414051422086027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4799652868421827e-06\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268143e+00 5.34086733e-05 3.00158744e-06 4.78600224e+01\n",
      " 7.54397834e+01 4.19501481e+01 9.45480940e+01 7.92861125e+01\n",
      " 1.18505741e+02 8.22892306e+01 1.28253425e+02 2.84870171e-05\n",
      " 1.16511603e-05 1.74552093e-05 5.75950191e-06 1.25770468e-05\n",
      " 3.40547740e-06 1.03369914e-05 2.36952759e-06 8.23259545e-06\n",
      " 1.52960605e-06 8.28326532e+01 1.29001698e+02]\n",
      "27-th iteration, loss: 0.24140514220835102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.466129573075931e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268173e+00 5.48611557e-05 2.60279788e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.78600238e+01 7.54397826e+01 4.19501451e+01\n",
      " 9.45480927e+01 7.92861104e+01 1.18505740e+02 8.22892304e+01\n",
      " 1.28253426e+02 2.96328580e-05 1.18527738e-05 1.86586971e-05\n",
      " 5.87475169e-06 1.38090310e-05 3.45849934e-06 1.15858250e-05\n",
      " 2.37140491e-06 9.49315285e-06 1.49075074e-06 8.28326545e+01\n",
      " 1.29001698e+02]\n",
      "28-th iteration, loss: 0.24140514220810216, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.390059746687982e-06\n",
      "28-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268201e+00 5.62917005e-05 2.18020312e-06 4.78600268e+01\n",
      " 7.54397818e+01 4.19501423e+01 9.45480914e+01 7.92861083e+01\n",
      " 1.18505739e+02 8.22892301e+01 1.28253426e+02 3.06832881e-05\n",
      " 1.19953876e-05 1.97677715e-05 5.92504721e-06 1.49471722e-05\n",
      " 3.44047149e-06 1.27410779e-05 2.29605353e-06 1.06601388e-05\n",
      " 1.36842982e-06 8.28326557e+01 1.29001698e+02]\n",
      "29-th iteration, loss: 0.24140514220788817, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3815638785810581e-06\n",
      "29-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268229e+00 5.76660291e-05 1.72689994e-06 4.78600282e+01\n",
      " 7.54397810e+01 4.19501395e+01 9.45480902e+01 7.92861063e+01\n",
      " 1.18505738e+02 8.22892298e+01 1.28253426e+02 3.16498636e-05\n",
      " 1.20861874e-05 2.07936970e-05 5.91804136e-06 1.60024132e-05\n",
      " 3.35951103e-06 1.38133415e-05 2.15205364e-06 1.17437626e-05\n",
      " 1.17168646e-06 8.28326568e+01 1.29001698e+02]\n",
      "30-th iteration, loss: 0.24140514220768913, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3709420510654484e-06\n",
      "30-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268257e+00 5.90369745e-05 1.25469346e-06 4.78600296e+01\n",
      " 7.54397802e+01 4.19501368e+01 9.45480890e+01 7.92861043e+01\n",
      " 1.18505737e+02 8.22892295e+01 1.28253427e+02 3.25465006e-05\n",
      " 1.21351904e-05 2.17501334e-05 5.86416260e-06 1.69881305e-05\n",
      " 3.22645658e-06 1.48156807e-05 1.95065448e-06 1.27567496e-05\n",
      " 9.12182320e-07 8.28326578e+01 1.29001698e+02]\n",
      "31-th iteration, loss: 0.2414051422075031, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.358428276640836e-06\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268284e+00 6.04026984e-05 7.63127066e-07 0.00000000e+00\n",
      " 5.29395592e-23 4.78600310e+01 7.54397795e+01 4.19501343e+01\n",
      " 9.45480879e+01 7.92861024e+01 1.18505736e+02 8.22892291e+01\n",
      " 1.28253427e+02 3.33822522e-05 1.21482127e-05 2.26459268e-05\n",
      " 5.76957086e-06 1.79129382e-05 3.04781206e-06 1.57564521e-05\n",
      " 1.69870591e-06 1.37071723e-05 5.97116771e-07 8.28326587e+01\n",
      " 1.29001698e+02]\n",
      "32-th iteration, loss: 0.24140514220731363, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2876167106340908e-06\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268311e+00 6.17499773e-05 2.49202833e-07 0.00000000e+00\n",
      " 3.30872245e-23 4.78600337e+01 7.54397788e+01 4.19501318e+01\n",
      " 9.45480868e+01 7.92861006e+01 1.18505735e+02 8.22892287e+01\n",
      " 1.28253427e+02 3.41635867e-05 1.21288659e-05 2.34873675e-05\n",
      " 5.63817796e-06 1.87829253e-05 2.82779064e-06 1.66415189e-05\n",
      " 1.40072573e-06 1.46006439e-05 2.31317163e-07 8.28326596e+01\n",
      " 1.29001698e+02]\n",
      "33-th iteration, loss: 0.2414051422071382, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.221996257742535e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268336e+00 4.78600993e+01 7.54397781e+01 4.19501294e+01\n",
      " 9.45480858e+01 7.92860988e+01 1.18505734e+02 8.22892284e+01\n",
      " 1.28253427e+02 3.48952966e-05 1.20793550e-05 2.42790879e-05\n",
      " 5.47245757e-06 1.96025419e-05 2.56913723e-06 1.74751267e-05\n",
      " 1.05973448e-06 8.28326759e+01 1.29001698e+02]\n",
      "34-th iteration, loss: 0.24140514220700623, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.26624624054199e-06\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268361e+00 4.78601005e+01 7.54397774e+01 4.19501270e+01\n",
      " 9.45480847e+01 7.92860970e+01 1.18505733e+02 8.22892279e+01\n",
      " 1.28253427e+02 3.55866402e-05 1.20058781e-05 2.50301971e-05\n",
      " 5.27885399e-06 2.03807274e-05 2.27854537e-06 1.82660236e-05\n",
      " 6.82680262e-07 8.28326767e+01 1.29001698e+02]\n",
      "35-th iteration, loss: 0.241405142206883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3032632126614188e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268386e+00 4.78601018e+01 7.54397768e+01 4.19501248e+01\n",
      " 9.45480838e+01 7.92860953e+01 1.18505732e+02 8.22892275e+01\n",
      " 1.28253427e+02 3.62525104e-05 1.19196836e-05 2.57554694e-05\n",
      " 5.06881639e-06 2.11321181e-05 1.96766917e-06 1.90286880e-05\n",
      " 2.81428532e-07 8.28326775e+01 1.29001698e+02]\n",
      "36-th iteration, loss: 0.24140514220676812, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.333385286225315e-06\n",
      "36-th iteration, new layer inserted. now 17 layers\n",
      "[1.88268412e+00 4.78601031e+01 7.54397762e+01 4.19501227e+01\n",
      " 9.45480829e+01 7.92860937e+01 1.18505731e+02 8.22892271e+01\n",
      " 1.28253427e+02 3.68950888e-05 1.18220217e-05 2.64570234e-05\n",
      " 4.84372295e-06 2.18587513e-05 1.63801941e-06 8.28326980e+01\n",
      " 1.29001698e+02]\n",
      "37-th iteration, loss: 0.24140514220666548, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3590181633906655e-06\n",
      "37-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268439e+00 0.00000000e+00 2.77555756e-16 4.78601045e+01\n",
      " 7.54397757e+01 4.19501207e+01 9.45480820e+01 7.92860921e+01\n",
      " 1.18505730e+02 8.22892267e+01 1.28253427e+02 3.75163859e-05\n",
      " 1.17133443e-05 2.71368128e-05 4.60414281e-06 2.25625063e-05\n",
      " 1.29028771e-06 8.28326987e+01 1.29001698e+02]\n",
      "38-th iteration, loss: 0.24140514220655282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3196272387778386e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268465e+00 1.36137226e-06 2.66891718e-07 4.78601058e+01\n",
      " 7.54397751e+01 4.19501188e+01 9.45480812e+01 7.92860906e+01\n",
      " 1.18505729e+02 8.22892263e+01 1.28253427e+02 3.81204673e-05\n",
      " 1.15956912e-05 2.77988490e-05 4.35222354e-06 2.32473226e-05\n",
      " 9.26734845e-07 8.28326994e+01 1.29001698e+02]\n",
      "39-th iteration, loss: 0.24140514220644632, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2823872307243689e-06\n",
      "39-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268491e+00 2.68014305e-06 5.05993369e-07 0.00000000e+00\n",
      " 9.26442286e-23 4.78601072e+01 7.54397746e+01 4.19501169e+01\n",
      " 9.45480804e+01 7.92860891e+01 1.18505728e+02 8.22892259e+01\n",
      " 1.28253427e+02 3.87058887e-05 1.14671471e-05 2.84416431e-05\n",
      " 4.08613764e-06 2.39116504e-05 5.45627024e-07 8.28327000e+01\n",
      " 1.29001698e+02]\n",
      "40-th iteration, loss: 0.24140514220633238, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.192238207267924e-06\n",
      "40-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268516e+00 3.94639049e-06 7.15596070e-07 1.27280303e-06\n",
      " 2.09602701e-07 0.00000000e+00 5.29395592e-23 4.78601084e+01\n",
      " 7.54397741e+01 4.19501151e+01 9.45480796e+01 7.92860877e+01\n",
      " 1.18505728e+02 8.22892255e+01 1.28253427e+02 3.92726819e-05\n",
      " 1.13271752e-05 2.90651729e-05 3.80544349e-06 2.45553974e-05\n",
      " 1.46623999e-07 8.28327007e+01 1.29001698e+02]\n",
      "41-th iteration, loss: 0.24140514220621667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0644188558423273e-06\n",
      "41-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268538e+00 5.10876470e-06 8.85394463e-07 2.44444842e-06\n",
      " 3.62910807e-07 1.17436097e-06 1.53308106e-07 4.78601096e+01\n",
      " 7.54397735e+01 4.19501132e+01 9.45480789e+01 7.92860863e+01\n",
      " 1.18505727e+02 8.22892252e+01 1.28253428e+02 3.98189760e-05\n",
      " 1.11733250e-05 2.96675111e-05 3.50778594e-06 8.28327265e+01\n",
      " 1.29001698e+02]\n",
      "42-th iteration, loss: 0.2414051422061148, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.628936749791983e-07\n",
      "42-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268557e+00 6.14317394e-06 1.01181097e-06 3.49032872e-06\n",
      " 4.57657330e-07 2.22494311e-06 2.32839745e-07 0.00000000e+00\n",
      " 5.95570041e-23 4.78601107e+01 7.54397729e+01 4.19501114e+01\n",
      " 9.45480781e+01 7.92860850e+01 1.18505726e+02 8.22892248e+01\n",
      " 1.28253428e+02 4.03433439e-05 1.10036439e-05 3.02471619e-05\n",
      " 3.19131719e-06 8.28327271e+01 1.29001698e+02]\n",
      "43-th iteration, loss: 0.24140514220601425, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.416413025217139e-07\n",
      "43-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268574e+00 7.06776489e-06 1.10029306e-06 4.42802858e-06\n",
      " 5.00919129e-07 3.16857233e-06 2.47275441e-07 9.46645860e-07\n",
      " 1.44356954e-08 4.78601116e+01 7.54397723e+01 4.19501095e+01\n",
      " 9.45480774e+01 7.92860837e+01 1.18505725e+02 8.22892244e+01\n",
      " 1.28253428e+02 4.08523679e-05 1.08225598e-05 3.08106293e-05\n",
      " 2.86057752e-06 8.28327277e+01 1.29001698e+02]\n",
      "44-th iteration, loss: 0.24140514220592268, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.481816157805908e-07\n",
      "44-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268588e+00 7.87553062e-06 1.15097051e-06 5.25004958e-06\n",
      " 4.94227561e-07 3.99708317e-06 1.99532221e-07 4.78601142e+01\n",
      " 7.54397716e+01 4.19501077e+01 9.45480766e+01 7.92860824e+01\n",
      " 1.18505724e+02 8.22892241e+01 1.28253428e+02 4.13465715e-05\n",
      " 1.06302454e-05 3.13583803e-05 2.51581961e-06 8.28327282e+01\n",
      " 1.29001698e+02]\n",
      "45-th iteration, loss: 0.2414051422058425, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.053431157369079e-07\n",
      "45-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268600e+00 8.60138452e-06 1.17300720e-06 5.99081531e-06\n",
      " 4.48245229e-07 4.74425206e-06 1.01764147e-07 4.78601150e+01\n",
      " 7.54397710e+01 4.19501058e+01 9.45480759e+01 7.92860812e+01\n",
      " 1.18505724e+02 8.22892237e+01 1.28253428e+02 4.18290931e-05\n",
      " 1.04294881e-05 3.18934979e-05 2.15990864e-06 8.28327288e+01\n",
      " 1.29001698e+02]\n",
      "46-th iteration, loss: 0.24140514220576703, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.683176993267606e-07\n",
      "46-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268612e+00 9.28712830e-06 1.17639186e-06 6.69175644e-06\n",
      " 3.74013599e-07 0.00000000e+00 1.05879118e-22 4.78601211e+01\n",
      " 7.54397703e+01 4.19501040e+01 9.45480752e+01 7.92860801e+01\n",
      " 1.18505723e+02 8.22892234e+01 1.28253428e+02 4.23027328e-05\n",
      " 1.02228136e-05 3.24187402e-05 1.79543278e-06 8.28327293e+01\n",
      " 1.29001698e+02]\n",
      "47-th iteration, loss: 0.2414051422056962, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.34976635388225e-07\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268623e+00 9.93879637e-06 1.16297293e-06 7.35866570e-06\n",
      " 2.73897068e-07 0.00000000e+00 5.95570041e-23 4.78601225e+01\n",
      " 7.54397696e+01 4.19501023e+01 9.45480745e+01 7.92860789e+01\n",
      " 1.18505722e+02 8.22892230e+01 1.28253428e+02 4.27681650e-05\n",
      " 1.00107101e-05 3.29347524e-05 1.42292916e-06 8.28327298e+01\n",
      " 1.29001698e+02]\n",
      "48-th iteration, loss: 0.24140514220562945, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.056665322885561e-07\n",
      "48-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268633e+00 1.05592991e-05 1.13383969e-06 7.99423580e-06\n",
      " 1.49425827e-07 0.00000000e+00 1.65436123e-23 4.78601238e+01\n",
      " 7.54397690e+01 4.19501005e+01 9.45480738e+01 7.92860779e+01\n",
      " 1.18505721e+02 8.22892227e+01 1.28253428e+02 4.32259499e-05\n",
      " 9.79356640e-06 3.34420680e-05 1.04283212e-06 8.28327303e+01\n",
      " 1.29001698e+02]\n",
      "49-th iteration, loss: 0.24140514220556653, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.795885775647849e-07\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268642e+00 1.11529948e-05 1.09036200e-06 8.60262139e-06\n",
      " 2.37577393e-09 4.78601250e+01 7.54397683e+01 4.19500989e+01\n",
      " 9.45480732e+01 7.92860768e+01 1.18505721e+02 8.22892224e+01\n",
      " 1.28253428e+02 4.36766145e-05 9.57175001e-06 3.39411888e-05\n",
      " 6.55552314e-07 8.28327309e+01 1.29001698e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5367213365606043\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         809.67084418]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6336695984730955\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 236.97683244   0.         572.69401174]\n",
      "2-th iteration, loss: 0.6050067963590977, 13 gd steps\n",
      "insert gradient: -0.6845947395742539\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.15317095  77.12431625 221.78396866  40.95625941 245.44029074\n",
      "   0.         327.25372099]\n",
      "3-th iteration, loss: 0.4625800514616977, 19 gd steps\n",
      "insert gradient: -0.5298361419073401\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          42.96282847 228.94543851  60.67538491 148.63482393\n",
      "  44.9059765  187.00212628   0.         140.25159471]\n",
      "4-th iteration, loss: 0.38015883925297916, 15 gd steps\n",
      "insert gradient: -0.693483076359026\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.79909379  49.14812795 117.08061311   0.         108.71771217\n",
      "  67.27680171 138.98726035  47.11641145 163.6666056   44.06578774\n",
      " 140.25159471]\n",
      "5-th iteration, loss: 0.3035307487711468, 14 gd steps\n",
      "insert gradient: -0.26943544901191385\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  4.26797872  54.43141506  88.32621319  27.77636276  85.89920685\n",
      "  57.95766486 148.01939362  57.16529279 145.1535385   63.88486531\n",
      " 140.25159471]\n",
      "6-th iteration, loss: 0.24140514501359497, 81 gd steps\n",
      "insert gradient: -2.3614363268371253e-05\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.87652279  47.86500481  75.43559163  41.95021016  94.54756156\n",
      "  79.28674048 118.50129327  82.29064167 128.26221529  82.82816949\n",
      " 140.25159471]\n",
      "7-th iteration, loss: 0.24140514220493367, 52 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.174821488883429e-07\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88269159  47.86021354  75.43966172  41.9500714   94.54811081\n",
      "  79.28607021 118.50572012  82.28923339 128.25325195  82.83287596\n",
      " 140.25159471]\n",
      "8-th iteration, loss: 0.24140514220491607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.917974088284352e-07\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269167e+00 0.00000000e+00 5.82867088e-16 4.78602142e+01\n",
      " 7.54396619e+01 4.19500712e+01 9.45481105e+01 7.92860695e+01\n",
      " 1.18505720e+02 8.22892334e+01 1.28253253e+02 8.28328767e+01\n",
      " 1.40251595e+02]\n",
      "9-th iteration, loss: 0.24140514220489556, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.721140921688558e-07\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269175e+00 6.81040505e-07 8.07656188e-08 4.78602149e+01\n",
      " 7.54396620e+01 4.19500709e+01 9.45481102e+01 7.92860687e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253253e+02 8.28328773e+01\n",
      " 1.40251595e+02]\n",
      "10-th iteration, loss: 0.2414051422048765, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.59797269865421e-07\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269182e+00 1.30906766e-06 1.41162236e-07 4.78602156e+01\n",
      " 7.54396622e+01 4.19500707e+01 9.45481099e+01 7.92860680e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253254e+02 8.28328780e+01\n",
      " 1.40251595e+02]\n",
      "11-th iteration, loss: 0.24140514220485876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.479629493788366e-07\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269188e+00 1.89031771e-06 1.83245631e-07 4.78602161e+01\n",
      " 7.54396623e+01 4.19500704e+01 9.45481096e+01 7.92860673e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253254e+02 8.28328787e+01\n",
      " 1.40251595e+02]\n",
      "12-th iteration, loss: 0.2414051422048421, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.365626362147421e-07\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[1.88269193e+00 2.43020514e-06 2.08811036e-07 4.78602167e+01\n",
      " 7.54396624e+01 4.19500701e+01 9.45481093e+01 7.92860667e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253255e+02 0.00000000e+00\n",
      " 3.73034936e-14 8.28328793e+01 1.40251595e+02]\n",
      "13-th iteration, loss: 0.24140514220482112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.130513589522634e-07\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269197e+00 2.93213687e-06 2.19197287e-07 4.78602172e+01\n",
      " 7.54396625e+01 4.19500698e+01 9.45481090e+01 7.92860660e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253255e+02 6.30590750e-07\n",
      " 5.10328108e-07 0.00000000e+00 3.97046694e-23 8.28328799e+01\n",
      " 1.40251595e+02]\n",
      "14-th iteration, loss: 0.2414051422047968, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.788851114778665e-07\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269201e+00 3.39711916e-06 2.15327716e-07 4.78602177e+01\n",
      " 7.54396625e+01 4.19500695e+01 9.45481087e+01 7.92860654e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253256e+02 1.23230152e-06\n",
      " 9.99393298e-07 6.04235774e-07 4.89065189e-07 8.28328805e+01\n",
      " 1.40251595e+02]\n",
      "15-th iteration, loss: 0.24140514220477463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.472478747113012e-07\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269204e+00 3.82751722e-06 1.98334184e-07 4.78602181e+01\n",
      " 7.54396626e+01 4.19500691e+01 9.45481084e+01 7.92860647e+01\n",
      " 1.18505717e+02 8.22892336e+01 1.28253256e+02 1.79573136e-06\n",
      " 1.46234004e-06 1.17261042e-06 9.49022292e-07 8.28328811e+01\n",
      " 1.40251595e+02]\n",
      "16-th iteration, loss: 0.2414051422047543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.17921561987412e-07\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269206e+00 4.22748735e-06 1.69540674e-07 4.78602185e+01\n",
      " 7.54396626e+01 4.19500688e+01 9.45481081e+01 7.92860641e+01\n",
      " 1.18505717e+02 8.22892335e+01 1.28253257e+02 2.32361580e-06\n",
      " 1.90120069e-06 1.70773023e-06 1.38208110e-06 0.00000000e+00\n",
      " 3.44107135e-22 8.28328817e+01 1.40251595e+02]\n",
      "17-th iteration, loss: 0.2414051422047324, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.807697541036638e-07\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269207e+00 4.59960045e-06 1.29915684e-07 4.78602189e+01\n",
      " 7.54396627e+01 4.19500685e+01 9.45481078e+01 7.92860635e+01\n",
      " 1.18505717e+02 8.22892335e+01 1.28253257e+02 2.81577133e-06\n",
      " 2.31598045e-06 2.20929254e-06 1.78841134e-06 5.08400572e-07\n",
      " 4.06330243e-07 8.28328822e+01 1.40251595e+02]\n",
      "18-th iteration, loss: 0.2414051422047128, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.47065927642791e-07\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269208e+00 4.94559386e-06 8.02936605e-08 4.78602192e+01\n",
      " 7.54396627e+01 4.19500682e+01 9.45481075e+01 7.92860628e+01\n",
      " 1.18505716e+02 8.22892334e+01 1.28253257e+02 3.26566543e-06\n",
      " 2.70350839e-06 2.67064565e-06 2.16500813e-06 9.78602400e-07\n",
      " 7.80411564e-07 8.28328826e+01 1.40251595e+02]\n",
      "19-th iteration, loss: 0.24140514220469514, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1645873851015697e-07\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269209e+00 5.26877533e-06 2.16865590e-08 4.78602195e+01\n",
      " 7.54396627e+01 4.19500678e+01 9.45481072e+01 7.92860622e+01\n",
      " 1.18505716e+02 8.22892333e+01 1.28253258e+02 3.67708556e-06\n",
      " 3.06645043e-06 3.09544222e-06 2.51473633e-06 1.41411100e-06\n",
      " 1.12529784e-06 8.28328831e+01 1.40251595e+02]\n",
      "20-th iteration, loss: 0.24140514220467912, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8861274908512565e-07\n",
      "20-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269209e+00 4.78602254e+01 7.54396628e+01 4.19500675e+01\n",
      " 9.45481069e+01 7.92860616e+01 1.18505716e+02 8.22892332e+01\n",
      " 1.28253258e+02 4.05343659e-06 3.40719519e-06 3.48696545e-06\n",
      " 2.84016544e-06 1.81807666e-06 1.44373020e-06 0.00000000e+00\n",
      " 5.82335151e-22 8.28328835e+01 1.40251595e+02]\n",
      "21-th iteration, loss: 0.2414051422046635, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5677151014204685e-07\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[1.88269209e+00 4.78602257e+01 7.54396628e+01 4.19500672e+01\n",
      " 9.45481066e+01 7.92860611e+01 1.18505715e+02 8.22892331e+01\n",
      " 1.28253258e+02 4.39608719e-06 3.72681775e-06 3.84647419e-06\n",
      " 3.14253518e-06 2.19163798e-06 1.73710444e-06 3.80704614e-07\n",
      " 2.93374242e-07 0.00000000e+00 1.98523347e-23 8.28328839e+01\n",
      " 1.40251595e+02]\n",
      "22-th iteration, loss: 0.2414051422046481, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.217289240832977e-07\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[1.88269208e+00 4.78602260e+01 7.54396628e+01 4.19500669e+01\n",
      " 9.45481063e+01 7.92860605e+01 1.18505715e+02 8.22892329e+01\n",
      " 1.28253259e+02 4.70004673e-06 4.02294813e-06 4.16887330e-06\n",
      " 3.41963395e-06 2.52958575e-06 2.00335942e-06 7.27247234e-07\n",
      " 5.57745564e-07 3.47994179e-07 2.64371322e-07 0.00000000e+00\n",
      " 3.30872245e-23 8.28328842e+01 1.40251595e+02]\n",
      "23-th iteration, loss: 0.24140514220463352, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.853012622632444e-07\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269208e+00 4.78602262e+01 7.54396629e+01 4.19500666e+01\n",
      " 9.45481061e+01 7.92860599e+01 1.18505715e+02 8.22892327e+01\n",
      " 1.28253259e+02 4.96181944e-06 4.29392061e-06 4.45055078e-06\n",
      " 3.66997965e-06 2.82818292e-06 2.24118923e-06 1.03575663e-06\n",
      " 7.91977095e-07 6.59263193e-07 4.96881046e-07 3.12577072e-07\n",
      " 2.32509724e-07 0.00000000e+00 1.98523347e-23 8.28328845e+01\n",
      " 1.40251595e+02]\n",
      "24-th iteration, loss: 0.2414051422046201, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.528020832827099e-07\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269207e+00 0.00000000e+00 2.22044605e-16 4.78602265e+01\n",
      " 7.54396629e+01 4.19500664e+01 9.45481058e+01 7.92860593e+01\n",
      " 1.18505714e+02 8.22892325e+01 1.28253259e+02 5.18002613e-06\n",
      " 4.53934971e-06 4.69000296e-06 3.89338829e-06 3.08579344e-06\n",
      " 2.45060457e-06 1.30445612e-06 9.96267716e-07 9.31881230e-07\n",
      " 6.97909761e-07 5.87653580e-07 4.31991869e-07 2.76226920e-07\n",
      " 1.99482145e-07 8.28328848e+01 1.40251595e+02]\n",
      "25-th iteration, loss: 0.2414051422046081, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3455412904486574e-07\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269207e+00 0.00000000e+00 3.33066907e-16 4.78602270e+01\n",
      " 7.54396629e+01 4.19500661e+01 9.45481055e+01 7.92860588e+01\n",
      " 1.18505714e+02 8.22892323e+01 1.28253260e+02 5.35612515e-06\n",
      " 4.76050942e-06 4.88856181e-06 4.09134277e-06 3.30361602e-06\n",
      " 2.63329114e-06 1.53440381e-06 1.17250009e-06 1.16675826e-06\n",
      " 8.69531373e-07 8.25983730e-07 6.00705886e-07 5.16694482e-07\n",
      " 3.66829446e-07 8.28328850e+01 1.40251595e+02]\n",
      "26-th iteration, loss: 0.24140514220459766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1908015688049584e-07\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269206e+00 4.78602275e+01 7.54396630e+01 4.19500658e+01\n",
      " 9.45481053e+01 7.92860582e+01 1.18505713e+02 8.22892321e+01\n",
      " 1.28253260e+02 5.49651675e-06 4.96134037e-06 5.05250749e-06\n",
      " 4.26798606e-06 3.48780487e-06 2.79358878e-06 1.73162168e-06\n",
      " 1.32520580e-06 1.36977743e-06 1.01646419e-06 1.03330517e-06\n",
      " 7.43551897e-07 7.26988099e-07 5.07118954e-07 8.28328853e+01\n",
      " 1.40251595e+02]\n",
      "27-th iteration, loss: 0.24140514220458878, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1498168409364778e-07\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269204e+00 0.00000000e+00 3.88578059e-16 4.78602277e+01\n",
      " 7.54396630e+01 4.19500656e+01 9.45481050e+01 7.92860577e+01\n",
      " 1.18505713e+02 8.22892318e+01 1.28253260e+02 5.60721592e-06\n",
      " 5.14578467e-06 5.18775441e-06 4.42743154e-06 3.64416896e-06\n",
      " 2.93577727e-06 1.90180790e-06 1.45882657e-06 1.54652051e-06\n",
      " 1.14330756e-06 1.21507752e-06 8.65282679e-07 9.12439397e-07\n",
      " 6.25252740e-07 8.28328854e+01 1.40251595e+02]\n",
      "28-th iteration, loss: 0.2414051422045804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0245389616705633e-07\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269203e+00 0.00000000e+00 2.22044605e-16 4.78602281e+01\n",
      " 7.54396631e+01 4.19500654e+01 9.45481047e+01 7.92860572e+01\n",
      " 1.18505713e+02 8.22892316e+01 1.28253260e+02 5.69304938e-06\n",
      " 5.31696543e-06 5.29904823e-06 4.57294428e-06 3.77736885e-06\n",
      " 3.06325938e-06 2.04953345e-06 1.57689890e-06 1.70146405e-06\n",
      " 1.25372801e-06 1.37567792e-06 9.69691163e-07 1.07732104e-06\n",
      " 7.25146652e-07 8.28328856e+01 1.40251595e+02]\n",
      "29-th iteration, loss: 0.24140514220457276, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.915677693586851e-07\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269201e+00 0.00000000e+00 3.33066907e-16 4.78602285e+01\n",
      " 7.54396631e+01 4.19500651e+01 9.45481045e+01 7.92860567e+01\n",
      " 1.18505712e+02 8.22892313e+01 1.28253260e+02 5.75744251e-06\n",
      " 5.47690406e-06 5.38974868e-06 4.70666425e-06 3.89069535e-06\n",
      " 3.17828966e-06 2.17801636e-06 1.68178849e-06 1.83774915e-06\n",
      " 1.35019909e-06 1.51816622e-06 1.05935565e-06 1.22460719e-06\n",
      " 8.09480764e-07 8.28328858e+01 1.40251595e+02]\n",
      "30-th iteration, loss: 0.24140514220456574, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8202524323115624e-07\n",
      "30-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269199e+00 4.78602289e+01 7.54396631e+01 4.19500649e+01\n",
      " 9.45481043e+01 7.92860562e+01 1.18505712e+02 8.22892311e+01\n",
      " 1.28253261e+02 5.80365316e-06 5.62765654e-06 5.46305800e-06\n",
      " 4.83074929e-06 3.98729233e-06 3.28312429e-06 2.29033891e-06\n",
      " 1.77584672e-06 1.95839286e-06 1.43516449e-06 1.64549045e-06\n",
      " 1.13680946e-06 1.35717290e-06 8.80875460e-07 8.28328859e+01\n",
      " 1.40251595e+02]\n",
      "31-th iteration, loss: 0.24140514220455955, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8121194441769932e-07\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269197e+00 4.78602291e+01 7.54396632e+01 4.19500647e+01\n",
      " 9.45481040e+01 7.92860557e+01 1.18505711e+02 8.22892308e+01\n",
      " 1.28253261e+02 5.83456152e-06 5.77110318e-06 5.52181093e-06\n",
      " 4.94716577e-06 4.06994688e-06 3.37981242e-06 2.38923771e-06\n",
      " 1.86120269e-06 2.06607821e-06 1.51083071e-06 1.76027672e-06\n",
      " 1.20433412e-06 1.47758388e-06 9.41685091e-07 8.28328860e+01\n",
      " 1.40251595e+02]\n",
      "32-th iteration, loss: 0.24140514220455375, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8032302389926632e-07\n",
      "32-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269195e+00 0.00000000e+00 2.22044605e-16 4.78602293e+01\n",
      " 7.54396632e+01 4.19500645e+01 9.45481038e+01 7.92860552e+01\n",
      " 1.18505711e+02 8.22892306e+01 1.28253261e+02 5.85264311e-06\n",
      " 5.90889833e-06 5.56844683e-06 5.05764005e-06 4.14106041e-06\n",
      " 3.47014939e-06 2.47707390e-06 1.93971818e-06 2.16312327e-06\n",
      " 1.57912365e-06 1.86479707e-06 1.26391755e-06 1.58806305e-06\n",
      " 9.93957721e-07 8.28328861e+01 1.40251595e+02]\n",
      "33-th iteration, loss: 0.2414051422045481, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7177701829865442e-07\n",
      "33-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269193e+00 4.78602296e+01 7.54396632e+01 4.19500643e+01\n",
      " 9.45481036e+01 7.92860548e+01 1.18505711e+02 8.22892303e+01\n",
      " 1.28253261e+02 5.85938446e-06 6.04186177e-06 5.60442429e-06\n",
      " 5.16305188e-06 4.20206209e-06 3.55507205e-06 2.55524522e-06\n",
      " 2.01238475e-06 2.25089193e-06 1.64108751e-06 1.96037894e-06\n",
      " 1.31665475e-06 1.68989853e-06 1.03883750e-06 8.28328862e+01\n",
      " 1.40251595e+02]\n",
      "34-th iteration, loss: 0.241405142204543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7129504181494238e-07\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269191e+00 0.00000000e+00 3.88578059e-16 4.78602298e+01\n",
      " 7.54396633e+01 4.19500641e+01 9.45481033e+01 7.92860544e+01\n",
      " 1.18505710e+02 8.22892301e+01 1.28253261e+02 5.85610701e-06\n",
      " 6.17073757e-06 5.63104082e-06 5.26419807e-06 4.25422441e-06\n",
      " 3.63542724e-06 2.62499735e-06 2.08009708e-06 2.33060096e-06\n",
      " 1.69766286e-06 2.04820774e-06 1.36353053e-06 1.78424189e-06\n",
      " 1.07735199e-06 8.28328863e+01 1.40251595e+02]\n",
      "35-th iteration, loss: 0.24140514220453801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6343703168247077e-07\n",
      "35-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269188e+00 4.78602301e+01 7.54396633e+01 4.19500639e+01\n",
      " 9.45481031e+01 7.92860539e+01 1.18505710e+02 8.22892298e+01\n",
      " 1.28253261e+02 5.84418242e-06 6.29639414e-06 5.64964786e-06\n",
      " 5.36199332e-06 4.29887770e-06 3.71217341e-06 2.68763803e-06\n",
      " 2.14385527e-06 2.40353356e-06 1.74988968e-06 2.12954005e-06\n",
      " 1.40562322e-06 1.87232069e-06 1.11061659e-06 8.28328864e+01\n",
      " 1.40251595e+02]\n",
      "36-th iteration, loss: 0.24140514220453355, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6316856463470246e-07\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269186e+00 4.78602303e+01 7.54396634e+01 4.19500637e+01\n",
      " 9.45481029e+01 7.92860535e+01 1.18505709e+02 8.22892296e+01\n",
      " 1.28253262e+02 5.82448235e-06 6.41928863e-06 5.66110116e-06\n",
      " 5.45693443e-06 4.33686112e-06 3.78584449e-06 2.74398857e-06\n",
      " 2.20422844e-06 2.47049148e-06 1.79837063e-06 2.20515609e-06\n",
      " 1.44356762e-06 1.95489148e-06 1.13929711e-06 8.28328865e+01\n",
      " 1.40251595e+02]\n",
      "37-th iteration, loss: 0.2414051422045293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.626545287026264e-07\n",
      "37-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269184e+00 0.00000000e+00 3.88578059e-16 4.78602305e+01\n",
      " 7.54396634e+01 4.19500635e+01 9.45481027e+01 7.92860531e+01\n",
      " 1.18505709e+02 8.22892293e+01 1.28253262e+02 5.79817124e-06\n",
      " 6.54022001e-06 5.66655148e-06 5.54985576e-06 4.36931131e-06\n",
      " 3.85730786e-06 2.79517040e-06 2.26211507e-06 2.53257940e-06\n",
      " 1.84403375e-06 2.27614200e-06 1.47832005e-06 2.03301988e-06\n",
      " 1.16437712e-06 8.28328866e+01 1.40251595e+02]\n",
      "38-th iteration, loss: 0.24140514220452508, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.550945785922936e-07\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269181e+00 0.00000000e+00 3.88578059e-16 4.78602308e+01\n",
      " 7.54396634e+01 4.19500634e+01 9.45481025e+01 7.92860528e+01\n",
      " 1.18505709e+02 8.22892291e+01 1.28253262e+02 5.76571286e-06\n",
      " 6.65936040e-06 5.66645287e-06 5.64095912e-06 4.39667233e-06\n",
      " 3.92679270e-06 2.84161664e-06 2.31776995e-06 2.59021815e-06\n",
      " 1.88715792e-06 2.34290468e-06 1.51018230e-06 2.10709700e-06\n",
      " 1.18618038e-06 8.28328867e+01 1.40251595e+02]\n",
      "39-th iteration, loss: 0.2414051422045211, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4820535579911038e-07\n",
      "39-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269179e+00 4.78602311e+01 7.54396635e+01 4.19500632e+01\n",
      " 9.45481024e+01 7.92860524e+01 1.18505708e+02 8.22892289e+01\n",
      " 1.28253262e+02 5.72740817e-06 6.77674402e-06 5.66109744e-06\n",
      " 5.73030620e-06 4.41922729e-06 3.99438589e-06 2.88360062e-06\n",
      " 2.37130336e-06 2.64367002e-06 1.92787543e-06 2.40569384e-06\n",
      " 1.53930757e-06 2.17735825e-06 1.20488014e-06 8.28328868e+01\n",
      " 1.40251595e+02]\n",
      "40-th iteration, loss: 0.24140514220451753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4811595292771753e-07\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269176e+00 0.00000000e+00 2.22044605e-16 4.78602313e+01\n",
      " 7.54396635e+01 4.19500630e+01 9.45481022e+01 7.92860520e+01\n",
      " 1.18505708e+02 8.22892286e+01 1.28253262e+02 5.68399911e-06\n",
      " 6.89286027e-06 5.65121842e-06 5.81841242e-06 4.43770075e-06\n",
      " 4.06062661e-06 2.92183753e-06 2.42327658e-06 2.69363969e-06\n",
      " 1.96676827e-06 2.46520226e-06 1.56629750e-06 2.24448287e-06\n",
      " 1.22109692e-06 8.28328868e+01 1.40251595e+02]\n",
      "41-th iteration, loss: 0.24140514220451395, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4168548031942137e-07\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269173e+00 4.78602315e+01 7.54396636e+01 4.19500629e+01\n",
      " 9.45481020e+01 7.92860517e+01 1.18505708e+02 8.22892284e+01\n",
      " 0.00000000e+00 3.55271368e-15 1.28253262e+02 5.63603057e-06\n",
      " 7.00804686e-06 5.63735447e-06 5.90563785e-06 4.45262519e-06\n",
      " 4.12589514e-06 2.95685318e-06 2.47408841e-06 2.74064525e-06\n",
      " 2.00425249e-06 2.52193902e-06 1.59158438e-06 2.30896936e-06\n",
      " 1.23527853e-06 8.28328869e+01 1.40251595e+02]\n",
      "42-th iteration, loss: 0.2414051422045106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.409715453292349e-07\n",
      "42-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269170e+00 4.78602317e+01 7.54396636e+01 4.19500627e+01\n",
      " 9.45481018e+01 7.92860514e+01 1.18505708e+02 8.22892282e+01\n",
      " 1.28253263e+02 5.58407439e-06 7.12273186e-06 5.62007278e-06\n",
      " 5.99243028e-06 4.46456348e-06 4.19065683e-06 2.98920562e-06\n",
      " 2.52422016e-06 2.78523900e-06 2.04082405e-06 2.57644943e-06\n",
      " 1.61567786e-06 2.37135463e-06 1.24794767e-06 8.28328870e+01\n",
      " 1.40251595e+02]\n",
      "43-th iteration, loss: 0.2414051422045075, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4066828952745595e-07\n",
      "43-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269168e+00 4.78602318e+01 7.54396636e+01 4.19500626e+01\n",
      " 9.45481017e+01 7.92860511e+01 1.18505707e+02 8.22892280e+01\n",
      " 1.28253263e+02 5.52685276e-06 7.23543451e-06 5.59809309e-06\n",
      " 6.07732587e-06 4.47223321e-06 4.25346260e-06 3.01760992e-06\n",
      " 2.57223591e-06 2.82613266e-06 2.07505897e-06 2.62744069e-06\n",
      " 1.63716500e-06 2.43033996e-06 1.25770181e-06 8.28328870e+01\n",
      " 1.40251595e+02]\n",
      "44-th iteration, loss: 0.24140514220450462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4010307081608912e-07\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269165e+00 0.00000000e+00 2.22044605e-16 4.78602320e+01\n",
      " 7.54396637e+01 4.19500624e+01 9.45481015e+01 7.92860507e+01\n",
      " 1.18505707e+02 8.22892278e+01 1.28253263e+02 5.46630566e-06\n",
      " 7.34790762e-06 5.57334556e-06 6.16210067e-06 4.47755514e-06\n",
      " 4.31610964e-06 3.04397718e-06 2.61995239e-06 2.86522685e-06\n",
      " 2.10879228e-06 2.67680188e-06 1.65789824e-06 2.48780153e-06\n",
      " 1.26641019e-06 8.28328871e+01 1.40251595e+02]\n",
      "45-th iteration, loss: 0.24140514220450168, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3795296804451164e-07\n",
      "45-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269162e+00 4.78602323e+01 7.54396637e+01 4.19500623e+01\n",
      " 9.45481014e+01 7.92860505e+01 1.18505707e+02 8.22892276e+01\n",
      " 1.28253263e+02 5.40240853e-06 7.46003006e-06 5.54580451e-06\n",
      " 6.24664724e-06 4.48050300e-06 4.37850211e-06 3.06828033e-06\n",
      " 2.66728384e-06 2.90249303e-06 2.14194714e-06 2.72450196e-06\n",
      " 1.67780879e-06 2.54370457e-06 1.27401158e-06 8.28328872e+01\n",
      " 1.40251595e+02]\n",
      "46-th iteration, loss: 0.24140514220449907, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.375559958382451e-07\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269159e+00 4.78602324e+01 7.54396638e+01 4.19500622e+01\n",
      " 9.45481012e+01 7.92860502e+01 1.18505706e+02 8.22892274e+01\n",
      " 0.00000000e+00 2.84217094e-14 1.28253263e+02 5.33520886e-06\n",
      " 7.57175011e-06 5.51551568e-06 6.33092769e-06 4.48112140e-06\n",
      " 4.44061388e-06 3.09056273e-06 2.71421434e-06 2.93797264e-06\n",
      " 2.17451665e-06 2.77057952e-06 1.69689799e-06 2.59808361e-06\n",
      " 1.28051501e-06 8.28328872e+01 1.40251595e+02]\n",
      "47-th iteration, loss: 0.24140514220449638, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.360042334539792e-07\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269156e+00 4.78602325e+01 7.54396638e+01 4.19500620e+01\n",
      " 9.45481011e+01 7.92860499e+01 1.18505706e+02 8.22892272e+01\n",
      " 1.28253263e+02 5.26540658e-06 7.68368854e-06 5.48317700e-06\n",
      " 6.41557638e-06 4.48010697e-06 4.50309083e-06 3.11151962e-06\n",
      " 2.76139977e-06 2.97235895e-06 2.20716555e-06 2.81572492e-06\n",
      " 1.71583859e-06 2.65162493e-06 1.28660080e-06 8.28328873e+01\n",
      " 1.40251595e+02]\n",
      "48-th iteration, loss: 0.24140514220449397, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3573866510818808e-07\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269153e+00 4.78602327e+01 7.54396638e+01 4.19500619e+01\n",
      " 9.45481009e+01 7.92860496e+01 1.18505706e+02 8.22892270e+01\n",
      " 0.00000000e+00 2.48689958e-14 1.28253264e+02 5.19139032e-06\n",
      " 7.79412740e-06 5.44717820e-06 6.49888550e-06 4.47585123e-06\n",
      " 4.56423322e-06 3.12954432e-06 2.80714695e-06 3.00404653e-06\n",
      " 2.23820607e-06 2.85833314e-06 1.73294744e-06 2.70272280e-06\n",
      " 1.29058991e-06 8.28328873e+01 1.40251595e+02]\n",
      "49-th iteration, loss: 0.24140514220449152, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3430731677847682e-07\n",
      "49-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269150e+00 4.78602328e+01 7.54396639e+01 4.19500618e+01\n",
      " 9.45481008e+01 7.92860494e+01 1.18505706e+02 8.22892268e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.28253264e+02 5.11512842e-06\n",
      " 7.90492215e-06 5.40948019e-06 6.58272864e-06 4.47030849e-06\n",
      " 4.62593069e-06 3.14658455e-06 2.85336002e-06 3.03497599e-06\n",
      " 2.26955569e-06 2.90033682e-06 1.75015457e-06 2.75330077e-06\n",
      " 1.29442448e-06 8.28328874e+01 1.40251595e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5328864240338573\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.91229239   0.         833.48469254]\n",
      "1-th iteration, loss: 0.7518055542540729, 11 gd steps\n",
      "insert gradient: -0.646337800987476\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.98565692  62.18124359 121.97336964   0.         711.5113229 ]\n",
      "2-th iteration, loss: 0.5091293589099802, 48 gd steps\n",
      "insert gradient: -0.6524902789050355\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.58216824  53.63831489  99.11799134  99.41280834 232.33022789\n",
      "   0.         479.18109501]\n",
      "3-th iteration, loss: 0.43122249789814926, 25 gd steps\n",
      "insert gradient: -0.2953960207596941\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  1.14560132  60.60140707  99.79272257  94.0744238  112.78673403\n",
      "   0.          94.97830234  46.82089987 479.18109501]\n",
      "4-th iteration, loss: 0.340824173917652, 38 gd steps\n",
      "insert gradient: -0.29264391299753206\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.05665697  83.10069653 126.49658851  77.14424388  98.47948316\n",
      "  43.43846085  66.72337203  47.22703745 124.23213574   0.\n",
      " 354.94895927]\n",
      "5-th iteration, loss: 0.2855948269010889, 17 gd steps\n",
      "insert gradient: -0.28387671389022034\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  2.8587922   93.47176748 108.63734981  82.1206607  104.08491935\n",
      "  52.68862047  66.83517465  37.08658234 121.40440077  53.4779588\n",
      " 354.94895927]\n",
      "6-th iteration, loss: 0.2739445789442682, 43 gd steps\n",
      "insert gradient: -0.13754287474005267\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[3.48154098e-01 9.02364580e+01 1.07989996e+02 9.18565791e+01\n",
      " 1.00792236e+02 4.84816857e+01 7.17554179e+01 4.20889856e+01\n",
      " 1.13636977e+02 5.39944622e+01 3.54948959e+02 0.00000000e+00\n",
      " 1.20792265e-13]\n",
      "7-th iteration, loss: 0.255161308450703, 17 gd steps\n",
      "insert gradient: -0.053879804999266596\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.56292211e+01 1.30214084e+02 8.90450678e+01\n",
      " 9.84761846e+01 5.08451011e+01 8.20916515e+01 3.27093094e+01\n",
      " 1.10348914e+02 6.59950925e+01 3.15062364e+02 3.41559620e+01\n",
      " 1.66566837e-13]\n",
      "8-th iteration, loss: 0.22867122414575505, 142 gd steps\n",
      "insert gradient: -0.06563701124623803\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[2.14322226e+00 6.42583019e+01 1.35930430e+02 8.06347824e+01\n",
      " 1.10002145e+02 5.08690062e+01 7.11683958e+01 3.48720918e+01\n",
      " 8.56248949e+01 9.04642815e+01 1.28046017e+02 0.00000000e+00\n",
      " 1.02436814e+02 6.05413009e+01 4.21598384e-13]\n",
      "9-th iteration, loss: 0.21651916699451115, 34 gd steps\n",
      "insert gradient: -0.048360323002242224\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[3.80530721e+00 6.80197565e+01 1.22372458e+02 8.93101542e+01\n",
      " 1.15728734e+02 0.00000000e+00 1.24344979e-14 5.76541573e+01\n",
      " 7.60641948e+01 3.32016056e+01 8.37525189e+01 9.48265958e+01\n",
      " 9.75848335e+01 3.09971733e+01 5.87145383e+01 6.83443381e+01\n",
      " 5.29705559e-13]\n",
      "10-th iteration, loss: 0.21004691860549093, 22 gd steps\n",
      "insert gradient: -0.044182126740155155\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[2.82261188e+00 6.75532349e+01 1.22227350e+02 9.49524616e+01\n",
      " 1.14145154e+02 6.06304816e+01 8.19330234e+01 3.29049849e+01\n",
      " 8.14504617e+01 9.46910726e+01 1.02481881e+02 3.85661406e+01\n",
      " 5.65139728e+01 5.77063052e+01 5.46756487e-13]\n",
      "11-th iteration, loss: 0.2078690358616768, 20 gd steps\n",
      "insert gradient: -0.03594531383933382\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[1.79995766e+00 6.53721335e+01 1.21221567e+02 9.65309140e+01\n",
      " 1.14640924e+02 6.39627392e+01 8.64499426e+01 3.22069073e+01\n",
      " 7.85302922e+01 6.48959096e+01 0.00000000e+00 2.94981407e+01\n",
      " 1.06501062e+02 4.24984077e+01 5.48869992e+01 5.37791312e+01\n",
      " 5.57903997e-13]\n",
      "12-th iteration, loss: 0.20681553464731356, 13 gd steps\n",
      "insert gradient: -0.03902925815673713\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[2.15363783e+00 6.70066655e+01 1.21037693e+02 9.71707537e+01\n",
      " 1.14160665e+02 6.28758424e+01 8.86191921e+01 3.35492790e+01\n",
      " 7.68886737e+01 6.35174341e+01 0.00000000e+00 7.10542736e-15\n",
      " 3.43044476e+00 2.79195177e+01 1.07948935e+02 4.40064136e+01\n",
      " 5.42797743e+01 5.22682863e+01 5.69222186e-13]\n",
      "13-th iteration, loss: 0.20006093881945097, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[2.64592283e+00 6.87446555e+01 1.24203413e+02 9.61568536e+01\n",
      " 1.16709358e+02 6.36012341e+01 9.17533073e+01 3.63395413e+01\n",
      " 7.92440295e+01 6.36032728e+01 2.62403611e+01 1.10546033e+01\n",
      " 1.63358488e+01 0.00000000e+00 9.80150926e+01 4.72356206e+01\n",
      " 4.98372621e+01 5.05494357e+01 6.00231081e-13]\n",
      "14-th iteration, loss: 0.1895096618561833, 57 gd steps\n",
      "insert gradient: -0.0038140869798465498\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[2.33801391e+00 7.13357566e+01 1.28563621e+02 9.62574911e+01\n",
      " 1.16131344e+02 6.61390196e+01 9.63360582e+01 3.98853371e+01\n",
      " 7.73028613e+01 5.27842840e+01 7.60927309e+01 1.10900978e+00\n",
      " 1.04557285e+02 5.05209968e+01 0.00000000e+00 1.15463195e-14\n",
      " 4.91438754e+01 4.80758054e+01 6.23619533e-13]\n",
      "15-th iteration, loss: 0.18881107442482548, 48 gd steps\n",
      "insert gradient: -0.001638374901153283\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[2.29821630e+00 7.04950493e+01 1.31651570e+02 9.67026875e+01\n",
      " 1.15649523e+02 6.62485472e+01 9.87952412e+01 4.05416642e+01\n",
      " 7.72733309e+01 5.00222285e+01 1.87170931e+02 5.08914994e+01\n",
      " 5.40676250e+01 4.45129258e+01 6.37954267e-13]\n",
      "16-th iteration, loss: 0.18869222571968178, 279 gd steps\n",
      "insert gradient: -0.0013744854382971214\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[2.53892398e+00 6.98829639e+01 1.33353546e+02 9.69498154e+01\n",
      " 1.15369673e+02 6.63337341e+01 1.00007347e+02 4.07507972e+01\n",
      " 7.70608267e+01 4.97486907e+01 9.38760239e+01 0.00000000e+00\n",
      " 9.38760239e+01 5.01111382e+01 5.99409458e+01 4.09900233e+01\n",
      " 9.23211497e-13]\n",
      "17-th iteration, loss: 0.18868935885361035, 51 gd steps\n",
      "insert gradient: -0.00033626143555386893\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[2.54151572e+00 6.98983659e+01 1.33549192e+02 9.69208762e+01\n",
      " 1.15382786e+02 0.00000000e+00 2.48689958e-14 6.63468034e+01\n",
      " 1.00064886e+02 4.07659539e+01 7.71108737e+01 4.98498876e+01\n",
      " 9.35295938e+01 3.60197452e-01 9.30502579e+01 5.02434306e+01\n",
      " 6.00168252e+01 4.09527578e+01 9.85853640e-13]\n",
      "18-th iteration, loss: 0.18868844766743298, 917 gd steps\n",
      "insert gradient: -4.0266121005492495e-05\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[2.54390787e+00 6.98924776e+01 1.33601893e+02 9.69149359e+01\n",
      " 1.15385523e+02 6.63755274e+01 1.00094211e+02 4.07762224e+01\n",
      " 7.71169754e+01 4.98422636e+01 9.42179205e+01 4.33727616e-01\n",
      " 9.21619599e+01 5.02667071e+01 6.00833898e+01 4.09228488e+01\n",
      " 1.11255925e-12]\n",
      "19-th iteration, loss: 0.1886874443258856, 126 gd steps\n",
      "insert gradient: -1.0288126279739883e-05\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[2.54579007e+00 6.98960266e+01 1.33645686e+02 9.69010534e+01\n",
      " 1.15387910e+02 6.63827980e+01 1.00106943e+02 4.07775866e+01\n",
      " 7.71386501e+01 4.98408601e+01 9.58772391e+01 5.06790354e-01\n",
      " 9.02776073e+01 5.03058876e+01 6.00866290e+01 4.09123616e+01\n",
      " 1.74493286e-12]\n",
      "20-th iteration, loss: 0.18868744353690303, 13 gd steps\n",
      "insert gradient: -2.5630256462051117e-05\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[2.54773032e+00 6.98963998e+01 1.33647530e+02 9.69008074e+01\n",
      " 1.15386974e+02 6.63798694e+01 1.00111409e+02 4.07784935e+01\n",
      " 7.71331714e+01 4.98428732e+01 9.58888577e+01 5.06450467e-01\n",
      " 9.02666534e+01 5.03045353e+01 6.00899981e+01 4.09104373e+01\n",
      " 1.76391637e-12]\n",
      "21-th iteration, loss: 0.18868743983634106, 64 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.261298683245403e-06\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[2.54629874e+00 0.00000000e+00 7.21644966e-16 6.98938365e+01\n",
      " 1.33646969e+02 9.69014523e+01 1.15388008e+02 6.63828314e+01\n",
      " 1.00109583e+02 4.07774370e+01 7.71358935e+01 4.98418669e+01\n",
      " 9.59212695e+01 5.06442994e-01 9.02334646e+01 5.03056766e+01\n",
      " 6.00913110e+01 4.09099403e+01 1.83931384e-12]\n",
      "22-th iteration, loss: 0.1886874398334359, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.056423888370797e-06\n",
      "22-th iteration, new layer inserted. now 19 layers\n",
      "[2.54630098e+00 8.20989079e-06 2.23811717e-06 6.98938447e+01\n",
      " 1.33646971e+02 9.69014532e+01 1.15388007e+02 6.63828260e+01\n",
      " 1.00109581e+02 4.07774353e+01 7.71358916e+01 4.98418634e+01\n",
      " 9.59212745e+01 5.06448587e-01 9.02334616e+01 5.03056774e+01\n",
      " 6.00913124e+01 4.09099425e+01 1.83931153e-12]\n",
      "23-th iteration, loss: 0.18868743983067965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.85591188297858e-06\n",
      "23-th iteration, new layer inserted. now 19 layers\n",
      "[2.54630316e+00 1.62030541e-05 4.37059151e-06 6.98938527e+01\n",
      " 1.33646973e+02 9.69014540e+01 1.15388005e+02 6.63828207e+01\n",
      " 1.00109579e+02 4.07774338e+01 7.71358898e+01 4.98418598e+01\n",
      " 9.59212794e+01 5.06453920e-01 9.02334586e+01 5.03056781e+01\n",
      " 6.00913138e+01 4.09099446e+01 1.83931722e-12]\n",
      "24-th iteration, loss: 0.18868743982806102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.660055478861386e-06\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[2.54630528e+00 2.39845193e-05 6.40003233e-06 0.00000000e+00\n",
      " 2.54109884e-21 6.98938605e+01 1.33646975e+02 9.69014547e+01\n",
      " 1.15388004e+02 6.63828156e+01 1.00109577e+02 4.07774324e+01\n",
      " 7.71358881e+01 4.98418563e+01 9.59212843e+01 5.06459016e-01\n",
      " 9.02334554e+01 5.03056787e+01 6.00913151e+01 4.09099467e+01\n",
      " 1.83932154e-12]\n",
      "25-th iteration, loss: 0.18868743982494496, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.2978848216022005e-06\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[2.54630734e+00 3.15145738e-05 8.31874704e-06 7.56817823e-06\n",
      " 1.91871472e-06 6.98938681e+01 1.33646977e+02 9.69014552e+01\n",
      " 1.15388003e+02 6.63828105e+01 1.00109575e+02 4.07774313e+01\n",
      " 7.71358864e+01 4.98418528e+01 9.59212891e+01 5.06463927e-01\n",
      " 9.02334522e+01 5.03056792e+01 6.00913164e+01 4.09099487e+01\n",
      " 1.83934321e-12]\n",
      "26-th iteration, loss: 0.1886874398220402, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.963505442698563e-06\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[2.54630931e+00 3.86675402e-05 1.00987817e-05 1.47706977e-05\n",
      " 3.65366720e-06 0.00000000e+00 3.17637355e-22 6.98938753e+01\n",
      " 1.33646979e+02 9.69014556e+01 1.15388001e+02 6.63828056e+01\n",
      " 1.00109574e+02 4.07774302e+01 7.71358848e+01 4.98418493e+01\n",
      " 9.59212938e+01 5.06468772e-01 9.02334490e+01 5.03056797e+01\n",
      " 6.00913177e+01 4.09099508e+01 1.83934496e-12]\n",
      "27-th iteration, loss: 0.1886874398188302, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.5010675133587164e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[2.54631118e+00 4.54320767e-05 1.17400838e-05 2.15953907e-05\n",
      " 5.20698316e-06 6.84645718e-06 1.55331596e-06 0.00000000e+00\n",
      " 2.11758237e-22 6.98938821e+01 1.33646980e+02 9.69014557e+01\n",
      " 1.15388000e+02 6.63828006e+01 1.00109572e+02 4.07774294e+01\n",
      " 7.71358833e+01 4.98418457e+01 9.59212985e+01 5.06473563e-01\n",
      " 9.02334457e+01 5.03056801e+01 6.00913190e+01 4.09099528e+01\n",
      " 1.83935887e-12]\n",
      "28-th iteration, loss: 0.1886874398154854, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.947459776834266e-06\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[2.54631293e+00 5.16827689e-05 1.32157872e-05 2.79160164e-05\n",
      " 6.55404692e-06 1.31980998e-05 2.85959666e-06 6.36089548e-06\n",
      " 1.30628070e-06 0.00000000e+00 1.58818678e-22 6.98938885e+01\n",
      " 1.33646982e+02 9.69014556e+01 1.15387998e+02 6.63827956e+01\n",
      " 1.00109571e+02 4.07774287e+01 7.71358818e+01 4.98418421e+01\n",
      " 9.59213032e+01 5.06478405e-01 9.02334424e+01 5.03056805e+01\n",
      " 6.00913203e+01 4.09099548e+01 1.83936473e-12]\n",
      "29-th iteration, loss: 0.188687439812165, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.341133630027295e-06\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[2.54631453e+00 5.73346662e-05 1.45092668e-05 3.36466377e-05\n",
      " 7.68123594e-06 1.89677624e-05 3.90816701e-06 1.21475921e-05\n",
      " 2.31696035e-06 5.79447793e-06 1.01067965e-06 0.00000000e+00\n",
      " 1.58818678e-22 6.98938943e+01 1.33646983e+02 9.69014553e+01\n",
      " 1.15387997e+02 6.63827905e+01 1.00109569e+02 4.07774280e+01\n",
      " 7.71358804e+01 4.98418384e+01 9.59213079e+01 5.06483362e-01\n",
      " 9.02334392e+01 5.03056807e+01 6.00913215e+01 4.09099568e+01\n",
      " 1.83937073e-12]\n",
      "30-th iteration, loss: 0.1886874398089898, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.4140305210701845e-06\n",
      "30-th iteration, new layer inserted. now 31 layers\n",
      "[2.54631598e+00 6.23443763e-05 1.56141517e-05 3.87427768e-05\n",
      " 8.58569405e-06 2.41096572e-05 4.69963756e-06 1.73127672e-05\n",
      " 3.03606983e-06 1.09734547e-05 1.69527244e-06 5.18499726e-06\n",
      " 6.84592793e-07 6.98938995e+01 1.33646984e+02 9.69014546e+01\n",
      " 1.15387995e+02 6.63827854e+01 1.00109568e+02 4.07774276e+01\n",
      " 7.71358790e+01 4.98418346e+01 9.59213125e+01 0.00000000e+00\n",
      " 3.55271368e-15 5.06488463e-01 9.02334359e+01 5.03056809e+01\n",
      " 6.00913228e+01 4.09099588e+01 1.83938115e-12]\n",
      "31-th iteration, loss: 0.18868743980575067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.884472978843182e-06\n",
      "31-th iteration, new layer inserted. now 33 layers\n",
      "[2.54631728e+00 6.67513105e-05 1.65439109e-05 4.32427215e-05\n",
      " 9.28466945e-06 2.86607455e-05 5.25499595e-06 2.18918504e-05\n",
      " 3.48829905e-06 1.55706232e-05 2.08213465e-06 9.79226421e-06\n",
      " 1.04056885e-06 6.98939041e+01 1.33646984e+02 9.69014536e+01\n",
      " 1.15387993e+02 6.63827802e+01 1.00109567e+02 4.07774272e+01\n",
      " 7.71358776e+01 4.98418307e+01 9.59213171e+01 4.85230916e-06\n",
      " 4.55401594e-06 0.00000000e+00 2.11758237e-22 5.06493315e-01\n",
      " 9.02334325e+01 5.03056810e+01 6.00913240e+01 4.09099607e+01\n",
      " 1.83938933e-12]\n",
      "32-th iteration, loss: 0.1886874398025356, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.166387555924061e-06\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[2.54631848e+00 7.07536469e-05 1.73495254e-05 4.73436071e-05\n",
      " 9.83269469e-06 3.28169382e-05 5.63229405e-06 2.60793462e-05\n",
      " 3.73519108e-06 1.97788983e-05 2.23627505e-06 1.40129422e-05\n",
      " 1.13637833e-06 6.98939083e+01 1.33646985e+02 9.69014525e+01\n",
      " 1.15387990e+02 6.63827752e+01 1.00109565e+02 4.07774268e+01\n",
      " 7.71358762e+01 4.98418264e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.59213214e+01 8.94248658e-06 8.80613009e-06 4.09051651e-06\n",
      " 4.25211415e-06 5.06497405e-01 9.02334287e+01 5.03056806e+01\n",
      " 6.00913252e+01 4.09099626e+01 1.83940867e-12]\n",
      "33-th iteration, loss: 0.188687439799561, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.849194309405082e-06\n",
      "33-th iteration, new layer inserted. now 35 layers\n",
      "[2.54631962e+00 7.44700351e-05 1.80610948e-05 5.11633433e-05\n",
      " 1.02622465e-05 3.66952460e-05 5.86636094e-06 2.99912047e-05\n",
      " 3.81390759e-06 2.37130067e-05 2.19717185e-06 1.79603718e-05\n",
      " 1.01380225e-06 6.98939123e+01 1.33646986e+02 9.69014513e+01\n",
      " 1.15387988e+02 6.63827703e+01 1.00109564e+02 4.07774265e+01\n",
      " 7.71358747e+01 4.98418217e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.59213293e+01 1.22917292e-05 1.27748052e-05 7.44041483e-06\n",
      " 8.22048466e-06 5.06500756e-01 9.02334247e+01 5.03056800e+01\n",
      " 6.00913262e+01 4.09099644e+01 1.83942401e-12]\n",
      "34-th iteration, loss: 0.18868743979691552, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.620915569895495e-06\n",
      "34-th iteration, new layer inserted. now 35 layers\n",
      "[2.54632068e+00 7.79139349e-05 1.86839181e-05 5.47148298e-05\n",
      " 1.05802987e-05 4.03078630e-05 5.96582560e-06 3.36387666e-05\n",
      " 3.73471949e-06 2.73832875e-05 1.97672910e-06 2.16437407e-05\n",
      " 6.86372334e-07 6.98939160e+01 1.33646986e+02 9.69014500e+01\n",
      " 1.15387986e+02 6.63827656e+01 1.00109563e+02 4.07774264e+01\n",
      " 7.71358732e+01 4.98418168e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.59213367e+01 1.49866396e-05 1.64850678e-05 1.01362764e-05\n",
      " 1.19301933e-05 5.06503452e-01 9.02334205e+01 5.03056790e+01\n",
      " 6.00913272e+01 4.09099661e+01 1.83943775e-12]\n",
      "35-th iteration, loss: 0.1886874397945113, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.454418211085945e-06\n",
      "35-th iteration, new layer inserted. now 35 layers\n",
      "[2.54632168e+00 8.10964351e-05 1.92218627e-05 5.80086268e-05\n",
      " 1.07923164e-05 4.36646851e-05 5.93773598e-06 3.70311262e-05\n",
      " 3.50624905e-06 3.07978942e-05 1.58514063e-06 2.50701225e-05\n",
      " 1.65855356e-07 6.98939194e+01 1.33646987e+02 9.69014486e+01\n",
      " 1.15387984e+02 6.63827610e+01 1.00109561e+02 4.07774263e+01\n",
      " 7.71358718e+01 4.98418116e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.59213438e+01 1.72017935e-05 2.00088098e-05 1.23526577e-05\n",
      " 1.54531807e-05 5.06505669e-01 9.02334160e+01 5.03056778e+01\n",
      " 6.00913281e+01 4.09099678e+01 1.83943821e-12]\n",
      "36-th iteration, loss: 0.1886874397922964, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3319139480239545e-06\n",
      "36-th iteration, new layer inserted. now 33 layers\n",
      "[2.54632263e+00 8.40368219e-05 1.96808186e-05 6.10635149e-05\n",
      " 1.09057247e-05 4.67838613e-05 5.79104086e-06 4.01856725e-05\n",
      " 3.13896527e-06 3.39733267e-05 1.03439843e-06 6.98939508e+01\n",
      " 1.33646987e+02 9.69014471e+01 1.15387982e+02 6.63827566e+01\n",
      " 1.00109560e+02 4.07774264e+01 7.71358703e+01 4.98418064e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59213505e+01 1.90560712e-05\n",
      " 2.33961435e-05 1.42084252e-05 1.88395946e-05 5.06507526e-01\n",
      " 9.02334115e+01 5.03056765e+01 6.00913290e+01 4.09099695e+01\n",
      " 1.83944471e-12]\n",
      "37-th iteration, loss: 0.18868743979032013, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.230917778714139e-06\n",
      "37-th iteration, new layer inserted. now 33 layers\n",
      "[2.54632352e+00 8.67673479e-05 2.00696039e-05 6.39112762e-05\n",
      " 1.09307649e-05 4.96965862e-05 5.53739706e-06 4.31328937e-05\n",
      " 2.64594157e-06 3.69392463e-05 3.39000642e-07 6.98939538e+01\n",
      " 1.33646987e+02 9.69014456e+01 1.15387979e+02 6.63827522e+01\n",
      " 1.00109559e+02 4.07774265e+01 7.71358690e+01 4.98418011e+01\n",
      " 0.00000000e+00 1.77635684e-15 9.59213571e+01 2.06249046e-05\n",
      " 2.66819267e-05 1.57790006e-05 2.21243200e-05 5.06509098e-01\n",
      " 9.02334068e+01 5.03056751e+01 6.00913299e+01 4.09099711e+01\n",
      " 1.83944647e-12]\n",
      "38-th iteration, loss: 0.1886874397884501, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1591403283945553e-06\n",
      "38-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632437e+00 8.93510242e-05 2.04042292e-05 6.66145036e-05\n",
      " 1.08846813e-05 5.24649264e-05 5.19527874e-06 4.59342194e-05\n",
      " 2.04688712e-06 6.98939964e+01 1.33646988e+02 9.69014441e+01\n",
      " 1.15387977e+02 6.63827480e+01 1.00109558e+02 4.07774268e+01\n",
      " 7.71358676e+01 4.98417958e+01 9.59213635e+01 2.19344238e-05\n",
      " 2.98857673e-05 1.70905065e-05 2.53269857e-05 5.06510411e-01\n",
      " 9.02334020e+01 5.03056736e+01 6.00913307e+01 4.09099727e+01\n",
      " 1.83945188e-12]\n",
      "39-th iteration, loss: 0.1886874397868468, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.149620430466098e-06\n",
      "39-th iteration, new layer inserted. now 31 layers\n",
      "[2.54632518e+00 9.17940829e-05 2.06874409e-05 6.91791068e-05\n",
      " 1.07710814e-05 5.50943679e-05 4.76915345e-06 4.85946084e-05\n",
      " 1.34713852e-06 6.98939991e+01 1.33646988e+02 9.69014425e+01\n",
      " 1.15387975e+02 6.63827439e+01 1.00109558e+02 4.07774273e+01\n",
      " 7.71358663e+01 4.98417905e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.59213666e+01 2.30043760e-05 3.30086236e-05 1.81626839e-05\n",
      " 2.84485695e-05 5.06511486e-01 9.02333972e+01 5.03056720e+01\n",
      " 6.00913315e+01 4.09099743e+01 1.83945965e-12]\n",
      "40-th iteration, loss: 0.18868743978520136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0929156013786883e-06\n",
      "40-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632597e+00 9.41494764e-05 2.09311802e-05 7.16577320e-05\n",
      " 1.06027322e-05 5.76371547e-05 4.27261563e-06 5.11658043e-05\n",
      " 5.61129779e-07 6.98940016e+01 1.33646988e+02 9.69014410e+01\n",
      " 1.15387973e+02 6.63827400e+01 1.00109557e+02 4.07774279e+01\n",
      " 7.71358652e+01 4.98417854e+01 9.59213729e+01 2.40120781e-05\n",
      " 3.61461930e-05 1.91728440e-05 3.15847865e-05 5.06512498e-01\n",
      " 9.02333923e+01 5.03056705e+01 6.00913323e+01 4.09099759e+01\n",
      " 1.83946684e-12]\n",
      "41-th iteration, loss: 0.1886874397837152, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1042775151200533e-06\n",
      "41-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632674e+00 9.64149540e-05 2.11370081e-05 7.40478931e-05\n",
      " 1.03817067e-05 6.00904746e-05 3.70825438e-06 6.98940578e+01\n",
      " 1.33646989e+02 9.69014394e+01 1.15387970e+02 6.63827361e+01\n",
      " 1.00109556e+02 4.07774286e+01 7.71358640e+01 4.98417803e+01\n",
      " 0.00000000e+00 8.88178420e-15 9.59213760e+01 2.48082938e-05\n",
      " 3.92178313e-05 1.99717511e-05 3.46549973e-05 5.06513299e-01\n",
      " 9.02333875e+01 5.03056689e+01 6.00913331e+01 4.09099775e+01\n",
      " 1.83947974e-12]\n",
      "42-th iteration, loss: 0.18868743978224353, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0555219555516026e-06\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[2.54632748e+00 9.86062851e-05 2.13078483e-05 7.63651338e-05\n",
      " 1.01114556e-05 6.24695575e-05 3.08005355e-06 6.98940602e+01\n",
      " 1.33646989e+02 9.69014379e+01 1.15387968e+02 6.63827324e+01\n",
      " 1.00109555e+02 4.07774295e+01 7.71358630e+01 4.98417754e+01\n",
      " 9.59213822e+01 2.55867087e-05 4.23189264e-05 2.07530864e-05\n",
      " 3.77546053e-05 5.06514083e-01 9.02333826e+01 5.03056674e+01\n",
      " 6.00913338e+01 4.09099790e+01 1.83947890e-12]\n",
      "43-th iteration, loss: 0.18868743978091002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0731566195435804e-06\n",
      "43-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632822e+00 1.00753534e-04 2.14526776e-05 7.86393097e-05\n",
      " 9.80139016e-06 6.48039658e-05 2.39786637e-06 6.98940625e+01\n",
      " 1.33646989e+02 9.69014363e+01 1.15387966e+02 6.63827288e+01\n",
      " 1.00109555e+02 4.07774304e+01 7.71358620e+01 4.98417705e+01\n",
      " 0.00000000e+00 1.77635684e-15 9.59213853e+01 2.61657305e-05\n",
      " 4.53626439e-05 2.13352595e-05 4.07967775e-05 5.06514668e-01\n",
      " 9.02333777e+01 5.03056658e+01 6.00913346e+01 4.09099806e+01\n",
      " 1.83949179e-12]\n",
      "44-th iteration, loss: 0.18868743977951308, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.038594164364344e-06\n",
      "44-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632893e+00 1.02856091e-04 2.15704365e-05 8.08696566e-05\n",
      " 9.45070733e-06 6.70926980e-05 1.66115606e-06 6.98940648e+01\n",
      " 1.33646990e+02 9.69014348e+01 1.15387964e+02 6.63827253e+01\n",
      " 1.00109554e+02 4.07774316e+01 7.71358612e+01 4.98417658e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59213914e+01 2.67383109e-05\n",
      " 4.84405798e-05 2.19112178e-05 4.38731247e-05 5.06515247e-01\n",
      " 9.02333728e+01 5.03056643e+01 6.00913353e+01 4.09099821e+01\n",
      " 1.83951340e-12]\n",
      "45-th iteration, loss: 0.18868743977814836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.016466567193415e-06\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[2.54632964e+00 1.04917462e-04 2.16632752e-05 8.30595194e-05\n",
      " 9.06181846e-06 6.93388573e-05 8.72605992e-07 6.98940671e+01\n",
      " 1.33646990e+02 9.69014334e+01 1.15387962e+02 6.63827219e+01\n",
      " 1.00109554e+02 4.07774328e+01 7.71358603e+01 4.98417612e+01\n",
      " 9.59213975e+01 2.71830678e-05 5.14913238e-05 2.23595820e-05\n",
      " 4.69222372e-05 5.06515699e-01 9.02333679e+01 5.03056627e+01\n",
      " 6.00913361e+01 4.09099836e+01 1.83953956e-12]\n",
      "46-th iteration, loss: 0.18868743977690858, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0500809780307986e-06\n",
      "46-th iteration, new layer inserted. now 29 layers\n",
      "[2.54633033e+00 1.06924800e-04 2.17289153e-05 8.51959022e-05\n",
      " 8.63268617e-06 7.15292199e-05 3.04323798e-08 6.98940693e+01\n",
      " 1.33646990e+02 9.69014319e+01 1.15387960e+02 6.63827186e+01\n",
      " 1.00109553e+02 4.07774341e+01 7.71358596e+01 4.98417566e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59214005e+01 2.75068033e-05\n",
      " 5.45078261e-05 2.26871520e-05 4.99370745e-05 5.06516030e-01\n",
      " 9.02333629e+01 5.03056612e+01 6.00913368e+01 4.09099852e+01\n",
      " 1.83954759e-12]\n",
      "47-th iteration, loss: 0.1886874397756068, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.027621959045647e-06\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633101e+00 1.08882638e-04 2.17676472e-05 8.72831755e-05\n",
      " 8.16391955e-06 6.98941451e+01 1.33646991e+02 9.69014305e+01\n",
      " 1.15387958e+02 6.63827154e+01 1.00109553e+02 4.07774355e+01\n",
      " 7.71358589e+01 4.98417522e+01 9.59214067e+01 2.78691028e-05\n",
      " 5.75689094e-05 2.30535107e-05 5.29964684e-05 5.06516400e-01\n",
      " 9.02333580e+01 5.03056597e+01 6.00913375e+01 4.09099867e+01\n",
      " 1.83955080e-12]\n",
      "48-th iteration, loss: 0.1886874397744734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0533095946512484e-06\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633167e+00 1.10791483e-04 2.17811676e-05 8.93216870e-05\n",
      " 7.65750784e-06 6.98941472e+01 1.33646991e+02 9.69014290e+01\n",
      " 1.15387957e+02 6.63827122e+01 1.00109553e+02 4.07774369e+01\n",
      " 7.71358582e+01 4.98417478e+01 9.59214097e+01 2.81175289e-05\n",
      " 6.05963631e-05 2.33062240e-05 5.60222054e-05 5.06516656e-01\n",
      " 9.02333531e+01 5.03056582e+01 6.00913382e+01 4.09099881e+01\n",
      " 1.83956286e-12]\n",
      "49-th iteration, loss: 0.18868743977336125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0735235351712152e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633232e+00 1.12679537e-04 2.17756657e-05 9.13394881e-05\n",
      " 7.11993067e-06 6.98941492e+01 1.33646991e+02 9.69014277e+01\n",
      " 1.15387955e+02 6.63827092e+01 1.00109552e+02 4.07774385e+01\n",
      " 7.71358576e+01 4.98417435e+01 9.59214127e+01 2.83599515e-05\n",
      " 6.36468073e-05 2.35531594e-05 5.90709141e-05 5.06516907e-01\n",
      " 9.02333481e+01 5.03056567e+01 6.00913389e+01 4.09099896e+01\n",
      " 1.83957898e-12]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.53615684102246\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         860.93116183]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6252791532618974\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 230.98153122   0.         629.94963061]\n",
      "2-th iteration, loss: 0.6036407791272141, 13 gd steps\n",
      "insert gradient: -0.6377458424663993\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.70763905  77.96399303 217.06852198  42.4114489  244.2661833\n",
      "   0.         385.68344731]\n",
      "3-th iteration, loss: 0.46276138578816994, 28 gd steps\n",
      "insert gradient: -0.717599980057436\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.30497788 227.36794006  48.07019182 161.49137796\n",
      "  50.7806192  363.64439318   0.          22.03905413]\n",
      "4-th iteration, loss: 0.37871581962501016, 14 gd steps\n",
      "insert gradient: -0.2944730264569776\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.55375599  61.83953572 220.20870363  61.02240831 144.9868635\n",
      "  45.04806695 239.22456294   0.         100.72613176  49.4793047\n",
      "  22.03905413]\n",
      "5-th iteration, loss: 0.29593148364295324, 47 gd steps\n",
      "insert gradient: -0.03814060834413912\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          75.22731587 106.51180226   0.         106.51180226\n",
      "  66.30003284 115.28820819  58.16579857 179.38697473  47.09146704\n",
      "  79.16159589  45.18307761  22.03905413]\n",
      "6-th iteration, loss: 0.29565340077572283, 13 gd steps\n",
      "insert gradient: -0.01708315133704025\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.61887364e+01 1.02916928e+02 0.00000000e+00\n",
      " 3.73034936e-14 2.64179118e+00 1.02273035e+02 6.71466875e+01\n",
      " 1.15758171e+02 5.81074491e+01 1.78908912e+02 4.78335201e+01\n",
      " 7.95921103e+01 4.46798475e+01 2.20390541e+01]\n",
      "7-th iteration, loss: 0.29558542707475655, 17 gd steps\n",
      "insert gradient: -0.006591361450998393\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.61852674e+01 1.03144951e+02 1.69954739e-01\n",
      " 1.82461272e-01 2.83666741e+00 1.01926663e+02 6.67518458e+01\n",
      " 1.15875584e+02 5.82916550e+01 1.79368809e+02 4.76557240e+01\n",
      " 7.95091771e+01 4.51206707e+01 2.20390541e+01]\n",
      "8-th iteration, loss: 0.29555242688556965, 22 gd steps\n",
      "insert gradient: -0.0024413146805199208\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.62735683e+01 1.03889841e+02 7.11915437e-03\n",
      " 7.06507981e-01 3.18171202e+00 1.00613309e+02 6.67521846e+01\n",
      " 1.16258544e+02 5.82856518e+01 1.79795172e+02 4.76977617e+01\n",
      " 7.97128660e+01 4.51407801e+01 2.20390541e+01]\n",
      "9-th iteration, loss: 0.29554345420127753, 202 gd steps\n",
      "insert gradient: -0.00033291403312604105\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.34528203 105.0516571    3.31887414 100.03738349\n",
      "  66.65532974 116.47552296  58.29158317 180.01840244  47.6955232\n",
      "  79.91282012  45.19232996  22.03905413]\n",
      "10-th iteration, loss: 0.29554326498722805, 150 gd steps\n",
      "insert gradient: -0.00036249992161231556\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63478790e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05062699e+02 3.32329178e+00 1.00014076e+02 6.66549096e+01\n",
      " 1.16483318e+02 5.82888488e+01 1.80027655e+02 4.76943246e+01\n",
      " 7.99182122e+01 4.51924000e+01 2.20390541e+01]\n",
      "11-th iteration, loss: 0.29554299785239546, 199 gd steps\n",
      "insert gradient: -0.000333839699749244\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63498678e+01 1.40051353e-02 2.91857809e-06\n",
      " 1.05076823e+02 3.32758813e+00 9.99830666e+01 6.66542330e+01\n",
      " 1.16492485e+02 5.82857120e+01 1.80038493e+02 4.76934910e+01\n",
      " 7.99253585e+01 4.51927688e+01 2.20390541e+01]\n",
      "12-th iteration, loss: 0.29554281760039863, 147 gd steps\n",
      "insert gradient: -0.0003256983611748408\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63510192e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05110463e+02 3.33012817e+00 9.99610206e+01 6.66537626e+01\n",
      " 1.16498439e+02 5.82836675e+01 1.80045278e+02 4.76928709e+01\n",
      " 7.99301474e+01 4.51928368e+01 2.20390541e+01]\n",
      "13-th iteration, loss: 0.2955426756911953, 124 gd steps\n",
      "insert gradient: -0.00032278387830824904\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63516656e+01 7.90221114e-03 9.49702612e-06\n",
      " 0.00000000e+00 1.48230766e-21 1.05118387e+02 3.33208778e+00\n",
      " 9.99430705e+01 6.66534656e+01 1.16502961e+02 5.82821361e+01\n",
      " 1.80050309e+02 4.76923999e+01 7.99339025e+01 4.51928437e+01\n",
      " 2.20390541e+01]\n",
      "14-th iteration, loss: 0.2955424763018284, 154 gd steps\n",
      "insert gradient: -0.0002888962821284706\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35181418 105.15525902   3.33324276  99.91912355\n",
      "  66.65265003 116.50841383  58.28039701 180.05645465  47.69209255\n",
      "  79.93874979  45.19282639  22.03905413]\n",
      "15-th iteration, loss: 0.2955424195409074, 60 gd steps\n",
      "insert gradient: -0.00032179398960825994\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63520166e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05158969e+02 3.33419609e+00 9.99105017e+01 6.66525831e+01\n",
      " 1.16510372e+02 5.82797426e+01 1.80058485e+02 4.76917840e+01\n",
      " 7.99403723e+01 4.51927082e+01 2.20390541e+01]\n",
      "16-th iteration, loss: 0.295542329505211, 87 gd steps\n",
      "insert gradient: -0.00031025698114949874\n",
      "16-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63522862e+01 0.00000000e+00 4.26325641e-14\n",
      " 1.05169637e+02 3.33566763e+00 9.98983730e+01 6.66526196e+01\n",
      " 1.16513016e+02 5.82788508e+01 1.80061155e+02 4.76913522e+01\n",
      " 7.99426143e+01 4.51925849e+01 2.20390541e+01]\n",
      "17-th iteration, loss: 0.2955422477852864, 81 gd steps\n",
      "insert gradient: -0.00030699926870489543\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35245476 105.17935702   3.3368628   99.8871523\n",
      "  66.65263084 116.51533772  58.27810896 180.06351537  47.69107035\n",
      "  79.94467543  45.19250822  22.03905413]\n",
      "18-th iteration, loss: 0.29554220336124304, 51 gd steps\n",
      "insert gradient: -0.00032333151699300727\n",
      "18-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35266104 105.18253661   3.33786934  99.88017299\n",
      "  66.65275603 116.51677363  58.27764446 180.06491082  47.69084001\n",
      "  79.94591585  45.19243497  22.03905413]\n",
      "19-th iteration, loss: 0.29554216131308103, 49 gd steps\n",
      "insert gradient: -0.00032941167817682513\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63529372e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05185693e+02 3.33903709e+00 9.98735761e+01 6.66529789e+01\n",
      " 1.16518115e+02 5.82771919e+01 1.80066159e+02 4.76905732e+01\n",
      " 7.99470566e+01 4.51923587e+01 2.20390541e+01]\n",
      "20-th iteration, loss: 0.2955420911942776, 72 gd steps\n",
      "insert gradient: -0.0003115892736571349\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63531575e+01 4.44781862e-03 2.36347011e-05\n",
      " 0.00000000e+00 8.47032947e-21 1.05190147e+02 3.34034324e+00\n",
      " 9.98638792e+01 6.66531761e+01 1.16519956e+02 5.82766017e+01\n",
      " 1.80067946e+02 4.76903538e+01 7.99487571e+01 4.51923190e+01\n",
      " 2.20390541e+01]\n",
      "21-th iteration, loss: 0.2955419864069977, 96 gd steps\n",
      "insert gradient: -0.0002825556058860012\n",
      "21-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63530528e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05211370e+02 3.34107164e+00 9.98503351e+01 6.66530289e+01\n",
      " 1.16522298e+02 5.82759256e+01 1.80070412e+02 4.76903531e+01\n",
      " 7.99511445e+01 4.51923112e+01 2.20390541e+01]\n",
      "22-th iteration, loss: 0.2955419270049164, 63 gd steps\n",
      "insert gradient: -0.00029404456373590104\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35300367 105.21854952   3.34163366  99.84164646\n",
      "  66.6529644  116.52377695  58.27548605 180.07191713  47.69027201\n",
      "  79.95259738  45.19223083  22.03905413]\n",
      "23-th iteration, loss: 0.29554189219932203, 42 gd steps\n",
      "insert gradient: -0.0003107753561439937\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63530942e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05221121e+02 3.34234157e+00 9.98358723e+01 6.66530661e+01\n",
      " 1.16524779e+02 5.82751675e+01 1.80072851e+02 4.76901129e+01\n",
      " 7.99535106e+01 4.51921372e+01 2.20390541e+01]\n",
      "24-th iteration, loss: 0.29554183720290456, 60 gd steps\n",
      "insert gradient: -0.00030036881574818257\n",
      "24-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63531862e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05228234e+02 3.34328236e+00 9.98278923e+01 6.66532192e+01\n",
      " 1.16526108e+02 5.82747368e+01 1.80074077e+02 4.76899094e+01\n",
      " 7.99547576e+01 4.51920321e+01 2.20390541e+01]\n",
      "25-th iteration, loss: 0.2955417849136966, 58 gd steps\n",
      "insert gradient: -0.000296872569931073\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532163e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05234955e+02 3.34405118e+00 9.98201937e+01 6.66533291e+01\n",
      " 1.16527337e+02 5.82743678e+01 1.80075240e+02 4.76897924e+01\n",
      " 7.99559654e+01 4.51919582e+01 2.20390541e+01]\n",
      "26-th iteration, loss: 0.29554173482239077, 56 gd steps\n",
      "insert gradient: -0.00029490591576657583\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532262e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05241414e+02 3.34475847e+00 9.98127531e+01 6.66534301e+01\n",
      " 1.16528489e+02 5.82740270e+01 1.80076327e+02 4.76896959e+01\n",
      " 7.99571145e+01 4.51918874e+01 2.20390541e+01]\n",
      "27-th iteration, loss: 0.2955416866945056, 54 gd steps\n",
      "insert gradient: -0.00029333791965852437\n",
      "27-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35322527 105.24765203   3.34542987  99.80555123\n",
      "  66.6535313  116.52957213  58.27370779 180.07734303  47.68960764\n",
      "  79.95820621  45.19181588  22.03905413]\n",
      "28-th iteration, loss: 0.295541657533137, 38 gd steps\n",
      "insert gradient: -0.0003065271370194082\n",
      "28-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532880e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05249913e+02 3.34606121e+00 9.98005472e+01 6.66536731e+01\n",
      " 1.16530331e+02 5.82734826e+01 1.80078020e+02 4.76895102e+01\n",
      " 7.99589406e+01 4.51917498e+01 2.20390541e+01]\n",
      "29-th iteration, loss: 0.29554161189231665, 52 gd steps\n",
      "insert gradient: -0.00029641494908436914\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3533433  105.25604516   3.34686427  99.79373111\n",
      "  66.65385622 116.53132089  58.27317365 180.07889695  47.68938284\n",
      "  79.95992581  45.1916713   22.03905413]\n",
      "30-th iteration, loss: 0.2955415843684699, 36 gd steps\n",
      "insert gradient: -0.00030735474082741707\n",
      "30-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.353413   105.25823431   3.34749949  99.78896745\n",
      "  66.65402055 116.53201113  58.27297236 180.07950012  47.68930082\n",
      "  79.96060763  45.19161624  22.03905413]\n",
      "31-th iteration, loss: 0.29554155763356943, 35 gd steps\n",
      "insert gradient: -0.00031252529397227536\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35352906 105.26043638   3.34823788  99.7843554\n",
      "  66.65423466 116.53267572  58.27276308 180.08004904  47.68918328\n",
      "  79.96124537  45.19155315  22.03905413]\n",
      "32-th iteration, loss: 0.2955415314888767, 35 gd steps\n",
      "insert gradient: -0.00031535409766542864\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35366306 105.26263377   3.34901103  99.7798466\n",
      "  66.65447236 116.53331484  58.27256011 180.08056548  47.68906674\n",
      "  79.96186103  45.19149608  22.03905413]\n",
      "33-th iteration, loss: 0.29554150589439493, 34 gd steps\n",
      "insert gradient: -0.0003171489020583961\n",
      "33-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63538044e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05264816e+02 3.34979086e+00 9.97754267e+01 6.66547211e+01\n",
      " 1.16533929e+02 5.82723668e+01 1.80081057e+02 4.76889617e+01\n",
      " 7.99624609e+01 4.51914477e+01 2.20390541e+01]\n",
      "34-th iteration, loss: 0.2955414646583072, 48 gd steps\n",
      "insert gradient: -0.00030169954566687946\n",
      "34-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63538947e+01 2.92003974e-03 7.94228014e-06\n",
      " 0.00000000e+00 1.48230766e-21 1.05267738e+02 3.35061959e+00\n",
      " 9.97692544e+01 6.66549720e+01 1.16534729e+02 5.82721249e+01\n",
      " 1.80081744e+02 4.76889006e+01 7.99633158e+01 4.51914154e+01\n",
      " 2.20390541e+01]\n",
      "35-th iteration, loss: 0.2955414038143116, 63 gd steps\n",
      "insert gradient: -0.0002763084014323868\n",
      "35-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35377204 105.28144541   3.35108847  99.76086595\n",
      "  66.65504751 116.53572493  58.27187168 180.08272634  47.6890055\n",
      "  79.96451677  45.19141588  22.03905413]\n",
      "36-th iteration, loss: 0.2955413791648044, 33 gd steps\n",
      "insert gradient: -0.0002955132661000742\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63537480e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05283381e+02 3.35144078e+00 9.97564474e+01 6.66551225e+01\n",
      " 1.16536264e+02 5.82717407e+01 1.80083239e+02 4.76890313e+01\n",
      " 7.99651255e+01 4.51913853e+01 2.20390541e+01]\n",
      "37-th iteration, loss: 0.295541342063183, 45 gd steps\n",
      "insert gradient: -0.0002904764925030565\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35374848 105.28854649   3.35199351  99.75068831\n",
      "  66.65526065 116.53695336  58.27153042 180.08384562  47.6889833\n",
      "  79.96587355  45.19131783  22.03905413]\n",
      "38-th iteration, loss: 0.2955413189277482, 32 gd steps\n",
      "insert gradient: -0.00030121896766729033\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35378408 105.29046644   3.35248775  99.74652874\n",
      "  66.65540512 116.53745707  58.27138584 180.08427348  47.68893849\n",
      "  79.9664033   45.19126354  22.03905413]\n",
      "39-th iteration, loss: 0.2955412963660613, 32 gd steps\n",
      "insert gradient: -0.0003066163489523183\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63538622e+01 0.00000000e+00 3.55271368e-14\n",
      " 1.05292406e+02 3.35308312e+00 9.97424885e+01 6.66555971e+01\n",
      " 1.16537947e+02 5.82712318e+01 1.80084659e+02 4.76888569e+01\n",
      " 7.99668973e+01 4.51912010e+01 2.20390541e+01]\n",
      "40-th iteration, loss: 0.295541260985948, 43 gd steps\n",
      "insert gradient: -0.0002946970200741749\n",
      "40-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35390578 105.29752025   3.35374672  99.7370244\n",
      "  66.65580993 116.53857212  58.2710375  180.08517164  47.68879146\n",
      "  79.96757337  45.19114174  22.03905413]\n",
      "41-th iteration, loss: 0.2955412391575383, 31 gd steps\n",
      "insert gradient: -0.00030319530249062687\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63539502e+01 0.00000000e+00 4.26325641e-14\n",
      " 1.05299390e+02 3.35425484e+00 9.97330701e+01 6.66559796e+01\n",
      " 1.16539025e+02 5.82709140e+01 1.80085546e+02 4.76887623e+01\n",
      " 7.99680638e+01 4.51911027e+01 2.20390541e+01]\n",
      "42-th iteration, loss: 0.29554120520744925, 42 gd steps\n",
      "insert gradient: -0.00029322115024775963\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63539761e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05304326e+02 3.35485740e+00 9.97277806e+01 6.66561786e+01\n",
      " 1.16539603e+02 5.82707425e+01 1.80086025e+02 4.76887266e+01\n",
      " 7.99687107e+01 4.51910559e+01 2.20390541e+01]\n",
      "43-th iteration, loss: 0.2955411722545243, 41 gd steps\n",
      "insert gradient: -0.0002888652240478643\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3539565  105.3090575    3.35533494  99.72257404\n",
      "  66.65633084 116.54015127  58.27059632 180.08650496  47.68873687\n",
      "  79.96935485  45.19102205  22.03905413]\n",
      "44-th iteration, loss: 0.29554115145120075, 30 gd steps\n",
      "insert gradient: -0.0002989661011359021\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35397687 105.31084293   3.35577323  99.71875273\n",
      "  66.65647887 116.5405606   58.27048917 180.08684774  47.68872629\n",
      "  79.96981424  45.19098596  22.03905413]\n",
      "45-th iteration, loss: 0.2955411311088864, 30 gd steps\n",
      "insert gradient: -0.0003041625884080799\n",
      "45-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63540374e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.05312648e+02 3.35630458e+00 9.97150315e+01 6.66566673e+01\n",
      " 1.16540960e+02 5.82703688e+01 1.80087154e+02 4.76886758e+01\n",
      " 7.99702404e+01 4.51909380e+01 2.20390541e+01]\n",
      "46-th iteration, loss: 0.2955410994035633, 40 gd steps\n",
      "insert gradient: -0.0002928984726349559\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35406661 105.31736497   3.35689456  99.71005312\n",
      "  66.65687689 116.54146559  58.27021469 180.08755629  47.6886387\n",
      "  79.97081458  45.19089116  22.03905413]\n",
      "47-th iteration, loss: 0.29554107961943904, 29 gd steps\n",
      "insert gradient: -0.00030075842501889793\n",
      "47-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35409891 105.31911277   3.35735145  99.7063962\n",
      "  66.65704378 116.54183843  58.27011677 180.08785672  47.68862749\n",
      "  79.97123769  45.19085957  22.03905413]\n",
      "48-th iteration, loss: 0.295541060191884, 29 gd steps\n",
      "insert gradient: -0.00030492422050669443\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3541617  105.3208753    3.35787743  99.70281767\n",
      "  66.65724051 116.54220267  58.27000887 180.08812921  47.68858685\n",
      "  79.97163501  45.19081925  22.03905413]\n",
      "49-th iteration, loss: 0.2955410410526909, 28 gd steps\n",
      "insert gradient: -0.00030733140606368966\n",
      "49-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35423848 105.32263998   3.35843307  99.69929704\n",
      "  66.65745309 116.54255695  58.26990065 180.08838492  47.6885394\n",
      "  79.97201863  45.19077951  22.03905413]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5364665204182852\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.80232488   0.         884.84591633]\n",
      "1-th iteration, loss: 0.7498498981810913, 11 gd steps\n",
      "insert gradient: -0.6341293753643171\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.18401986  62.29250037 237.39768487   0.         647.44823146]\n",
      "2-th iteration, loss: 0.605030487631589, 13 gd steps\n",
      "insert gradient: -0.6831801842439986\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.07183426  77.02886188 222.09171429  40.84729674 237.83812584\n",
      "   0.         409.61010562]\n",
      "3-th iteration, loss: 0.47108647062245856, 19 gd steps\n",
      "insert gradient: -0.7362093262722418\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.61169508 225.60297918  57.50168636 163.41189533\n",
      "  47.12467507 374.50066799   0.          35.10943762]\n",
      "4-th iteration, loss: 0.3805374092409368, 13 gd steps\n",
      "insert gradient: -0.3933043536427769\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          63.37034187 225.04471606  59.39619166 149.60983497\n",
      "  49.83274757 222.7882043    0.         111.39410215  49.78118097\n",
      "  35.10943762]\n",
      "5-th iteration, loss: 0.3020543932246289, 20 gd steps\n",
      "insert gradient: -0.09631657393249264\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[2.68279657e+00 7.36594211e+01 2.06842814e+02 7.08577296e+01\n",
      " 1.07083282e+02 6.01659911e+01 1.87549877e+02 0.00000000e+00\n",
      " 8.52651283e-14 3.99889202e+01 7.30024322e+01 4.81089361e+01\n",
      " 3.51094376e+01]\n",
      "6-th iteration, loss: 0.2961292699571588, 73 gd steps\n",
      "insert gradient: -0.057260471855894786\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  0.61877635  75.87070827 105.61654563   0.         105.61654563\n",
      "  67.4304729  114.06513542  57.89492127 181.12828742  46.82435071\n",
      "  78.23722056  45.93127885  35.10943762]\n",
      "7-th iteration, loss: 0.29562043315980324, 18 gd steps\n",
      "insert gradient: -0.0076009753655646205\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.58660625e+01 1.04004284e+02 2.46538915e+00\n",
      " 1.03316045e+02 6.64637438e+01 1.16284683e+02 5.85518428e+01\n",
      " 1.79314874e+02 0.00000000e+00 2.48689958e-14 4.72008624e+01\n",
      " 7.98506165e+01 4.54460374e+01 3.51094376e+01]\n",
      "8-th iteration, loss: 0.29557367092777326, 18 gd steps\n",
      "insert gradient: -0.004963880092817772\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.62366322e+01 1.03886006e+02 0.00000000e+00\n",
      " 1.59872116e-14 2.86395431e+00 1.02279119e+02 6.65532635e+01\n",
      " 1.16248970e+02 5.81712316e+01 1.79845244e+02 1.31133099e-01\n",
      " 1.56960478e-01 4.76028083e+01 7.96990732e+01 4.50905010e+01\n",
      " 3.51094376e+01]\n",
      "9-th iteration, loss: 0.2955614174616655, 13 gd steps\n",
      "insert gradient: -0.001932618223352608\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63212550e+01 1.03935078e+02 9.58498104e-02\n",
      " 3.93692165e-02 2.96157797e+00 1.01872680e+02 6.65243862e+01\n",
      " 1.16395108e+02 5.82900855e+01 1.79962457e+02 1.39042688e-02\n",
      " 9.00318782e-02 4.75978071e+01 7.98319331e+01 4.51838895e+01\n",
      " 3.51094376e+01]\n",
      "10-th iteration, loss: 0.2955521811372716, 636 gd steps\n",
      "insert gradient: -0.000434035796886964\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63729582e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04162737e+02 1.43278161e-01 1.72374702e-01 3.07620249e+00\n",
      " 1.01085294e+02 6.65635455e+01 1.16540980e+02 5.82648403e+01\n",
      " 1.80123717e+02 4.76579082e+01 7.99509929e+01 4.51838188e+01\n",
      " 3.51094376e+01]\n",
      "11-th iteration, loss: 0.29554761154530934, 689 gd steps\n",
      "insert gradient: -0.00032055210745193623\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63606724e+01 1.04482154e+02 1.01279446e-01\n",
      " 2.63633591e-01 3.14961817e+00 1.00609095e+02 6.65833559e+01\n",
      " 1.16557449e+02 5.82635167e+01 1.80122203e+02 4.76663440e+01\n",
      " 7.99735127e+01 4.51815596e+01 3.51094376e+01]\n",
      "12-th iteration, loss: 0.2955462691172362, 687 gd steps\n",
      "insert gradient: -0.0003480015795472855\n",
      "12-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63601141e+01 0.00000000e+00 3.19744231e-14\n",
      " 1.04541516e+02 8.28151213e-02 3.03265929e-01 3.18884892e+00\n",
      " 1.00444857e+02 6.65958543e+01 1.16559539e+02 5.82633272e+01\n",
      " 1.80119409e+02 4.76684210e+01 7.99781390e+01 4.51820617e+01\n",
      " 3.51094376e+01]\n",
      "13-th iteration, loss: 0.295545109693043, 556 gd steps\n",
      "insert gradient: -0.00029330225466350643\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63558150e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04634447e+02 5.76055062e-02 3.36847022e-01 3.21968098e+00\n",
      " 1.00309489e+02 6.66024506e+01 1.16560044e+02 5.82633979e+01\n",
      " 1.80118333e+02 4.76711236e+01 7.99819117e+01 4.51824437e+01\n",
      " 3.51094376e+01]\n",
      "14-th iteration, loss: 0.29554449019771584, 394 gd steps\n",
      "insert gradient: -0.00027583939602609034\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63527116e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04683095e+02 4.20707508e-02 3.55953331e-01 3.23883281e+00\n",
      " 1.00234521e+02 6.66060521e+01 1.16560489e+02 5.82635253e+01\n",
      " 1.80117680e+02 4.76722107e+01 7.99838723e+01 4.51825423e+01\n",
      " 3.51094376e+01]\n",
      "15-th iteration, loss: 0.29554405028218217, 308 gd steps\n",
      "insert gradient: -0.0002637167527085539\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63504732e+01 1.04717289e+02 2.99874699e-02\n",
      " 3.70259468e-01 3.25365734e+00 1.00180327e+02 6.66087245e+01\n",
      " 1.16560823e+02 5.82636272e+01 1.80117153e+02 4.76729745e+01\n",
      " 7.99852790e+01 4.51826358e+01 3.51094376e+01]\n",
      "16-th iteration, loss: 0.29554382923111105, 182 gd steps\n",
      "insert gradient: -0.0002777617609503838\n",
      "16-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63497598e+01 1.04726984e+02 2.39087117e-02\n",
      " 3.78796731e-01 3.26300506e+00 1.00151160e+02 6.66107167e+01\n",
      " 1.16561063e+02 5.82636552e+01 1.80116678e+02 4.76731869e+01\n",
      " 7.99859082e+01 4.51826594e+01 3.51094376e+01]\n",
      "17-th iteration, loss: 0.2955436272978505, 169 gd steps\n",
      "insert gradient: -0.0002786159393790651\n",
      "17-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63493568e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.04736094e+02 1.81018003e-02 3.87065553e-01 3.27184909e+00\n",
      " 1.00124609e+02 6.66127207e+01 1.16561232e+02 5.82636991e+01\n",
      " 1.80116181e+02 4.76734891e+01 7.99864962e+01 4.51827780e+01\n",
      " 3.51094376e+01]\n",
      "18-th iteration, loss: 0.2955433324334386, 226 gd steps\n",
      "insert gradient: -0.0002552302415944117\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63482345e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.04759692e+02 8.54428946e-03 3.98123874e-01 3.28295123e+00\n",
      " 1.00088120e+02 6.66147265e+01 1.16561292e+02 5.82637616e+01\n",
      " 1.80115735e+02 4.76742533e+01 7.99874689e+01 4.51830134e+01\n",
      " 3.51094376e+01]\n",
      "19-th iteration, loss: 0.2955430808292688, 184 gd steps\n",
      "insert gradient: -0.0002472165762747389\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63469996e+01 1.05187150e+02 0.00000000e+00\n",
      " 3.55271368e-14 3.29280578e+00 1.00056628e+02 6.66162586e+01\n",
      " 1.16561355e+02 5.82637916e+01 1.80115412e+02 4.76748080e+01\n",
      " 7.99882687e+01 4.51831102e+01 3.51094376e+01]\n",
      "20-th iteration, loss: 0.2955429682815254, 99 gd steps\n",
      "insert gradient: -0.00020754778029909588\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63460798e+01 1.05191454e+02 2.68048493e-03\n",
      " 4.27176505e-03 3.29553205e+00 1.00039939e+02 6.66168109e+01\n",
      " 1.16561547e+02 5.82641612e+01 1.80115476e+02 4.76755210e+01\n",
      " 7.99888598e+01 4.51832172e+01 3.51094376e+01]\n",
      "21-th iteration, loss: 0.2955428724939761, 91 gd steps\n",
      "insert gradient: -0.00021036107129608205\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63456268e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05195167e+02 4.15057833e-03 7.91679478e-03 3.29712121e+00\n",
      " 1.00025185e+02 6.66174917e+01 1.16561916e+02 5.82645038e+01\n",
      " 1.80115500e+02 4.76760371e+01 7.99893070e+01 4.51831780e+01\n",
      " 3.51094376e+01]\n",
      "22-th iteration, loss: 0.2955427594730731, 104 gd steps\n",
      "insert gradient: -0.00020122393792729832\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63452450e+01 1.05203434e+02 5.18213637e-03\n",
      " 1.19433002e-02 3.29837666e+00 1.00008348e+02 6.66182704e+01\n",
      " 1.16562368e+02 5.82647831e+01 1.80115471e+02 4.76765544e+01\n",
      " 7.99898023e+01 4.51831159e+01 3.51094376e+01]\n",
      "23-th iteration, loss: 0.29554267749745033, 81 gd steps\n",
      "insert gradient: -0.00021202206922845876\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63451312e+01 1.05206736e+02 6.05353574e-03\n",
      " 1.51463755e-02 3.29948574e+00 9.99954390e+01 6.66190109e+01\n",
      " 1.16562740e+02 5.82649086e+01 1.80115376e+02 4.76768336e+01\n",
      " 7.99901366e+01 4.51830443e+01 3.51094376e+01]\n",
      "24-th iteration, loss: 0.2955426003876867, 77 gd steps\n",
      "insert gradient: -0.00021499144958593433\n",
      "24-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63451349e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05209967e+02 6.89277833e-03 1.82679474e-02 3.30060537e+00\n",
      " 9.99832614e+01 6.66198260e+01 1.16563089e+02 5.82649857e+01\n",
      " 1.80115227e+02 4.76770568e+01 7.99904366e+01 4.51829977e+01\n",
      " 3.51094376e+01]\n",
      "25-th iteration, loss: 0.2955425066598961, 90 gd steps\n",
      "insert gradient: -0.0002051711975141483\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63450534e+01 1.05217278e+02 7.47912583e-03\n",
      " 2.17815271e-02 3.30158322e+00 9.99690888e+01 6.66206762e+01\n",
      " 1.16563444e+02 5.82650608e+01 1.80115076e+02 4.76774060e+01\n",
      " 7.99908388e+01 4.51830078e+01 3.51094376e+01]\n",
      "26-th iteration, loss: 0.2955424380029718, 71 gd steps\n",
      "insert gradient: -0.00021483560603985502\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63450602e+01 1.05220211e+02 8.04400483e-03\n",
      " 2.45946010e-02 3.30250423e+00 9.99580798e+01 6.66214059e+01\n",
      " 1.16563720e+02 5.82650850e+01 1.80114935e+02 4.76776286e+01\n",
      " 7.99911290e+01 4.51830053e+01 3.51094376e+01]\n",
      "27-th iteration, loss: 0.2955423726468191, 68 gd steps\n",
      "insert gradient: -0.00021721873663354098\n",
      "27-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63451244e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.05223098e+02 8.63124402e-03 2.73568536e-02 3.30347606e+00\n",
      " 9.99475848e+01 6.66221760e+01 1.16563974e+02 5.82650904e+01\n",
      " 1.80114766e+02 4.76778116e+01 7.99913893e+01 4.51830102e+01\n",
      " 3.51094376e+01]\n",
      "28-th iteration, loss: 0.29554229231754103, 80 gd steps\n",
      "insert gradient: -0.0002072033956689064\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63450944e+01 0.00000000e+00 3.19744231e-14\n",
      " 1.05229659e+02 9.00502142e-03 3.04831439e-02 3.30435087e+00\n",
      " 9.99352882e+01 6.66229740e+01 1.16564228e+02 5.82651050e+01\n",
      " 1.80114600e+02 4.76781081e+01 7.99917365e+01 4.51830604e+01\n",
      " 3.51094376e+01]\n",
      "29-th iteration, loss: 0.2955422165820008, 76 gd steps\n",
      "insert gradient: -0.0002052031240408257\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63450063e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05235807e+02 9.23886528e-03 3.34051734e-02 3.30511269e+00\n",
      " 9.99235766e+01 6.66236697e+01 1.16564447e+02 5.82651087e+01\n",
      " 1.80114466e+02 4.76784051e+01 7.99920769e+01 4.51831099e+01\n",
      " 3.51094376e+01]\n",
      "30-th iteration, loss: 0.29554214467339396, 73 gd steps\n",
      "insert gradient: -0.00020381724823870378\n",
      "30-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63448976e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05241671e+02 9.43257852e-03 3.61877040e-02 3.30585853e+00\n",
      " 9.99123854e+01 6.66243223e+01 1.16564644e+02 5.82650994e+01\n",
      " 1.80114337e+02 4.76786684e+01 7.99923932e+01 4.51831487e+01\n",
      " 3.51094376e+01]\n",
      "31-th iteration, loss: 0.29554207615433037, 71 gd steps\n",
      "insert gradient: -0.00020260959247628282\n",
      "31-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63447746e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.05247289e+02 9.59515998e-03 3.88498143e-02 3.30659438e+00\n",
      " 9.99016599e+01 6.66249466e+01 1.16564824e+02 5.82650861e+01\n",
      " 1.80114214e+02 4.76789070e+01 7.99926900e+01 4.51831820e+01\n",
      " 3.51094376e+01]\n",
      "32-th iteration, loss: 0.29554201070582836, 68 gd steps\n",
      "insert gradient: -0.0002015039166157539\n",
      "32-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63446417e+01 1.05252684e+02 9.73029547e-03\n",
      " 4.14037262e-02 3.30732110e+00 9.98913590e+01 6.66255484e+01\n",
      " 1.16564990e+02 5.82650723e+01 1.80114094e+02 4.76791267e+01\n",
      " 7.99929705e+01 4.51832126e+01 3.51094376e+01]\n",
      "33-th iteration, loss: 0.2955419607468768, 56 gd steps\n",
      "insert gradient: -0.00021043001810471974\n",
      "33-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63445972e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.05254955e+02 9.98238084e-03 4.35543517e-02 3.30808561e+00\n",
      " 9.98830330e+01 6.66261150e+01 1.16565133e+02 5.82650546e+01\n",
      " 1.80113975e+02 4.76792597e+01 7.99931723e+01 4.51832232e+01\n",
      " 3.51094376e+01]\n",
      "34-th iteration, loss: 0.29554189983849816, 64 gd steps\n",
      "insert gradient: -0.00020246644622898483\n",
      "34-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63445083e+01 1.05260142e+02 1.01442268e-02\n",
      " 4.60060231e-02 3.30887246e+00 9.98734145e+01 6.66267452e+01\n",
      " 1.16565279e+02 5.82650399e+01 1.80113836e+02 4.76794374e+01\n",
      " 7.99934165e+01 4.51832556e+01 3.51094376e+01]\n",
      "35-th iteration, loss: 0.2955418532640853, 53 gd steps\n",
      "insert gradient: -0.00021050136674309806\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63444717e+01 1.05262304e+02 1.03474655e-02\n",
      " 4.80496664e-02 3.30961424e+00 9.98655905e+01 6.66272989e+01\n",
      " 1.16565402e+02 5.82650306e+01 1.80113720e+02 4.76795709e+01\n",
      " 7.99936071e+01 4.51832780e+01 3.51094376e+01]\n",
      "36-th iteration, loss: 0.29554180812684905, 52 gd steps\n",
      "insert gradient: -0.00021267825968345\n",
      "36-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63444758e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05264459e+02 1.05980540e-02 5.00867456e-02 3.31041435e+00\n",
      " 9.98580094e+01 6.66278838e+01 1.16565520e+02 5.82650138e+01\n",
      " 1.80113585e+02 4.76796754e+01 7.99937766e+01 4.51832975e+01\n",
      " 3.51094376e+01]\n",
      "37-th iteration, loss: 0.2955417527561651, 60 gd steps\n",
      "insert gradient: -0.0002037128132584577\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63444137e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05269332e+02 1.06938523e-02 5.23834128e-02 3.31117539e+00\n",
      " 9.98491971e+01 6.66284953e+01 1.16565632e+02 5.82650075e+01\n",
      " 1.80113448e+02 4.76798520e+01 7.99940004e+01 4.51833467e+01\n",
      " 3.51094376e+01]\n",
      "38-th iteration, loss: 0.2955416995256612, 58 gd steps\n",
      "insert gradient: -0.00020158234954512663\n",
      "38-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63443100e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05273970e+02 1.06906220e-02 5.45664542e-02 3.31184562e+00\n",
      " 9.98406474e+01 6.66290368e+01 1.16565729e+02 5.82650064e+01\n",
      " 1.80113340e+02 4.76800502e+01 7.99942311e+01 4.51833999e+01\n",
      " 3.51094376e+01]\n",
      "39-th iteration, loss: 0.2955416480922608, 57 gd steps\n",
      "insert gradient: -0.0002002589905841098\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63441937e+01 1.05278458e+02 1.06659634e-02\n",
      " 5.66781687e-02 3.31250344e+00 9.98323430e+01 6.66295466e+01\n",
      " 1.16565816e+02 5.82649986e+01 1.80113236e+02 4.76802329e+01\n",
      " 7.99944505e+01 4.51834445e+01 3.51094376e+01]\n",
      "40-th iteration, loss: 0.29554160811899405, 47 gd steps\n",
      "insert gradient: -0.0002082957263999071\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63441448e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.05280377e+02 1.07591743e-02 5.84864873e-02 3.31318609e+00\n",
      " 9.98254898e+01 6.66300228e+01 1.16565897e+02 5.82649852e+01\n",
      " 1.80113135e+02 4.76803466e+01 7.99946109e+01 4.51834663e+01\n",
      " 3.51094376e+01]\n",
      "41-th iteration, loss: 0.2955415594160548, 54 gd steps\n",
      "insert gradient: -0.00020099822413984766\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440606e+01 1.05284746e+02 1.07775114e-02\n",
      " 6.05426636e-02 3.31390571e+00 9.98176095e+01 6.66305498e+01\n",
      " 1.16565976e+02 5.82649699e+01 1.80113015e+02 4.76804862e+01\n",
      " 7.99947994e+01 4.51835009e+01 3.51094376e+01]\n",
      "42-th iteration, loss: 0.29554152153726254, 45 gd steps\n",
      "insert gradient: -0.00020813874489576344\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440160e+01 1.05286596e+02 1.08467883e-02\n",
      " 6.22845248e-02 3.31457974e+00 9.98110734e+01 6.66310168e+01\n",
      " 1.16566048e+02 5.82649627e+01 1.80112916e+02 4.76805971e+01\n",
      " 7.99949511e+01 4.51835266e+01 3.51094376e+01]\n",
      "43-th iteration, loss: 0.29554148457415896, 45 gd steps\n",
      "insert gradient: -0.0002102154558900777\n",
      "43-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440044e+01 1.05288448e+02 1.09640070e-02\n",
      " 6.40296820e-02 3.31530856e+00 9.98047003e+01 6.66315107e+01\n",
      " 1.16566117e+02 5.82649481e+01 1.80112800e+02 4.76806810e+01\n",
      " 7.99950849e+01 4.51835471e+01 3.51094376e+01]\n",
      "44-th iteration, loss: 0.29554144845764946, 44 gd steps\n",
      "insert gradient: -0.00021128454312232183\n",
      "44-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63440048e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.05290282e+02 1.10759202e-02 6.57583528e-02 3.31603875e+00\n",
      " 9.97984633e+01 6.66320098e+01 1.16566182e+02 5.82649374e+01\n",
      " 1.80112683e+02 4.76807668e+01 7.99952155e+01 4.51835726e+01\n",
      " 3.51094376e+01]\n",
      "45-th iteration, loss: 0.29554140416338176, 51 gd steps\n",
      "insert gradient: -0.00020262619002846117\n",
      "45-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439479e+01 1.05294407e+02 1.10464462e-02\n",
      " 6.76976500e-02 3.31674256e+00 9.97912433e+01 6.66325261e+01\n",
      " 1.16566239e+02 5.82649340e+01 1.80112564e+02 4.76809090e+01\n",
      " 7.99953880e+01 4.51836213e+01 3.51094376e+01]\n",
      "46-th iteration, loss: 0.2955413696928012, 42 gd steps\n",
      "insert gradient: -0.00020908714625392827\n",
      "46-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439167e+01 1.05296150e+02 1.10587973e-02\n",
      " 6.93379346e-02 3.31738602e+00 9.97852285e+01 6.66329695e+01\n",
      " 1.16566290e+02 5.82649334e+01 1.80112470e+02 4.76810252e+01\n",
      " 7.99955290e+01 4.51836573e+01 3.51094376e+01]\n",
      "47-th iteration, loss: 0.2955413359634496, 42 gd steps\n",
      "insert gradient: -0.00021095515670304198\n",
      "47-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439129e+01 1.05297896e+02 1.11169515e-02\n",
      " 7.09824157e-02 3.31808104e+00 9.97793481e+01 6.66334324e+01\n",
      " 1.16566339e+02 5.82649222e+01 1.80112361e+02 4.76811125e+01\n",
      " 7.99956521e+01 4.51836850e+01 3.51094376e+01]\n",
      "48-th iteration, loss: 0.2955413029255095, 41 gd steps\n",
      "insert gradient: -0.0002118504817316265\n",
      "48-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439180e+01 1.05299628e+02 1.11731305e-02\n",
      " 7.26134350e-02 3.31877963e+00 9.97735802e+01 6.66338980e+01\n",
      " 1.16566385e+02 5.82649126e+01 1.80112251e+02 4.76811983e+01\n",
      " 7.99957713e+01 4.51837147e+01 3.51094376e+01]\n",
      "49-th iteration, loss: 0.29554127053216095, 41 gd steps\n",
      "insert gradient: -0.00021248487653877085\n",
      "49-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63439276e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05301343e+02 1.12186207e-02 7.42285041e-02 3.31947322e+00\n",
      " 9.97679139e+01 6.66343610e+01 1.16566427e+02 5.82649051e+01\n",
      " 1.80112140e+02 4.76812862e+01 7.99958885e+01 4.51837471e+01\n",
      " 3.51094376e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.532378848404032\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 46.01319852   0.         908.76067082]\n",
      "1-th iteration, loss: 0.7519772466020608, 11 gd steps\n",
      "insert gradient: -0.648075865769649\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 44.05862423  62.16864421 121.90691926   0.         786.85375157]\n",
      "2-th iteration, loss: 0.509119037923581, 49 gd steps\n",
      "insert gradient: -0.6566803892666219\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.46710872  53.46604916  99.19547274  99.46009194 224.81535759\n",
      "   0.         562.03839398]\n",
      "3-th iteration, loss: 0.4357263470496145, 13 gd steps\n",
      "insert gradient: -0.5745348413125594\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[6.13030512e-02 6.65198261e+01 9.40230769e+01 9.23204106e+01\n",
      " 1.11309507e+02 0.00000000e+00 9.37343219e+01 5.00676913e+01\n",
      " 5.62038394e+02]\n",
      "4-th iteration, loss: 0.33997510990533286, 146 gd steps\n",
      "insert gradient: -0.3225109975132139\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[3.85517615e-01 8.56428906e+01 1.23427334e+02 7.88137866e+01\n",
      " 9.88066834e+01 4.48725766e+01 6.83417660e+01 4.77158866e+01\n",
      " 5.41222157e+02 0.00000000e+00 2.08162368e+01]\n",
      "5-th iteration, loss: 0.29281158670410534, 53 gd steps\n",
      "insert gradient: -0.10382560485710843\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.76985609  69.62385486 137.40869471  84.32270537 100.77443407\n",
      "  46.6315234   66.98117451  42.06431547 177.36634632   0.\n",
      " 310.39110605  58.69456809  20.81623681]\n",
      "6-th iteration, loss: 0.2287663694856964, 96 gd steps\n",
      "insert gradient: -0.05932295652486771\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.76605107  64.08058261 135.86834163  81.47620652 109.96584624\n",
      "  50.82870415  72.17044591  34.83596738  84.69416716  90.67370001\n",
      " 116.02735768   0.         116.02735768  61.68650946  20.81623681]\n",
      "7-th iteration, loss: 0.22345544121944608, 19 gd steps\n",
      "insert gradient: -0.023747538480940952\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[3.54218087e+00 6.45291270e+01 1.34832685e+02 8.15379423e+01\n",
      " 1.12136721e+02 0.00000000e+00 1.59872116e-14 5.30095862e+01\n",
      " 6.91878908e+01 3.39329204e+01 8.99534310e+01 9.27603730e+01\n",
      " 9.82427300e+01 1.53806590e+01 9.07416355e+01 6.41160260e+01\n",
      " 2.08162368e+01]\n",
      "8-th iteration, loss: 0.21711305029066127, 37 gd steps\n",
      "insert gradient: -0.045524501898154934\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  2.20771537  62.44714577 127.89998744  88.48987962 115.38085789\n",
      "  60.05570021  74.60964683  32.55769478  84.25194549  96.00857799\n",
      "  97.33841149  25.87612305  70.07844535  67.40791747  20.81623681]\n",
      "9-th iteration, loss: 0.20871990094694115, 37 gd steps\n",
      "insert gradient: -0.04153240493320586\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  2.09058409  65.29917251 120.0730366   96.13631275 114.26583525\n",
      "  62.73930342  86.56281819  31.78970875  80.27813895  63.18705932\n",
      "   0.          28.7213906  111.35817723  39.06511697  56.87487454\n",
      "  53.23048451  20.81623681]\n",
      "10-th iteration, loss: 0.2075391101716561, 14 gd steps\n",
      "insert gradient: -0.05026670156057329\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[1.69170077e+00 6.56635860e+01 1.20673806e+02 9.67994602e+01\n",
      " 1.14425985e+02 6.32356202e+01 8.82645222e+01 3.30817012e+01\n",
      " 7.69799936e+01 0.00000000e+00 1.24344979e-14 6.16958147e+01\n",
      " 4.24548084e+00 2.66729785e+01 1.12548703e+02 4.24980421e+01\n",
      " 5.46112208e+01 5.15945427e+01 2.08162368e+01]\n",
      "11-th iteration, loss: 0.20559941987418687, 14 gd steps\n",
      "insert gradient: -0.05229779555490137\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[  1.99455791  66.1947935  121.3350015   97.22905861 115.18847274\n",
      "  63.56543589  89.48001988  34.2340126   77.62656757  62.2678747\n",
      "   9.93842658  21.05452732 114.58971429  44.61407091  52.45487996\n",
      "  51.5703828   20.81623681]\n",
      "12-th iteration, loss: 0.18921854424132864, 62 gd steps\n",
      "insert gradient: -0.005434665316047452\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[  2.47169283  71.38516533 128.7821308   96.27768244 116.41883529\n",
      "  66.23497993  97.29131082  40.25026572  77.02762513  50.66461515\n",
      " 186.69367315  51.44310513  48.91253856  47.04490822  20.81623681]\n",
      "13-th iteration, loss: 0.18869222573378236, 251 gd steps\n",
      "insert gradient: -0.0013743373227520406\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[  2.53889121  69.88314107 133.35299874  96.94975411 115.36977581\n",
      "  66.33356724 100.0070211   40.7506943   77.06084685  49.74892913\n",
      "  93.87575267   0.          93.87575267  50.11145231  59.93929493\n",
      "  40.99087594  20.81623681]\n",
      "14-th iteration, loss: 0.18868816751566805, 153 gd steps\n",
      "insert gradient: -1.6241565914552696e-05\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[2.54632852e+00 6.98947636e+01 1.33610472e+02 9.69112410e+01\n",
      " 1.15383717e+02 6.63749159e+01 1.00096723e+02 4.07780670e+01\n",
      " 7.71175115e+01 4.98521536e+01 0.00000000e+00 1.33226763e-14\n",
      " 9.45044850e+01 4.51228991e-01 9.18165256e+01 5.02695376e+01\n",
      " 6.00784325e+01 4.09256284e+01 2.08162368e+01]\n",
      "15-th iteration, loss: 0.18868811077116085, 54 gd steps\n",
      "insert gradient: -2.1255487107328786e-05\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[  2.5441898   69.89332193 133.61470166  96.91156397 115.38606887\n",
      "  66.37739422 100.09745783  40.77771997  77.122838    49.83834637\n",
      "  94.57982      0.44897888  91.75846863  50.27605407  60.08167127\n",
      "  40.92191804  20.81623681]\n",
      "16-th iteration, loss: 0.18868790400222357, 16 gd steps\n",
      "insert gradient: -0.0002698629671659858\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[2.52573777e+00 0.00000000e+00 7.21644966e-16 6.98661571e+01\n",
      " 1.33616682e+02 9.69084263e+01 1.15413319e+02 6.64087827e+01\n",
      " 1.00072804e+02 4.07607469e+01 7.71773785e+01 4.98153326e+01\n",
      " 9.51356639e+01 4.68594789e-01 9.11163678e+01 5.03312586e+01\n",
      " 6.00179532e+01 4.09604400e+01 2.08162368e+01]\n",
      "17-th iteration, loss: 0.1886876531225761, 274 gd steps\n",
      "insert gradient: -2.067402555736575e-05\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[2.54503591e+00 6.98950585e+01 1.33630803e+02 9.69053981e+01\n",
      " 1.15387781e+02 6.63800251e+01 1.00102663e+02 4.07770828e+01\n",
      " 7.71297068e+01 4.98455857e+01 0.00000000e+00 1.15463195e-14\n",
      " 9.52134497e+01 4.86487547e-01 9.09996641e+01 5.02955580e+01\n",
      " 6.00754337e+01 4.09225701e+01 2.08162368e+01]\n",
      "18-th iteration, loss: 0.1886876314362251, 46 gd steps\n",
      "insert gradient: -2.143817125006503e-05\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54535865  69.89483218 133.6337581   96.90513522 115.387657\n",
      "  66.37990158 100.10393897  40.77690989  77.12969297  49.84295414\n",
      "  95.26865955   0.48373807  90.95951735  50.29233242  60.08251837\n",
      "  40.91754608  20.81623681]\n",
      "19-th iteration, loss: 0.18868744111675576, 190 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.389968480600635e-06\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[  2.5464745   69.89468498 133.64727864  96.90131943 115.38785989\n",
      "  66.3828346  100.10973312  40.77783355  77.13564798  49.84136985\n",
      "  95.90661202   0.50690411  90.2485133   50.30520575  60.09153688\n",
      "  40.91003644  20.81623681]\n",
      "20-th iteration, loss: 0.18868744111490082, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.732217769641752e-06\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54647461  69.89468575 133.64727938  96.90132082 115.38785934\n",
      "  66.38283052 100.10973292  40.77783749  77.13565118  49.84137604\n",
      "  95.90661684   0.50690126  90.24850925  50.30521069  60.09153993\n",
      "  40.9100398   20.81623681]\n",
      "21-th iteration, loss: 0.18868744111322008, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.164711398531601e-06\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[2.54647471e+00 6.98946865e+01 1.33647280e+02 9.69013222e+01\n",
      " 1.15387859e+02 6.63828265e+01 1.00109733e+02 4.07778410e+01\n",
      " 7.71356541e+01 0.00000000e+00 8.88178420e-15 4.98413816e+01\n",
      " 9.59066215e+01 5.06898176e-01 9.02485050e+01 5.03052152e+01\n",
      " 6.00915428e+01 4.09100430e+01 2.08162368e+01]\n",
      "22-th iteration, loss: 0.18868744111138241, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3492943271215275e-06\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647479e+00 6.98946871e+01 1.33647281e+02 9.69013235e+01\n",
      " 1.15387858e+02 6.63828224e+01 1.00109732e+02 4.07778441e+01\n",
      " 7.71356567e+01 4.92890127e-06 2.60341118e-06 0.00000000e+00\n",
      " 3.70576914e-22 4.98413865e+01 9.59066259e+01 5.06894809e-01\n",
      " 9.02485006e+01 5.03052194e+01 6.00915456e+01 4.09100461e+01\n",
      " 2.08162368e+01]\n",
      "23-th iteration, loss: 0.18868744110959493, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.965183899186987e-06\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647487e+00 6.98946876e+01 1.33647282e+02 9.69013248e+01\n",
      " 1.15387857e+02 6.63828183e+01 1.00109732e+02 4.07778465e+01\n",
      " 7.71356589e+01 9.00039763e-06 4.77768969e-06 4.08831152e-06\n",
      " 2.17427851e-06 4.98413906e+01 9.59066300e+01 5.06891093e-01\n",
      " 9.02484959e+01 5.03052230e+01 6.00915482e+01 4.09100490e+01\n",
      " 2.08162368e+01]\n",
      "24-th iteration, loss: 0.18868744110809213, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7688868031882896e-06\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647493e+00 6.98946881e+01 1.33647282e+02 9.69013260e+01\n",
      " 1.15387856e+02 6.63828141e+01 1.00109731e+02 4.07778485e+01\n",
      " 7.71356607e+01 1.22031731e-05 6.53142999e-06 7.32194572e-06\n",
      " 3.90161276e-06 4.98413939e+01 9.59066339e+01 5.06887113e-01\n",
      " 9.02484909e+01 5.03052262e+01 6.00915507e+01 4.09100517e+01\n",
      " 2.08162368e+01]\n",
      "25-th iteration, loss: 0.18868744110676372, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6394867562672904e-06\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647497e+00 6.98946884e+01 1.33647283e+02 9.69013271e+01\n",
      " 1.15387855e+02 6.63828100e+01 1.00109730e+02 4.07778499e+01\n",
      " 7.71356622e+01 1.47561585e-05 7.96459673e-06 9.91711711e-06\n",
      " 5.28748759e-06 4.98413965e+01 9.59066377e+01 5.06883004e-01\n",
      " 9.02484858e+01 5.03052290e+01 6.00915531e+01 4.09100543e+01\n",
      " 2.08162368e+01]\n",
      "26-th iteration, loss: 0.18868744110554805, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5578753534758213e-06\n",
      "26-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647500e+00 6.98946887e+01 1.33647284e+02 9.69013282e+01\n",
      " 1.15387854e+02 6.63828058e+01 1.00109729e+02 4.07778511e+01\n",
      " 7.71356635e+01 1.68209994e-05 9.15157844e-06 1.20334008e-05\n",
      " 6.41041527e-06 4.98413986e+01 9.59066413e+01 5.06878860e-01\n",
      " 9.02484806e+01 5.03052316e+01 6.00915553e+01 4.09100568e+01\n",
      " 2.08162368e+01]\n",
      "27-th iteration, loss: 0.1886874411044099, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.510177616196315e-06\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647502e+00 6.98946889e+01 1.33647285e+02 9.69013292e+01\n",
      " 1.15387853e+02 6.63828016e+01 1.00109728e+02 4.07778520e+01\n",
      " 7.71356646e+01 1.85172374e-05 1.01479061e-05 1.37887483e-05\n",
      " 7.32901990e-06 4.98414004e+01 9.59066449e+01 5.06874744e-01\n",
      " 9.02484753e+01 5.03052339e+01 6.00915575e+01 4.09100593e+01\n",
      " 2.08162368e+01]\n",
      "28-th iteration, loss: 0.18868744110332872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4863395570462414e-06\n",
      "28-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647502e+00 6.98946890e+01 1.33647286e+02 9.69013302e+01\n",
      " 1.15387852e+02 6.63827974e+01 1.00109727e+02 4.07778527e+01\n",
      " 7.71356656e+01 1.99334479e-05 1.09951888e-05 1.52705037e-05\n",
      " 8.08724172e-06 4.98414020e+01 9.59066484e+01 5.06870699e-01\n",
      " 9.02484700e+01 5.03052361e+01 6.00915596e+01 4.09100616e+01\n",
      " 2.08162368e+01]\n",
      "29-th iteration, loss: 0.188687441102292, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.479094734785635e-06\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647501e+00 6.98946891e+01 1.33647286e+02 9.69013312e+01\n",
      " 1.15387851e+02 6.63827932e+01 1.00109726e+02 4.07778532e+01\n",
      " 7.71356664e+01 2.11354133e-05 1.17247408e-05 1.65434865e-05\n",
      " 8.71816220e-06 4.98414033e+01 9.59066519e+01 5.06866751e-01\n",
      " 9.02484647e+01 5.03052382e+01 6.00915616e+01 4.09100638e+01\n",
      " 2.08162368e+01]\n",
      "30-th iteration, loss: 0.18868744110129185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.48320836437205e-06\n",
      "30-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647499e+00 6.98946892e+01 1.33647287e+02 9.69013321e+01\n",
      " 1.15387850e+02 6.63827891e+01 1.00109724e+02 4.07778535e+01\n",
      " 7.71356672e+01 2.21721228e-05 1.23602498e-05 1.76559256e-05\n",
      " 9.24681747e-06 4.98414045e+01 9.59066554e+01 5.06862917e-01\n",
      " 9.02484593e+01 5.03052402e+01 6.00915636e+01 4.09100660e+01\n",
      " 2.08162368e+01]\n",
      "31-th iteration, loss: 0.188687441100323, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4949246561756847e-06\n",
      "31-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647497e+00 6.98946891e+01 1.33647288e+02 9.69013330e+01\n",
      " 1.15387849e+02 6.63827850e+01 1.00109723e+02 4.07778538e+01\n",
      " 7.71356679e+01 2.30801789e-05 1.29197403e-05 1.86438161e-05\n",
      " 9.69226904e-06 4.98414055e+01 9.59066589e+01 5.06859205e-01\n",
      " 9.02484540e+01 5.03052421e+01 6.00915656e+01 4.09100682e+01\n",
      " 2.08162368e+01]\n",
      "32-th iteration, loss: 0.18868744109938176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.511562934762468e-06\n",
      "32-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647493e+00 6.98946891e+01 1.33647288e+02 9.69013339e+01\n",
      " 1.15387848e+02 6.63827809e+01 1.00109722e+02 4.07778540e+01\n",
      " 7.71356686e+01 2.38870345e-05 1.34170206e-05 1.95341198e-05\n",
      " 1.00691296e-05 4.98414065e+01 9.59066625e+01 5.06855618e-01\n",
      " 9.02484487e+01 5.03052440e+01 6.00915675e+01 4.09100703e+01\n",
      " 2.08162368e+01]\n",
      "33-th iteration, loss: 0.1886874410984654, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.531222648437379e-06\n",
      "33-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647488e+00 6.98946890e+01 1.33647289e+02 9.69013348e+01\n",
      " 1.15387847e+02 6.63827768e+01 1.00109721e+02 4.07778541e+01\n",
      " 7.71356692e+01 2.46133730e-05 1.38627498e-05 2.03471183e-05\n",
      " 1.03886886e-05 4.98414073e+01 9.59066660e+01 5.06852156e-01\n",
      " 9.02484434e+01 5.03052458e+01 6.00915694e+01 4.09100723e+01\n",
      " 2.08162368e+01]\n",
      "34-th iteration, loss: 0.18868744109757185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5525680572217546e-06\n",
      "34-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647483e+00 6.98946888e+01 1.33647290e+02 9.69013356e+01\n",
      " 1.15387846e+02 6.63827728e+01 1.00109720e+02 4.07778542e+01\n",
      " 7.71356697e+01 2.52748589e-05 1.42652258e-05 2.10981434e-05\n",
      " 1.06597432e-05 4.98414082e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.59066696e+01 5.06848816e-01 9.02484381e+01 5.03052476e+01\n",
      " 6.00915712e+01 4.09100743e+01 2.08162368e+01]\n",
      "35-th iteration, loss: 0.18868744109655908, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5167363407449975e-06\n",
      "35-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647477e+00 6.98946887e+01 1.33647290e+02 9.69013364e+01\n",
      " 1.15387845e+02 6.63827688e+01 1.00109718e+02 4.07778542e+01\n",
      " 7.71356702e+01 2.58425308e-05 1.46150504e-05 2.17579538e-05\n",
      " 1.08732956e-05 4.98414089e+01 3.55689889e-06 7.28661418e-07\n",
      " 9.59066731e+01 5.06845514e-01 9.02484329e+01 5.03052493e+01\n",
      " 6.00915730e+01 4.09100762e+01 2.08162368e+01]\n",
      "36-th iteration, loss: 0.18868744109556793, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.497928907200881e-06\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647471e+00 6.98946885e+01 1.33647291e+02 9.69013372e+01\n",
      " 1.15387843e+02 6.63827649e+01 1.00109717e+02 4.07778541e+01\n",
      " 7.71356707e+01 2.62759321e-05 1.49009395e-05 2.22857533e-05\n",
      " 1.10186507e-05 4.98414095e+01 7.08209733e-06 1.30872113e-06\n",
      " 0.00000000e+00 2.11758237e-22 9.59066767e+01 5.06842216e-01\n",
      " 9.02484276e+01 5.03052509e+01 6.00915748e+01 4.09100781e+01\n",
      " 2.08162368e+01]\n",
      "37-th iteration, loss: 0.1886874410944644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.439920151562162e-06\n",
      "37-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647465e+00 6.98946883e+01 1.33647292e+02 9.69013380e+01\n",
      " 1.15387842e+02 6.63827611e+01 1.00109716e+02 4.07778539e+01\n",
      " 7.71356711e+01 2.65798658e-05 1.51274789e-05 2.26859317e-05\n",
      " 1.11012470e-05 4.98414100e+01 1.05636807e-05 1.74428869e-06\n",
      " 3.48819525e-06 4.35567558e-07 9.59066801e+01 5.06838871e-01\n",
      " 9.02484222e+01 5.03052524e+01 6.00915764e+01 4.09100799e+01\n",
      " 2.08162368e+01]\n",
      "38-th iteration, loss: 0.18868744109338073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.406671655527159e-06\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647459e+00 6.98946881e+01 1.33647292e+02 9.69013388e+01\n",
      " 1.15387841e+02 6.63827573e+01 1.00109715e+02 4.07778537e+01\n",
      " 7.71356714e+01 2.67453147e-05 1.52957496e-05 2.29490886e-05\n",
      " 1.11229899e-05 4.98414103e+01 1.39945586e-05 2.02577864e-06\n",
      " 6.92788558e-06 6.99434532e-07 0.00000000e+00 1.85288457e-22\n",
      " 9.59066836e+01 5.06835470e-01 9.02484168e+01 5.03052538e+01\n",
      " 6.00915781e+01 4.09100817e+01 2.08162368e+01]\n",
      "39-th iteration, loss: 0.18868744109219074, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.346504212132848e-06\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647453e+00 6.98946879e+01 1.33647293e+02 9.69013396e+01\n",
      " 1.15387840e+02 6.63827536e+01 1.00109713e+02 4.07778534e+01\n",
      " 7.71356717e+01 2.67953569e-05 1.54174875e-05 2.30979257e-05\n",
      " 1.10965002e-05 4.98414105e+01 1.73733984e-05 2.17575588e-06\n",
      " 1.03169600e-05 8.14410855e-07 3.39260806e-06 1.14976323e-07\n",
      " 9.59066870e+01 5.06831987e-01 9.02484114e+01 5.03052551e+01\n",
      " 6.00915797e+01 4.09100834e+01 2.08162368e+01]\n",
      "40-th iteration, loss: 0.18868744109101696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.311950856022234e-06\n",
      "40-th iteration, new layer inserted. now 29 layers\n",
      "[2.54647447e+00 6.98946877e+01 1.33647293e+02 9.69013404e+01\n",
      " 1.15387839e+02 6.63827499e+01 1.00109712e+02 4.07778531e+01\n",
      " 7.71356720e+01 2.67387297e-05 1.55001090e-05 2.31408796e-05\n",
      " 1.10299328e-05 4.98414106e+01 2.07023725e-05 2.20260833e-06\n",
      " 1.36569264e-05 7.89140183e-07 6.73668905e-06 7.25655908e-08\n",
      " 0.00000000e+00 9.92616735e-24 9.59066903e+01 5.06828424e-01\n",
      " 9.02484059e+01 5.03052563e+01 6.00915812e+01 4.09100850e+01\n",
      " 2.08162368e+01]\n",
      "41-th iteration, loss: 0.18868744108974106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2513240382224866e-06\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647441e+00 6.98946876e+01 1.33647294e+02 9.69013412e+01\n",
      " 1.15387838e+02 6.63827464e+01 1.00109711e+02 4.07778527e+01\n",
      " 7.71356722e+01 2.66020823e-05 1.55562557e-05 2.31043471e-05\n",
      " 1.09366130e-05 4.98414107e+01 2.39841271e-05 2.13272573e-06\n",
      " 1.69498090e-05 6.50260348e-07 0.00000000e+00 7.94093388e-23\n",
      " 9.59067070e+01 5.06824763e-01 9.02484004e+01 5.03052574e+01\n",
      " 6.00915827e+01 4.09100866e+01 2.08162368e+01]\n",
      "42-th iteration, loss: 0.18868744108859167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.248477986220215e-06\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647436e+00 6.98946874e+01 1.33647295e+02 9.69013420e+01\n",
      " 1.15387837e+02 6.63827429e+01 1.00109710e+02 4.07778524e+01\n",
      " 7.71356724e+01 2.64071068e-05 1.55960239e-05 2.30098491e-05\n",
      " 1.08271508e-05 4.98414106e+01 2.72371677e-05 1.98769498e-06\n",
      " 2.02136246e-05 4.19596132e-07 0.00000000e+00 6.61744490e-23\n",
      " 9.59067135e+01 5.06821059e-01 9.02483948e+01 5.03052585e+01\n",
      " 6.00915841e+01 4.09100882e+01 2.08162368e+01]\n",
      "43-th iteration, loss: 0.18868744108745242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2473986421672847e-06\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647430e+00 6.98946872e+01 1.33647295e+02 9.69013428e+01\n",
      " 1.15387836e+02 6.63827395e+01 1.00109709e+02 4.07778521e+01\n",
      " 7.71356726e+01 2.62109892e-05 1.56397895e-05 2.29144659e-05\n",
      " 1.07222962e-05 4.98414106e+01 3.04902278e-05 1.82463699e-06\n",
      " 2.34767268e-05 1.54415315e-07 0.00000000e+00 1.98523347e-23\n",
      " 9.59067200e+01 5.06817367e-01 9.02483893e+01 5.03052595e+01\n",
      " 6.00915855e+01 4.09100897e+01 2.08162368e+01]\n",
      "44-th iteration, loss: 0.18868744108632332, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2444417872021756e-06\n",
      "44-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647425e+00 6.98946871e+01 1.33647296e+02 9.69013436e+01\n",
      " 1.15387835e+02 6.63827362e+01 1.00109707e+02 4.07778518e+01\n",
      " 7.71356728e+01 2.60161488e-05 1.56882818e-05 2.28206424e-05\n",
      " 1.06227846e-05 4.98414106e+01 3.37450299e-05 1.64602653e-06\n",
      " 9.59067533e+01 5.06813691e-01 9.02483837e+01 5.03052604e+01\n",
      " 6.00915869e+01 4.09100912e+01 2.08162368e+01]\n",
      "45-th iteration, loss: 0.18868744108543456, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.320254008662244e-06\n",
      "45-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647419e+00 6.98946869e+01 1.33647297e+02 9.69013444e+01\n",
      " 1.15387834e+02 6.63827330e+01 1.00109706e+02 4.07778515e+01\n",
      " 7.71356731e+01 2.58598705e-05 1.57533578e-05 2.27656943e-05\n",
      " 1.05404626e-05 4.98414106e+01 3.70378714e-05 1.48920479e-06\n",
      " 0.00000000e+00 2.11758237e-22 9.59067566e+01 5.06810139e-01\n",
      " 9.02483782e+01 5.03052614e+01 6.00915883e+01 4.09100927e+01\n",
      " 2.08162368e+01]\n",
      "46-th iteration, loss: 0.1886874410844415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.336759492390121e-06\n",
      "46-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647413e+00 6.98946867e+01 1.33647297e+02 9.69013451e+01\n",
      " 1.15387833e+02 6.63827298e+01 1.00109705e+02 4.07778512e+01\n",
      " 7.71356733e+01 2.57718219e-05 1.58430885e-05 2.27793962e-05\n",
      " 1.04831503e-05 4.98414107e+01 4.03760929e-05 1.38386541e-06\n",
      " 9.59067633e+01 5.06806710e-01 9.02483727e+01 5.03052624e+01\n",
      " 6.00915896e+01 4.09100942e+01 2.08162368e+01]\n",
      "47-th iteration, loss: 0.18868744108357954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3914860859834686e-06\n",
      "47-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647407e+00 6.98946865e+01 1.33647298e+02 9.69013459e+01\n",
      " 1.15387832e+02 6.63827266e+01 1.00109704e+02 4.07778510e+01\n",
      " 7.71356736e+01 2.57173208e-05 1.59446633e-05 2.28272252e-05\n",
      " 1.04375936e-05 4.98414108e+01 4.37512322e-05 1.29541759e-06\n",
      " 9.59067667e+01 5.06803402e-01 9.02483673e+01 5.03052634e+01\n",
      " 6.00915910e+01 4.09100956e+01 2.08162368e+01]\n",
      "48-th iteration, loss: 0.1886874410827303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.433900740537277e-06\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647400e+00 6.98946863e+01 1.33647298e+02 9.69013467e+01\n",
      " 1.15387831e+02 6.63827235e+01 1.00109703e+02 4.07778508e+01\n",
      " 7.71356739e+01 2.57199283e-05 1.60649178e-05 2.29328188e-05\n",
      " 1.04104076e-05 4.98414110e+01 4.71749341e-05 1.24738839e-06\n",
      " 0.00000000e+00 5.29395592e-23 9.59067701e+01 5.06800238e-01\n",
      " 9.02483619e+01 5.03052644e+01 6.00915923e+01 4.09100970e+01\n",
      " 2.08162368e+01]\n",
      "49-th iteration, loss: 0.18868744108176524, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4256886820189315e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647393e+00 6.98946861e+01 1.33647299e+02 9.69013474e+01\n",
      " 1.15387831e+02 6.63827205e+01 1.00109702e+02 4.07778506e+01\n",
      " 7.71356742e+01 2.57315651e-05 1.61857066e-05 2.30482185e-05\n",
      " 1.03830740e-05 4.98414112e+01 5.06138904e-05 1.19169256e-06\n",
      " 0.00000000e+00 1.58818678e-22 9.59067770e+01 5.06797129e-01\n",
      " 9.02483565e+01 5.03052655e+01 6.00915936e+01 4.09100984e+01\n",
      " 2.08162368e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535388046575191\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.28873282   0.         936.61076467]\n",
      "1-th iteration, loss: 0.7469808567750643, 11 gd steps\n",
      "insert gradient: -0.6220056359276027\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.09207687  62.36451321 239.86373241   0.         696.74703225]\n",
      "2-th iteration, loss: 0.607514803743586, 13 gd steps\n",
      "insert gradient: -0.7002161897951394\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.4635834   77.04469709 224.34700949  39.44269071 241.72856221\n",
      "   0.         455.01847004]\n",
      "3-th iteration, loss: 0.47567491598931855, 20 gd steps\n",
      "insert gradient: -0.7173363768445731\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          56.6490121  228.89137612  55.21989105 154.84319619\n",
      "  39.6159821  377.01530375   0.          78.00316629]\n",
      "4-th iteration, loss: 0.35587699484558405, 35 gd steps\n",
      "insert gradient: -0.19680346595430365\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          55.43792894 110.17216906   0.         102.30272841\n",
      "  53.70489687 138.79631748  60.53255303 322.37820145  39.4555736\n",
      "  78.00316629]\n",
      "5-th iteration, loss: 0.3239215196415179, 30 gd steps\n",
      "insert gradient: -0.06648194891291852\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  1.65701798  52.58100067  79.81016487  11.95695782  90.57886181\n",
      "  60.23249322 125.55406352  62.82146105 288.70787379  59.59830943\n",
      "  78.00316629]\n",
      "6-th iteration, loss: 0.3224880629792657, 50 gd steps\n",
      "insert gradient: -0.10893489127631974\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  2.00919157  53.25580821  79.12614285  10.51758422  92.17257836\n",
      "  58.98736724 122.77018052  63.29640148 127.26232904   0.\n",
      " 152.71479485  62.27060034  78.00316629]\n",
      "7-th iteration, loss: 0.3187450840779734, 21 gd steps\n",
      "insert gradient: -0.0303442776118737\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  2.4847488   52.71148459  84.5380406    8.50817749  91.79890231\n",
      "  57.59936115 128.21405917  61.40348084 117.72450762   8.72211358\n",
      " 146.49538428  58.65789785  78.00316629]\n",
      "8-th iteration, loss: 0.3156624104555752, 48 gd steps\n",
      "insert gradient: -0.05172544142028196\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  2.35368225  52.25266688  88.16244666   5.90778929  91.15466014\n",
      "  56.93625964 125.34356933  63.914357    96.37303278  15.66139482\n",
      " 142.90519597  60.82247307  78.00316629]\n",
      "9-th iteration, loss: 0.28634125404641925, 136 gd steps\n",
      "insert gradient: -0.026568655408213487\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[  4.95170284  55.33727514 179.26854897  58.32924604 152.85071938\n",
      "  53.3493096   80.49340011  36.88844147  98.65504656  60.08686821\n",
      "  78.00316629]\n",
      "10-th iteration, loss: 0.27900915349824834, 18 gd steps\n",
      "insert gradient: -0.006092829206851561\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[8.13448838e+00 6.09600664e+01 1.61636792e+02 5.79014842e+01\n",
      " 1.60408933e+02 5.20191711e+01 8.09376487e+01 3.85054217e+01\n",
      " 8.94693862e+01 0.00000000e+00 3.55271368e-14 6.29977463e+01\n",
      " 7.80031663e+01]\n",
      "11-th iteration, loss: 0.2781876517562937, 41 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7395663736314672e-17\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.37512445  62.65207945 157.00042158  59.3869409  158.51827645\n",
      "  50.51244318  78.95410799  39.28344745  87.65208933  63.53397534\n",
      "  78.00316629]\n",
      "12-th iteration, loss: 0.27817860321462046, 29 gd steps\n",
      "insert gradient: -8.96286971273526e-05\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.94131425  62.63501271 156.52981595  59.56184338 158.16797234\n",
      "  50.53688723  78.7607765   39.24158048  87.58439894  63.4604792\n",
      "  78.00316629]\n",
      "13-th iteration, loss: 0.2781782386753313, 21 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.471648042302706e-06\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[1.08906392e+01 6.26325129e+01 1.56486312e+02 5.95965856e+01\n",
      " 1.58091943e+02 5.05368217e+01 7.87058567e+01 3.92454649e+01\n",
      " 0.00000000e+00 5.77315973e-15 8.76097676e+01 6.34365234e+01\n",
      " 7.80031663e+01]\n",
      "14-th iteration, loss: 0.27817819337037736, 75 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9881849356194023e-06\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86871803  62.64095689 156.46848365  59.60248616 158.05896446\n",
      "  50.53517705  78.69241248  39.24252443  87.61842741  63.42623912\n",
      "  78.00316629]\n",
      "15-th iteration, loss: 0.2781781930257328, 29 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5801514264834582e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457914  62.64162696 156.46679377  59.60372832 158.05638597\n",
      "  50.53547054  78.69069454  39.2423359   87.61913702  63.42566446\n",
      "  78.00316629]\n",
      "16-th iteration, loss: 0.2781781930254889, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5414018404575024e-07\n",
      "16-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645786e+01 6.26416261e+01 1.56466791e+02 5.96037251e+01\n",
      " 1.58056383e+02 5.05354707e+01 7.86906941e+01 0.00000000e+00\n",
      " 8.88178420e-16 3.92423361e+01 8.76191363e+01 6.34256618e+01\n",
      " 7.80031663e+01]\n",
      "17-th iteration, loss: 0.27817819302526337, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.321231943664684e-07\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645780e+01 6.26416253e+01 1.56466789e+02 5.96037221e+01\n",
      " 1.58056381e+02 0.00000000e+00 5.50670620e-14 5.05354709e+01\n",
      " 7.86906936e+01 3.92423365e+01 8.76191355e+01 6.34256591e+01\n",
      " 7.80031663e+01]\n",
      "18-th iteration, loss: 0.2781781930250543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.094101176054646e-07\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457753  62.64162463 156.46678663  59.60371942 158.05637794\n",
      "  50.53547165  78.69069317  39.2423367   87.61913476  63.42565634\n",
      "  78.00316629]\n",
      "19-th iteration, loss: 0.27817819302485763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.959854222344822e-07\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457703  62.64162405 156.46678439  59.60371686 158.05637539\n",
      "  50.53547207  78.69069272  39.24233686  87.61913403  63.42565362\n",
      "  78.00316629]\n",
      "20-th iteration, loss: 0.2781781930246729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.699015964884739e-07\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645765e+01 6.26416236e+01 1.56466782e+02 5.96037145e+01\n",
      " 1.58056373e+02 0.00000000e+00 1.59872116e-14 5.05354726e+01\n",
      " 7.86906923e+01 3.92423370e+01 8.76191333e+01 6.34256509e+01\n",
      " 7.80031663e+01]\n",
      "21-th iteration, loss: 0.2781781930244971, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.015114339296286e-07\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457608  62.64162318 156.46678011  59.60371232 158.05637048\n",
      "  50.53547372  78.69069182  39.24233716  87.61913261  63.42564822\n",
      "  78.00316629]\n",
      "22-th iteration, loss: 0.27817819302433117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.573527007959342e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457562  62.64162283 156.46677804  59.60371025 158.05636808\n",
      "  50.53547431  78.69069137  39.24233728  87.61913192  63.42564554\n",
      "  78.00316629]\n",
      "23-th iteration, loss: 0.2781781930241734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.043179792853982e-07\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457518  62.64162255 156.46677602  59.60370834 158.05636574\n",
      "  50.53547494  78.6906909   39.24233739  87.61913124  63.42564289\n",
      "  78.00316629]\n",
      "24-th iteration, loss: 0.2781781930240229, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.434987948260482e-07\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457475  62.64162233 156.46677405  59.60370655 158.05636343\n",
      "  50.53547562  78.69069044  39.24233749  87.61913058  63.42564025\n",
      "  78.00316629]\n",
      "25-th iteration, loss: 0.278178193023879, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.75859296838513e-07\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457433  62.64162216 156.46677213  59.60370489 158.05636117\n",
      "  50.53547634  78.69068997  39.24233758  87.61912993  63.42563763\n",
      "  78.00316629]\n",
      "26-th iteration, loss: 0.2781781930237409, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.022508795784448e-07\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645739e+01 6.26416220e+01 1.56466770e+02 5.96037033e+01\n",
      " 1.58056359e+02 0.00000000e+00 1.59872116e-14 5.05354771e+01\n",
      " 7.86906895e+01 3.92423376e+01 8.76191293e+01 6.34256350e+01\n",
      " 7.80031663e+01]\n",
      "27-th iteration, loss: 0.2781781930236048, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.831820176737147e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457352  62.64162196 156.46676841  59.6037019  158.05635676\n",
      "  50.53547863  78.69068901  39.24233771  87.61912868  63.42563247\n",
      "  78.00316629]\n",
      "28-th iteration, loss: 0.27817819302347696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.034053767341242e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457312  62.64162189 156.4667666   59.60370051 158.05635459\n",
      "  50.53547937  78.69068851  39.24233775  87.61912807  63.42562995\n",
      "  78.00316629]\n",
      "29-th iteration, loss: 0.2781781930233537, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.192749130884123e-07\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645727e+01 6.26416219e+01 1.56466765e+02 5.96036992e+01\n",
      " 1.58056352e+02 0.00000000e+00 3.73034936e-14 5.05354801e+01\n",
      " 7.86906880e+01 3.92423378e+01 8.76191275e+01 6.34256274e+01\n",
      " 7.80031663e+01]\n",
      "30-th iteration, loss: 0.27817819302323127, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.908339519209588e-07\n",
      "30-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645723e+01 6.26416219e+01 1.56466763e+02 5.96036980e+01\n",
      " 1.58056350e+02 0.00000000e+00 2.30926389e-14 5.05354817e+01\n",
      " 7.86906875e+01 3.92423378e+01 8.76191269e+01 6.34256250e+01\n",
      " 7.80031663e+01]\n",
      "31-th iteration, loss: 0.278178193023113, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.641241600396788e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457197  62.64162189 156.46676139  59.60369687 158.05634827\n",
      "  50.5354832   78.69068696  39.24233778  87.61912633  63.42562256\n",
      "  78.00316629]\n",
      "32-th iteration, loss: 0.2781781930230016, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.767653397338992e-07\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457159  62.64162191 156.4667597   59.60369577 158.05634619\n",
      "  50.53548391  78.69068641  39.24233776  87.61912577  63.42562017\n",
      "  78.00316629]\n",
      "33-th iteration, loss: 0.278178193022894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.862040162084937e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457122  62.64162196 156.46675804  59.60369473 158.05634414\n",
      "  50.53548464  78.69068585  39.24233772  87.61912523  63.42561782\n",
      "  78.00316629]\n",
      "34-th iteration, loss: 0.2781781930227897, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.928572420156045e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457086  62.64162203 156.46675641  59.60369376 158.05634211\n",
      "  50.53548538  78.69068529  39.24233768  87.61912471  63.42561549\n",
      "  78.00316629]\n",
      "35-th iteration, loss: 0.2781781930226886, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.970926769436118e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.8645705   62.64162211 156.46675481  59.60369285 158.0563401\n",
      "  50.53548611  78.69068472  39.24233764  87.6191242   63.4256132\n",
      "  78.00316629]\n",
      "36-th iteration, loss: 0.27817819302259056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.992342983047747e-07\n",
      "36-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645701e+01 6.26416222e+01 1.56466753e+02 5.96036920e+01\n",
      " 1.58056338e+02 0.00000000e+00 5.50670620e-14 5.05354869e+01\n",
      " 7.86906841e+01 3.92423376e+01 8.76191237e+01 6.34256109e+01\n",
      " 7.80031663e+01]\n",
      "37-th iteration, loss: 0.27817819302249197, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.608421433806709e-07\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645698e+01 6.26416223e+01 1.56466752e+02 5.96036912e+01\n",
      " 1.58056336e+02 0.00000000e+00 3.73034936e-14 5.05354884e+01\n",
      " 7.86906836e+01 3.92423375e+01 8.76191232e+01 6.34256087e+01\n",
      " 7.80031663e+01]\n",
      "38-th iteration, loss: 0.2781781930223965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.261672886782125e-07\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456945  62.64162247 156.46675015  59.60369044 158.05633421\n",
      "  50.53548977  78.69068296  39.24233745  87.61912275  63.42560654\n",
      "  78.00316629]\n",
      "39-th iteration, loss: 0.27817819302230645, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.300011746307338e-07\n",
      "39-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645691e+01 6.26416226e+01 1.56466749e+02 5.96036897e+01\n",
      " 1.58056332e+02 0.00000000e+00 1.59872116e-14 5.05354904e+01\n",
      " 7.86906823e+01 3.92423374e+01 8.76191223e+01 6.34256044e+01\n",
      " 7.80031663e+01]\n",
      "40-th iteration, loss: 0.27817819302221625, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.966769457691546e-07\n",
      "40-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645688e+01 6.26416227e+01 1.56466747e+02 5.96036890e+01\n",
      " 1.58056330e+02 0.00000000e+00 2.30926389e-14 5.05354918e+01\n",
      " 7.86906817e+01 3.92423373e+01 8.76191218e+01 6.34256023e+01\n",
      " 7.80031663e+01]\n",
      "41-th iteration, loss: 0.2781781930221287, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.665185941728005e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456842  62.64162286 156.46674568  59.60368835 158.05632844\n",
      "  50.53549309  78.69068109  39.24233715  87.61912141  63.42560021\n",
      "  78.00316629]\n",
      "42-th iteration, loss: 0.27817819302204577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.714439330987864e-07\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456807  62.64162299 156.46674422  59.60368771 158.05632653\n",
      "  50.5354937   78.69068044  39.24233703  87.61912099  63.42559818\n",
      "  78.00316629]\n",
      "43-th iteration, loss: 0.27817819302196506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.745769433952203e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456774  62.64162313 156.46674278  59.60368711 158.05632464\n",
      "  50.53549431  78.69067979  39.24233691  87.61912059  63.42559618\n",
      "  78.00316629]\n",
      "44-th iteration, loss: 0.2781781930218864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.761551448836682e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645674e+01 6.26416233e+01 1.56466741e+02 5.96036865e+01\n",
      " 1.58056323e+02 0.00000000e+00 1.59872116e-14 5.05354949e+01\n",
      " 7.86906791e+01 3.92423368e+01 8.76191202e+01 6.34255942e+01\n",
      " 7.80031663e+01]\n",
      "45-th iteration, loss: 0.27817819302180746, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.439084292588941e-07\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645671e+01 6.26416234e+01 1.56466740e+02 5.96036860e+01\n",
      " 1.58056321e+02 0.00000000e+00 2.30926389e-14 5.05354962e+01\n",
      " 7.86906785e+01 3.92423367e+01 8.76191198e+01 6.34255923e+01\n",
      " 7.80031663e+01]\n",
      "46-th iteration, loss: 0.2781781930217307, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.149281428821911e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456674  62.64162361 156.46673858  59.60368555 158.05631908\n",
      "  50.53549735  78.6906778   39.24233654  87.61911944  63.42559036\n",
      "  78.00316629]\n",
      "47-th iteration, loss: 0.2781781930216577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.183811133770314e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456641  62.64162376 156.4667372   59.60368507 158.05631723\n",
      "  50.5354979   78.69067711  39.2423364   87.61911909  63.42558849\n",
      "  78.00316629]\n",
      "48-th iteration, loss: 0.2781781930215864, 0 gd steps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./../../designer/script/')\n",
    "from design import Design\n",
    "from film import FilmSimple\n",
    "from spectrum import Spectrum\n",
    "\n",
    "\n",
    "def make_reflection_design(init_film: FilmSimple):\n",
    "    inc_ang = 0.\n",
    "    wls = np.linspace(400, 1000, 500) # when wls = 50, ~100 min\n",
    "    target_spec = [Spectrum(inc_ang, wls, np.ones(wls.shape[0], dtype='float'))]\n",
    "    \n",
    "    design = Design(target_spec, init_film)\n",
    "    return design\n",
    "\n",
    "result_d_ls = []\n",
    "for run_num, init_ot in enumerate(np.linspace(0, 5000, 200)):\n",
    "    \n",
    "    d_init = np.array([init_ot], dtype='float')\n",
    "    film = FilmSimple('SiO2', 'TiO2', 'SiO2', d_init)\n",
    "    design = make_reflection_design(film)\n",
    "    try:\n",
    "        design.TFNN_train(epoch=50)\n",
    "    except Exception as e:\n",
    "        print(e.args[0])\n",
    "\n",
    "    np.savetxt(\n",
    "        f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final', \n",
    "        design.film.get_d() / 1000 # in \\mu m\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfeng6\\AppData\\Local\\Temp\\ipykernel_13436\\3791610427.py:4: UserWarning: loadtxt: input contained no data: \"./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/0_final\"\n",
      "  d = np.loadtxt(f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final') * 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "design_ls: list[Design] = []\n",
    "for run_num, init_gt in enumerate(np.linspace(0, 5000, 200)):\n",
    "    d = np.loadtxt(f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final') * 1000.\n",
    "    if d.shape == (0, ):\n",
    "        continue\n",
    "    f = FilmSimple('SiO2', 'TiO2', 'SiO2', d)\n",
    "    \n",
    "    inc_ang = 0.\n",
    "    wls = np.linspace(400, 1000, 500)\n",
    "    target_spec = [Spectrum(inc_ang, wls, np.ones(wls.shape[0], dtype='float'))]\n",
    "    \n",
    "    design_ls.append(Design(target_spec, FilmSimple('SiO2', 'TiO2', 'SiO2', np.array([init_gt])), f))\n",
    "\n",
    "init_ot, loss = [], []\n",
    "for d in design_ls:\n",
    "    init_ot.append(d.get_init_ot())\n",
    "    loss.append(d.calculate_loss())\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(init_ot, np.log10(loss))\n",
    "ax.set_xlabel('init ot / nm')\n",
    "ax.set_ylabel('log 10 loss')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(wls, design_ls[198].film.get_spec().get_R())\n",
    "ax.set_ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
