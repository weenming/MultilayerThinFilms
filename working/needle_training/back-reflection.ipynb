{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\rfeng6\\Desktop\\TFNN\\Thin-Film-Design\\working\\needle_training\\./../../designer/script\\optimizer\\LM_gradient_descent.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rho = (F_d - F_dnew) / np.dot(h.T, mu * h - g).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design terminated: zero layers\n",
      "0-th iteration, loss: 0.9545036783633585, 6 gd steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rfeng6\\.conda\\envs\\cuda\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert gradient: -1.9865456241651303\n",
      "0-th iteration, new layer inserted. now 2 layers\n",
      "[25.12562814  0.        ]\n",
      "1-th iteration, loss: 0.7465113875206747, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9156536339285143e-15\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[2.98971315e+01 7.52391188e+01 0.00000000e+00 3.70814490e-14]\n",
      "2-th iteration, loss: 0.6598330591444459, 13 gd steps\n",
      "insert gradient: -0.011180187198537962\n",
      "2-th iteration, new layer inserted. now 4 layers\n",
      "[1.20649550e+00 0.00000000e+00 3.26128013e-16 5.52737458e+01]\n",
      "3-th iteration, loss: 0.6598181064726139, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0853385803732504e-16\n",
      "3-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79889184 55.24403845  0.        ]\n",
      "4-th iteration, loss: 0.6598180977378222, 7 gd steps\n",
      "insert gradient: -0.0002516729579486868\n",
      "4-th iteration, new layer inserted. now 2 layers\n",
      "[ 1.79629626 55.23511496]\n",
      "5-th iteration, loss: 0.6598180969660132, 7 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4778355223477435e-15\n",
      "5-th iteration, new layer inserted. now 4 layers\n",
      "[1.79710075e+00 5.52377582e+01 0.00000000e+00 1.38777878e-14]\n",
      "6-th iteration, loss: 0.6598180968911103, 6 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.0712105349610603e-14\n",
      "6-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693521e+00 5.52371452e+01 0.00000000e+00 1.10467191e-13]\n",
      "7-th iteration, loss: 0.6598180968911073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.356503847953933e-15\n",
      "7-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693619e+00 5.52371451e+01 0.00000000e+00 8.60422844e-14]\n",
      "8-th iteration, loss: 0.6598180968911045, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5004352501125248e-14\n",
      "8-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693714e+00 5.52371450e+01 0.00000000e+00 1.56319402e-13]\n",
      "9-th iteration, loss: 0.6598180968911019, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.363573330910042e-15\n",
      "9-th iteration, new layer inserted. now 4 layers\n",
      "[1.79693807e+00 5.52371449e+01 0.00000000e+00 8.60422844e-14]\n",
      "10-th iteration, loss: 0.6598180968910995, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8101009363040587e-16\n",
      "10-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693897 55.23714483  0.        ]\n",
      "11-th iteration, loss: 0.6598180968910972, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9121591517570153e-16\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693985 55.2371447   0.        ]\n",
      "12-th iteration, loss: 0.6598180968910948, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1468047152164157e-16\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969407  55.23714457  0.        ]\n",
      "13-th iteration, loss: 0.6598180968910928, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.224031782464662e-15\n",
      "13-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694153e+00 5.52371444e+01 0.00000000e+00 5.28466160e-14]\n",
      "14-th iteration, loss: 0.6598180968910907, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9892941155157363e-16\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694233 55.23714429  0.        ]\n",
      "15-th iteration, loss: 0.6598180968910888, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.583708300486293e-14\n",
      "15-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694312e+00 5.52371441e+01 0.00000000e+00 1.64424030e-13]\n",
      "16-th iteration, loss: 0.6598180968910871, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1320377413810817e-14\n",
      "16-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694388e+00 5.52371440e+01 0.00000000e+00 1.17350574e-13]\n",
      "17-th iteration, loss: 0.6598180968910853, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0362565054452338e-16\n",
      "17-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694463 55.23714383  0.        ]\n",
      "18-th iteration, loss: 0.6598180968910837, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8272995209182463e-16\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694535 55.23714368  0.        ]\n",
      "19-th iteration, loss: 0.6598180968910822, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0867651663162567e-16\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694606 55.23714352  0.        ]\n",
      "20-th iteration, loss: 0.6598180968910806, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.067160651266847e-15\n",
      "20-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694675e+00 5.52371434e+01 0.00000000e+00 9.30366895e-14]\n",
      "21-th iteration, loss: 0.6598180968910792, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.778350666659227e-15\n",
      "21-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694742e+00 5.52371432e+01 0.00000000e+00 1.01141318e-13]\n",
      "22-th iteration, loss: 0.6598180968910778, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.970134152566637e-16\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694808 55.23714305  0.        ]\n",
      "23-th iteration, loss: 0.6598180968910765, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.186994245199285e-15\n",
      "23-th iteration, new layer inserted. now 4 layers\n",
      "[1.79694871e+00 5.52371429e+01 0.00000000e+00 8.38218384e-14]\n",
      "24-th iteration, loss: 0.6598180968910753, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9505313046758664e-16\n",
      "24-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694934 55.23714274  0.        ]\n",
      "25-th iteration, loss: 0.6598180968910742, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7565438987808222e-16\n",
      "25-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694995 55.23714259  0.        ]\n",
      "26-th iteration, loss: 0.659818096891073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1980288825624532e-16\n",
      "26-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695054 55.23714244  0.        ]\n",
      "27-th iteration, loss: 0.6598180968910718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.783813964193745e-15\n",
      "27-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695112e+00 5.52371423e+01 0.00000000e+00 7.96029909e-14]\n",
      "28-th iteration, loss: 0.659818096891071, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.798331060581357e-15\n",
      "28-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695168e+00 5.52371421e+01 0.00000000e+00 7.96029909e-14]\n",
      "29-th iteration, loss: 0.65981809689107, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5814719355613968e-14\n",
      "29-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695223e+00 5.52371420e+01 0.00000000e+00 1.64424030e-13]\n",
      "30-th iteration, loss: 0.6598180968910691, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1731740719721552e-14\n",
      "30-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695277e+00 5.52371419e+01 0.00000000e+00 1.21458399e-13]\n",
      "31-th iteration, loss: 0.6598180968910682, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7601182191783408e-16\n",
      "31-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695329 55.23714172  0.        ]\n",
      "32-th iteration, loss: 0.6598180968910673, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1159624206210483e-16\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969538  55.23714158  0.        ]\n",
      "33-th iteration, loss: 0.6598180968910665, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1723686954017003e-14\n",
      "33-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695430e+00 5.52371414e+01 0.00000000e+00 1.21458399e-13]\n",
      "34-th iteration, loss: 0.6598180968910657, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.858573286350223e-15\n",
      "34-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695479e+00 5.52371413e+01 0.00000000e+00 7.03881398e-14]\n",
      "35-th iteration, loss: 0.659818096891065, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1168193098152713e-16\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695526 55.23714119  0.        ]\n",
      "36-th iteration, loss: 0.6598180968910643, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.807369573266401e-16\n",
      "36-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695573e+00 5.52371411e+01 0.00000000e+00 6.99440506e-15]\n",
      "37-th iteration, loss: 0.6598180968910637, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8612468418800893e-16\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695618 55.23714094  0.        ]\n",
      "38-th iteration, loss: 0.659818096891063, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1272231093831333e-16\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695662 55.23714081  0.        ]\n",
      "39-th iteration, loss: 0.6598180968910624, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.260055636190348e-15\n",
      "39-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695705e+00 5.52371407e+01 0.00000000e+00 3.25295346e-14]\n",
      "40-th iteration, loss: 0.6598180968910619, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0945556278062414e-16\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79695747 55.23714058  0.        ]\n",
      "41-th iteration, loss: 0.6598180968910613, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.559431604874859e-15\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695789e+00 5.52371405e+01 0.00000000e+00 4.58522109e-14]\n",
      "42-th iteration, loss: 0.6598180968910607, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.584862155836908e-15\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695829e+00 5.52371404e+01 0.00000000e+00 6.73905376e-14]\n",
      "43-th iteration, loss: 0.6598180968910603, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7044282020962538e-14\n",
      "43-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695868e+00 5.52371402e+01 0.00000000e+00 1.77746706e-13]\n",
      "44-th iteration, loss: 0.6598180968910599, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.057837662737118e-15\n",
      "44-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695906e+00 5.52371401e+01 0.00000000e+00 9.30366895e-14]\n",
      "45-th iteration, loss: 0.6598180968910594, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.012125159192662e-14\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[1.79695943e+00 5.52371400e+01 0.00000000e+00 1.04694031e-13]\n",
      "46-th iteration, loss: 0.659818096891059, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9000377479086705e-16\n",
      "46-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969598  55.23713993  0.        ]\n",
      "47-th iteration, loss: 0.6598180968910585, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.510554523068066e-15\n",
      "47-th iteration, new layer inserted. now 4 layers\n",
      "[1.79696015e+00 5.52371398e+01 0.00000000e+00 5.57331958e-14]\n",
      "48-th iteration, loss: 0.6598180968910582, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8879085328442603e-16\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969605  55.23713973  0.        ]\n",
      "49-th iteration, loss: 0.6598180968910577, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.82067625661617e-16\n",
      "49-th iteration, new layer inserted. now 4 layers\n",
      "[1.79696084e+00 5.52371396e+01 0.00000000e+00 6.99440506e-15]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248375\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.19688805  0.          6.05436823]\n",
      "1-th iteration, loss: 0.7487306396172311, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.0225770581121275e-15\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.27472286e+01 6.23322519e+01 0.00000000e+00 6.37268016e-14\n",
      " 6.05436823e+00]\n",
      "2-th iteration, loss: 0.6614935451217976, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8780096804950728e-16\n",
      "2-th iteration, new layer inserted. now 3 layers\n",
      "[ 8.13284728 54.69001242  6.05436823]\n",
      "3-th iteration, loss: 0.6598229421478641, 20 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.983755953758121e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[1.99893171e+00 5.53284154e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "4-th iteration, loss: 0.6598183930225427, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.487867609514557e-15\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[1.87044516e+00 5.52431740e+01 0.00000000e+00 5.57331958e-14\n",
      " 6.05436823e+00]\n",
      "5-th iteration, loss: 0.6598181362125486, 12 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.3327453423744195e-16\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.82772077 55.23445791  6.05436823]\n",
      "6-th iteration, loss: 0.6598180968947274, 27 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.0973310194762192e-14\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[1.79685097e+00 5.52372964e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "7-th iteration, loss: 0.6598180968914297, 5 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.300776349944795e-08\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[1.79687517e+00 0.00000000e+00 5.89805982e-16 5.52371574e+01\n",
      " 6.05436823e+00]\n",
      "8-th iteration, loss: 0.6598180968913921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.369993456034439e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[1.79687770e+00 1.42961152e-09 0.00000000e+00 6.86213531e-08\n",
      " 2.52829478e-06 5.52371575e+01 6.05436823e+00]\n",
      "9-th iteration, loss: 0.6598180968913434, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.729235551779377e-16\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79688733 55.23715744  6.05436823]\n",
      "10-th iteration, loss: 0.6598180968913293, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.620790945729256e-16\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79688948e+00 5.52371572e+01 0.00000000e+00 6.88338275e-15\n",
      " 6.05436823e+00]\n",
      "11-th iteration, loss: 0.659818096891316, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.897177369990208e-16\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79689157 55.23715694  6.05436823]\n",
      "12-th iteration, loss: 0.6598180968913033, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.079421411969583e-15\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689361e+00 5.52371567e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "13-th iteration, loss: 0.6598180968912915, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.8968001770470386e-16\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79689558 55.23715635  6.05436823]\n",
      "14-th iteration, loss: 0.65981809689128, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1159519332459903e-14\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689751e+00 5.52371560e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "15-th iteration, loss: 0.6598180968912691, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.846960872546786e-15\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[1.79689938e+00 5.52371557e+01 0.00000000e+00 5.97299987e-14\n",
      " 6.05436823e+00]\n",
      "16-th iteration, loss: 0.6598180968912588, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.951150578501617e-15\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690121e+00 5.52371553e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "17-th iteration, loss: 0.659818096891249, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.4057386078566874e-15\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690298e+00 5.52371550e+01 0.00000000e+00 5.35127498e-14\n",
      " 6.05436823e+00]\n",
      "18-th iteration, loss: 0.6598180968912397, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1195098280413984e-16\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79690471 55.23715464  6.05436823]\n",
      "19-th iteration, loss: 0.6598180968912307, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0592369028021686e-15\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690640e+00 5.52371543e+01 0.00000000e+00 2.81996648e-14\n",
      " 6.05436823e+00]\n",
      "20-th iteration, loss: 0.6598180968912223, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.6160976629067334e-16\n",
      "20-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79690804 55.23715391  6.05436823]\n",
      "21-th iteration, loss: 0.6598180968912142, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1164160175615541e-14\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[1.79690964e+00 5.52371535e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "22-th iteration, loss: 0.6598180968912065, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.6331408571684236e-15\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691120e+00 5.52371532e+01 0.00000000e+00 3.48610030e-14\n",
      " 6.05436823e+00]\n",
      "23-th iteration, loss: 0.6598180968911992, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.187726860111706e-16\n",
      "23-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79691273 55.23715282  6.05436823]\n",
      "24-th iteration, loss: 0.6598180968911922, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.635034846991289e-15\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691421e+00 5.52371525e+01 0.00000000e+00 8.79296636e-14\n",
      " 6.05436823e+00]\n",
      "25-th iteration, loss: 0.6598180968911856, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.664244979514424e-17\n",
      "25-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79691566 55.2371521   6.05436823]\n",
      "26-th iteration, loss: 0.6598180968911792, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.908640544581488e-15\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691707e+00 5.52371517e+01 0.00000000e+00 8.08242362e-14\n",
      " 6.05436823e+00]\n",
      "27-th iteration, loss: 0.6598180968911732, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.21448462509023e-15\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691845e+00 5.52371514e+01 0.00000000e+00 9.48130463e-14\n",
      " 6.05436823e+00]\n",
      "28-th iteration, loss: 0.6598180968911674, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.2537483566943716e-15\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[1.79691979e+00 5.52371511e+01 0.00000000e+00 5.35127498e-14\n",
      " 6.05436823e+00]\n",
      "29-th iteration, loss: 0.6598180968911619, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.012018982157213e-15\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692110e+00 5.52371507e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "30-th iteration, loss: 0.6598180968911567, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.1405282396704544e-16\n",
      "30-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692238 55.23715038  6.05436823]\n",
      "31-th iteration, loss: 0.6598180968911517, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.997453183682767e-15\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692363e+00 5.52371500e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "32-th iteration, loss: 0.6598180968911469, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.17035931420074e-15\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692484e+00 5.52371497e+01 0.00000000e+00 2.10942375e-14\n",
      " 6.05436823e+00]\n",
      "33-th iteration, loss: 0.6598180968911425, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.013911228755424e-15\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692603e+00 5.52371494e+01 0.00000000e+00 9.25926003e-14\n",
      " 6.05436823e+00]\n",
      "34-th iteration, loss: 0.6598180968911381, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4562210047939078e-16\n",
      "34-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692719 55.2371491   6.05436823]\n",
      "35-th iteration, loss: 0.659818096891134, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.3707140467404284e-16\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79692832 55.23714879  6.05436823]\n",
      "36-th iteration, loss: 0.6598180968911301, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.037583100720405e-15\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[1.79692942e+00 5.52371485e+01 0.00000000e+00 8.08242362e-14\n",
      " 6.05436823e+00]\n",
      "37-th iteration, loss: 0.6598180968911264, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.377671027088762e-15\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693050e+00 5.52371482e+01 0.00000000e+00 6.50590692e-14\n",
      " 6.05436823e+00]\n",
      "38-th iteration, loss: 0.6598180968911228, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.493193568268102e-16\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693155 55.2371479   6.05436823]\n",
      "39-th iteration, loss: 0.6598180968911194, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1129333009102296e-14\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693258e+00 5.52371476e+01 0.00000000e+00 1.13908882e-13\n",
      " 6.05436823e+00]\n",
      "40-th iteration, loss: 0.659818096891116, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.252191611747597e-15\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693358e+00 5.52371473e+01 0.00000000e+00 6.50590692e-14\n",
      " 6.05436823e+00]\n",
      "41-th iteration, loss: 0.6598180968911129, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.2209255954305026e-16\n",
      "41-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693456 55.23714707  6.05436823]\n",
      "42-th iteration, loss: 0.6598180968911102, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.9807497732101035e-15\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693552e+00 5.52371468e+01 0.00000000e+00 1.88737914e-14\n",
      " 6.05436823e+00]\n",
      "43-th iteration, loss: 0.6598180968911073, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.9520768788016154e-15\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693645e+00 5.52371465e+01 0.00000000e+00 6.03961325e-14\n",
      " 6.05436823e+00]\n",
      "44-th iteration, loss: 0.6598180968911046, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0206705463414516e-16\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79693736 55.23714629  6.05436823]\n",
      "45-th iteration, loss: 0.6598180968911022, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.9458291835299476e-15\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693825e+00 5.52371460e+01 0.00000000e+00 2.81996648e-14\n",
      " 6.05436823e+00]\n",
      "46-th iteration, loss: 0.6598180968910996, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.5864186119777466e-15\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693911e+00 5.52371458e+01 0.00000000e+00 6.72795153e-14\n",
      " 6.05436823e+00]\n",
      "47-th iteration, loss: 0.6598180968910973, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.684204866681388e-15\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79693996e+00 5.52371456e+01 0.00000000e+00 4.70734562e-14\n",
      " 6.05436823e+00]\n",
      "48-th iteration, loss: 0.6598180968910952, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.396647831132266e-16\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694078 55.23714533  6.05436823]\n",
      "49-th iteration, loss: 0.6598180968910929, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0162389007964004e-16\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79694159 55.2371451   6.05436823]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248406\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.19688805  0.         31.17999637]\n",
      "1-th iteration, loss: 0.7487306396172311, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.099184917260901e-15\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[4.27472286e+01 6.23322519e+01 0.00000000e+00 5.44009282e-14\n",
      " 3.11799964e+01]\n",
      "2-th iteration, loss: 0.6614935451217976, 14 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.853610500483975e-15\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[8.13284728e+00 5.46900124e+01 0.00000000e+00 4.01900735e-14\n",
      " 3.11799964e+01]\n",
      "3-th iteration, loss: 0.6601012007629534, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 6.9332941248416625e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[0.00000000e+00 5.65980576e+01 0.00000000e+00 7.12763182e-14\n",
      " 3.11799964e+01]\n",
      "4-th iteration, loss: 0.6598337483318801, 13 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.215411978265948e-15\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[2.13293007e+00 5.54155477e+01 0.00000000e+00 8.59312621e-14\n",
      " 3.11799964e+01]\n",
      "5-th iteration, loss: 0.6598181525924166, 46 gd steps\n",
      "insert gradient: -0.00011824888684448805\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.83508581 55.22751142 31.17999637]\n",
      "6-th iteration, loss: 0.6598180968917238, 20 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.264314567140658e-06\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703945 55.23706697 31.17999637]\n",
      "7-th iteration, loss: 0.6598180968916124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.5524918364192316e-06\n",
      "7-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703922 55.23707317 31.17999637]\n",
      "8-th iteration, loss: 0.6598180968915247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.92619553302848e-06\n",
      "8-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703883 55.23707867 31.17999637]\n",
      "9-th iteration, loss: 0.6598180968914551, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.375031559096475e-06\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703832 55.23708354 31.17999637]\n",
      "10-th iteration, loss: 0.6598180968913996, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8898719009975185e-06\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703770e+00 0.00000000e+00 4.23272528e-16 5.52370879e+01\n",
      " 3.11799964e+01]\n",
      "11-th iteration, loss: 0.6598180968913179, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0256767097898178e-06\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703697e+00 0.00000000e+00 2.53269627e-15 5.52370955e+01\n",
      " 3.11799964e+01]\n",
      "12-th iteration, loss: 0.6598180968912668, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.361855202894515e-06\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703607 55.23710144 31.17999637]\n",
      "13-th iteration, loss: 0.6598180968912477, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.117541239960738e-06\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703506 55.23710378 31.17999637]\n",
      "14-th iteration, loss: 0.6598180968912316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9019701333574368e-06\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703401 55.23710588 31.17999637]\n",
      "15-th iteration, loss: 0.6598180968912178, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7116634577925293e-06\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703294e+00 0.00000000e+00 2.08860707e-15 5.52371078e+01\n",
      " 3.11799964e+01]\n",
      "16-th iteration, loss: 0.6598180968911986, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3506602139284799e-06\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703186 55.23711113 31.17999637]\n",
      "17-th iteration, loss: 0.6598180968911895, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2253054048852317e-06\n",
      "17-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703072 55.23711247 31.17999637]\n",
      "18-th iteration, loss: 0.6598180968911812, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1142972939242786e-06\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702958 55.23711368 31.17999637]\n",
      "19-th iteration, loss: 0.6598180968911738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0159094520524132e-06\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702845 55.23711479 31.17999637]\n",
      "20-th iteration, loss: 0.659818096891167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.286254316031056e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702732e+00 0.00000000e+00 2.10942375e-15 5.52371158e+01\n",
      " 3.11799964e+01]\n",
      "21-th iteration, loss: 0.6598180968911586, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.460414263621368e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702621 55.23711763 31.17999637]\n",
      "22-th iteration, loss: 0.6598180968911531, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.897759499853552e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702508 55.23711837 31.17999637]\n",
      "23-th iteration, loss: 0.659818096891148, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.395461774032062e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702396e+00 0.00000000e+00 3.35148576e-15 5.52371191e+01\n",
      " 3.11799964e+01]\n",
      "24-th iteration, loss: 0.6598180968911421, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.220162840118871e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702287e+00 0.00000000e+00 2.74780199e-15 5.52371203e+01\n",
      " 3.11799964e+01]\n",
      "25-th iteration, loss: 0.659818096891137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.311053140356156e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702177e+00 0.00000000e+00 1.68615122e-15 5.52371213e+01\n",
      " 3.11799964e+01]\n",
      "26-th iteration, loss: 0.6598180968911325, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.606283165024757e-07\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702068e+00 0.00000000e+00 2.10942375e-15 5.52371222e+01\n",
      " 3.11799964e+01]\n",
      "27-th iteration, loss: 0.6598180968911282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0584135104979846e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701960e+00 0.00000000e+00 1.88737914e-15 5.52371229e+01\n",
      " 3.11799964e+01]\n",
      "28-th iteration, loss: 0.6598180968911244, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.631056543952492e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701854e+00 0.00000000e+00 3.18495230e-15 5.52371235e+01\n",
      " 3.11799964e+01]\n",
      "29-th iteration, loss: 0.6598180968911206, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.296301334843892e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970175  55.23712405 31.17999637]\n",
      "30-th iteration, loss: 0.6598180968911174, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2984301618965836e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701647e+00 0.00000000e+00 1.67227343e-15 5.52371243e+01\n",
      " 3.11799964e+01]\n",
      "31-th iteration, loss: 0.659818096891114, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0278873227500972e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701547e+00 0.00000000e+00 2.53269627e-15 5.52371247e+01\n",
      " 3.11799964e+01]\n",
      "32-th iteration, loss: 0.6598180968911109, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8138963988199931e-07\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701449 55.23712515 31.17999637]\n",
      "33-th iteration, loss: 0.659818096891108, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8544921452134612e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701353e+00 0.00000000e+00 2.69922973e-15 5.52371253e+01\n",
      " 3.11799964e+01]\n",
      "34-th iteration, loss: 0.6598180968911053, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6685252802862932e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701260e+00 0.00000000e+00 3.18495230e-15 5.52371257e+01\n",
      " 3.11799964e+01]\n",
      "35-th iteration, loss: 0.6598180968911026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.519861618813484e-07\n",
      "35-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701168 55.23712603 31.17999637]\n",
      "36-th iteration, loss: 0.6598180968911, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5775153351916073e-07\n",
      "36-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701079 55.23712619 31.17999637]\n",
      "37-th iteration, loss: 0.6598180968910977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6223860037682135e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700991e+00 0.00000000e+00 2.07472928e-15 5.52371263e+01\n",
      " 3.11799964e+01]\n",
      "38-th iteration, loss: 0.6598180968910954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4674725191795938e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700907e+00 0.00000000e+00 2.53269627e-15 5.52371267e+01\n",
      " 3.11799964e+01]\n",
      "39-th iteration, loss: 0.6598180968910933, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3431900200434232e-07\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700823e+00 0.00000000e+00 1.02695630e-15 5.52371270e+01\n",
      " 3.11799964e+01]\n",
      "40-th iteration, loss: 0.659818096891091, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.242533814663726e-07\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700742 55.23712724 31.17999637]\n",
      "41-th iteration, loss: 0.6598180968910892, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3057771209219962e-07\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700663e+00 0.00000000e+00 8.11850587e-16 5.52371274e+01\n",
      " 3.11799964e+01]\n",
      "42-th iteration, loss: 0.6598180968910873, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2036481789831688e-07\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700585e+00 0.00000000e+00 2.15105711e-16 5.52371276e+01\n",
      " 3.11799964e+01]\n",
      "43-th iteration, loss: 0.6598180968910855, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1203633676906384e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700510e+00 0.00000000e+00 2.13717932e-15 5.52371279e+01\n",
      " 3.11799964e+01]\n",
      "44-th iteration, loss: 0.6598180968910837, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0516522260259463e-07\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700436 55.23712809 31.17999637]\n",
      "45-th iteration, loss: 0.6598180968910822, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1178958427987709e-07\n",
      "45-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700364 55.2371282  31.17999637]\n",
      "46-th iteration, loss: 0.6598180968910806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1714586304332269e-07\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700293e+00 0.00000000e+00 1.88737914e-15 5.52371283e+01\n",
      " 3.11799964e+01]\n",
      "47-th iteration, loss: 0.6598180968910792, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0771957673484245e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700225e+00 0.00000000e+00 4.23272528e-16 5.52371285e+01\n",
      " 3.11799964e+01]\n",
      "48-th iteration, loss: 0.6598180968910778, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0005184769999815e-07\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700158 55.23712876 31.17999637]\n",
      "49-th iteration, loss: 0.6598180968910764, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0548176794838784e-07\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700093 55.23712886 31.17999637]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355636\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.39870033  0.         56.10381223]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2783751519858121e-16\n",
      "1-th iteration, new layer inserted. now 3 layers\n",
      "[42.89280919 62.32063131 56.10381223]\n",
      "2-th iteration, loss: 0.6614980133516497, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.507305171713015e-16\n",
      "2-th iteration, new layer inserted. now 3 layers\n",
      "[ 8.08544013 54.78446635 56.10381223]\n",
      "3-th iteration, loss: 0.6598183807845738, 30 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.8647711019298356e-18\n",
      "3-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.86777631 55.24408677 56.10381223]\n",
      "4-th iteration, loss: 0.6598180970048395, 31 gd steps\n",
      "insert gradient: -9.322925055799605e-05\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[1.79696278e+00 0.00000000e+00 4.23272528e-16 5.52363331e+01\n",
      " 5.61038122e+01]\n",
      "5-th iteration, loss: 0.6598180968912126, 12 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6036420502801359e-06\n",
      "5-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703282 55.23710872 56.10381223]\n",
      "6-th iteration, loss: 0.6598180968912016, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.44846919806782e-06\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703172 55.23711031 56.10381223]\n",
      "7-th iteration, loss: 0.6598180968911918, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3112529321602298e-06\n",
      "7-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703061 55.23711175 56.10381223]\n",
      "8-th iteration, loss: 0.6598180968911831, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1898274767332669e-06\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702949e+00 0.00000000e+00 5.89805982e-16 5.52371130e+01\n",
      " 5.61038122e+01]\n",
      "9-th iteration, loss: 0.6598180968911718, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.479138085369971e-07\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702838e+00 0.00000000e+00 5.89805982e-16 5.52371154e+01\n",
      " 5.61038122e+01]\n",
      "10-th iteration, loss: 0.6598180968911629, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.615136892422476e-07\n",
      "10-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702724 55.23711726 56.10381223]\n",
      "11-th iteration, loss: 0.6598180968911572, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.040585346958925e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702609e+00 0.00000000e+00 8.11850587e-16 5.52371180e+01\n",
      " 5.61038122e+01]\n",
      "12-th iteration, loss: 0.6598180968911507, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.728786329349799e-07\n",
      "12-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702495 55.2371194  56.10381223]\n",
      "13-th iteration, loss: 0.6598180968911459, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.366354990340606e-07\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702381 55.23711997 56.10381223]\n",
      "14-th iteration, loss: 0.6598180968911412, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.040135754126766e-07\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970227  55.23712051 56.10381223]\n",
      "15-th iteration, loss: 0.6598180968911369, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.745903844858363e-07\n",
      "15-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7970216  55.23712101 56.10381223]\n",
      "16-th iteration, loss: 0.6598180968911329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.479945412715544e-07\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702053 55.23712148 56.10381223]\n",
      "17-th iteration, loss: 0.6598180968911289, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2389944214508384e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701947e+00 0.00000000e+00 2.15105711e-16 5.52371219e+01\n",
      " 5.61038122e+01]\n",
      "18-th iteration, loss: 0.6598180968911247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.536456363914294e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701844e+00 0.00000000e+00 8.11850587e-16 5.52371228e+01\n",
      " 5.61038122e+01]\n",
      "19-th iteration, loss: 0.6598180968911209, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.990655607186766e-07\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701742e+00 0.00000000e+00 3.35148576e-15 5.52371235e+01\n",
      " 5.61038122e+01]\n",
      "20-th iteration, loss: 0.6598180968911173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5652337607789277e-07\n",
      "20-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701641 55.23712406 56.10381223]\n",
      "21-th iteration, loss: 0.6598180968911141, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.527909192950026e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701542 55.23712432 56.10381223]\n",
      "22-th iteration, loss: 0.6598180968911111, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.488608351776035e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701445 55.23712457 56.10381223]\n",
      "23-th iteration, loss: 0.6598180968911084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4477267964964485e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701351e+00 0.00000000e+00 3.55965257e-15 5.52371248e+01\n",
      " 5.61038122e+01]\n",
      "24-th iteration, loss: 0.6598180968911054, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1237394617598347e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701259e+00 0.00000000e+00 2.74086309e-15 5.52371253e+01\n",
      " 5.61038122e+01]\n",
      "25-th iteration, loss: 0.6598180968911027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8692172484483314e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701168e+00 0.00000000e+00 1.90819582e-15 5.52371257e+01\n",
      " 5.61038122e+01]\n",
      "26-th iteration, loss: 0.6598180968911002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6681187462588299e-07\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701079e+00 0.00000000e+00 1.46410661e-15 5.52371261e+01\n",
      " 5.61038122e+01]\n",
      "27-th iteration, loss: 0.6598180968910977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5081371253473397e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700992e+00 0.00000000e+00 1.02695630e-15 5.52371264e+01\n",
      " 5.61038122e+01]\n",
      "28-th iteration, loss: 0.6598180968910954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3798307243895285e-07\n",
      "28-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700907 55.23712674 56.10381223]\n",
      "29-th iteration, loss: 0.6598180968910933, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4373106142749183e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700824 55.23712688 56.10381223]\n",
      "30-th iteration, loss: 0.6598180968910913, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4824377960125475e-07\n",
      "30-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700743 55.23712703 56.10381223]\n",
      "31-th iteration, loss: 0.6598180968910893, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5168440570198007e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700664e+00 0.00000000e+00 1.04777298e-15 5.52371272e+01\n",
      " 5.61038122e+01]\n",
      "32-th iteration, loss: 0.6598180968910874, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3657351486862967e-07\n",
      "32-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700587 55.23712748 56.10381223]\n",
      "33-th iteration, loss: 0.6598180968910856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4040245408608427e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700512e+00 0.00000000e+00 3.78169718e-15 5.52371276e+01\n",
      " 5.61038122e+01]\n",
      "34-th iteration, loss: 0.659818096891084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2695104700972158e-07\n",
      "34-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700438 55.2371279  56.10381223]\n",
      "35-th iteration, loss: 0.6598180968910823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3096913907176313e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700367e+00 0.00000000e+00 3.55965257e-15 5.52371280e+01\n",
      " 5.61038122e+01]\n",
      "36-th iteration, loss: 0.6598180968910807, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1879261716718422e-07\n",
      "36-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700297 55.23712829 56.10381223]\n",
      "37-th iteration, loss: 0.6598180968910793, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2286911857384983e-07\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700228 55.23712841 56.10381223]\n",
      "38-th iteration, loss: 0.659818096891078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.260090710048752e-07\n",
      "38-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700162 55.23712853 56.10381223]\n",
      "39-th iteration, loss: 0.6598180968910766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2833705267791535e-07\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700097e+00 0.00000000e+00 1.02695630e-15 5.52371287e+01\n",
      " 5.61038122e+01]\n",
      "40-th iteration, loss: 0.6598180968910753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1506731582963412e-07\n",
      "40-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700034 55.23712891 56.10381223]\n",
      "41-th iteration, loss: 0.6598180968910742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1787797628036033e-07\n",
      "41-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699972 55.23712903 56.10381223]\n",
      "42-th iteration, loss: 0.6598180968910731, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1994712387416712e-07\n",
      "42-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699912 55.23712915 56.10381223]\n",
      "43-th iteration, loss: 0.659818096891072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2137456836738731e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699853e+00 0.00000000e+00 1.67227343e-15 5.52371293e+01\n",
      " 5.61038122e+01]\n",
      "44-th iteration, loss: 0.659818096891071, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0818108625124424e-07\n",
      "44-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699796 55.23712951 56.10381223]\n",
      "45-th iteration, loss: 0.65981809689107, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1026968936725593e-07\n",
      "45-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969974  55.23712962 56.10381223]\n",
      "46-th iteration, loss: 0.659818096891069, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1174177457788375e-07\n",
      "46-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699686 55.23712973 56.10381223]\n",
      "47-th iteration, loss: 0.6598180968910681, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.126810585590531e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699632e+00 0.00000000e+00 1.92207361e-15 5.52371298e+01\n",
      " 5.61038122e+01]\n",
      "48-th iteration, loss: 0.6598180968910673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0011173439508334e-07\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699581 55.23713007 56.10381223]\n",
      "49-th iteration, loss: 0.6598180968910664, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0176682799972635e-07\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.7969953  55.23713017 56.10381223]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235569\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[44.39870033  0.         81.22944037]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2432348551227353e-16\n",
      "1-th iteration, new layer inserted. now 3 layers\n",
      "[42.89280919 62.32063131 81.22944037]\n",
      "2-th iteration, loss: 0.6614980133516497, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.812223874962927e-15\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[8.08544013e+00 5.47844664e+01 0.00000000e+00 8.08242362e-14\n",
      " 8.12294404e+01]\n",
      "3-th iteration, loss: 0.6600317106660547, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.14375534259676e-15\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[7.99154559e-02 5.63837005e+01 0.00000000e+00 8.59312621e-14\n",
      " 8.12294404e+01]\n",
      "4-th iteration, loss: 0.6598182337738671, 28 gd steps\n",
      "insert gradient: -0.003208067918111697\n",
      "4-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.78943183 55.21113106 81.22944037]\n",
      "5-th iteration, loss: 0.6598181090174507, 8 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.8926260933401645e-15\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[1.79933416e+00 5.52448434e+01 0.00000000e+00 6.03961325e-14\n",
      " 8.12294404e+01]\n",
      "6-th iteration, loss: 0.6598180968911934, 7 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.5562500280027116e-07\n",
      "6-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703541 55.23711806 81.22944037]\n",
      "7-th iteration, loss: 0.6598180968911866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.403167056127459e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703398e+00 0.00000000e+00 1.24206201e-15 5.52371185e+01\n",
      " 8.12294404e+01]\n",
      "8-th iteration, loss: 0.6598180968911798, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7542264101091106e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[1.79703259e+00 0.00000000e+00 1.46410661e-15 5.52371194e+01\n",
      " 8.12294404e+01]\n",
      "9-th iteration, loss: 0.6598180968911733, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.247278513368432e-07\n",
      "9-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79703122 55.23712013 81.22944037]\n",
      "10-th iteration, loss: 0.6598180968911675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.224398091968964e-07\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702987e+00 0.00000000e+00 2.10942375e-15 5.52371205e+01\n",
      " 8.12294404e+01]\n",
      "11-th iteration, loss: 0.6598180968911618, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.823357574441111e-07\n",
      "11-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702855 55.2371211  81.22944037]\n",
      "12-th iteration, loss: 0.6598180968911564, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.834114019849022e-07\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702726e+00 0.00000000e+00 2.28289609e-15 5.52371214e+01\n",
      " 8.12294404e+01]\n",
      "13-th iteration, loss: 0.6598180968911513, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.507276905423268e-07\n",
      "13-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.797026   55.23712195 81.22944037]\n",
      "14-th iteration, loss: 0.6598180968911466, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.539438997843736e-07\n",
      "14-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702476 55.2371222  81.22944037]\n",
      "15-th iteration, loss: 0.659818096891142, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5596769411086797e-07\n",
      "15-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702356 55.23712246 81.22944037]\n",
      "16-th iteration, loss: 0.6598180968911377, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.569633775459286e-07\n",
      "16-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702239 55.23712271 81.22944037]\n",
      "17-th iteration, loss: 0.6598180968911336, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5707476943174e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[1.79702125e+00 0.00000000e+00 1.07552856e-15 5.52371230e+01\n",
      " 8.12294404e+01]\n",
      "18-th iteration, loss: 0.6598180968911295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2670975559073653e-07\n",
      "18-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79702013 55.23712348 81.22944037]\n",
      "19-th iteration, loss: 0.6598180968911257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2899126890603915e-07\n",
      "19-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701904 55.23712371 81.22944037]\n",
      "20-th iteration, loss: 0.6598180968911221, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3028582441794517e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701798e+00 0.00000000e+00 3.18495230e-15 5.52371239e+01\n",
      " 8.12294404e+01]\n",
      "21-th iteration, loss: 0.6598180968911187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0407831508847308e-07\n",
      "21-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701694 55.2371244  81.22944037]\n",
      "22-th iteration, loss: 0.6598180968911155, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.070010194276275e-07\n",
      "22-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701592 55.2371246  81.22944037]\n",
      "23-th iteration, loss: 0.6598180968911124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0890878117249461e-07\n",
      "23-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701493 55.23712481 81.22944037]\n",
      "24-th iteration, loss: 0.6598180968911095, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0994091056933794e-07\n",
      "24-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701397 55.23712502 81.22944037]\n",
      "25-th iteration, loss: 0.6598180968911067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1021937977336854e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701303e+00 0.00000000e+00 1.07552856e-15 5.52371252e+01\n",
      " 8.12294404e+01]\n",
      "26-th iteration, loss: 0.6598180968911039, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.855445940165025e-07\n",
      "26-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701211 55.23712565 81.22944037]\n",
      "27-th iteration, loss: 0.6598180968911014, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8754809822067574e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[1.79701121e+00 0.00000000e+00 2.35228503e-15 5.52371258e+01\n",
      " 8.12294404e+01]\n",
      "28-th iteration, loss: 0.6598180968910988, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6699345501771797e-07\n",
      "28-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79701033 55.23712621 81.22944037]\n",
      "29-th iteration, loss: 0.6598180968910966, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7007203353320814e-07\n",
      "29-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700947 55.23712638 81.22944037]\n",
      "30-th iteration, loss: 0.6598180968910944, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7222010691256964e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700864e+00 0.00000000e+00 2.53269627e-15 5.52371265e+01\n",
      " 8.12294404e+01]\n",
      "31-th iteration, loss: 0.6598180968910923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5360165451289382e-07\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700782e+00 0.00000000e+00 1.46410661e-15 5.52371269e+01\n",
      " 8.12294404e+01]\n",
      "32-th iteration, loss: 0.6598180968910902, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3879477723976764e-07\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700702e+00 0.00000000e+00 2.08860707e-15 5.52371272e+01\n",
      " 8.12294404e+01]\n",
      "33-th iteration, loss: 0.6598180968910883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2692404723932084e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700624e+00 0.00000000e+00 2.15105711e-16 5.52371275e+01\n",
      " 8.12294404e+01]\n",
      "34-th iteration, loss: 0.6598180968910864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1731778951375393e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700548e+00 0.00000000e+00 2.53269627e-15 5.52371277e+01\n",
      " 8.12294404e+01]\n",
      "35-th iteration, loss: 0.6598180968910846, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.094605399412967e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700473e+00 0.00000000e+00 2.10942375e-15 5.52371280e+01\n",
      " 8.12294404e+01]\n",
      "36-th iteration, loss: 0.659818096891083, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0295671605909823e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700400e+00 0.00000000e+00 1.88737914e-15 5.52371282e+01\n",
      " 8.12294404e+01]\n",
      "37-th iteration, loss: 0.6598180968910814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.750255558156067e-08\n",
      "37-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700329 55.2371284  81.22944037]\n",
      "38-th iteration, loss: 0.65981809689108, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0435118917547732e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700259e+00 0.00000000e+00 1.46410661e-15 5.52371285e+01\n",
      " 8.12294404e+01]\n",
      "39-th iteration, loss: 0.6598180968910785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.768143019628885e-08\n",
      "39-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79700191 55.23712871 81.22944037]\n",
      "40-th iteration, loss: 0.6598180968910772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0361248125253091e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700125e+00 0.00000000e+00 1.92207361e-15 5.52371288e+01\n",
      " 8.12294404e+01]\n",
      "41-th iteration, loss: 0.6598180968910758, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.626745612044189e-08\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[1.79700061e+00 0.00000000e+00 2.94902991e-15 5.52371290e+01\n",
      " 8.12294404e+01]\n",
      "42-th iteration, loss: 0.6598180968910745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.022220009921526e-08\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699998e+00 0.00000000e+00 1.67227343e-15 5.52371292e+01\n",
      " 8.12294404e+01]\n",
      "43-th iteration, loss: 0.6598180968910734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.518393253626606e-08\n",
      "43-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699936 55.23712939 81.22944037]\n",
      "44-th iteration, loss: 0.6598180968910723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.095649905957464e-08\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699876e+00 0.00000000e+00 1.68615122e-15 5.52371295e+01\n",
      " 8.12294404e+01]\n",
      "45-th iteration, loss: 0.6598180968910713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.497915564933918e-08\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699818e+00 0.00000000e+00 1.88737914e-15 5.52371297e+01\n",
      " 8.12294404e+01]\n",
      "46-th iteration, loss: 0.6598180968910702, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.002163814941832e-08\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[1.79699761e+00 0.00000000e+00 2.07472928e-15 5.52371298e+01\n",
      " 8.12294404e+01]\n",
      "47-th iteration, loss: 0.6598180968910693, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.585541719245531e-08\n",
      "47-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699705 55.23712999 81.22944037]\n",
      "48-th iteration, loss: 0.6598180968910683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.124295476411425e-08\n",
      "48-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699651 55.23713007 81.22944037]\n",
      "49-th iteration, loss: 0.6598180968910675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.562711576522187e-08\n",
      "49-th iteration, new layer inserted. now 3 layers\n",
      "[ 1.79699598 55.23713015 81.22944037]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248375\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.19688805   0.         106.55688079]\n",
      "1-th iteration, loss: 0.7487306396172312, 11 gd steps\n",
      "insert gradient: -0.4426815814718002\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[ 42.74722856  62.33225191 106.55688079   0.        ]\n",
      "2-th iteration, loss: 0.6620460808349823, 13 gd steps\n",
      "insert gradient: -0.19515671517286143\n",
      "2-th iteration, new layer inserted. now 6 layers\n",
      "[8.67643970e+00 5.72808245e+01 1.09236238e+02 0.00000000e+00\n",
      " 7.06101844e-14 6.54319283e+00]\n",
      "3-th iteration, loss: 0.559528716994699, 18 gd steps\n",
      "insert gradient: -0.05828571445518015\n",
      "3-th iteration, new layer inserted. now 6 layers\n",
      "[2.58755478e+01 6.32132346e+01 0.00000000e+00 3.59712260e-14\n",
      " 8.66327832e+01 1.28286210e+02]\n",
      "4-th iteration, loss: 0.5121896352851273, 88 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.371603278151749e-18\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[  0.88549031  57.63878907  98.18852654 104.2947256    0.        ]\n",
      "5-th iteration, loss: 0.509132790197679, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[5.23225683e+00 5.34194613e+01 9.88913595e+01 9.92933165e+01\n",
      " 6.84992590e-17 0.00000000e+00 3.28796443e-15]\n",
      "6-th iteration, loss: 0.5091165531826751, 31 gd steps\n",
      "insert gradient: -0.00011495806574585649\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[4.61874413e+00 5.34185231e+01 9.90729953e+01 9.94105245e+01\n",
      " 1.51912838e-15]\n",
      "7-th iteration, loss: 0.5091165386165154, 20 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.423024500897606e-06\n",
      "7-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116327e+00 0.00000000e+00 1.66533454e-15 5.34267348e+01\n",
      " 9.90616161e+01 9.94166678e+01]\n",
      "8-th iteration, loss: 0.5091165386161828, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.803088718343046e-06\n",
      "8-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116392e+00 5.02126764e-06 6.41747066e-07 5.34267398e+01\n",
      " 9.90616197e+01 0.00000000e+00 3.10862447e-14 9.94166728e+01]\n",
      "9-th iteration, loss: 0.5091165386158576, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.364901857062935e-06\n",
      "9-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116421e+00 8.54060611e-06 6.19843775e-07 5.34267434e+01\n",
      " 9.90616227e+01 4.62800696e-06 3.02339803e-06 9.94166774e+01]\n",
      "10-th iteration, loss: 0.5091165386156401, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.989503425245668e-06\n",
      "10-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116425e+00 1.09648723e-05 1.24110440e-07 5.34267458e+01\n",
      " 9.90616251e+01 8.70025236e-06 5.21417490e-06 0.00000000e+00\n",
      " 6.35274710e-22 9.94166816e+01]\n",
      "11-th iteration, loss: 0.5091165386154303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5119234386950014e-06\n",
      "11-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116412e+00 5.34267601e+01 9.90616271e+01 1.22650158e-05\n",
      " 6.72016048e-06 3.82234078e-06 1.50598558e-06 9.94166854e+01]\n",
      "12-th iteration, loss: 0.509116538615293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1230061964095837e-06\n",
      "12-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116386e+00 5.34267612e+01 9.90616286e+01 1.52420050e-05\n",
      " 7.61780514e-06 7.13130225e-06 2.21480875e-06 9.94166888e+01]\n",
      "13-th iteration, loss: 0.5091165386151915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.797067661313409e-06\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116356e+00 5.34267621e+01 9.90616298e+01 1.77723018e-05\n",
      " 8.08497509e-06 1.00379144e-05 2.32969641e-06 0.00000000e+00\n",
      " 2.64697796e-22 9.94166918e+01]\n",
      "14-th iteration, loss: 0.5091165386150864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.429880744543104e-06\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116321e+00 5.34267627e+01 9.90616308e+01 1.99381168e-05\n",
      " 8.20795077e-06 1.26031227e-05 1.95680486e-06 0.00000000e+00\n",
      " 3.97046694e-22 9.94166972e+01]\n",
      "15-th iteration, loss: 0.5091165386150052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1236147243095274e-06\n",
      "15-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116285e+00 5.34267633e+01 9.90616316e+01 2.17711990e-05\n",
      " 8.04102252e-06 1.48416731e-05 1.16728960e-06 9.94167019e+01]\n",
      "16-th iteration, loss: 0.5091165386149534, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.935195330584988e-06\n",
      "16-th iteration, new layer inserted. now 10 layers\n",
      "[4.60116247e+00 5.34267638e+01 9.90616323e+01 2.33789250e-05\n",
      " 7.65549404e-06 1.68466212e-05 4.85910346e-08 0.00000000e+00\n",
      " 2.02659250e-23 9.94167039e+01]\n",
      "17-th iteration, loss: 0.5091165386149008, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6843957131856278e-06\n",
      "17-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116210e+00 5.34267643e+01 9.90616329e+01 2.48661077e-05\n",
      " 7.12884113e-06 0.00000000e+00 2.27640105e-21 9.94167264e+01]\n",
      "18-th iteration, loss: 0.5091165386148694, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5266194695243366e-06\n",
      "18-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116172e+00 5.34267646e+01 9.90616334e+01 2.61509063e-05\n",
      " 6.40630722e-06 0.00000000e+00 5.55865372e-21 9.94167296e+01]\n",
      "19-th iteration, loss: 0.5091165386148423, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3865458720533173e-06\n",
      "19-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116134e+00 5.34267650e+01 9.90616339e+01 2.73200897e-05\n",
      " 5.55413710e-06 9.94167326e+01]\n",
      "20-th iteration, loss: 0.5091165386148251, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3083936450747006e-06\n",
      "20-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116095e+00 5.34267653e+01 9.90616343e+01 2.84066984e-05\n",
      " 4.59127683e-06 0.00000000e+00 9.26442286e-22 9.94167340e+01]\n",
      "21-th iteration, loss: 0.5091165386148024, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1912016160581643e-06\n",
      "21-th iteration, new layer inserted. now 6 layers\n",
      "[4.60116057e+00 5.34267656e+01 9.90616346e+01 2.94556976e-05\n",
      " 3.55449715e-06 9.94167365e+01]\n",
      "22-th iteration, loss: 0.5091165386147867, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1263253618230792e-06\n",
      "22-th iteration, new layer inserted. now 8 layers\n",
      "[4.60116020e+00 5.34267659e+01 9.90616350e+01 3.04501049e-05\n",
      " 2.42890028e-06 0.00000000e+00 3.04402465e-22 9.94167377e+01]\n",
      "23-th iteration, loss: 0.5091165386147667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0266642321720043e-06\n",
      "23-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115983e+00 5.34267662e+01 9.90616353e+01 3.14301874e-05\n",
      " 1.24875758e-06 0.00000000e+00 4.76456033e-22 9.94167399e+01]\n",
      "24-th iteration, loss: 0.5091165386147479, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.364258856559679e-07\n",
      "24-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115946 53.42676654 99.06163559 99.41677428]\n",
      "25-th iteration, loss: 0.5091165386147434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.991923982916023e-07\n",
      "25-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011591  53.42676682 99.06163587 99.4167752 ]\n",
      "26-th iteration, loss: 0.5091165386147395, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.638275184299479e-07\n",
      "26-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115873e+00 5.34267671e+01 9.90616361e+01 0.00000000e+00\n",
      " 6.70574707e-14 9.94167761e+01]\n",
      "27-th iteration, loss: 0.5091165386147329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.97970191530365e-07\n",
      "27-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115836e+00 5.34267673e+01 9.90616363e+01 8.42342867e-07\n",
      " 2.12014835e-07 0.00000000e+00 8.76811449e-23 9.94167769e+01]\n",
      "28-th iteration, loss: 0.5091165386147254, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.10910080641519e-07\n",
      "28-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115799e+00 5.34267674e+01 9.90616365e+01 1.60189596e-06\n",
      " 3.34243607e-07 7.70026519e-07 1.22228772e-07 9.94167777e+01]\n",
      "29-th iteration, loss: 0.5091165386147193, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.371321430788544e-07\n",
      "29-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115761e+00 5.34267676e+01 9.90616366e+01 2.26671788e-06\n",
      " 3.70108621e-07 1.45135992e-06 1.20054845e-07 9.94167784e+01]\n",
      "30-th iteration, loss: 0.5091165386147144, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.73882713845461e-07\n",
      "30-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115724e+00 5.34267677e+01 9.90616367e+01 2.85954812e-06\n",
      " 3.38322489e-07 2.06247335e-06 1.65722312e-08 9.94167790e+01]\n",
      "31-th iteration, loss: 0.5091165386147103, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.164636370836422e-07\n",
      "31-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115687e+00 5.34267678e+01 9.90616367e+01 3.39851612e-06\n",
      " 2.53204446e-07 0.00000000e+00 8.76811449e-23 9.94167822e+01]\n",
      "32-th iteration, loss: 0.5091165386147067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.655301415795781e-07\n",
      "32-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115651e+00 5.34267679e+01 9.90616368e+01 3.88699507e-06\n",
      " 1.18563674e-07 9.94167832e+01]\n",
      "33-th iteration, loss: 0.5091165386147045, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3510276020063646e-07\n",
      "33-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115615e+00 5.34267680e+01 9.90616368e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94167880e+01]\n",
      "34-th iteration, loss: 0.5091165386147026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.049709176162558e-07\n",
      "34-th iteration, new layer inserted. now 8 layers\n",
      "[4.60115580e+00 5.34267681e+01 9.90616368e+01 4.25474397e-07\n",
      " 4.82479656e-09 0.00000000e+00 2.68833699e-24 9.94167884e+01]\n",
      "35-th iteration, loss: 0.5091165386147003, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6380455506712807e-07\n",
      "35-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115545 53.42676821 99.06163679 99.41679   ]\n",
      "36-th iteration, loss: 0.5091165386146992, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.514181003698071e-07\n",
      "36-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011551  53.42676831 99.06163676 99.41679036]\n",
      "37-th iteration, loss: 0.5091165386146983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.395372064381214e-07\n",
      "37-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115476e+00 5.34267684e+01 9.90616367e+01 0.00000000e+00\n",
      " 7.23865412e-14 9.94167907e+01]\n",
      "38-th iteration, loss: 0.509116538614697, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1662111216918323e-07\n",
      "38-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115442 53.42676849 99.0616367  99.41679137]\n",
      "39-th iteration, loss: 0.5091165386146961, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0614953764134813e-07\n",
      "39-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115409 53.42676858 99.06163666 99.41679169]\n",
      "40-th iteration, loss: 0.5091165386146953, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.960917428428432e-07\n",
      "40-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115376 53.42676866 99.06163661 99.41679199]\n",
      "41-th iteration, loss: 0.5091165386146945, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.864289144299447e-07\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115344 53.42676875 99.06163657 99.41679228]\n",
      "42-th iteration, loss: 0.5091165386146937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7714332816428173e-07\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115312 53.42676883 99.06163651 99.41679257]\n",
      "43-th iteration, loss: 0.5091165386146931, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6821818436630965e-07\n",
      "43-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115281e+00 5.34267689e+01 9.90616365e+01 0.00000000e+00\n",
      " 1.11466392e-13 9.94167928e+01]\n",
      "44-th iteration, loss: 0.5091165386146921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5052121375976044e-07\n",
      "44-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115250e+00 5.34267690e+01 9.90616364e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167934e+01]\n",
      "45-th iteration, loss: 0.5091165386146912, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3412202320982744e-07\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115219 53.42676909 99.06163634 99.41679386]\n",
      "46-th iteration, loss: 0.5091165386146906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2689000049931032e-07\n",
      "46-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115189e+00 5.34267692e+01 9.90616363e+01 0.00000000e+00\n",
      " 1.37667655e-14 9.94167941e+01]\n",
      "47-th iteration, loss: 0.5091165386146899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.122009734956615e-07\n",
      "47-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115159e+00 5.34267693e+01 9.90616362e+01 0.00000000e+00\n",
      " 6.70574707e-14 9.94167945e+01]\n",
      "48-th iteration, loss: 0.5091165386146892, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9858021900163887e-07\n",
      "48-th iteration, new layer inserted. now 6 layers\n",
      "[4.60115130e+00 5.34267693e+01 9.90616361e+01 0.00000000e+00\n",
      " 4.84057239e-14 9.94167950e+01]\n",
      "49-th iteration, loss: 0.5091165386146885, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8594621959260348e-07\n",
      "49-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60115101 53.42676942 99.06163605 99.41679534]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536802874516846\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         131.37979052]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6205102372394407\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 123.36882768   0.           8.01096284]\n",
      "2-th iteration, loss: 0.5091175827746208, 55 gd steps\n",
      "insert gradient: -0.002857187333261249\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.59861158 53.40007459 99.06864677 99.52958242  8.01096284]\n",
      "3-th iteration, loss: 0.5091165386152302, 33 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.065014823455803e-06\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118862 53.42673467 99.06159311 99.41683838  8.01096284]\n",
      "4-th iteration, loss: 0.5091165386151245, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.352388968971235e-06\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118890e+00 0.00000000e+00 2.08166817e-15 5.34267396e+01\n",
      " 9.90615950e+01 9.94168378e+01 8.01096284e+00]\n",
      "5-th iteration, loss: 0.5091165386149921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.232676016093853e-06\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118900e+00 4.08078894e-06 9.96726995e-08 5.34267436e+01\n",
      " 9.90615966e+01 9.94168371e+01 8.01096284e+00]\n",
      "6-th iteration, loss: 0.5091165386149171, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4101032066933867e-06\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118886 53.42675379 99.06159777 99.41683641  8.01096284]\n",
      "7-th iteration, loss: 0.5091165386148906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0799708419487044e-06\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118856 53.42675612 99.06159878 99.41683575  8.01096284]\n",
      "8-th iteration, loss: 0.50911653861487, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.797436281221257e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118820e+00 0.00000000e+00 3.30291350e-15 5.34267581e+01\n",
      " 9.90615997e+01 9.94168351e+01 8.01096284e+00]\n",
      "9-th iteration, loss: 0.5091165386148444, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3447067099779261e-06\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118777 53.42676151 99.06160043 99.41683442  8.01096284]\n",
      "10-th iteration, loss: 0.5091165386148336, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1676369019154933e-06\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118725 53.42676281 99.06160108 99.41683376  8.01096284]\n",
      "11-th iteration, loss: 0.5091165386148246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.015883006683058e-06\n",
      "11-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118671e+00 0.00000000e+00 1.41553436e-15 5.34267639e+01\n",
      " 9.90616017e+01 9.94168331e+01 8.01096284e+00]\n",
      "12-th iteration, loss: 0.509116538614814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.657251567217315e-07\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118613e+00 0.00000000e+00 4.16333634e-16 5.34267659e+01\n",
      " 9.90616022e+01 9.94168325e+01 8.01096284e+00]\n",
      "13-th iteration, loss: 0.5091165386148063, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.80252989042138e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118551e+00 0.00000000e+00 4.16333634e-16 5.34267673e+01\n",
      " 9.90616026e+01 9.94168319e+01 8.01096284e+00]\n",
      "14-th iteration, loss: 0.5091165386148004, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4264042599018157e-07\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118486 53.42676842 99.06160304 99.41683124  8.01096284]\n",
      "15-th iteration, loss: 0.509116538614796, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.938157210106524e-07\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118419 53.42676886 99.0616034  99.41683065  8.01096284]\n",
      "16-th iteration, loss: 0.5091165386147919, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5168498873078e-07\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118352e+00 0.00000000e+00 4.30211422e-15 5.34267692e+01\n",
      " 9.90616037e+01 9.94168301e+01 8.01096284e+00]\n",
      "17-th iteration, loss: 0.5091165386147877, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0942488726450083e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60118285 53.42676992 99.06160408 99.4168295   8.01096284]\n",
      "18-th iteration, loss: 0.509116538614784, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0154222015168226e-07\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60118217e+00 5.34267702e+01 0.00000000e+00 6.39488462e-14\n",
      " 9.90616044e+01 9.94168289e+01 8.01096284e+00]\n",
      "19-th iteration, loss: 0.50911653861478, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.748371920290345e-07\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118149e+00 5.34267704e+01 2.94622120e-07 2.31646057e-07\n",
      " 0.00000000e+00 1.24077092e-22 9.90616047e+01 9.94168284e+01\n",
      " 8.01096284e+00]\n",
      "20-th iteration, loss: 0.509116538614776, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.415199316041322e-07\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118081e+00 5.34267706e+01 5.39791589e-07 3.71499970e-07\n",
      " 2.66609000e-07 1.39853914e-07 9.90616049e+01 9.94168279e+01\n",
      " 8.01096284e+00]\n",
      "21-th iteration, loss: 0.5091165386147726, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2274551535335392e-07\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[4.60118012e+00 5.34267707e+01 7.30487452e-07 4.30529932e-07\n",
      " 4.91688291e-07 1.74208420e-07 9.90616052e+01 9.94168273e+01\n",
      " 8.01096284e+00]\n",
      "22-th iteration, loss: 0.5091165386147692, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1151892776973463e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117944e+00 5.34267708e+01 8.96251415e-07 4.46002074e-07\n",
      " 6.97299083e-07 1.44173347e-07 0.00000000e+00 8.10637000e-23\n",
      " 9.90616054e+01 9.94168268e+01 8.01096284e+00]\n",
      "23-th iteration, loss: 0.5091165386147659, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9624036688738334e-07\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117876e+00 5.34267708e+01 1.05290479e-06 4.36596735e-07\n",
      " 8.95231276e-07 7.02308973e-08 0.00000000e+00 2.48154184e-24\n",
      " 9.90616058e+01 9.94168263e+01 8.01096284e+00]\n",
      "24-th iteration, loss: 0.5091165386147626, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8313467558046534e-07\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117809e+00 5.34267709e+01 1.20379980e-06 4.07442181e-07\n",
      " 0.00000000e+00 2.24993127e-22 9.90616073e+01 9.94168258e+01\n",
      " 8.01096284e+00]\n",
      "25-th iteration, loss: 0.5091165386147595, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7655558735565587e-07\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117744e+00 5.34267710e+01 1.35025486e-06 3.58302358e-07\n",
      " 9.90616077e+01 9.94168253e+01 8.01096284e+00]\n",
      "26-th iteration, loss: 0.5091165386147567, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7755431661296364e-07\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117678e+00 5.34267710e+01 1.49560118e-06 2.95600027e-07\n",
      " 0.00000000e+00 4.96308368e-23 9.90616079e+01 9.94168248e+01\n",
      " 8.01096284e+00]\n",
      "27-th iteration, loss: 0.5091165386147538, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7121360839419658e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117614e+00 5.34267711e+01 1.64721667e-06 2.26045760e-07\n",
      " 0.00000000e+00 9.09898674e-23 9.90616082e+01 9.94168244e+01\n",
      " 8.01096284e+00]\n",
      "28-th iteration, loss: 0.509116538614751, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6560633369781565e-07\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117551e+00 5.34267712e+01 1.79935117e-06 1.42958868e-07\n",
      " 9.90616086e+01 9.94168239e+01 8.01096284e+00]\n",
      "29-th iteration, loss: 0.5091165386147486, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6690065759079695e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117489e+00 5.34267713e+01 1.95380688e-06 4.76203432e-08\n",
      " 9.90616087e+01 9.94168235e+01 8.01096284e+00]\n",
      "30-th iteration, loss: 0.5091165386147462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.652087399148199e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60117428 53.42677133 99.06161101 99.41682302  8.01096284]\n",
      "31-th iteration, loss: 0.5091165386147438, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6765007312980443e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60117368e+00 5.34267714e+01 0.00000000e+00 1.90958360e-14\n",
      " 9.90616112e+01 9.94168226e+01 8.01096284e+00]\n",
      "32-th iteration, loss: 0.5091165386147415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6049020124714165e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117308e+00 5.34267715e+01 1.66622582e-07 7.79617363e-08\n",
      " 0.00000000e+00 8.27180613e-25 9.90616113e+01 9.94168222e+01\n",
      " 8.01096284e+00]\n",
      "33-th iteration, loss: 0.5091165386147392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.481815359800327e-07\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117249e+00 5.34267716e+01 3.17775016e-07 1.23534696e-07\n",
      " 1.58368026e-07 4.55729598e-08 9.90616115e+01 9.94168218e+01\n",
      " 8.01096284e+00]\n",
      "34-th iteration, loss: 0.5091165386147369, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.411428349713736e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[4.60117191e+00 5.34267716e+01 4.49906844e-07 1.38506273e-07\n",
      " 3.01933357e-07 4.58871053e-08 0.00000000e+00 2.23338765e-23\n",
      " 9.90616117e+01 9.94168214e+01 8.01096284e+00]\n",
      "35-th iteration, loss: 0.5091165386147347, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.316036532153377e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117133e+00 5.34267716e+01 5.73872857e-07 1.35482490e-07\n",
      " 4.38718537e-07 1.49184894e-08 9.90616119e+01 9.94168210e+01\n",
      " 8.01096284e+00]\n",
      "36-th iteration, loss: 0.5091165386147326, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2792405664785642e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117076e+00 5.34267717e+01 6.92834323e-07 1.19214329e-07\n",
      " 0.00000000e+00 6.20385459e-23 9.90616126e+01 9.94168206e+01\n",
      " 8.01096284e+00]\n",
      "37-th iteration, loss: 0.5091165386147307, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.241060528702231e-07\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[4.60117019e+00 5.34267717e+01 8.10667843e-07 9.29321542e-08\n",
      " 0.00000000e+00 5.54211010e-23 9.90616129e+01 9.94168202e+01\n",
      " 8.01096284e+00]\n",
      "38-th iteration, loss: 0.5091165386147288, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.207254343526074e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116964e+00 5.34267717e+01 9.27381293e-07 5.72298398e-08\n",
      " 9.90616131e+01 9.94168198e+01 8.01096284e+00]\n",
      "39-th iteration, loss: 0.5091165386147269, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2221040691575937e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116909e+00 5.34267718e+01 1.04451582e-06 1.36582818e-08\n",
      " 9.90616133e+01 9.94168195e+01 8.01096284e+00]\n",
      "40-th iteration, loss: 0.5091165386147252, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.217157069828043e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60116856 53.42677184 99.06161456 99.41681912  8.01096284]\n",
      "41-th iteration, loss: 0.5091165386147236, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2503560689220583e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116803e+00 5.34267719e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90616147e+01 9.94168188e+01 8.01096284e+00]\n",
      "42-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2114846805828598e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116750e+00 5.34267719e+01 1.24791342e-07 5.30460467e-08\n",
      " 0.00000000e+00 2.27474668e-23 9.90616148e+01 9.94168184e+01\n",
      " 8.01096284e+00]\n",
      "43-th iteration, loss: 0.5091165386147203, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1300347289066253e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[4.60116699e+00 5.34267720e+01 2.39843551e-07 8.52580876e-08\n",
      " 1.19961778e-07 3.22120410e-08 0.00000000e+00 2.89513214e-24\n",
      " 9.90616149e+01 9.94168181e+01 8.01096284e+00]\n",
      "44-th iteration, loss: 0.5091165386147187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0438194581151545e-07\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[4.60116648e+00 5.34267720e+01 3.41132163e-07 9.53757678e-08\n",
      " 2.29141279e-07 3.12269000e-08 0.00000000e+00 1.03397577e-23\n",
      " 9.90616152e+01 9.94168178e+01 8.01096284e+00]\n",
      "45-th iteration, loss: 0.5091165386147171, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.849898089296709e-08\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116597e+00 5.34267720e+01 4.34174263e-07 9.15505810e-08\n",
      " 3.31010691e-07 6.19400449e-09 9.90616154e+01 9.94168175e+01\n",
      " 8.01096284e+00]\n",
      "46-th iteration, loss: 0.5091165386147156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.632997765868812e-08\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116547e+00 5.34267721e+01 5.24856152e-07 8.02566584e-08\n",
      " 0.00000000e+00 2.39882378e-23 9.90616159e+01 9.94168172e+01\n",
      " 8.01096284e+00]\n",
      "47-th iteration, loss: 0.5091165386147143, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.414132212031372e-08\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116498e+00 5.34267721e+01 6.14680622e-07 6.22973296e-08\n",
      " 9.90616161e+01 9.94168169e+01 8.01096284e+00]\n",
      "48-th iteration, loss: 0.509116538614713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.563497077844682e-08\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60116450e+00 5.34267721e+01 7.04480400e-07 3.90990997e-08\n",
      " 9.90616162e+01 9.94168166e+01 8.01096284e+00]\n",
      "49-th iteration, loss: 0.5091165386147116, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.661302737413419e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[4.60116402e+00 5.34267722e+01 7.97762796e-07 1.34802419e-08\n",
      " 0.00000000e+00 1.16839262e-23 9.90616163e+01 9.94168163e+01\n",
      " 8.01096284e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355685\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         156.6063248 ]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6184134081367765\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 124.1391599    0.          32.4671649 ]\n",
      "2-th iteration, loss: 0.5091165615223171, 66 gd steps\n",
      "insert gradient: -0.0006350363902161577\n",
      "2-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.61191766 53.41584316 99.07248885 99.4238018  32.4671649 ]\n",
      "3-th iteration, loss: 0.5091165386236958, 19 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.114817208604025e-06\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6009542  53.42698916 99.06151165 99.41663291 32.4671649 ]\n",
      "4-th iteration, loss: 0.5091165386149656, 9 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0023778509777978e-16\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010349  53.42680407 99.06163953 99.41681291 32.4671649 ]\n",
      "5-th iteration, loss: 0.5091165386149524, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.037107531448759e-15\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103600e+00 5.34268029e+01 9.90616388e+01 9.94168124e+01\n",
      " 0.00000000e+00 1.00364161e-13 3.24671649e+01]\n",
      "6-th iteration, loss: 0.509116538614941, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.3845596693891704e-18\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60103711 53.42680187 99.06163811 99.41681191 32.4671649 ]\n",
      "7-th iteration, loss: 0.5091165386149306, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6297197084166053e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103823e+00 5.34268010e+01 9.90616375e+01 9.94168115e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "8-th iteration, loss: 0.5091165386149212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.954585203368061e-17\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60103936 53.42680017 99.06163692 99.41681103 32.4671649 ]\n",
      "9-th iteration, loss: 0.5091165386149126, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.156540759662093e-18\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104049 53.42679947 99.0616364  99.41681063 32.4671649 ]\n",
      "10-th iteration, loss: 0.5091165386149046, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.072849450698253e-17\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104163 53.42679884 99.06163593 99.41681024 32.4671649 ]\n",
      "11-th iteration, loss: 0.5091165386148969, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4897692592589256e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104276 53.42679827 99.06163549 99.41680988 32.4671649 ]\n",
      "12-th iteration, loss: 0.5091165386148899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1960588812721723e-17\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010439  53.42679776 99.06163508 99.41680954 32.4671649 ]\n",
      "13-th iteration, loss: 0.5091165386148832, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6009662303774784e-17\n",
      "13-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104502 53.4267973  99.06163471 99.41680922 32.4671649 ]\n",
      "14-th iteration, loss: 0.5091165386148768, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.120427388024693e-16\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104614 53.42679689 99.06163436 99.41680891 32.4671649 ]\n",
      "15-th iteration, loss: 0.5091165386148706, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.4864937949280603e-15\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60104725e+00 5.34267965e+01 9.90616340e+01 9.94168086e+01\n",
      " 0.00000000e+00 1.14575016e-13 3.24671649e+01]\n",
      "16-th iteration, loss: 0.5091165386148648, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.5182445022955804e-17\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104836 53.42679615 99.06163372 99.41680835 32.4671649 ]\n",
      "17-th iteration, loss: 0.5091165386148591, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.3714181369137146e-17\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60104945 53.42679582 99.06163344 99.41680809 32.4671649 ]\n",
      "18-th iteration, loss: 0.5091165386148536, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.59897560283836e-15\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105053e+00 5.34267955e+01 9.90616332e+01 9.94168078e+01\n",
      " 0.00000000e+00 8.43769499e-14 3.24671649e+01]\n",
      "19-th iteration, loss: 0.5091165386148484, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.122566036257092e-15\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105160e+00 5.34267952e+01 9.90616329e+01 9.94168076e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "20-th iteration, loss: 0.5091165386148433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.08234471792363e-17\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60105266 53.42679496 99.06163267 99.41680739 32.4671649 ]\n",
      "21-th iteration, loss: 0.5091165386148384, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1455776657445783e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105371e+00 5.34267947e+01 9.90616324e+01 9.94168072e+01\n",
      " 0.00000000e+00 7.46069873e-14 3.24671649e+01]\n",
      "22-th iteration, loss: 0.5091165386148336, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7181879823072777e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105474e+00 5.34267945e+01 9.90616322e+01 9.94168070e+01\n",
      " 0.00000000e+00 5.68434189e-14 3.24671649e+01]\n",
      "23-th iteration, loss: 0.5091165386148291, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2104799596131088e-15\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105577e+00 5.34267942e+01 9.90616320e+01 9.94168068e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "24-th iteration, loss: 0.5091165386148246, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2339313827730386e-15\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105678e+00 5.34267940e+01 9.90616318e+01 9.94168066e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "25-th iteration, loss: 0.5091165386148202, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6641455666890749e-15\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105777e+00 5.34267938e+01 9.90616317e+01 9.94168064e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "26-th iteration, loss: 0.5091165386148162, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5444073563777963e-17\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60105876 53.42679358 99.06163148 99.41680628 32.4671649 ]\n",
      "27-th iteration, loss: 0.5091165386148121, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2359532904889408e-15\n",
      "27-th iteration, new layer inserted. now 7 layers\n",
      "[4.60105973e+00 5.34267934e+01 9.90616313e+01 9.94168061e+01\n",
      " 0.00000000e+00 4.17443857e-14 3.24671649e+01]\n",
      "28-th iteration, loss: 0.5091165386148081, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.158103158173363e-15\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106069e+00 5.34267932e+01 9.90616311e+01 9.94168060e+01\n",
      " 0.00000000e+00 7.01660952e-14 3.24671649e+01]\n",
      "29-th iteration, loss: 0.5091165386148043, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.74350932681397e-18\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106164 53.42679298 99.06163099 99.41680585 32.4671649 ]\n",
      "30-th iteration, loss: 0.5091165386148007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7387975320274398e-17\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106257 53.42679279 99.06163085 99.41680571 32.4671649 ]\n",
      "31-th iteration, loss: 0.509116538614797, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.808985910564859e-17\n",
      "31-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106349 53.4267926  99.06163071 99.41680559 32.4671649 ]\n",
      "32-th iteration, loss: 0.5091165386147936, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.627467489908381e-15\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106440e+00 5.34267924e+01 9.90616306e+01 9.94168055e+01\n",
      " 0.00000000e+00 5.32907052e-14 3.24671649e+01]\n",
      "33-th iteration, loss: 0.5091165386147902, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6419578082163515e-15\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106530e+00 5.34267922e+01 9.90616304e+01 9.94168054e+01\n",
      " 0.00000000e+00 5.68434189e-14 3.24671649e+01]\n",
      "34-th iteration, loss: 0.5091165386147869, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.141600401327268e-17\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106618 53.42679206 99.06163032 99.41680525 32.4671649 ]\n",
      "35-th iteration, loss: 0.5091165386147837, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.132113495586646e-17\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60106706 53.42679188 99.0616302  99.41680515 32.4671649 ]\n",
      "36-th iteration, loss: 0.5091165386147806, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6636515059025463e-15\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106792e+00 5.34267917e+01 9.90616301e+01 9.94168051e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "37-th iteration, loss: 0.5091165386147776, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1923500900228173e-15\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106877e+00 5.34267915e+01 9.90616300e+01 9.94168050e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "38-th iteration, loss: 0.5091165386147747, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6687632442541796e-15\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60106960e+00 5.34267914e+01 9.90616299e+01 9.94168049e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "39-th iteration, loss: 0.5091165386147718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.058139924201221e-15\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107043e+00 5.34267912e+01 9.90616298e+01 9.94168048e+01\n",
      " 0.00000000e+00 7.01660952e-14 3.24671649e+01]\n",
      "40-th iteration, loss: 0.509116538614769, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.953977574336948e-16\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107125e+00 5.34267910e+01 9.90616297e+01 9.94168047e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "41-th iteration, loss: 0.5091165386147664, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0053809367671108e-16\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107205 53.42679087 99.06162958 99.41680465 32.4671649 ]\n",
      "42-th iteration, loss: 0.5091165386147638, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.7626467327824937e-15\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107284e+00 5.34267907e+01 9.90616295e+01 9.94168046e+01\n",
      " 0.00000000e+00 8.88178420e-14 3.24671649e+01]\n",
      "43-th iteration, loss: 0.5091165386147614, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.876627873059228e-17\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107362 53.42679055 99.06162941 99.41680452 32.4671649 ]\n",
      "44-th iteration, loss: 0.5091165386147588, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.900319490504165e-17\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107439 53.42679039 99.06162932 99.41680445 32.4671649 ]\n",
      "45-th iteration, loss: 0.5091165386147564, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.855634020617389e-16\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107515e+00 5.34267902e+01 9.90616292e+01 9.94168044e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "46-th iteration, loss: 0.5091165386147541, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.4423105813651585e-15\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107590e+00 5.34267901e+01 9.90616292e+01 9.94168043e+01\n",
      " 0.00000000e+00 1.14575016e-13 3.24671649e+01]\n",
      "47-th iteration, loss: 0.5091165386147519, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.809470480367603e-16\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107664e+00 5.34267899e+01 9.90616291e+01 9.94168043e+01\n",
      " 0.00000000e+00 2.75335310e-14 3.24671649e+01]\n",
      "48-th iteration, loss: 0.5091165386147496, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0390663487420435e-15\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107737e+00 5.34267898e+01 9.90616290e+01 9.94168042e+01\n",
      " 0.00000000e+00 1.00364161e-13 3.24671649e+01]\n",
      "49-th iteration, loss: 0.5091165386147475, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.1164799588398936e-15\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107809e+00 5.34267896e+01 9.90616290e+01 9.94168042e+01\n",
      " 0.00000000e+00 3.90798505e-14 3.24671649e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368028745168436\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         181.6310468 ]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6204761741881215\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 124.04071489   0.          57.59033191]\n",
      "2-th iteration, loss: 0.5091170879368359, 50 gd steps\n",
      "insert gradient: -0.002823694092044642\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[4.58849421e+00 5.34134044e+01 9.90512501e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.93423484e+01 5.75903319e+01]\n",
      "3-th iteration, loss: 0.5091165409529652, 11 gd steps\n",
      "insert gradient: -0.00012759241660170124\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.5994115  53.42818335 99.06473892 99.41174489 57.59033191]\n",
      "4-th iteration, loss: 0.5091165386303973, 16 gd steps\n",
      "insert gradient: -1.368276059659435e-05\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60103003e+00 5.34268367e+01 9.90618591e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94163118e+01 5.75903319e+01]\n",
      "5-th iteration, loss: 0.5091165386149508, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8090738994637445e-07\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109387 53.42676423 99.06170605 99.41676443 57.59033191]\n",
      "6-th iteration, loss: 0.5091165386149404, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9417920364904945e-07\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109428 53.42676394 99.06170453 99.41676472 57.59033191]\n",
      "7-th iteration, loss: 0.5091165386149308, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.058517849248523e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109471 53.42676374 99.06170307 99.41676502 57.59033191]\n",
      "8-th iteration, loss: 0.5091165386149219, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1610541136961433e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109515 53.4267636  99.06170166 99.41676533 57.59033191]\n",
      "9-th iteration, loss: 0.5091165386149136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.25096874375932e-07\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109560e+00 5.34267635e+01 9.90617003e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167656e+01 5.75903319e+01]\n",
      "10-th iteration, loss: 0.5091165386149052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.210867003320641e-07\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109606e+00 5.34267635e+01 9.90616990e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94167663e+01 5.75903319e+01]\n",
      "11-th iteration, loss: 0.5091165386148973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.167465273550666e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109651 53.42676353 99.06169772 99.41676696 57.59033191]\n",
      "12-th iteration, loss: 0.5091165386148899, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.236872456014151e-07\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109697e+00 5.34267636e+01 9.90616965e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167673e+01 5.75903319e+01]\n",
      "13-th iteration, loss: 0.5091165386148826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1802563799845507e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109743e+00 5.34267636e+01 9.90616953e+01 0.00000000e+00\n",
      " 7.19424520e-14 9.94167679e+01 5.75903319e+01]\n",
      "14-th iteration, loss: 0.5091165386148755, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1230096549354134e-07\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109789 53.42676375 99.0616941  99.41676858 57.59033191]\n",
      "15-th iteration, loss: 0.509116538614869, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1786055005263166e-07\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109834e+00 5.34267639e+01 9.90616929e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167689e+01 5.75903319e+01]\n",
      "16-th iteration, loss: 0.5091165386148625, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.112552256504448e-07\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109880e+00 5.34267640e+01 9.90616918e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167695e+01 5.75903319e+01]\n",
      "17-th iteration, loss: 0.5091165386148562, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0475123630274266e-07\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109925 53.42676415 99.06169073 99.41677016 57.59033191]\n",
      "18-th iteration, loss: 0.5091165386148504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.093585860712213e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109969 53.42676429 99.06168964 99.41677047 57.59033191]\n",
      "19-th iteration, loss: 0.5091165386148448, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.133909149858172e-07\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110013e+00 5.34267644e+01 9.90616886e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167708e+01 5.75903319e+01]\n",
      "20-th iteration, loss: 0.5091165386148392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0564476696289256e-07\n",
      "20-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110057e+00 5.34267646e+01 9.90616876e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167714e+01 5.75903319e+01]\n",
      "21-th iteration, loss: 0.5091165386148335, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9816737336798185e-07\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110101e+00 5.34267648e+01 9.90616866e+01 0.00000000e+00\n",
      " 7.01660952e-14 9.94167720e+01 5.75903319e+01]\n",
      "22-th iteration, loss: 0.5091165386148282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.909503793981057e-07\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110144e+00 5.34267650e+01 9.90616856e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.94167726e+01 5.75903319e+01]\n",
      "23-th iteration, loss: 0.509116538614823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8398490359386446e-07\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110186 53.42676517 99.0616846  99.41677321 57.59033191]\n",
      "24-th iteration, loss: 0.5091165386148183, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8746942784587135e-07\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110227e+00 5.34267653e+01 9.90616836e+01 0.00000000e+00\n",
      " 3.90798505e-14 9.94167735e+01 5.75903319e+01]\n",
      "25-th iteration, loss: 0.5091165386148134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.80202851893275e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110269 53.42676553 99.0616827  99.41677407 57.59033191]\n",
      "26-th iteration, loss: 0.5091165386148091, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.832778055465411e-07\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110309e+00 5.34267657e+01 9.90616818e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167743e+01 5.75903319e+01]\n",
      "27-th iteration, loss: 0.5091165386148045, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7580511055766884e-07\n",
      "27-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110350e+00 5.34267659e+01 9.90616809e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.94167749e+01 5.75903319e+01]\n",
      "28-th iteration, loss: 0.5091165386148001, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6865192596144397e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110389 53.42676608 99.06168    99.41677547 57.59033191]\n",
      "29-th iteration, loss: 0.5091165386147961, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.714318299654945e-07\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110428 53.42676626 99.06167912 99.41677574 57.59033191]\n",
      "30-th iteration, loss: 0.5091165386147922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.73844412995204e-07\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110467 53.42676643 99.06167827 99.41677601 57.59033191]\n",
      "31-th iteration, loss: 0.5091165386147886, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.759147931695573e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110505e+00 5.34267666e+01 9.90616774e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167763e+01 5.75903319e+01]\n",
      "32-th iteration, loss: 0.5091165386147847, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6782270004538956e-07\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110542e+00 5.34267668e+01 9.90616766e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167768e+01 5.75903319e+01]\n",
      "33-th iteration, loss: 0.509116538614781, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.601308858115458e-07\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110579e+00 5.34267670e+01 9.90616758e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167774e+01 5.75903319e+01]\n",
      "34-th iteration, loss: 0.5091165386147772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.528116334673536e-07\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110616e+00 5.34267671e+01 9.90616750e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167779e+01 5.75903319e+01]\n",
      "35-th iteration, loss: 0.5091165386147738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.45839448831736e-07\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110652e+00 5.34267673e+01 9.90616742e+01 0.00000000e+00\n",
      " 5.68434189e-14 9.94167784e+01 5.75903319e+01]\n",
      "36-th iteration, loss: 0.5091165386147705, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.391908957846423e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110687 53.42676749 99.06167346 99.4167789  57.59033191]\n",
      "37-th iteration, loss: 0.5091165386147674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4140863176274687e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110722 53.42676765 99.0616727  99.41677914 57.59033191]\n",
      "38-th iteration, loss: 0.5091165386147645, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.433227705529929e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110756e+00 5.34267678e+01 9.90616719e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167794e+01 5.75903319e+01]\n",
      "39-th iteration, loss: 0.5091165386147614, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3626900636914234e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110790e+00 5.34267680e+01 9.90616712e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167799e+01 5.75903319e+01]\n",
      "40-th iteration, loss: 0.5091165386147584, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.295631133678563e-07\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110823 53.42676813 99.06167051 99.41678034 57.59033191]\n",
      "41-th iteration, loss: 0.5091165386147557, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3138647450381094e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110856e+00 5.34267683e+01 9.90616698e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167806e+01 5.75903319e+01]\n",
      "42-th iteration, loss: 0.5091165386147529, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2468196802560368e-07\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110888 53.42676844 99.0616691  99.41678103 57.59033191]\n",
      "43-th iteration, loss: 0.5091165386147504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.263337124680571e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011092  53.42676858 99.06166842 99.41678126 57.59033191]\n",
      "44-th iteration, loss: 0.5091165386147479, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2773089403654638e-07\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110951e+00 5.34267687e+01 9.90616677e+01 0.00000000e+00\n",
      " 8.52651283e-14 9.94167815e+01 5.75903319e+01]\n",
      "45-th iteration, loss: 0.5091165386147454, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2077853047784126e-07\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110982 53.42676888 99.06166709 99.41678194 57.59033191]\n",
      "46-th iteration, loss: 0.5091165386147432, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2205780266296124e-07\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111012e+00 5.34267690e+01 9.90616664e+01 0.00000000e+00\n",
      " 8.88178420e-14 9.94167822e+01 5.75903319e+01]\n",
      "47-th iteration, loss: 0.5091165386147407, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.152041394911115e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111042 53.42676916 99.06166581 99.4167826  57.59033191]\n",
      "48-th iteration, loss: 0.5091165386147386, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1637822578152962e-07\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111071e+00 5.34267693e+01 9.90616652e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167828e+01 5.75903319e+01]\n",
      "49-th iteration, loss: 0.5091165386147364, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0963483581328384e-07\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111101e+00 5.34267694e+01 9.90616646e+01 0.00000000e+00\n",
      " 2.75335310e-14 9.94167833e+01 5.75903319e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355685\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         206.85758108]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6185213411276936\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 123.61001796   0.          83.24756312]\n",
      "2-th iteration, loss: 0.50917051042473, 45 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.459896391438879e-15\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[3.61241440e+00 5.35208071e+01 9.98414342e+01 9.92299661e+01\n",
      " 0.00000000e+00 1.14575016e-13 8.32475631e+01]\n",
      "3-th iteration, loss: 0.5091166040187136, 36 gd steps\n",
      "insert gradient: -0.00046139220112038687\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6360841  53.42804166 99.03333583 99.43257542 83.24756312]\n",
      "4-th iteration, loss: 0.5091165386198226, 26 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.350232595579709e-06\n",
      "4-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110324 53.42690786 99.06160509 99.41664891 83.24756312]\n",
      "5-th iteration, loss: 0.5091165386148037, 7 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.722181601640729e-07\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110946e+00 5.34267945e+01 9.90616510e+01 0.00000000e+00\n",
      " 7.10542736e-14 9.94167877e+01 8.32475631e+01]\n",
      "6-th iteration, loss: 0.5091165386147763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.831346706410901e-07\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011092  53.42679204 99.0616499  99.4167881  83.24756312]\n",
      "7-th iteration, loss: 0.5091165386147559, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.977161597963257e-07\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110902 53.42678996 99.06164892 99.4167883  83.24756312]\n",
      "8-th iteration, loss: 0.5091165386147408, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0913958353114511e-07\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110891 53.42678818 99.06164805 99.41678851 83.24756312]\n",
      "9-th iteration, loss: 0.5091165386147297, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1789915555740283e-07\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110886e+00 5.34267867e+01 9.90616473e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167887e+01 8.32475631e+01]\n",
      "10-th iteration, loss: 0.5091165386147212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.163165510355324e-07\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110887e+00 5.34267854e+01 9.90616466e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167892e+01 8.32475631e+01]\n",
      "11-th iteration, loss: 0.5091165386147146, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1333044387662787e-07\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110891 53.42678426 99.061646   99.41678964 83.24756312]\n",
      "12-th iteration, loss: 0.5091165386147097, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1704310663782193e-07\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110899 53.4267833  99.06164544 99.41678986 83.24756312]\n",
      "13-th iteration, loss: 0.509116538614706, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.193999084002067e-07\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110910e+00 5.34267825e+01 9.90616449e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167901e+01 8.32475631e+01]\n",
      "14-th iteration, loss: 0.5091165386147029, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.127505351472637e-07\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110923e+00 5.34267818e+01 9.90616445e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167905e+01 8.32475631e+01]\n",
      "15-th iteration, loss: 0.5091165386147004, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0575695786698844e-07\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110939e+00 5.34267812e+01 9.90616441e+01 0.00000000e+00\n",
      " 7.19424520e-14 9.94167910e+01 8.32475631e+01]\n",
      "16-th iteration, loss: 0.5091165386146983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9855649144199138e-07\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110956 53.42678067 99.06164367 99.41679137 83.24756312]\n",
      "17-th iteration, loss: 0.5091165386146967, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9831104835707108e-07\n",
      "17-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110974e+00 5.34267802e+01 9.90616433e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167916e+01 8.32475631e+01]\n",
      "18-th iteration, loss: 0.5091165386146953, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9047696201753164e-07\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110994 53.42677985 99.06164297 99.41679197 83.24756312]\n",
      "19-th iteration, loss: 0.5091165386146942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.894715866483669e-07\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111015 53.42677952 99.06164265 99.41679216 83.24756312]\n",
      "20-th iteration, loss: 0.5091165386146932, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.880615002547127e-07\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111036 53.42677924 99.06164235 99.41679235 83.24756312]\n",
      "21-th iteration, loss: 0.5091165386146923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8632042239703927e-07\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111058 53.426779   99.06164207 99.41679254 83.24756312]\n",
      "22-th iteration, loss: 0.5091165386146916, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8431082751717994e-07\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111081 53.42677879 99.0616418  99.41679273 83.24756312]\n",
      "23-th iteration, loss: 0.5091165386146909, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8208558903666247e-07\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111104e+00 5.34267786e+01 9.90616415e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167929e+01 8.32475631e+01]\n",
      "24-th iteration, loss: 0.5091165386146901, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.733386380629188e-07\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111127 53.42677847 99.0616413  99.41679327 83.24756312]\n",
      "25-th iteration, loss: 0.5091165386146894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7107286568332305e-07\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011115  53.42677833 99.06164106 99.41679345 83.24756312]\n",
      "26-th iteration, loss: 0.5091165386146889, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6869239579803002e-07\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111174e+00 5.34267782e+01 9.90616408e+01 0.00000000e+00\n",
      " 1.14575016e-13 9.94167936e+01 8.32475631e+01]\n",
      "27-th iteration, loss: 0.5091165386146883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6035550346314166e-07\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111197 53.42677813 99.06164062 99.41679395 83.24756312]\n",
      "28-th iteration, loss: 0.5091165386146878, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5806779384010432e-07\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011122  53.42677804 99.06164041 99.41679411 83.24756312]\n",
      "29-th iteration, loss: 0.5091165386146872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5572364123400979e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111244e+00 5.34267780e+01 9.90616402e+01 0.00000000e+00\n",
      " 4.17443857e-14 9.94167943e+01 8.32475631e+01]\n",
      "30-th iteration, loss: 0.5091165386146866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4792877495660778e-07\n",
      "30-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111267e+00 5.34267779e+01 9.90616400e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167946e+01 8.32475631e+01]\n",
      "31-th iteration, loss: 0.5091165386146861, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4060129741268345e-07\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111290e+00 5.34267779e+01 9.90616398e+01 0.00000000e+00\n",
      " 7.01660952e-14 9.94167949e+01 8.32475631e+01]\n",
      "32-th iteration, loss: 0.5091165386146856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3371484378475405e-07\n",
      "32-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111313e+00 5.34267778e+01 9.90616396e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94167952e+01 8.32475631e+01]\n",
      "33-th iteration, loss: 0.5091165386146851, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2724395653976695e-07\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111336e+00 5.34267778e+01 9.90616394e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167954e+01 8.32475631e+01]\n",
      "34-th iteration, loss: 0.5091165386146846, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.211641611801704e-07\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111359 53.42677773 99.06163923 99.41679567 83.24756312]\n",
      "35-th iteration, loss: 0.5091165386146843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1967760707687809e-07\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111381 53.4267777  99.06163904 99.41679579 83.24756312]\n",
      "36-th iteration, loss: 0.5091165386146839, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1816458613845167e-07\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111403e+00 5.34267777e+01 9.90616389e+01 0.00000000e+00\n",
      " 4.17443857e-14 9.94167959e+01 8.32475631e+01]\n",
      "37-th iteration, loss: 0.5091165386146834, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1251589616962068e-07\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111425e+00 5.34267777e+01 9.90616387e+01 0.00000000e+00\n",
      " 1.13686838e-13 9.94167961e+01 8.32475631e+01]\n",
      "38-th iteration, loss: 0.509116538614683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0721905481193475e-07\n",
      "38-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111447e+00 5.34267776e+01 9.90616385e+01 0.00000000e+00\n",
      " 1.42108547e-14 9.94167964e+01 8.32475631e+01]\n",
      "39-th iteration, loss: 0.5091165386146826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0225085985134943e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111468e+00 5.34267776e+01 9.90616383e+01 0.00000000e+00\n",
      " 5.77315973e-14 9.94167966e+01 8.32475631e+01]\n",
      "40-th iteration, loss: 0.5091165386146823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.758957713660134e-08\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111489 53.4267776  99.06163816 99.41679678 83.24756312]\n",
      "41-th iteration, loss: 0.509116538614682, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.662778376662009e-08\n",
      "41-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011151  53.42677759 99.06163799 99.41679688 83.24756312]\n",
      "42-th iteration, loss: 0.5091165386146818, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.564119664621636e-08\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111531e+00 5.34267776e+01 9.90616378e+01 0.00000000e+00\n",
      " 8.43769499e-14 9.94167970e+01 8.32475631e+01]\n",
      "43-th iteration, loss: 0.5091165386146813, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.129277405480157e-08\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111551e+00 5.34267776e+01 9.90616377e+01 0.00000000e+00\n",
      " 8.52651283e-14 9.94167972e+01 8.32475631e+01]\n",
      "44-th iteration, loss: 0.5091165386146811, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.721614772608333e-08\n",
      "44-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111572 53.42677756 99.06163751 99.41679735 83.24756312]\n",
      "45-th iteration, loss: 0.5091165386146806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.644616759757935e-08\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111592e+00 5.34267775e+01 9.90616373e+01 0.00000000e+00\n",
      " 1.00364161e-13 9.94167974e+01 8.32475631e+01]\n",
      "46-th iteration, loss: 0.5091165386146804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.262770286286964e-08\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111611 53.42677754 99.06163719 99.41679761 83.24756312]\n",
      "47-th iteration, loss: 0.5091165386146801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.194142670805027e-08\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111631e+00 5.34267775e+01 9.90616370e+01 0.00000000e+00\n",
      " 1.27897692e-13 9.94167977e+01 8.32475631e+01]\n",
      "48-th iteration, loss: 0.5091165386146799, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.836271248430662e-08\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011165  53.42677753 99.06163689 99.41679785 83.24756312]\n",
      "49-th iteration, loss: 0.5091165386146795, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.775365036994375e-08\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111669 53.42677753 99.06163674 99.41679793 83.24756312]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355756\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         231.98320922]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6292589800370129\n",
      "1-th iteration, new layer inserted. now 4 layers\n",
      "[ 42.89280919  62.32063131 231.98320922   0.        ]\n",
      "2-th iteration, loss: 0.6034846706022171, 13 gd steps\n",
      "insert gradient: -0.44655276234212676\n",
      "2-th iteration, new layer inserted. now 6 layers\n",
      "[  3.45157305  77.7695492  115.92076696   0.         101.8697649\n",
      "  42.36759133]\n",
      "3-th iteration, loss: 0.5141481088059235, 33 gd steps\n",
      "insert gradient: -0.05968592998493505\n",
      "3-th iteration, new layer inserted. now 6 layers\n",
      "[5.63880738e+00 0.00000000e+00 9.32587341e-15 5.43376558e+01\n",
      " 9.38145533e+01 1.09026079e+02]\n",
      "4-th iteration, loss: 0.5091167136537478, 37 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.0550315235655874e-15\n",
      "4-th iteration, new layer inserted. now 6 layers\n",
      "[4.59547386e+00 5.34131460e+01 9.91260018e+01 9.94008908e+01\n",
      " 0.00000000e+00 9.90318938e-14]\n",
      "5-th iteration, loss: 0.5091165393082356, 16 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.4351878172544115e-17\n",
      "5-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60151615 53.4273449  99.06384517 99.41665619  0.        ]\n",
      "6-th iteration, loss: 0.509116538708158, 6 gd steps\n",
      "insert gradient: -4.219195233470164e-05\n",
      "6-th iteration, new layer inserted. now 6 layers\n",
      "[4.60114269e+00 0.00000000e+00 3.52495810e-15 5.34260552e+01\n",
      " 9.90629052e+01 9.94164825e+01]\n",
      "7-th iteration, loss: 0.509116538615995, 9 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.803100346864696e-06\n",
      "7-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112339 53.42678184 99.0617143  99.41665839]\n",
      "8-th iteration, loss: 0.50911653861589, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6998255454428097e-06\n",
      "8-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112268 53.42677852 99.06171308 99.41666218]\n",
      "9-th iteration, loss: 0.5091165386158006, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.596797178710269e-06\n",
      "9-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112209e+00 5.34267757e+01 9.90617120e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94166659e+01]\n",
      "10-th iteration, loss: 0.5091165386156761, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3715497838141046e-06\n",
      "10-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112159e+00 5.34267732e+01 9.90617110e+01 0.00000000e+00\n",
      " 1.25677246e-13 9.94166729e+01]\n",
      "11-th iteration, loss: 0.5091165386155697, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1602642761971385e-06\n",
      "11-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112119e+00 5.34267711e+01 9.90617100e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94166796e+01]\n",
      "12-th iteration, loss: 0.5091165386154783, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9622370221334585e-06\n",
      "12-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112085e+00 5.34267693e+01 9.90617090e+01 0.00000000e+00\n",
      " 1.38999923e-13 9.94166858e+01]\n",
      "13-th iteration, loss: 0.5091165386153997, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7767702659188515e-06\n",
      "13-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112058e+00 5.34267677e+01 9.90617081e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94166916e+01]\n",
      "14-th iteration, loss: 0.5091165386153316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.603178300942294e-06\n",
      "14-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112036 53.42676636 99.0617072  99.41669711]\n",
      "15-th iteration, loss: 0.5091165386152974, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.529797020545773e-06\n",
      "15-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112020e+00 5.34267652e+01 9.90617063e+01 0.00000000e+00\n",
      " 1.37667655e-14 9.94166997e+01]\n",
      "16-th iteration, loss: 0.5091165386152429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3713327661522094e-06\n",
      "16-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112007 53.42676423 99.06170543 99.41670468]\n",
      "17-th iteration, loss: 0.5091165386152157, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3043796287358524e-06\n",
      "17-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111998 53.42676341 99.06170456 99.41670704]\n",
      "18-th iteration, loss: 0.5091165386151908, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2388753366557976e-06\n",
      "18-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111993 53.42676271 99.06170373 99.41670932]\n",
      "19-th iteration, loss: 0.5091165386151677, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1749095890573726e-06\n",
      "19-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011199  53.42676213 99.06170292 99.41671154]\n",
      "20-th iteration, loss: 0.5091165386151462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.112546941724929e-06\n",
      "20-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111989 53.42676164 99.06170213 99.4167137 ]\n",
      "21-th iteration, loss: 0.5091165386151262, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0518310156137918e-06\n",
      "21-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.6011199  53.42676124 99.06170135 99.4167158 ]\n",
      "22-th iteration, loss: 0.5091165386151073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.992788032145511e-06\n",
      "22-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111993 53.42676091 99.06170059 99.41671783]\n",
      "23-th iteration, loss: 0.5091165386150897, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.935429784592568e-06\n",
      "23-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60111998 53.42676065 99.06169984 99.41671981]\n",
      "24-th iteration, loss: 0.5091165386150729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8797562302874908e-06\n",
      "24-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112003e+00 5.34267604e+01 9.90616991e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167217e+01]\n",
      "25-th iteration, loss: 0.5091165386150442, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7615767024838682e-06\n",
      "25-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112010e+00 5.34267603e+01 9.90616984e+01 0.00000000e+00\n",
      " 9.76996262e-14 9.94167254e+01]\n",
      "26-th iteration, loss: 0.5091165386150187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6516822264768653e-06\n",
      "26-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112018 53.42676016 99.06169761 99.41672889]\n",
      "27-th iteration, loss: 0.5091165386150058, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6059467222848888e-06\n",
      "27-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112026 53.42676007 99.06169684 99.41673052]\n",
      "28-th iteration, loss: 0.5091165386149935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5615751784892932e-06\n",
      "28-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112035 53.42676001 99.06169607 99.41673212]\n",
      "29-th iteration, loss: 0.509116538614982, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5185499182882804e-06\n",
      "29-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112044 53.42675998 99.06169532 99.41673367]\n",
      "30-th iteration, loss: 0.5091165386149709, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4768487846702715e-06\n",
      "30-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112054e+00 5.34267600e+01 9.90616946e+01 0.00000000e+00\n",
      " 1.01252340e-13 9.94167352e+01]\n",
      "31-th iteration, loss: 0.5091165386149522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3859394431284462e-06\n",
      "31-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112065e+00 5.34267600e+01 9.90616938e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167381e+01]\n",
      "32-th iteration, loss: 0.5091165386149354, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3014454817503766e-06\n",
      "32-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112076 53.42676004 99.06169305 99.4167408 ]\n",
      "33-th iteration, loss: 0.5091165386149266, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.267474456704376e-06\n",
      "33-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112087e+00 5.34267601e+01 9.90616923e+01 0.00000000e+00\n",
      " 1.52766688e-13 9.94167421e+01]\n",
      "34-th iteration, loss: 0.5091165386149121, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1910995083109037e-06\n",
      "34-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112098e+00 5.34267602e+01 9.90616915e+01 0.00000000e+00\n",
      " 7.23865412e-14 9.94167446e+01]\n",
      "35-th iteration, loss: 0.5091165386148989, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1200879722497653e-06\n",
      "35-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112110e+00 5.34267602e+01 9.90616907e+01 0.00000000e+00\n",
      " 6.88338275e-14 9.94167469e+01]\n",
      "36-th iteration, loss: 0.509116538614887, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0540442509304598e-06\n",
      "36-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112121e+00 5.34267603e+01 9.90616899e+01 0.00000000e+00\n",
      " 2.44249065e-14 9.94167491e+01]\n",
      "37-th iteration, loss: 0.509116538614876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.926026961673016e-07\n",
      "37-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112133e+00 5.34267604e+01 9.90616891e+01 0.00000000e+00\n",
      " 2.44249065e-14 9.94167512e+01]\n",
      "38-th iteration, loss: 0.509116538614866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.354253358797386e-07\n",
      "38-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112145e+00 5.34267605e+01 9.90616883e+01 0.00000000e+00\n",
      " 9.41469125e-14 9.94167532e+01]\n",
      "39-th iteration, loss: 0.5091165386148567, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.821996938459807e-07\n",
      "39-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112157e+00 5.34267607e+01 9.90616875e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167550e+01]\n",
      "40-th iteration, loss: 0.5091165386148482, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.32636773790055e-07\n",
      "40-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112169e+00 5.34267608e+01 9.90616867e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167568e+01]\n",
      "41-th iteration, loss: 0.5091165386148403, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.864692743465465e-07\n",
      "41-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112181 53.42676093 99.06168592 99.4167584 ]\n",
      "42-th iteration, loss: 0.5091165386148352, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.705944920624335e-07\n",
      "42-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112193 53.42676106 99.06168511 99.41675918]\n",
      "43-th iteration, loss: 0.5091165386148303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.55057723329563e-07\n",
      "43-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112205e+00 5.34267612e+01 9.90616843e+01 0.00000000e+00\n",
      " 6.21724894e-14 9.94167599e+01]\n",
      "44-th iteration, loss: 0.5091165386148234, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.137999286635362e-07\n",
      "44-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112217 53.42676137 99.06168353 99.41676144]\n",
      "45-th iteration, loss: 0.509116538614819, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.000210753300185e-07\n",
      "45-th iteration, new layer inserted. now 4 layers\n",
      "[ 4.60112229 53.42676152 99.06168274 99.41676214]\n",
      "46-th iteration, loss: 0.5091165386148145, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.86526911101746e-07\n",
      "46-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112241e+00 5.34267617e+01 9.90616820e+01 0.00000000e+00\n",
      " 1.35447209e-13 9.94167628e+01]\n",
      "47-th iteration, loss: 0.5091165386148085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.49596551496966e-07\n",
      "47-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112253e+00 5.34267619e+01 9.90616812e+01 0.00000000e+00\n",
      " 9.76996262e-14 9.94167642e+01]\n",
      "48-th iteration, loss: 0.5091165386148029, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.151842140115341e-07\n",
      "48-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112265e+00 5.34267620e+01 9.90616804e+01 0.00000000e+00\n",
      " 2.70894418e-14 9.94167655e+01]\n",
      "49-th iteration, loss: 0.5091165386147977, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.831032473721061e-07\n",
      "49-th iteration, new layer inserted. now 6 layers\n",
      "[4.60112277e+00 5.34267622e+01 9.90616797e+01 0.00000000e+00\n",
      " 1.35447209e-13 9.94167667e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5364665204182857\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.80232488   0.         256.70521281]\n",
      "1-th iteration, loss: 0.7498498981810913, 11 gd steps\n",
      "insert gradient: -0.6347981907359495\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.18401986  62.29250037 234.79135318   0.          21.91385963]\n",
      "2-th iteration, loss: 0.6040366479862144, 13 gd steps\n",
      "insert gradient: -0.4409384390366693\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1922579   77.35207373 116.74055884   0.         103.27049435\n",
      "  41.72503259  21.91385963]\n",
      "3-th iteration, loss: 0.5094713963188506, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7512866751076424e-17\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[  2.18581045  54.60749015  97.88579313 101.16888107  21.91385963]\n",
      "4-th iteration, loss: 0.5091165406028058, 34 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.2343484860321998e-15\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60091336e+00 5.34303134e+01 9.90577282e+01 9.94189383e+01\n",
      " 0.00000000e+00 3.90798505e-14 2.19138596e+01]\n",
      "5-th iteration, loss: 0.5091165386413579, 13 gd steps\n",
      "insert gradient: -1.2175552311110703e-05\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60096683e+00 5.34269713e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90609229e+01 9.94173675e+01 2.19138596e+01]\n",
      "6-th iteration, loss: 0.5091165386150943, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.210635993452795e-18\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60107859 53.42680132 99.06163615 99.41686349 21.91385963]\n",
      "7-th iteration, loss: 0.509116538615048, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.7617083249047845e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107887e+00 5.34267993e+01 9.90616345e+01 9.94168613e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "8-th iteration, loss: 0.5091165386150103, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.5827423714735345e-15\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107922e+00 5.34267975e+01 9.90616330e+01 9.94168592e+01\n",
      " 0.00000000e+00 1.14575016e-13 2.19138596e+01]\n",
      "9-th iteration, loss: 0.509116538614979, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.807033786149365e-15\n",
      "9-th iteration, new layer inserted. now 7 layers\n",
      "[4.60107962e+00 5.34267961e+01 9.90616317e+01 9.94168572e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "10-th iteration, loss: 0.5091165386149522, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.278382616027425e-15\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108007e+00 5.34267948e+01 9.90616305e+01 9.94168553e+01\n",
      " 0.00000000e+00 7.46069873e-14 2.19138596e+01]\n",
      "11-th iteration, loss: 0.5091165386149294, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.101959332894695e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108056 53.42679378 99.06162947 99.41685343 21.91385963]\n",
      "12-th iteration, loss: 0.5091165386149095, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6476484820416026e-17\n",
      "12-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108107 53.42679289 99.06162849 99.41685166 21.91385963]\n",
      "13-th iteration, loss: 0.509116538614892, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.696435611074485e-15\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108161e+00 5.34267921e+01 9.90616276e+01 9.94168500e+01\n",
      " 0.00000000e+00 5.68434189e-14 2.19138596e+01]\n",
      "14-th iteration, loss: 0.5091165386148767, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.944852430980597e-15\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108217e+00 5.34267915e+01 9.90616268e+01 9.94168484e+01\n",
      " 0.00000000e+00 1.27897692e-13 2.19138596e+01]\n",
      "15-th iteration, loss: 0.5091165386148631, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.093954914580368e-15\n",
      "15-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108274e+00 5.34267910e+01 9.90616261e+01 9.94168468e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "16-th iteration, loss: 0.5091165386148506, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1160073531290597e-15\n",
      "16-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108333e+00 5.34267905e+01 9.90616254e+01 9.94168454e+01\n",
      " 0.00000000e+00 7.10542736e-14 2.19138596e+01]\n",
      "17-th iteration, loss: 0.5091165386148395, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 7.945781924756327e-16\n",
      "17-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108392e+00 5.34267901e+01 9.90616248e+01 9.94168440e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "18-th iteration, loss: 0.5091165386148293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.565305451907113e-17\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108451 53.42678983 99.06162425 99.41684262 21.91385963]\n",
      "19-th iteration, loss: 0.5091165386148199, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1252790730502623e-15\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108511e+00 5.34267895e+01 9.90616237e+01 9.94168413e+01\n",
      " 0.00000000e+00 7.19424520e-14 2.19138596e+01]\n",
      "20-th iteration, loss: 0.5091165386148114, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.052582831997984e-15\n",
      "20-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108571e+00 5.34267893e+01 9.90616233e+01 9.94168401e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "21-th iteration, loss: 0.5091165386148035, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.267796493968059e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108630e+00 5.34267891e+01 9.90616228e+01 9.94168389e+01\n",
      " 0.00000000e+00 7.46069873e-14 2.19138596e+01]\n",
      "22-th iteration, loss: 0.5091165386147962, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.666112444189505e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60108690e+00 5.34267889e+01 9.90616224e+01 9.94168377e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "23-th iteration, loss: 0.5091165386147894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.728685926835555e-17\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108749 53.4267888  99.06162209 99.41683663 21.91385963]\n",
      "24-th iteration, loss: 0.5091165386147832, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5131255929754893e-17\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108808 53.42678866 99.06162176 99.41683556 21.91385963]\n",
      "25-th iteration, loss: 0.5091165386147773, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.184894970686345e-17\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108866 53.42678854 99.06162145 99.41683453 21.91385963]\n",
      "26-th iteration, loss: 0.5091165386147719, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.596066655321674e-17\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60108923 53.42678844 99.06162117 99.41683355 21.91385963]\n",
      "27-th iteration, loss: 0.5091165386147668, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.619083834025772e-17\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010898  53.42678834 99.06162091 99.4168326  21.91385963]\n",
      "28-th iteration, loss: 0.509116538614762, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.241419362392719e-17\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109037 53.42678826 99.06162068 99.41683168 21.91385963]\n",
      "29-th iteration, loss: 0.5091165386147577, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.3489765787752024e-16\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109092e+00 5.34267882e+01 9.90616205e+01 9.94168308e+01\n",
      " 0.00000000e+00 1.42108547e-14 2.19138596e+01]\n",
      "30-th iteration, loss: 0.5091165386147536, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.3417107790371953e-17\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109148 53.42678811 99.06162027 99.41682997 21.91385963]\n",
      "31-th iteration, loss: 0.5091165386147497, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5939436836330419e-15\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109202e+00 5.34267880e+01 9.90616201e+01 9.94168292e+01\n",
      " 0.00000000e+00 5.32907052e-14 2.19138596e+01]\n",
      "32-th iteration, loss: 0.5091165386147462, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 5.630312068339595e-17\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109256 53.42678797 99.06161993 99.41682838 21.91385963]\n",
      "33-th iteration, loss: 0.5091165386147427, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.0511949831839097e-15\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109308e+00 5.34267879e+01 9.90616198e+01 9.94168276e+01\n",
      " 0.00000000e+00 7.01660952e-14 2.19138596e+01]\n",
      "34-th iteration, loss: 0.5091165386147396, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9728007362605147e-17\n",
      "34-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109361 53.42678784 99.06161965 99.41682691 21.91385963]\n",
      "35-th iteration, loss: 0.5091165386147365, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.30156338141989e-17\n",
      "35-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109412 53.42678777 99.06161953 99.41682622 21.91385963]\n",
      "36-th iteration, loss: 0.5091165386147337, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.224662698542142e-16\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109463e+00 5.34267877e+01 9.90616194e+01 9.94168255e+01\n",
      " 0.00000000e+00 1.42108547e-14 2.19138596e+01]\n",
      "37-th iteration, loss: 0.509116538614731, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7905175692206597e-15\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109513e+00 5.34267876e+01 9.90616193e+01 9.94168249e+01\n",
      " 0.00000000e+00 5.77315973e-14 2.19138596e+01]\n",
      "38-th iteration, loss: 0.5091165386147286, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.895609012908725e-17\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60109562 53.42678758 99.06161925 99.41682428 21.91385963]\n",
      "39-th iteration, loss: 0.5091165386147262, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.254120149514949e-17\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010961  53.42678752 99.06161917 99.41682368 21.91385963]\n",
      "40-th iteration, loss: 0.509116538614724, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1577571209990733e-15\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109658e+00 5.34267874e+01 9.90616191e+01 9.94168231e+01\n",
      " 0.00000000e+00 7.01660952e-14 2.19138596e+01]\n",
      "41-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.6967641702413162e-15\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109705e+00 5.34267874e+01 9.90616191e+01 9.94168226e+01\n",
      " 0.00000000e+00 8.88178420e-14 2.19138596e+01]\n",
      "42-th iteration, loss: 0.5091165386147198, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.163361591490623e-15\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109752e+00 5.34267873e+01 9.90616190e+01 9.94168220e+01\n",
      " 0.00000000e+00 1.00364161e-13 2.19138596e+01]\n",
      "43-th iteration, loss: 0.509116538614718, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.239145151064029e-16\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109797e+00 5.34267872e+01 9.90616190e+01 9.94168215e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "44-th iteration, loss: 0.5091165386147162, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.504436255781421e-15\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109842e+00 5.34267872e+01 9.90616189e+01 9.94168210e+01\n",
      " 0.00000000e+00 1.14575016e-13 2.19138596e+01]\n",
      "45-th iteration, loss: 0.5091165386147145, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 8.386394193304624e-16\n",
      "45-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109886e+00 5.34267871e+01 9.90616189e+01 9.94168205e+01\n",
      " 0.00000000e+00 2.75335310e-14 2.19138596e+01]\n",
      "46-th iteration, loss: 0.509116538614713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.059540305763039e-17\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6010993  53.42678704 99.06161889 99.41682007 21.91385963]\n",
      "47-th iteration, loss: 0.5091165386147114, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6108179403182298e-15\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109973e+00 5.34267870e+01 9.90616189e+01 9.94168196e+01\n",
      " 0.00000000e+00 5.32907052e-14 2.19138596e+01]\n",
      "48-th iteration, loss: 0.50911653861471, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.500525522502344e-17\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110015 53.42678689 99.06161888 99.41681919 21.91385963]\n",
      "49-th iteration, loss: 0.5091165386147085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7368169260311371e-09\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60110057 53.42678681 99.06161887 99.41681877 21.91385963]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536721336560604\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         282.03265323]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6333108339474548\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.32015942   0.          44.7124938 ]\n",
      "2-th iteration, loss: 0.6051562988423069, 13 gd steps\n",
      "insert gradient: -0.44287306577339\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.13890924  77.08377718 117.82668764   0.         104.2313006\n",
      "  40.82946936  44.7124938 ]\n",
      "3-th iteration, loss: 0.5094819386284526, 50 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.1637720451077163e-15\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[2.09441160e+00 5.46242065e+01 9.80241166e+01 1.01144724e+02\n",
      " 0.00000000e+00 7.19424520e-14 4.47124938e+01]\n",
      "4-th iteration, loss: 0.5091172269371531, 17 gd steps\n",
      "insert gradient: -0.0016064205639812212\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.63972049e+00 5.34657345e+01 9.90510432e+01 0.00000000e+00\n",
      " 7.46069873e-14 9.93712684e+01 4.47124938e+01]\n",
      "5-th iteration, loss: 0.509116542510028, 28 gd steps\n",
      "insert gradient: -2.9084962098561204e-05\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.61251497e+00 5.34248560e+01 0.00000000e+00 5.72875081e-14\n",
      " 9.90592495e+01 9.94189124e+01 4.47124938e+01]\n",
      "6-th iteration, loss: 0.5091165386436309, 19 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6007246952170097e-06\n",
      "6-th iteration, new layer inserted. now 7 layers\n",
      "[4.60099134e+00 0.00000000e+00 9.15933995e-16 5.34265308e+01\n",
      " 9.90624673e+01 9.94165833e+01 4.47124938e+01]\n",
      "7-th iteration, loss: 0.509116538615336, 11 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7390092186916205e-15\n",
      "7-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111376e+00 5.34267904e+01 9.90616724e+01 9.94168642e+01\n",
      " 0.00000000e+00 5.68434189e-14 4.47124938e+01]\n",
      "8-th iteration, loss: 0.5091165386152379, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1074673196754186e-17\n",
      "8-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011133  53.4267873  99.06166956 99.4168615  44.7124938 ]\n",
      "9-th iteration, loss: 0.509116538615159, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2776061822782934e-16\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111295 53.42678473 99.06166693 99.41685888 44.7124938 ]\n",
      "10-th iteration, loss: 0.5091165386150956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.270882550252856e-17\n",
      "10-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111271 53.4267826  99.06166454 99.41685641 44.7124938 ]\n",
      "11-th iteration, loss: 0.5091165386150437, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.640707828780049e-17\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111256 53.42678084 99.06166235 99.41685405 44.7124938 ]\n",
      "12-th iteration, loss: 0.5091165386150007, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.432274732709646e-15\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111248e+00 5.34267794e+01 9.90616604e+01 9.94168518e+01\n",
      " 0.00000000e+00 1.14575016e-13 4.47124938e+01]\n",
      "13-th iteration, loss: 0.5091165386149651, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.425414447078349e-15\n",
      "13-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111246e+00 5.34267782e+01 9.90616585e+01 9.94168497e+01\n",
      " 0.00000000e+00 8.43769499e-14 4.47124938e+01]\n",
      "14-th iteration, loss: 0.5091165386149348, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4022459190324148e-17\n",
      "14-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011125  53.42677728 99.06165684 99.41684771 44.7124938 ]\n",
      "15-th iteration, loss: 0.5091165386149085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3837180533090305e-18\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111257 53.42677651 99.06165527 99.41684578 44.7124938 ]\n",
      "16-th iteration, loss: 0.5091165386148856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.388599456144329e-17\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111268 53.4267759  99.06165382 99.41684394 44.7124938 ]\n",
      "17-th iteration, loss: 0.5091165386148656, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.184188438390914e-17\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111282 53.42677543 99.06165247 99.41684218 44.7124938 ]\n",
      "18-th iteration, loss: 0.509116538614848, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.6251558541142734e-15\n",
      "18-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111298e+00 5.34267751e+01 9.90616512e+01 9.94168405e+01\n",
      " 0.00000000e+00 5.68434189e-14 4.47124938e+01]\n",
      "19-th iteration, loss: 0.5091165386148324, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5952148412479132e-16\n",
      "19-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111317 53.4267748  99.06165004 99.41683893 44.7124938 ]\n",
      "20-th iteration, loss: 0.5091165386148184, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7876556249292795e-17\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111336 53.42677461 99.06164894 99.41683741 44.7124938 ]\n",
      "21-th iteration, loss: 0.5091165386148058, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.12597815340521e-15\n",
      "21-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111357e+00 5.34267745e+01 9.90616479e+01 9.94168360e+01\n",
      " 0.00000000e+00 7.19424520e-14 4.47124938e+01]\n",
      "22-th iteration, loss: 0.5091165386147946, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.643961739420902e-15\n",
      "22-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111379e+00 5.34267744e+01 9.90616469e+01 9.94168346e+01\n",
      " 0.00000000e+00 8.88178420e-14 4.47124938e+01]\n",
      "23-th iteration, loss: 0.5091165386147843, 0 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.5416242987368004e-15\n",
      "23-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111402e+00 5.34267744e+01 9.90616460e+01 9.94168333e+01\n",
      " 0.00000000e+00 5.32907052e-14 4.47124938e+01]\n",
      "24-th iteration, loss: 0.509116538614775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5477050853825678e-08\n",
      "24-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111425 53.4267744  99.06164519 99.41683201 44.7124938 ]\n",
      "25-th iteration, loss: 0.5091165386147665, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.144702560342823e-08\n",
      "25-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60111448 53.42677443 99.06164437 99.41683079 44.7124938 ]\n",
      "26-th iteration, loss: 0.5091165386147587, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.225230270469821e-08\n",
      "26-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111471e+00 0.00000000e+00 2.35922393e-15 5.34267745e+01\n",
      " 9.90616436e+01 9.94168296e+01 4.47124938e+01]\n",
      "27-th iteration, loss: 0.5091165386147515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.261795389816296e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111495e+00 2.07194510e-09 0.00000000e+00 7.04461334e-08\n",
      " 2.34816203e-07 5.34267746e+01 9.90616429e+01 9.94168285e+01\n",
      " 4.47124938e+01]\n",
      "28-th iteration, loss: 0.5091165386147446, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1403510720591362e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111517e+00 2.12586358e-09 0.00000000e+00 5.52724532e-08\n",
      " 2.27549351e-07 1.25772505e-07 4.57929892e-07 5.34267746e+01\n",
      " 9.90616422e+01 9.94168275e+01 4.47124938e+01]\n",
      "29-th iteration, loss: 0.509116538614738, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9992819524145863e-07\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[4.60111539e+00 8.10207389e-10 0.00000000e+00 1.70143552e-08\n",
      " 2.15408257e-07 7.09711522e-08 4.39477361e-07 1.55798919e-07\n",
      " 6.61938597e-07 5.34267747e+01 9.90616415e+01 9.94168264e+01\n",
      " 4.47124938e+01]\n",
      "30-th iteration, loss: 0.509116538614732, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.851635021568765e-07\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111621e+00 1.97793667e-09 0.00000000e+00 5.14263534e-08\n",
      " 6.35799415e-07 1.65903884e-07 8.48450724e-07 5.34267747e+01\n",
      " 9.90616409e+01 9.94168255e+01 4.47124938e+01]\n",
      "31-th iteration, loss: 0.5091165386147267, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7626772222034098e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111659e+00 3.11272239e-10 0.00000000e+00 8.09307821e-09\n",
      " 8.20023925e-07 1.62915185e-07 1.02222904e-06 5.34267748e+01\n",
      " 9.90616403e+01 9.94168245e+01 4.47124938e+01]\n",
      "32-th iteration, loss: 0.509116538614722, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6618772578283382e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111794e+00 4.44518352e-09 0.00000000e+00 1.51136240e-07\n",
      " 1.19012039e-06 5.34267748e+01 9.90616397e+01 9.94168237e+01\n",
      " 4.47124938e+01]\n",
      "33-th iteration, loss: 0.5091165386147175, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6073265166838715e-07\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111828e+00 3.77268818e-09 0.00000000e+00 1.28271398e-07\n",
      " 1.34878875e-06 5.34267749e+01 9.90616391e+01 9.94168228e+01\n",
      " 4.47124938e+01]\n",
      "34-th iteration, loss: 0.5091165386147136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5522958829732472e-07\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111860e+00 3.01152732e-09 0.00000000e+00 1.02391929e-07\n",
      " 1.50333829e-06 5.34267750e+01 9.90616386e+01 9.94168220e+01\n",
      " 4.47124938e+01]\n",
      "35-th iteration, loss: 0.50911653861471, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.497450383972368e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111892e+00 2.14094383e-09 0.00000000e+00 7.27920901e-08\n",
      " 1.65393561e-06 5.34267750e+01 9.90616381e+01 9.94168212e+01\n",
      " 4.47124938e+01]\n",
      "36-th iteration, loss: 0.5091165386147066, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4432313764503513e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111922e+00 1.14812295e-09 0.00000000e+00 3.90361802e-08\n",
      " 1.80085216e-06 5.34267751e+01 9.90616376e+01 9.94168205e+01\n",
      " 4.47124938e+01]\n",
      "37-th iteration, loss: 0.5091165386147036, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.389924365970731e-07\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111951e+00 2.56443845e-11 0.00000000e+00 8.71909074e-10\n",
      " 1.94442669e-06 5.34267752e+01 9.90616372e+01 9.94168198e+01\n",
      " 4.47124938e+01]\n",
      "38-th iteration, loss: 0.5091165386147007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.778287915536088e-08\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112188 53.42677526 99.06163673 99.41681908 44.7124938 ]\n",
      "39-th iteration, loss: 0.5091165386146984, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.310176877501799e-08\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112201 53.42677533 99.06163631 99.41681843 44.7124938 ]\n",
      "40-th iteration, loss: 0.5091165386146962, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.69070613509461e-08\n",
      "40-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112215 53.42677541 99.06163592 99.41681781 44.7124938 ]\n",
      "41-th iteration, loss: 0.5091165386146942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.945235811404187e-08\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112228e+00 0.00000000e+00 4.96824804e-15 5.34267755e+01\n",
      " 9.90616355e+01 9.94168172e+01 4.47124938e+01]\n",
      "42-th iteration, loss: 0.5091165386146922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2291328367775442e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[4.60112241e+00 2.33141997e-09 0.00000000e+00 7.92682788e-08\n",
      " 1.29888291e-07 5.34267756e+01 9.90616352e+01 9.94168166e+01\n",
      " 4.47124938e+01]\n",
      "43-th iteration, loss: 0.5091165386146904, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1350991835691805e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112253e+00 2.28637120e-09 0.00000000e+00 5.94456512e-08\n",
      " 1.22927966e-07 1.38668881e-07 2.47825104e-07 5.34267756e+01\n",
      " 9.90616348e+01 9.94168161e+01 4.47124938e+01]\n",
      "44-th iteration, loss: 0.5091165386146888, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.039718926797924e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[4.60112264e+00 1.29133651e-09 0.00000000e+00 2.71180667e-08\n",
      " 1.13688596e-07 8.55686827e-08 2.32873547e-07 1.72532111e-07\n",
      " 3.49039356e-07 5.34267757e+01 9.90616345e+01 9.94168156e+01\n",
      " 4.47124938e+01]\n",
      "45-th iteration, loss: 0.5091165386146872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.573997600546809e-08\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[4.60112285e+00 9.13319653e-10 0.00000000e+00 1.91797127e-08\n",
      " 2.16629159e-07 8.57020855e-08 3.30426251e-07 1.87328472e-07\n",
      " 4.35728521e-07 5.34267757e+01 9.90616342e+01 9.94168151e+01\n",
      " 4.47124938e+01]\n",
      "46-th iteration, loss: 0.5091165386146858, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.915223086607371e-08\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112336e+00 2.62731695e-09 0.00000000e+00 6.83102408e-08\n",
      " 4.20684299e-07 1.93369338e-07 5.14191373e-07 5.34267758e+01\n",
      " 9.90616339e+01 9.94168146e+01 4.47124938e+01]\n",
      "47-th iteration, loss: 0.5091165386146845, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.49789527777623e-08\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112354e+00 1.59219715e-09 0.00000000e+00 4.13971260e-08\n",
      " 5.06652276e-07 1.94536943e-07 5.87983787e-07 5.34267758e+01\n",
      " 9.90616336e+01 9.94168141e+01 4.47124938e+01]\n",
      "48-th iteration, loss: 0.5091165386146833, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.110187742313197e-08\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[4.60112371e+00 3.26097810e-10 0.00000000e+00 8.47854307e-09\n",
      " 5.90179730e-07 1.93845979e-07 6.59262161e-07 5.34267758e+01\n",
      " 9.90616333e+01 9.94168137e+01 4.47124938e+01]\n",
      "49-th iteration, loss: 0.5091165386146823, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.612395475543989e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[4.60112455e+00 5.48016420e-09 0.00000000e+00 1.86325583e-07\n",
      " 7.28803278e-07 5.34267759e+01 9.90616330e+01 9.94168133e+01\n",
      " 4.47124938e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5365154209642973\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.79326351   0.         307.96553046]\n",
      "1-th iteration, loss: 0.7479635372963356, 11 gd steps\n",
      "insert gradient: -0.6308616976652868\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.45607731  62.35062607 236.60766365   0.          71.35786681]\n",
      "2-th iteration, loss: 0.6054338475143709, 13 gd steps\n",
      "insert gradient: -0.45076273962597946\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.41006398  77.3179864  117.58506071   0.         104.0175537\n",
      "  40.90133603  71.35786681]\n",
      "3-th iteration, loss: 0.5094596098736359, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.724385749047338e-17\n",
      "3-th iteration, new layer inserted. now 5 layers\n",
      "[  2.22457638  54.58246062  97.89056962 101.14251937  71.35786681]\n",
      "4-th iteration, loss: 0.509116539241859, 37 gd steps\n",
      "insert gradient: -7.991871506944366e-05\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.60142087e+00 5.34279174e+01 9.90593730e+01 0.00000000e+00\n",
      " 5.32907052e-14 9.94153673e+01 7.13578668e+01]\n",
      "5-th iteration, loss: 0.509116538616539, 15 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5496290395444653e-06\n",
      "5-th iteration, new layer inserted. now 7 layers\n",
      "[4.60109939e+00 0.00000000e+00 1.02695630e-15 5.34267944e+01\n",
      " 9.90615414e+01 9.94169858e+01 7.13578668e+01]\n",
      "6-th iteration, loss: 0.5091165386164169, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1311927254112953e-06\n",
      "6-th iteration, new layer inserted. now 7 layers\n",
      "[4.60110022e+00 1.47625667e-06 8.35286826e-07 5.34267959e+01\n",
      " 9.90615415e+01 9.94169807e+01 7.13578668e+01]\n",
      "7-th iteration, loss: 0.5091165386163107, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.311779213142963e-07\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[4.60110094e+00 2.50895830e-06 1.46465958e-06 0.00000000e+00\n",
      " 7.80858498e-22 5.34267970e+01 9.90615416e+01 9.94169758e+01\n",
      " 7.13578668e+01]\n",
      "8-th iteration, loss: 0.5091165386162133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.416955867971397e-07\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110158e+00 1.18373502e-07 0.00000000e+00 3.07771105e-06\n",
      " 1.94004364e-06 7.79348828e-07 4.75384066e-07 5.34267978e+01\n",
      " 9.90615417e+01 9.94169710e+01 7.13578668e+01]\n",
      "9-th iteration, loss: 0.5091165386161256, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.690638434772865e-07\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110212e+00 2.01136147e-08 0.00000000e+00 4.22385908e-07\n",
      " 5.34081866e-07 3.40183707e-06 2.28033685e-06 1.22563007e-06\n",
      " 7.66605415e-07 5.34267982e+01 9.90615417e+01 9.94169664e+01\n",
      " 7.13578668e+01]\n",
      "10-th iteration, loss: 0.5091165386160452, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.174035750545395e-07\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[4.60110259e+00 3.32702188e-09 0.00000000e+00 5.65593720e-08\n",
      " 4.67396973e-07 4.62158687e-07 9.74883231e-07 3.47523842e-06\n",
      " 2.50694088e-06 1.44261328e-06 9.16037407e-07 5.34267985e+01\n",
      " 9.90615418e+01 9.94169619e+01 7.13578668e+01]\n",
      "11-th iteration, loss: 0.5091165386159701, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.772479598893315e-07\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110431e+00 1.60381894e-08 0.00000000e+00 3.36801978e-07\n",
      " 1.36271420e-06 3.42730361e-06 2.67595278e-06 1.55252851e-06\n",
      " 9.94214870e-07 5.34267987e+01 9.90615418e+01 9.94169576e+01\n",
      " 7.13578668e+01]\n",
      "12-th iteration, loss: 0.5091165386159007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.55605224464375e-07\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[4.60110508e+00 6.50543189e-09 0.00000000e+00 1.36614070e-07\n",
      " 1.72339484e-06 3.31942470e-06 2.82083257e-06 1.61314151e-06\n",
      " 1.04133938e-06 5.34267988e+01 9.90615420e+01 9.94169535e+01\n",
      " 7.13578668e+01]\n",
      "13-th iteration, loss: 0.509116538615836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.262746117250606e-07\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110787e+00 1.18159911e-07 0.00000000e+00 3.07215770e-06\n",
      " 2.96407895e-06 1.66164873e-06 1.08301397e-06 5.34267989e+01\n",
      " 9.90615421e+01 9.94169495e+01 7.13578668e+01]\n",
      "14-th iteration, loss: 0.5091165386157759, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.145097674831355e-07\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110854e+00 1.07767999e-07 0.00000000e+00 2.80196796e-06\n",
      " 3.10093917e-06 1.68586084e-06 1.11524812e-06 5.34267990e+01\n",
      " 9.90615423e+01 9.94169456e+01 7.13578668e+01]\n",
      "15-th iteration, loss: 0.5091165386157198, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.025707480501758e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110919e+00 9.73846855e-08 0.00000000e+00 2.53200182e-06\n",
      " 3.24289921e-06 1.70853068e-06 1.15105757e-06 5.34267991e+01\n",
      " 9.90615426e+01 9.94169418e+01 7.13578668e+01]\n",
      "16-th iteration, loss: 0.5091165386156673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2629044317013235e-07\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[4.60110980e+00 2.34722795e-06 3.38983696e-06 1.72794664e-06\n",
      " 1.19041732e-06 5.34267992e+01 0.00000000e+00 2.39808173e-14\n",
      " 9.90615429e+01 9.94169382e+01 7.13578668e+01]\n",
      "17-th iteration, loss: 0.5091165386156179, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.497509935368353e-07\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[4.60111010e+00 2.13836001e-06 3.53964616e-06 1.73252052e-06\n",
      " 1.23142598e-06 5.34267993e+01 3.37446526e-07 7.95287879e-08\n",
      " 0.00000000e+00 2.56425990e-23 9.90615432e+01 9.94169347e+01\n",
      " 7.13578668e+01]\n",
      "18-th iteration, loss: 0.5091165386155709, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6193858085042553e-07\n",
      "18-th iteration, new layer inserted. now 15 layers\n",
      "[4.60111039e+00 1.89087112e-06 3.68882443e-06 1.70790622e-06\n",
      " 1.27151572e-06 5.34267993e+01 6.89280752e-07 1.01219827e-07\n",
      " 3.59194849e-07 2.16910387e-08 0.00000000e+00 3.30872245e-24\n",
      " 9.90615436e+01 9.94169313e+01 7.13578668e+01]\n",
      "19-th iteration, loss: 0.5091165386155259, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.643312088061833e-07\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111066e+00 1.61033817e-06 3.84108377e-06 1.65964091e-06\n",
      " 1.31623635e-06 5.34267993e+01 1.04985738e-06 6.92208269e-08\n",
      " 9.90615451e+01 9.94169280e+01 7.13578668e+01]\n",
      "20-th iteration, loss: 0.509116538615484, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.949654581977994e-07\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111092e+00 1.30930003e-06 4.00130821e-06 1.60045745e-06\n",
      " 1.37196113e-06 5.34267994e+01 0.00000000e+00 2.70894418e-14\n",
      " 9.90615469e+01 9.94169249e+01 7.13578668e+01]\n",
      "21-th iteration, loss: 0.5091165386154446, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1636131209317743e-07\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111118e+00 1.00760710e-06 4.17580771e-06 1.55070778e-06\n",
      " 1.44568747e-06 5.34267994e+01 4.06465990e-07 3.66362031e-08\n",
      " 9.90615473e+01 9.94169218e+01 7.13578668e+01]\n",
      "22-th iteration, loss: 0.5091165386154073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3713466123012844e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111142e+00 6.89498439e-07 4.36107937e-06 1.49552979e-06\n",
      " 1.53331846e-06 5.34267994e+01 8.31014444e-07 3.48666316e-08\n",
      " 9.90615477e+01 9.94169188e+01 7.13578668e+01]\n",
      "23-th iteration, loss: 0.509116538615372, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.5650826083181043e-07\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111167e+00 3.58377247e-07 4.55904262e-06 1.43900494e-06\n",
      " 1.63711533e-06 5.34267995e+01 9.90615494e+01 9.94169159e+01\n",
      " 7.13578668e+01]\n",
      "24-th iteration, loss: 0.5091165386153392, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.886569584081363e-07\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111190e+00 1.97192823e-08 4.77181356e-06 1.38740813e-06\n",
      " 1.75927900e-06 5.34267995e+01 9.90615499e+01 9.94169130e+01\n",
      " 7.13578668e+01]\n",
      "25-th iteration, loss: 0.5091165386153087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.04132519685508e-07\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111713e+00 1.34382659e-06 1.90076975e-06 5.34267996e+01\n",
      " 9.90615504e+01 9.94169103e+01 7.13578668e+01]\n",
      "26-th iteration, loss: 0.5091165386152803, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.228358291760236e-07\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111735e+00 1.25516583e-06 2.02895823e-06 5.34267996e+01\n",
      " 0.00000000e+00 3.15303339e-14 9.90615509e+01 9.94169077e+01\n",
      " 7.13578668e+01]\n",
      "27-th iteration, loss: 0.5091165386152523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.215406882397523e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[4.60111755e+00 1.13660390e-06 2.15237438e-06 5.34267996e+01\n",
      " 5.27541403e-07 9.19193487e-09 0.00000000e+00 9.30578189e-25\n",
      " 9.90615514e+01 9.94169051e+01 7.13578668e+01]\n",
      "28-th iteration, loss: 0.5091165386152248, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.032051082439788e-07\n",
      "28-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111774e+00 9.78185293e-07 2.27056500e-06 5.34267996e+01\n",
      " 9.90615535e+01 9.94169027e+01 7.13578668e+01]\n",
      "29-th iteration, loss: 0.5091165386152007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.264471875399688e-07\n",
      "29-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111792e+00 7.85948174e-07 2.38726958e-06 5.34267996e+01\n",
      " 9.90615541e+01 9.94169002e+01 7.13578668e+01]\n",
      "30-th iteration, loss: 0.5091165386151778, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.472276960105089e-07\n",
      "30-th iteration, new layer inserted. now 7 layers\n",
      "[4.60111809e+00 5.95050896e-07 2.51322994e-06 5.34267995e+01\n",
      " 9.90615546e+01 9.94168979e+01 7.13578668e+01]\n",
      "31-th iteration, loss: 0.5091165386151563, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.659116271958049e-07\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111826e+00 4.02495611e-07 2.64772922e-06 5.34267995e+01\n",
      " 0.00000000e+00 3.77475828e-14 9.90615552e+01 9.94168956e+01\n",
      " 7.13578668e+01]\n",
      "32-th iteration, loss: 0.5091165386151345, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.618173307888983e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[4.60111843e+00 1.99016853e-07 2.78889950e-06 5.34267995e+01\n",
      " 0.00000000e+00 1.64313008e-14 9.90615563e+01 9.94168934e+01\n",
      " 7.13578668e+01]\n",
      "33-th iteration, loss: 0.5091165386151139, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.575739907150986e-07\n",
      "33-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112152 53.4267994  99.06155743 99.41689131 71.35786681]\n",
      "34-th iteration, loss: 0.5091165386150956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.681655929531871e-07\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112167e+00 5.34267993e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90615580e+01 9.94168892e+01 7.13578668e+01]\n",
      "35-th iteration, loss: 0.5091165386150771, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.565978413136549e-07\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112181e+00 5.34267993e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90615591e+01 9.94168872e+01 7.13578668e+01]\n",
      "36-th iteration, loss: 0.5091165386150596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.46385049208558e-07\n",
      "36-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112195 53.42679914 99.06156026 99.41688526 71.35786681]\n",
      "37-th iteration, loss: 0.509116538615044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.579651025541763e-07\n",
      "37-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112207 53.426799   99.06156081 99.41688335 71.35786681]\n",
      "38-th iteration, loss: 0.5091165386150293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.683757567188375e-07\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6011222  53.42679886 99.06156137 99.4168815  71.35786681]\n",
      "39-th iteration, loss: 0.5091165386150153, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.777016165435082e-07\n",
      "39-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112232e+00 5.34267987e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90615619e+01 9.94168797e+01 7.13578668e+01]\n",
      "40-th iteration, loss: 0.5091165386150007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.641767373409481e-07\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112243e+00 5.34267986e+01 0.00000000e+00 4.13002965e-14\n",
      " 9.90615631e+01 9.94168780e+01 7.13578668e+01]\n",
      "41-th iteration, loss: 0.5091165386149868, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.52002853656945e-07\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112254e+00 5.34267984e+01 0.00000000e+00 4.26325641e-14\n",
      " 9.90615642e+01 9.94168763e+01 7.13578668e+01]\n",
      "42-th iteration, loss: 0.5091165386149735, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.40937277871605e-07\n",
      "42-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112264e+00 5.34267982e+01 0.00000000e+00 5.90638649e-14\n",
      " 9.90615654e+01 9.94168746e+01 7.13578668e+01]\n",
      "43-th iteration, loss: 0.5091165386149609, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.307791280910284e-07\n",
      "43-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112274 53.426798   99.06156645 99.41687302 71.35786681]\n",
      "44-th iteration, loss: 0.5091165386149499, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.416865108910824e-07\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112283e+00 5.34267978e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90615670e+01 9.94168714e+01 7.13578668e+01]\n",
      "45-th iteration, loss: 0.5091165386149382, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.30454089356228e-07\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112292 53.42679756 99.06156808 99.41686992 71.35786681]\n",
      "46-th iteration, loss: 0.5091165386149283, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.403775203393498e-07\n",
      "46-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.601123   53.42679733 99.06156862 99.41686844 71.35786681]\n",
      "47-th iteration, loss: 0.5091165386149189, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.489059327845704e-07\n",
      "47-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112309 53.42679712 99.06156917 99.416867   71.35786681]\n",
      "48-th iteration, loss: 0.5091165386149098, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.56186012694776e-07\n",
      "48-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60112318 53.42679691 99.06156972 99.4168656  71.35786681]\n",
      "49-th iteration, loss: 0.5091165386149012, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.623462500946394e-07\n",
      "49-th iteration, new layer inserted. now 7 layers\n",
      "[4.60112326e+00 5.34267967e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90615703e+01 9.94168642e+01 7.13578668e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536648578875073\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.89416964   0.         332.99025247]\n",
      "1-th iteration, loss: 0.7481568929324234, 11 gd steps\n",
      "insert gradient: -0.6317802871014003\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.52886631  62.34663952 235.52969077   0.          97.4605617 ]\n",
      "2-th iteration, loss: 0.6049244714380901, 13 gd steps\n",
      "insert gradient: -0.4493983797547323\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.4231431   77.4251234  117.11640185   0.         103.60297087\n",
      "  41.29093827  97.4605617 ]\n",
      "3-th iteration, loss: 0.5093962299073077, 46 gd steps\n",
      "insert gradient: -0.004143449235379363\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[2.52687069e+00 5.44613615e+01 0.00000000e+00 2.97539771e-14\n",
      " 9.77951608e+01 1.01016079e+02 9.74605617e+01]\n",
      "4-th iteration, loss: 0.5091259512142529, 29 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 3.574344316312493e-15\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[4.10070158e+00 5.36175090e+01 9.89524018e+01 9.95931410e+01\n",
      " 0.00000000e+00 1.13686838e-13 9.74605617e+01]\n",
      "5-th iteration, loss: 0.5091165388925496, 33 gd steps\n",
      "insert gradient: -0.0001285211202457494\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[4.60140241e+00 0.00000000e+00 2.88657986e-15 5.34258407e+01\n",
      " 9.90611860e+01 9.94163038e+01 6.52542723e-16 1.17075077e-03\n",
      " 9.74605617e+01]\n",
      "6-th iteration, loss: 0.5091165386211892, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3625865647593567e-06\n",
      "6-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60142489 53.42679318 99.06135333 99.41704939 97.4605617 ]\n",
      "7-th iteration, loss: 0.5091165386209902, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3567519280167423e-06\n",
      "7-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60142075 53.42679284 99.0613567  99.41704466 97.4605617 ]\n",
      "8-th iteration, loss: 0.5091165386207978, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.347201802452666e-06\n",
      "8-th iteration, new layer inserted. now 7 layers\n",
      "[4.60141668e+00 5.34267926e+01 0.00000000e+00 3.68594044e-14\n",
      " 9.90613601e+01 9.94170401e+01 9.74605617e+01]\n",
      "9-th iteration, loss: 0.5091165386205686, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.210023867567041e-06\n",
      "9-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60141266 53.42679225 99.06136675 99.41703555 97.4605617 ]\n",
      "10-th iteration, loss: 0.5091165386203891, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2050017457537955e-06\n",
      "10-th iteration, new layer inserted. now 7 layers\n",
      "[4.60140869e+00 5.34267919e+01 0.00000000e+00 5.90638649e-14\n",
      " 9.90613700e+01 9.94170311e+01 9.74605617e+01]\n",
      "11-th iteration, loss: 0.5091165386201759, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0769139479885836e-06\n",
      "11-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60140477 53.42679154 99.06137637 99.41702679 97.4605617 ]\n",
      "12-th iteration, loss: 0.5091165386200085, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.075021237038052e-06\n",
      "12-th iteration, new layer inserted. now 7 layers\n",
      "[4.60140089e+00 5.34267912e+01 0.00000000e+00 1.90958360e-14\n",
      " 9.90613795e+01 9.94170225e+01 9.74605617e+01]\n",
      "13-th iteration, loss: 0.5091165386198101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.954437865053403e-06\n",
      "13-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60139707 53.42679077 99.06138561 99.41701836 97.4605617 ]\n",
      "14-th iteration, loss: 0.5091165386196537, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9546797015563493e-06\n",
      "14-th iteration, new layer inserted. now 7 layers\n",
      "[4.60139329e+00 5.34267904e+01 0.00000000e+00 3.41948692e-14\n",
      " 9.90613886e+01 9.94170143e+01 9.74605617e+01]\n",
      "15-th iteration, loss: 0.5091165386194689, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.840437186814864e-06\n",
      "15-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60138957 53.42678996 99.06139448 99.41701026 97.4605617 ]\n",
      "16-th iteration, loss: 0.509116538619323, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8421078087843506e-06\n",
      "16-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6013859  53.42678954 99.06139733 99.41700633 97.4605617 ]\n",
      "17-th iteration, loss: 0.509116538619182, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.839442020977499e-06\n",
      "17-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60138228 53.42678917 99.06140019 99.41700249 97.4605617 ]\n",
      "18-th iteration, loss: 0.5091165386190458, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8330440908887572e-06\n",
      "18-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60137873 53.42678885 99.06140304 99.41699874 97.4605617 ]\n",
      "19-th iteration, loss: 0.509116538618914, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8234342338970485e-06\n",
      "19-th iteration, new layer inserted. now 7 layers\n",
      "[4.60137524e+00 5.34267886e+01 0.00000000e+00 5.72875081e-14\n",
      " 9.90614059e+01 9.94169951e+01 9.74605617e+01]\n",
      "20-th iteration, loss: 0.5091165386187559, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7060695427553277e-06\n",
      "20-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60137179 53.42678829 99.06141152 99.41699151 97.4605617 ]\n",
      "21-th iteration, loss: 0.5091165386186328, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.700211620059498e-06\n",
      "21-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136838 53.42678796 99.06141423 99.41698799 97.4605617 ]\n",
      "22-th iteration, loss: 0.5091165386185137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6911875862575664e-06\n",
      "22-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136503 53.42678768 99.06141695 99.41698456 97.4605617 ]\n",
      "23-th iteration, loss: 0.5091165386183986, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.679443791445919e-06\n",
      "23-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60136173 53.42678743 99.06141965 99.4169812  97.4605617 ]\n",
      "24-th iteration, loss: 0.5091165386182871, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.66536482795754e-06\n",
      "24-th iteration, new layer inserted. now 7 layers\n",
      "[4.60135848e+00 5.34267872e+01 0.00000000e+00 2.62012634e-14\n",
      " 9.90614223e+01 9.94169779e+01 9.74605617e+01]\n",
      "25-th iteration, loss: 0.5091165386181519, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5503142258882544e-06\n",
      "25-th iteration, new layer inserted. now 7 layers\n",
      "[4.60135528e+00 5.34267870e+01 0.00000000e+00 3.19744231e-14\n",
      " 9.90614276e+01 9.94169747e+01 9.74605617e+01]\n",
      "26-th iteration, loss: 0.5091165386180226, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.445912689936802e-06\n",
      "26-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.6013521  53.42678668 99.06143274 99.41697155 97.4605617 ]\n",
      "27-th iteration, loss: 0.5091165386179219, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4420145449723814e-06\n",
      "27-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134896 53.42678634 99.06143519 99.41696843 97.4605617 ]\n",
      "28-th iteration, loss: 0.5091165386178246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4349353677717023e-06\n",
      "28-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134587 53.42678605 99.06143765 99.41696538 97.4605617 ]\n",
      "29-th iteration, loss: 0.5091165386177303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4251309104195413e-06\n",
      "29-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60134283 53.42678579 99.06144009 99.4169624  97.4605617 ]\n",
      "30-th iteration, loss: 0.5091165386176391, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.41299321607541e-06\n",
      "30-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60133984 53.42678557 99.06144252 99.4169595  97.4605617 ]\n",
      "31-th iteration, loss: 0.5091165386175508, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.39885974435423e-06\n",
      "31-th iteration, new layer inserted. now 7 layers\n",
      "[4.60133690e+00 5.34267854e+01 0.00000000e+00 2.88657986e-14\n",
      " 9.90614449e+01 9.94169567e+01 9.74605617e+01]\n",
      "32-th iteration, loss: 0.5091165386174433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.293965661698472e-06\n",
      "32-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60133399 53.42678516 99.06144972 99.41695388 97.4605617 ]\n",
      "33-th iteration, loss: 0.5091165386173607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2842124417905107e-06\n",
      "33-th iteration, new layer inserted. now 7 layers\n",
      "[4.60133112e+00 5.34267849e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90614520e+01 9.94169511e+01 9.74605617e+01]\n",
      "34-th iteration, loss: 0.5091165386172607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1873133630649984e-06\n",
      "34-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132829e+00 5.34267847e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90614566e+01 9.94169484e+01 9.74605617e+01]\n",
      "35-th iteration, loss: 0.509116538617165, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.099121275574982e-06\n",
      "35-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132548e+00 5.34267844e+01 0.00000000e+00 1.37667655e-14\n",
      " 9.90614609e+01 9.94169458e+01 9.74605617e+01]\n",
      "36-th iteration, loss: 0.5091165386170734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0184457622581513e-06\n",
      "36-th iteration, new layer inserted. now 7 layers\n",
      "[4.60132270e+00 5.34267840e+01 0.00000000e+00 6.57252031e-14\n",
      " 9.90614651e+01 9.94169432e+01 9.74605617e+01]\n",
      "37-th iteration, loss: 0.5091165386169855, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9442798551531775e-06\n",
      "37-th iteration, new layer inserted. now 7 layers\n",
      "[4.60131995e+00 5.34267837e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90614692e+01 9.94169406e+01 9.74605617e+01]\n",
      "38-th iteration, loss: 0.5091165386169011, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.875770815395197e-06\n",
      "38-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60131724 53.42678326 99.06147308 99.41693801 97.4605617 ]\n",
      "39-th iteration, loss: 0.5091165386168338, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8827055348821934e-06\n",
      "39-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60131456 53.42678286 99.06147496 99.41693549 97.4605617 ]\n",
      "40-th iteration, loss: 0.5091165386167688, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8856641327600474e-06\n",
      "40-th iteration, new layer inserted. now 7 layers\n",
      "[4.60131193e+00 5.34267825e+01 0.00000000e+00 3.41948692e-14\n",
      " 9.90614769e+01 9.94169330e+01 9.74605617e+01]\n",
      "41-th iteration, loss: 0.5091165386166921, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8146985799779228e-06\n",
      "41-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130934e+00 5.34267822e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.90614806e+01 9.94169306e+01 9.74605617e+01]\n",
      "42-th iteration, loss: 0.5091165386166188, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7493274005810112e-06\n",
      "42-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60130679 53.42678188 99.06148426 99.41692826 97.4605617 ]\n",
      "43-th iteration, loss: 0.5091165386165601, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7544847176894599e-06\n",
      "43-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130426e+00 5.34267815e+01 0.00000000e+00 2.88657986e-14\n",
      " 9.90614860e+01 9.94169259e+01 9.74605617e+01]\n",
      "44-th iteration, loss: 0.5091165386164915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6904511507852722e-06\n",
      "44-th iteration, new layer inserted. now 7 layers\n",
      "[4.60130178e+00 5.34267812e+01 0.00000000e+00 4.39648318e-14\n",
      " 9.90614895e+01 9.94169236e+01 9.74605617e+01]\n",
      "45-th iteration, loss: 0.5091165386164257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6312769010118217e-06\n",
      "45-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60129933 53.42678091 99.06149291 99.4169214  97.4605617 ]\n",
      "46-th iteration, loss: 0.5091165386163727, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6376116545960274e-06\n",
      "46-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129691e+00 5.34267806e+01 0.00000000e+00 1.37667655e-14\n",
      " 9.90614946e+01 9.94169192e+01 9.74605617e+01]\n",
      "47-th iteration, loss: 0.5091165386163112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5791250629422343e-06\n",
      "47-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129454e+00 5.34267803e+01 0.00000000e+00 1.95399252e-14\n",
      " 9.90614978e+01 9.94169170e+01 9.74605617e+01]\n",
      "48-th iteration, loss: 0.5091165386162522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5249576303005705e-06\n",
      "48-th iteration, new layer inserted. now 7 layers\n",
      "[4.60129219e+00 5.34267800e+01 0.00000000e+00 3.55271368e-14\n",
      " 9.90615010e+01 9.94169149e+01 9.74605617e+01]\n",
      "49-th iteration, loss: 0.5091165386161955, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4745642843768111e-06\n",
      "49-th iteration, new layer inserted. now 5 layers\n",
      "[ 4.60128987 53.42677967 99.06150405 99.41691278 97.4605617 ]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224633\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         358.41859902]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6306377752902012\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 236.03176033   0.         122.38683869]\n",
      "2-th iteration, loss: 0.6053204051370064, 13 gd steps\n",
      "insert gradient: -0.601544884542837\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.48858706  77.41183875 221.16730282  41.05484644 119.8891481\n",
      "   0.           2.49769059]\n",
      "3-th iteration, loss: 0.46612598501238417, 33 gd steps\n",
      "insert gradient: -0.18687363939791926\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         101.98170961 106.33312455   0.          79.74984341\n",
      "  57.62253391 100.73799881  58.08011102   2.49769059]\n",
      "4-th iteration, loss: 0.4565502198648592, 19 gd steps\n",
      "insert gradient: -0.2413655063829578\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         100.95705006  98.70806018  12.26706368  71.45861816\n",
      "  56.38804621 103.70246795  63.11943575   2.49769059]\n",
      "5-th iteration, loss: 0.34167297461453605, 86 gd steps\n",
      "insert gradient: -0.1281305633055248\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[2.66343621e+00 8.32691456e+01 1.27313316e+02 7.62433188e+01\n",
      " 9.80309389e+01 0.00000000e+00 3.64153152e-14 4.21058314e+01\n",
      " 6.59014028e+01 4.84630326e+01 2.49769059e+00]\n",
      "6-th iteration, loss: 0.3400102133021442, 49 gd steps\n",
      "insert gradient: -0.004279676441337002\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  1.42128433  84.88283555 124.41513058  78.430674    98.90392544\n",
      "  44.70258387  68.10902609  47.96219196   2.49769059]\n",
      "7-th iteration, loss: 0.33998323257926916, 13 gd steps\n",
      "insert gradient: -0.003836935921141341\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[3.82148275e-01 0.00000000e+00 1.35308431e-16 8.54757700e+01\n",
      " 1.23454902e+02 7.88245457e+01 9.87035499e+01 4.47194963e+01\n",
      " 6.83256745e+01 4.80029837e+01 2.49769059e+00]\n",
      "8-th iteration, loss: 0.3399751351597464, 43 gd steps\n",
      "insert gradient: -0.0001231978606845064\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.39368871  85.62953539 123.45427426  78.80163664  98.79900465\n",
      "  44.87706704  68.31930119  47.7226276    2.49769059]\n",
      "9-th iteration, loss: 0.33997510991237917, 48 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.478298930568304e-06\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565624  85.642724   123.42742087  78.81367414  98.80671331\n",
      "  44.87255808  68.34189538  47.71588117   2.49769059]\n",
      "10-th iteration, loss: 0.3399751099112745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.05221141008726e-06\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856574   85.64273025 123.42742348  78.81367989  98.80671232\n",
      "  44.87255044  68.34189029  47.71587609   2.49769059]\n",
      "11-th iteration, loss: 0.33997510991034857, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.659745264240285e-06\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658460e-01 0.00000000e+00 2.63677968e-16 8.56427361e+01\n",
      " 1.23427426e+02 7.88136852e+01 9.88067114e+01 4.48725436e+01\n",
      " 6.83418856e+01 4.77158715e+01 2.49769059e+00]\n",
      "12-th iteration, loss: 0.3399751099094087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.147506235958417e-06\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[3.85659396e-01 5.35040525e-06 9.36028554e-07 0.00000000e+00\n",
      " 2.77932686e-22 8.56427414e+01 1.23427428e+02 7.88136901e+01\n",
      " 9.88067106e+01 4.48725373e+01 6.83418814e+01 4.77158674e+01\n",
      " 2.49769059e+00]\n",
      "13-th iteration, loss: 0.3399751099085181, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.554028582441382e-06\n",
      "13-th iteration, new layer inserted. now 15 layers\n",
      "[3.85660186e-01 1.01081629e-05 1.66829069e-06 4.76791393e-06\n",
      " 7.32262137e-07 0.00000000e+00 9.26442286e-23 8.56427462e+01\n",
      " 1.23427430e+02 7.88136945e+01 9.88067099e+01 4.48725318e+01\n",
      " 6.83418775e+01 4.77158637e+01 2.49769059e+00]\n",
      "14-th iteration, loss: 0.33997510990772883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.921027643032778e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[3.85660817e-01 1.42179756e-05 2.18984245e-06 8.89582820e-06\n",
      " 1.20208032e-06 4.13585958e-06 4.69818181e-07 8.56427503e+01\n",
      " 1.23427431e+02 7.88136985e+01 9.88067092e+01 4.48725269e+01\n",
      " 6.83418741e+01 4.77158604e+01 2.49769059e+00]\n",
      "15-th iteration, loss: 0.339975109907133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5159871352236242e-06\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661302e-01 1.77382622e-05 2.52064205e-06 1.24398753e-05\n",
      " 1.43635700e-06 7.69294973e-06 6.59219303e-07 8.56427539e+01\n",
      " 1.23427433e+02 7.88137021e+01 9.88067087e+01 4.48725227e+01\n",
      " 6.83418710e+01 4.77158575e+01 2.49769059e+00]\n",
      "16-th iteration, loss: 0.3399751099066804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1988539668523456e-06\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[3.85661663e-01 2.07579131e-05 2.68932641e-06 1.54868761e-05\n",
      " 1.47006427e-06 1.07555354e-05 6.09455328e-07 8.56427570e+01\n",
      " 1.23427434e+02 0.00000000e+00 3.73034936e-14 7.88137053e+01\n",
      " 9.88067081e+01 4.48725190e+01 6.83418682e+01 4.77158549e+01\n",
      " 2.49769059e+00]\n",
      "17-th iteration, loss: 0.3399751099062848, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.829621786232623e-06\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[3.85661909e-01 2.33063069e-05 2.71005929e-06 1.80644500e-05\n",
      " 1.32275904e-06 1.33490601e-05 3.45448698e-07 8.56427596e+01\n",
      " 1.23427434e+02 2.93213258e-06 8.05024692e-07 0.00000000e+00\n",
      " 9.26442286e-23 7.88137083e+01 9.88067077e+01 4.48725158e+01\n",
      " 6.83418658e+01 4.77158527e+01 2.49769059e+00]\n",
      "18-th iteration, loss: 0.3399751099059534, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.435837008479167e-06\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[3.85662053e-01 2.54340781e-05 2.60098863e-06 2.02216264e-05\n",
      " 1.01768269e-06 8.56427773e+01 1.23427435e+02 5.47752869e-06\n",
      " 1.32812106e-06 2.55824904e-06 5.23096365e-07 0.00000000e+00\n",
      " 9.26442286e-23 7.88137108e+01 9.88067072e+01 4.48725130e+01\n",
      " 6.83418636e+01 4.77158507e+01 2.49769059e+00]\n",
      "19-th iteration, loss: 0.33997510990570573, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0590183642946555e-06\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[3.85662116e-01 2.72289832e-05 2.38784062e-06 2.20447531e-05\n",
      " 5.85122874e-07 8.56427791e+01 1.23427435e+02 7.64496677e-06\n",
      " 1.59712641e-06 4.74689175e-06 7.51257000e-07 2.19699441e-06\n",
      " 2.28160636e-07 7.88137130e+01 9.88067068e+01 4.48725107e+01\n",
      " 6.83418617e+01 4.77158490e+01 2.49769059e+00]\n",
      "20-th iteration, loss: 0.3399751099055249, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7494307570503479e-06\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[3.85662118e-01 2.87726385e-05 2.09445958e-06 2.36143173e-05\n",
      " 5.25484765e-08 8.56427807e+01 1.23427436e+02 9.46708222e-06\n",
      " 1.65211730e-06 6.59450674e-06 7.30459552e-07 4.05660388e-06\n",
      " 1.72286223e-07 0.00000000e+00 2.31610572e-23 7.88137149e+01\n",
      " 9.88067064e+01 4.48725087e+01 6.83418600e+01 4.77158475e+01\n",
      " 2.49769059e+00]\n",
      "21-th iteration, loss: 0.339975109905379, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4495358920104744e-06\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[3.85662067e-01 3.00951267e-05 1.73142461e-06 8.56428070e+01\n",
      " 1.23427436e+02 1.09923756e-05 1.52680841e-06 8.14617764e-06\n",
      " 4.99863495e-07 0.00000000e+00 9.26442286e-23 7.88137236e+01\n",
      " 9.88067060e+01 4.48725070e+01 6.83418585e+01 4.77158463e+01\n",
      " 2.49769059e+00]\n",
      "22-th iteration, loss: 0.33997510990528906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2480211281593639e-06\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661980e-01 3.12708337e-05 1.31815183e-06 8.56428082e+01\n",
      " 1.23427436e+02 1.22942152e-05 1.26395372e-06 9.47239406e-06\n",
      " 1.06947832e-07 7.88137263e+01 9.88067057e+01 4.48725056e+01\n",
      " 6.83418572e+01 4.77158451e+01 2.49769059e+00]\n",
      "23-th iteration, loss: 0.3399751099052265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1048106772158143e-06\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[3.85661868e-01 3.23401325e-05 8.66436482e-07 8.56428093e+01\n",
      " 1.23427435e+02 1.34393222e-05 9.04147016e-07 0.00000000e+00\n",
      " 1.98523347e-22 7.88137381e+01 9.88067053e+01 4.48725044e+01\n",
      " 6.83418560e+01 4.77158442e+01 2.49769059e+00]\n",
      "24-th iteration, loss: 0.3399751099051757, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.790521654412725e-07\n",
      "24-th iteration, new layer inserted. now 13 layers\n",
      "[3.85661732e-01 3.33113865e-05 3.79725602e-07 8.56428102e+01\n",
      " 1.23427435e+02 1.44590259e-05 4.64706979e-07 7.88137402e+01\n",
      " 9.88067050e+01 4.48725033e+01 6.83418549e+01 4.77158433e+01\n",
      " 2.49769059e+00]\n",
      "25-th iteration, loss: 0.3399751099051389, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.947111755461576e-07\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566158  85.64284535 123.42743487  78.81375648  98.80670466\n",
      "  44.87250243  68.34185391  47.71584257   2.49769059]\n",
      "26-th iteration, loss: 0.3399751099051182, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.450319062288227e-07\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[3.85661413e-01 8.56428462e+01 1.23427435e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88137574e+01 9.88067044e+01 4.48725017e+01\n",
      " 6.83418530e+01 4.77158419e+01 2.49769059e+00]\n",
      "27-th iteration, loss: 0.3399751099050965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.761740169721811e-07\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566123  85.642847   123.42743425  78.81375898  98.80670408\n",
      "  44.87250099  68.34185212  47.71584129   2.49769059]\n",
      "28-th iteration, loss: 0.33997510990508106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.35255318631416e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85661047e-01 8.56428477e+01 1.23427434e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88137597e+01 9.88067038e+01 4.48725004e+01\n",
      " 6.83418513e+01 4.77158407e+01 2.49769059e+00]\n",
      "29-th iteration, loss: 0.33997510990506463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.912575837882989e-07\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85660848e-01 0.00000000e+00 6.59194921e-17 8.56428485e+01\n",
      " 1.23427434e+02 7.88137612e+01 9.88067035e+01 4.48724999e+01\n",
      " 6.83418505e+01 4.77158403e+01 2.49769059e+00]\n",
      "30-th iteration, loss: 0.33997510990505037, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.408814090595308e-07\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566064  85.64284978 123.42743314  78.8137618   98.8067033\n",
      "  44.87249949  68.34184983  47.71583981   2.49769059]\n",
      "31-th iteration, loss: 0.33997510990503993, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.118317999486597e-07\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38566042  85.6428504  123.42743273  78.81376242  98.80670306\n",
      "  44.87249913  68.34184915  47.71583942   2.49769059]\n",
      "32-th iteration, loss: 0.33997510990503055, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.851445957107139e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856602   85.642851   123.4274323   78.81376302  98.80670284\n",
      "  44.87249882  68.34184851  47.71583907   2.49769059]\n",
      "33-th iteration, loss: 0.339975109905022, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.606158290485923e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85659968e-01 0.00000000e+00 1.76941795e-16 8.56428516e+01\n",
      " 1.23427432e+02 7.88137636e+01 9.88067026e+01 4.48724986e+01\n",
      " 6.83418479e+01 4.77158388e+01 2.49769059e+00]\n",
      "34-th iteration, loss: 0.3399751099050125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.240215705109872e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85659731e-01 8.56428526e+01 1.23427431e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88137641e+01 9.88067024e+01 4.48724984e+01\n",
      " 6.83418473e+01 4.77158385e+01 2.49769059e+00]\n",
      "35-th iteration, loss: 0.33997510990500385, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.981981023572174e-07\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565949  85.64285316 123.42743097  78.81376513  98.8067022\n",
      "  44.8724982   68.34184677  47.71583823   2.49769059]\n",
      "36-th iteration, loss: 0.3399751099049973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.801732333341555e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565924  85.64285364 123.4274305   78.81376561  98.80670201\n",
      "  44.87249808  68.34184624  47.71583801   2.49769059]\n",
      "37-th iteration, loss: 0.33997510990499114, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.63580125568065e-07\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658988e-01 0.00000000e+00 1.00613962e-16 8.56428541e+01\n",
      " 1.23427430e+02 7.88137661e+01 9.88067018e+01 4.48724980e+01\n",
      " 6.83418457e+01 4.77158378e+01 2.49769059e+00]\n",
      "38-th iteration, loss: 0.33997510990498414, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3621018246521483e-07\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565873  85.642855   123.42742954  78.8137665   98.80670163\n",
      "  44.87249793  68.34184526  47.71583764   2.49769059]\n",
      "39-th iteration, loss: 0.33997510990497865, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.226604200445897e-07\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[3.85658470e-01 0.00000000e+00 1.83880688e-16 8.56428554e+01\n",
      " 1.23427429e+02 7.88137669e+01 9.88067014e+01 4.48724979e+01\n",
      " 6.83418448e+01 4.77158375e+01 2.49769059e+00]\n",
      "40-th iteration, loss: 0.33997510990497243, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.991299020569328e-07\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565821  85.64285625 123.42742855  78.81376733  98.80670127\n",
      "  44.8724979   68.34184435  47.71583736   2.49769059]\n",
      "41-th iteration, loss: 0.3399751099049674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8810023860934307e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657938e-01 0.00000000e+00 5.20417043e-17 8.56428566e+01\n",
      " 1.23427428e+02 7.88137677e+01 9.88067011e+01 4.48724979e+01\n",
      " 6.83418439e+01 4.77158372e+01 2.49769059e+00]\n",
      "42-th iteration, loss: 0.33997510990496166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.715028868387082e-07\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565767  85.64285739 123.42742755  78.81376809  98.80670093\n",
      "  44.87249796  68.3418435   47.71583715   2.49769059]\n",
      "43-th iteration, loss: 0.339975109904957, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6142497058909145e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657393e-01 8.56428577e+01 1.23427427e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88137685e+01 9.88067008e+01 4.48724980e+01\n",
      " 6.83418431e+01 4.77158371e+01 2.49769059e+00]\n",
      "44-th iteration, loss: 0.3399751099049517, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4687588871315934e-07\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[3.85657118e-01 0.00000000e+00 1.14491749e-16 8.56428581e+01\n",
      " 1.23427427e+02 7.88137692e+01 9.88067006e+01 4.48724981e+01\n",
      " 6.83418427e+01 4.77158370e+01 2.49769059e+00]\n",
      "45-th iteration, loss: 0.33997510990494667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3046961225744226e-07\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[3.85656839e-01 0.00000000e+00 1.87350135e-16 8.56428588e+01\n",
      " 1.23427426e+02 7.88137695e+01 9.88067004e+01 4.48724982e+01\n",
      " 6.83418423e+01 4.77158369e+01 2.49769059e+00]\n",
      "46-th iteration, loss: 0.33997510990494173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.197347338099567e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85656558e-01 8.56428594e+01 1.23427425e+02 0.00000000e+00\n",
      " 6.57252031e-14 7.88137698e+01 9.88067003e+01 4.48724983e+01\n",
      " 6.83418420e+01 4.77158369e+01 2.49769059e+00]\n",
      "47-th iteration, loss: 0.33997510990493696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.06644139655494e-07\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38565628  85.64285972 123.42742495  78.81377044  98.80670013\n",
      "  44.8724984   68.34184161  47.71583686   2.49769059]\n",
      "48-th iteration, loss: 0.3399751099049328, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.018641331500698e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[3.85655991e-01 0.00000000e+00 2.63677968e-16 8.56428600e+01\n",
      " 1.23427424e+02 7.88137707e+01 9.88067000e+01 4.48724985e+01\n",
      " 6.83418413e+01 4.77158368e+01 2.49769059e+00]\n",
      "49-th iteration, loss: 0.33997510990492824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9014443731187036e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85655706e-01 8.56428606e+01 1.23427424e+02 0.00000000e+00\n",
      " 8.52651283e-14 7.88137710e+01 9.88066998e+01 4.48724987e+01\n",
      " 6.83418409e+01 4.77158368e+01 2.49769059e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5367213365606\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         382.53516579]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6324887568650404\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.91821287   0.         144.61695292]\n",
      "2-th iteration, loss: 0.6054314507011219, 13 gd steps\n",
      "insert gradient: -0.5912024952973778\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.11499841  77.01458289 222.53525413  40.59979633 118.05465545\n",
      "   0.          26.56229748]\n",
      "3-th iteration, loss: 0.46422438221632906, 53 gd steps\n",
      "insert gradient: -0.12778897775100037\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         107.33114882 103.69264498   0.          77.76948373\n",
      "  55.99704096 101.16078815  59.66216168  26.56229748]\n",
      "4-th iteration, loss: 0.3960516218135036, 45 gd steps\n",
      "insert gradient: -0.086173202733914\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[4.47306884e-01 8.06086017e+01 1.16281904e+02 5.22139810e+01\n",
      " 8.54730510e+01 0.00000000e+00 1.33226763e-14 1.98697011e+01\n",
      " 8.94420670e+01 6.82666463e+01 2.65622975e+01]\n",
      "5-th iteration, loss: 0.3570384548149841, 40 gd steps\n",
      "insert gradient: -0.25870729499796946\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[3.46600140e+00 0.00000000e+00 1.55431223e-15 7.24394783e+01\n",
      " 1.24679083e+02 7.60600853e+01 9.91818489e+01 3.39862477e+01\n",
      " 6.93803725e+01 5.63475370e+01 2.65622975e+01]\n",
      "6-th iteration, loss: 0.3427465394651828, 14 gd steps\n",
      "insert gradient: -0.09274209872129408\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[2.50740684e+00 0.00000000e+00 3.05311332e-16 8.05433149e+01\n",
      " 1.27038338e+02 7.67678287e+01 9.83862084e+01 4.25273753e+01\n",
      " 6.68475247e+01 5.22870887e+01 2.65622975e+01]\n",
      "7-th iteration, loss: 0.34003661491030635, 20 gd steps\n",
      "insert gradient: -0.006998928604309705\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[1.51187247e+00 8.47446097e+01 1.24716517e+02 7.80560328e+01\n",
      " 9.87638570e+01 0.00000000e+00 3.46389584e-14 4.47426155e+01\n",
      " 6.77189453e+01 4.78002585e+01 2.65622975e+01]\n",
      "8-th iteration, loss: 0.3399826139335977, 12 gd steps\n",
      "insert gradient: -0.006645889822940358\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.60457205  85.31129962 123.50710905  78.74536952  98.79325356\n",
      "  44.72793053  68.49087624  47.64867592  26.56229748]\n",
      "9-th iteration, loss: 0.3399751099112865, 76 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.585685420388472e-06\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688039e-01 0.00000000e+00 1.83880688e-16 8.56426574e+01\n",
      " 1.23427540e+02 7.88136547e+01 9.88066652e+01 4.48724863e+01\n",
      " 6.83417463e+01 4.77160011e+01 2.65622975e+01]\n",
      "10-th iteration, loss: 0.33997510991054064, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.127897264138434e-06\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688835e-01 5.28230224e-06 7.95938859e-07 8.56426627e+01\n",
      " 1.23427541e+02 7.88136591e+01 9.88066652e+01 4.48724829e+01\n",
      " 6.83417433e+01 4.77159958e+01 2.65622975e+01]\n",
      "11-th iteration, loss: 0.339975109909922, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.71299644812659e-06\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[3.85689522e-01 1.01235714e-05 1.42513467e-06 0.00000000e+00\n",
      " 5.55865372e-22 8.56426675e+01 1.23427542e+02 7.88136633e+01\n",
      " 9.88066652e+01 4.48724800e+01 6.83417407e+01 4.77159909e+01\n",
      " 2.65622975e+01]\n",
      "12-th iteration, loss: 0.33997510990930246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2184122800441454e-06\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[3.85690089e-01 1.44769463e-05 1.88319592e-06 4.36883811e-06\n",
      " 4.58061252e-07 8.56426719e+01 1.23427543e+02 7.88136670e+01\n",
      " 9.88066653e+01 4.48724775e+01 6.83417384e+01 4.77159864e+01\n",
      " 2.65622975e+01]\n",
      "13-th iteration, loss: 0.3399751099088052, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7810024546542e-06\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[3.85690542e-01 1.83628483e-05 2.17887940e-06 8.27517339e-06\n",
      " 7.06341294e-07 8.56426758e+01 1.23427544e+02 7.88136704e+01\n",
      " 9.88066654e+01 4.48724755e+01 6.83417363e+01 4.77159821e+01\n",
      " 2.65622975e+01]\n",
      "14-th iteration, loss: 0.3399751099084033, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.394001513913673e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[3.85690894e-01 2.18381646e-05 2.33152250e-06 1.17741313e-05\n",
      " 7.69195842e-07 0.00000000e+00 2.77932686e-22 8.56426793e+01\n",
      " 1.23427545e+02 7.88136736e+01 9.88066655e+01 4.48724739e+01\n",
      " 6.83417346e+01 4.77159782e+01 2.65622975e+01]\n",
      "15-th iteration, loss: 0.3399751099080252, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.971429332384482e-06\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691144e-01 2.48931916e-05 2.34481708e-06 1.48544562e-05\n",
      " 6.54736940e-07 8.56426855e+01 1.23427545e+02 7.88136764e+01\n",
      " 9.88066657e+01 4.48724727e+01 6.83417330e+01 4.77159746e+01\n",
      " 2.65622975e+01]\n",
      "16-th iteration, loss: 0.3399751099077657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.675854853358395e-06\n",
      "16-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691313e-01 2.76176770e-05 2.24315237e-06 1.76043837e-05\n",
      " 3.91896153e-07 8.56426882e+01 1.23427545e+02 7.88136791e+01\n",
      " 9.88066659e+01 4.48724718e+01 6.83417317e+01 4.77159712e+01\n",
      " 2.65622975e+01]\n",
      "17-th iteration, loss: 0.3399751099075498, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.429306277275514e-06\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691416e-01 3.00725376e-05 2.04637095e-06 2.00835832e-05\n",
      " 4.10098592e-09 8.56426907e+01 1.23427545e+02 7.88136815e+01\n",
      " 9.88066661e+01 4.48724712e+01 6.83417306e+01 4.77159680e+01\n",
      " 2.65622975e+01]\n",
      "18-th iteration, loss: 0.3399751099073699, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2578518272123705e-06\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691461e-01 3.22912951e-05 1.76579160e-06 8.56427153e+01\n",
      " 1.23427545e+02 7.88136838e+01 9.88066664e+01 4.48724708e+01\n",
      " 6.83417296e+01 4.77159650e+01 2.65622975e+01]\n",
      "19-th iteration, loss: 0.33997510990724056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1236012104821445e-06\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691464e-01 3.43329873e-05 1.41770591e-06 8.56427174e+01\n",
      " 1.23427545e+02 7.88136860e+01 9.88066666e+01 4.48724706e+01\n",
      " 6.83417288e+01 4.77159622e+01 2.65622975e+01]\n",
      "20-th iteration, loss: 0.3399751099071265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.000834088412041e-06\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[3.85691432e-01 3.62315099e-05 1.01321179e-06 8.56427193e+01\n",
      " 1.23427544e+02 0.00000000e+00 3.37507799e-14 7.88136880e+01\n",
      " 9.88066669e+01 4.48724705e+01 6.83417281e+01 4.77159596e+01\n",
      " 2.65622975e+01]\n",
      "21-th iteration, loss: 0.33997510990700525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.835096472164758e-06\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691364e-01 3.79770436e-05 5.52181251e-07 8.56427210e+01\n",
      " 1.23427544e+02 7.88136918e+01 9.88066672e+01 4.48724706e+01\n",
      " 6.83417274e+01 4.77159570e+01 2.65622975e+01]\n",
      "22-th iteration, loss: 0.3399751099069164, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.734778558598726e-06\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691270e-01 3.96136940e-05 4.62790132e-08 8.56427227e+01\n",
      " 1.23427544e+02 7.88136935e+01 9.88066675e+01 4.48724708e+01\n",
      " 6.83417269e+01 4.77159546e+01 2.65622975e+01]\n",
      "23-th iteration, loss: 0.33997510990683755, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6416345192010994e-06\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569115  85.64276535 123.4275432   78.81369519  98.80666777\n",
      "  44.87247112  68.34172649  47.71595234  26.56229748]\n",
      "24-th iteration, loss: 0.33997510990677815, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.570254114205771e-06\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[3.85691016e-01 8.56427668e+01 1.23427543e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.88136968e+01 9.88066681e+01 4.48724715e+01\n",
      " 6.83417261e+01 4.77159502e+01 2.65622975e+01]\n",
      "25-th iteration, loss: 0.3399751099067101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4608511032204291e-06\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690864e-01 8.56427682e+01 1.23427542e+02 0.00000000e+00\n",
      " 5.68434189e-14 7.88136998e+01 9.88066683e+01 4.48724720e+01\n",
      " 6.83417258e+01 4.77159481e+01 2.65622975e+01]\n",
      "26-th iteration, loss: 0.3399751099066483, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3607825471022833e-06\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3856907   85.64276951 123.4275415   78.81370256  98.80666862\n",
      "  44.87247253  68.34172554  47.71594606  26.56229748]\n",
      "27-th iteration, loss: 0.3399751099066014, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3064004339490099e-06\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569052  85.64277079 123.42754086  78.81370388  98.80666889\n",
      "  44.87247313  68.34172534  47.71594414  26.56229748]\n",
      "28-th iteration, loss: 0.3399751099065571, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2555666840790078e-06\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690331e-01 8.56427720e+01 1.23427540e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88137051e+01 9.88066692e+01 4.48724738e+01\n",
      " 6.83417252e+01 4.77159423e+01 2.65622975e+01]\n",
      "29-th iteration, loss: 0.3399751099065069, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1775890558089533e-06\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85690133e-01 0.00000000e+00 2.25514052e-16 8.56427732e+01\n",
      " 1.23427539e+02 7.88137075e+01 9.88066694e+01 4.48724744e+01\n",
      " 6.83417251e+01 4.77159405e+01 2.65622975e+01]\n",
      "30-th iteration, loss: 0.33997510990646035, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1196619013097735e-06\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568992  85.64277543 123.42753876  78.81370867  98.80666966\n",
      "  44.87247513  68.34172497  47.71593878  26.56229748]\n",
      "31-th iteration, loss: 0.3399751099064232, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0804503873067462e-06\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689701e-01 8.56427765e+01 1.23427538e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88137098e+01 9.88066699e+01 4.48724759e+01\n",
      " 6.83417249e+01 4.77159371e+01 2.65622975e+01]\n",
      "32-th iteration, loss: 0.3399751099063814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0398718714998658e-06\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568947  85.64277754 123.42753723  78.81371182  98.80667015\n",
      "  44.87247661  68.3417249   47.71593551  26.56229748]\n",
      "33-th iteration, loss: 0.33997510990634766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0131305147304315e-06\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689237e-01 0.00000000e+00 1.35308431e-16 8.56427786e+01\n",
      " 1.23427536e+02 7.88137128e+01 9.88066704e+01 4.48724774e+01\n",
      " 6.83417249e+01 4.77159340e+01 2.65622975e+01]\n",
      "34-th iteration, loss: 0.3399751099063098, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.62028429376246e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688993e-01 0.00000000e+00 1.83880688e-16 8.56427805e+01\n",
      " 1.23427536e+02 7.88137137e+01 9.88066706e+01 4.48724782e+01\n",
      " 6.83417249e+01 4.77159325e+01 2.65622975e+01]\n",
      "35-th iteration, loss: 0.3399751099062739, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.149751211891346e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688738e-01 0.00000000e+00 1.83880688e-16 8.56427823e+01\n",
      " 1.23427535e+02 7.88137146e+01 9.88066708e+01 4.48724790e+01\n",
      " 6.83417250e+01 4.77159310e+01 2.65622975e+01]\n",
      "36-th iteration, loss: 0.33997510990623964, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.716134693416218e-07\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568847  85.64278406 123.42753393  78.8137155   98.80667104\n",
      "  44.87247979  68.34172507  47.71592959  26.56229748]\n",
      "37-th iteration, loss: 0.339975109906211, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.54102741758964e-07\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[3.85688204e-01 0.00000000e+00 1.76941795e-16 8.56427849e+01\n",
      " 1.23427533e+02 7.88137163e+01 9.88066713e+01 4.48724806e+01\n",
      " 6.83417252e+01 4.77159282e+01 2.65622975e+01]\n",
      "38-th iteration, loss: 0.33997510990617935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.358867334999364e-07\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568793  85.64278653 123.42753218  78.81371716  98.80667145\n",
      "  44.87248143  68.34172528  47.71592688  26.56229748]\n",
      "39-th iteration, loss: 0.3399751099061526, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.299469323035286e-07\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568765  85.64278732 123.42753129  78.81371795  98.80667165\n",
      "  44.87248226  68.34172541  47.71592558  26.56229748]\n",
      "40-th iteration, loss: 0.3399751099061265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.231320367565178e-07\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38568737  85.6427881  123.4275304   78.81371872  98.80667184\n",
      "  44.87248308  68.34172555  47.71592431  26.56229748]\n",
      "41-th iteration, loss: 0.3399751099061013, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.155521919166896e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85687081e-01 8.56427889e+01 1.23427529e+02 7.88137195e+01\n",
      " 9.88066720e+01 0.00000000e+00 2.93098879e-14 4.48724839e+01\n",
      " 6.83417257e+01 4.77159231e+01 2.65622975e+01]\n",
      "42-th iteration, loss: 0.3399751099060729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.714050938375107e-07\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[3.85686795e-01 8.56427896e+01 1.23427529e+02 7.88137202e+01\n",
      " 9.88066722e+01 7.96324494e-07 1.74916625e-07 4.48724847e+01\n",
      " 6.83417258e+01 4.77159219e+01 2.65622975e+01]\n",
      "43-th iteration, loss: 0.3399751099060458, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.67245522401178e-07\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[3.85686509e-01 0.00000000e+00 1.17961196e-16 8.56427904e+01\n",
      " 1.23427528e+02 7.88137209e+01 9.88066724e+01 1.53840161e-06\n",
      " 3.10246438e-07 4.48724854e+01 6.83417260e+01 4.77159207e+01\n",
      " 2.65622975e+01]\n",
      "44-th iteration, loss: 0.3399751099060167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.424525045460171e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[3.85686221e-01 0.00000000e+00 9.02056208e-17 8.56427918e+01\n",
      " 1.23427527e+02 7.88137216e+01 9.88066725e+01 2.23130532e-06\n",
      " 4.06331869e-07 4.48724861e+01 6.83417261e+01 4.77159195e+01\n",
      " 2.65622975e+01]\n",
      "45-th iteration, loss: 0.33997510990598884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.186234881709821e-07\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685929e-01 8.56427932e+01 1.23427526e+02 7.88137223e+01\n",
      " 9.88066726e+01 2.89091196e-06 4.69785484e-07 4.48724868e+01\n",
      " 6.83417262e+01 4.77159183e+01 2.65622975e+01]\n",
      "46-th iteration, loss: 0.3399751099059649, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.143716085953062e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685636e-01 8.56427939e+01 1.23427525e+02 7.88137230e+01\n",
      " 9.88066727e+01 3.52235335e-06 5.05712791e-07 4.48724875e+01\n",
      " 6.83417264e+01 4.77159171e+01 2.65622975e+01]\n",
      "47-th iteration, loss: 0.33997510990594176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.09600836479954e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685343e-01 8.56427946e+01 1.23427524e+02 7.88137237e+01\n",
      " 9.88066729e+01 4.11941048e-06 5.12777101e-07 4.48724881e+01\n",
      " 6.83417265e+01 4.77159160e+01 2.65622975e+01]\n",
      "48-th iteration, loss: 0.3399751099059194, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.044306737139008e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[3.85685050e-01 8.56427953e+01 1.23427523e+02 7.88137243e+01\n",
      " 9.88066729e+01 4.68635911e-06 4.93186729e-07 4.48724886e+01\n",
      " 6.83417266e+01 4.77159149e+01 2.65622975e+01]\n",
      "49-th iteration, loss: 0.33997510990589774, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.98961298616846e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85684758e-01 8.56427960e+01 1.23427522e+02 7.88137250e+01\n",
      " 9.88066730e+01 5.22693123e-06 4.48889545e-07 4.48724892e+01\n",
      " 6.83417268e+01 4.77159138e+01 2.65622975e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224646\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         408.6698553 ]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6306424795698111\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 234.23759999   0.         174.43225531]\n",
      "2-th iteration, loss: 0.6046347771589909, 13 gd steps\n",
      "insert gradient: -0.6047968613702339\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.55936244  77.6127003  219.71573098  41.6177556  121.03462613\n",
      "   0.          53.39762918]\n",
      "3-th iteration, loss: 0.4641745801326174, 101 gd steps\n",
      "insert gradient: -0.11599961014627878\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         107.59132309 104.57611445   0.          78.43208583\n",
      "  55.82003969 101.3862897   59.81730964  53.39762918]\n",
      "4-th iteration, loss: 0.42960286513562307, 30 gd steps\n",
      "insert gradient: -0.1663587609723339\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 8.26300994e+01 1.11746698e+02 6.07924261e+01\n",
      " 0.00000000e+00 4.26325641e-14 2.99583558e+01 4.14721431e+01\n",
      " 1.06617681e+02 6.91121606e+01 5.33976292e+01]\n",
      "5-th iteration, loss: 0.39788442796506857, 47 gd steps\n",
      "insert gradient: -0.028390431432818296\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[1.45469950e-01 8.20284868e+01 1.10972422e+02 5.46640035e+01\n",
      " 0.00000000e+00 2.48689958e-14 6.36965116e+01 2.93694521e+01\n",
      " 9.63943499e+01 6.88460208e+01 5.33976292e+01]\n",
      "6-th iteration, loss: 0.39367329259726036, 37 gd steps\n",
      "insert gradient: -0.01492457965988628\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.54261255  80.02779027 115.07527118  52.68106507  77.48170054\n",
      "  24.45377573  90.40837291  70.5584737   53.39762918]\n",
      "7-th iteration, loss: 0.37765360163966555, 29 gd steps\n",
      "insert gradient: -0.14914068713594344\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[4.32573256e+00 0.00000000e+00 2.77555756e-15 6.21256752e+01\n",
      " 1.39508118e+02 6.21142348e+01 8.91582993e+01 2.64054620e+01\n",
      " 7.50954096e+01 5.20885748e+01 5.33976292e+01]\n",
      "8-th iteration, loss: 0.35211207346759993, 17 gd steps\n",
      "insert gradient: -0.1748016845167724\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  3.83059645  74.59924128 127.53205084  71.36325961  93.57710219\n",
      "  34.47682007  73.15053961  49.84728529  53.39762918]\n",
      "9-th iteration, loss: 0.3400029902668376, 44 gd steps\n",
      "insert gradient: -0.009237863594801977\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.8819494   85.10048644 124.08526566  78.40492723  98.84088284\n",
      "  45.00232383  68.18741687  47.81760837  53.39762918]\n",
      "10-th iteration, loss: 0.33997595640438105, 32 gd steps\n",
      "insert gradient: -0.0018821012544554014\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.4023804   85.58151547 123.45637032  78.77704247  98.81270191\n",
      "  44.85396296  68.36016903  47.77429015  53.39762918]\n",
      "11-th iteration, loss: 0.3399751710014429, 13 gd steps\n",
      "insert gradient: -0.0006914426293477419\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[4.04253284e-01 8.56249553e+01 1.23455047e+02 7.87981072e+01\n",
      " 9.88072339e+01 0.00000000e+00 3.46389584e-14 4.48636952e+01\n",
      " 6.83303418e+01 4.77050215e+01 5.33976292e+01]\n",
      "12-th iteration, loss: 0.3399751195863385, 16 gd steps\n",
      "insert gradient: -0.0002115773416453164\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.39966262  85.6326284  123.44358721  78.80712139  98.80702959\n",
      "  44.8683359   68.33761796  47.71541097  53.39762918]\n",
      "13-th iteration, loss: 0.3399751099049552, 66 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1615063783296065e-07\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[3.85279735e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 6.57252031e-14 7.88138014e+01 9.88066723e+01 4.48725295e+01\n",
      " 6.83417790e+01 4.77158560e+01 5.33976292e+01]\n",
      "14-th iteration, loss: 0.3399751099049517, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0521604732217834e-07\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528042  85.64299069 123.42732493  78.8138018   98.80667241\n",
      "  44.87252964  68.34177902  47.71585615  53.39762918]\n",
      "15-th iteration, loss: 0.33997510990494856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0058697988684302e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[3.85281098e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138020e+01 9.88066725e+01 4.48725298e+01\n",
      " 6.83417791e+01 4.77158563e+01 5.33976292e+01]\n",
      "16-th iteration, loss: 0.3399751099049452, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.905816583876583e-07\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[3.85281775e-01 8.56429907e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138024e+01 9.88066725e+01 4.48725299e+01\n",
      " 6.83417791e+01 4.77158564e+01 5.33976292e+01]\n",
      "17-th iteration, loss: 0.339975109904942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8121060930840753e-07\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[3.85282449e-01 8.56429906e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138028e+01 9.88066726e+01 4.48725300e+01\n",
      " 6.83417791e+01 4.77158565e+01 5.33976292e+01]\n",
      "18-th iteration, loss: 0.33997510990493884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.724293049069663e-07\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528312  85.64299061 123.42732468  78.8138031   98.80667262\n",
      "  44.87253005  68.34177915  47.7158566   53.39762918]\n",
      "19-th iteration, loss: 0.33997510990493585, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6893305140221473e-07\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528379  85.64299058 123.42732461  78.81380327  98.80667267\n",
      "  44.87253014  68.34177918  47.7158567   53.39762918]\n",
      "20-th iteration, loss: 0.3399751099049329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.655090475057553e-07\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[3.85284455e-01 8.56429905e+01 1.23427325e+02 0.00000000e+00\n",
      " 4.79616347e-14 7.88138034e+01 9.88066727e+01 4.48725302e+01\n",
      " 6.83417792e+01 4.77158568e+01 5.33976292e+01]\n",
      "21-th iteration, loss: 0.33997510990492985, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.576060419535387e-07\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528512  85.64299051 123.42732446  78.81380375  98.80667275\n",
      "  44.87253031  68.34177924  47.7158569   53.39762918]\n",
      "22-th iteration, loss: 0.339975109904927, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5454250579840903e-07\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528578  85.64299047 123.42732438  78.81380391  98.80667279\n",
      "  44.87253039  68.34177926  47.71585699  53.39762918]\n",
      "23-th iteration, loss: 0.3399751099049242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.515432737811418e-07\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38528644  85.64299043 123.42732429  78.81380406  98.80667282\n",
      "  44.87253047  68.34177929  47.71585708  53.39762918]\n",
      "24-th iteration, loss: 0.3399751099049214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4861005561573754e-07\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[3.85287096e-01 8.56429904e+01 1.23427324e+02 0.00000000e+00\n",
      " 9.23705556e-14 7.88138042e+01 9.88066729e+01 4.48725305e+01\n",
      " 6.83417793e+01 4.77158572e+01 5.33976292e+01]\n",
      "25-th iteration, loss: 0.33997510990491847, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4164022439351884e-07\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85287750e-01 8.56429903e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138045e+01 9.88066729e+01 4.48725306e+01\n",
      " 6.83417793e+01 4.77158572e+01 5.33976292e+01]\n",
      "26-th iteration, loss: 0.33997510990491564, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.351143953397422e-07\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[3.85288402e-01 8.56429903e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88138048e+01 9.88066729e+01 4.48725307e+01\n",
      " 6.83417794e+01 4.77158573e+01 5.33976292e+01]\n",
      "27-th iteration, loss: 0.3399751099049128, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.290009072447194e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[3.85289051e-01 8.56429902e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.26325641e-14 7.88138050e+01 9.88066729e+01 4.48725307e+01\n",
      " 6.83417794e+01 4.77158574e+01 5.33976292e+01]\n",
      "28-th iteration, loss: 0.3399751099049101, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2327058399599382e-07\n",
      "28-th iteration, new layer inserted. now 9 layers\n",
      "[  0.3852897   85.64299018 123.42732384  78.81380528  98.80667296\n",
      "  44.87253081  68.3417794   47.71585748  53.39762918]\n",
      "29-th iteration, loss: 0.33997510990490737, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2132174263158974e-07\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[3.85290342e-01 8.56429901e+01 1.23427324e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.88138054e+01 9.88066730e+01 4.48725309e+01\n",
      " 6.83417794e+01 4.77158575e+01 5.33976292e+01]\n",
      "30-th iteration, loss: 0.3399751099049047, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1602870229608971e-07\n",
      "30-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529098  85.64299006 123.42732365  78.81380563  98.806673\n",
      "  44.87253093  68.34177944  47.71585762  53.39762918]\n",
      "31-th iteration, loss: 0.33997510990490204, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1429663907879238e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85291624e-01 8.56429900e+01 1.23427324e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138057e+01 9.88066730e+01 4.48725310e+01\n",
      " 6.83417795e+01 4.77158577e+01 5.33976292e+01]\n",
      "32-th iteration, loss: 0.3399751099048994, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0940278327810435e-07\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529226  85.64298993 123.42732345  78.81380597  98.80667304\n",
      "  44.87253104  68.34177948  47.71585775  53.39762918]\n",
      "33-th iteration, loss: 0.3399751099048968, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0786803485281753e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85292898e-01 8.56429899e+01 1.23427323e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138061e+01 9.88066731e+01 4.48725311e+01\n",
      " 6.83417795e+01 4.77158578e+01 5.33976292e+01]\n",
      "34-th iteration, loss: 0.3399751099048942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0333850837161342e-07\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529353  85.6429898  123.42732324  78.81380628  98.80667306\n",
      "  44.87253114  68.34177951  47.71585788  53.39762918]\n",
      "35-th iteration, loss: 0.33997510990489166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0198322928288804e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85294163e-01 8.56429897e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138064e+01 9.88066731e+01 4.48725312e+01\n",
      " 6.83417795e+01 4.77158579e+01 5.33976292e+01]\n",
      "36-th iteration, loss: 0.3399751099048891, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.778617213402768e-08\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[3.85294792e-01 8.56429897e+01 1.23427323e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.88138066e+01 9.88066731e+01 4.48725312e+01\n",
      " 6.83417795e+01 4.77158580e+01 5.33976292e+01]\n",
      "37-th iteration, loss: 0.33997510990488655, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.38503713245958e-08\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529542  85.64298958 123.42732292  78.81380677  98.80667309\n",
      "  44.87253128  68.34177956  47.71585805  53.39762918]\n",
      "38-th iteration, loss: 0.33997510990488405, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.279530106498568e-08\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[3.85296045e-01 8.56429895e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.08562073e-14 7.88138069e+01 9.88066731e+01 4.48725313e+01\n",
      " 6.83417796e+01 4.77158581e+01 5.33976292e+01]\n",
      "39-th iteration, loss: 0.33997510990488156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.913504028397377e-08\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[3.85296668e-01 8.56429894e+01 1.23427323e+02 0.00000000e+00\n",
      " 7.99360578e-14 7.88138070e+01 9.88066731e+01 4.48725314e+01\n",
      " 6.83417796e+01 4.77158582e+01 5.33976292e+01]\n",
      "40-th iteration, loss: 0.33997510990487906, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.570077275871551e-08\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[3.85297289e-01 8.56429894e+01 1.23427323e+02 0.00000000e+00\n",
      " 4.44089210e-14 7.88138072e+01 9.88066731e+01 4.48725314e+01\n",
      " 6.83417796e+01 4.77158582e+01 5.33976292e+01]\n",
      "41-th iteration, loss: 0.3399751099048766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.247655434307828e-08\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529791  85.64298928 123.42732247  78.81380739  98.80667312\n",
      "  44.87253145  68.34177962  47.71585825  53.39762918]\n",
      "42-th iteration, loss: 0.3399751099048742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.17802011750019e-08\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529853  85.6429892  123.42732236  78.81380747  98.80667312\n",
      "  44.87253149  68.34177963  47.7158583   53.39762918]\n",
      "43-th iteration, loss: 0.3399751099048718, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.107635479108649e-08\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[3.85299141e-01 8.56429891e+01 1.23427322e+02 0.00000000e+00\n",
      " 7.10542736e-14 7.88138075e+01 9.88066731e+01 4.48725315e+01\n",
      " 6.83417796e+01 4.77158583e+01 5.33976292e+01]\n",
      "44-th iteration, loss: 0.33997510990486934, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.807262289162491e-08\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38529975  85.64298904 123.42732213  78.81380771  98.80667312\n",
      "  44.87253156  68.34177965  47.71585839  53.39762918]\n",
      "45-th iteration, loss: 0.339975109904867, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.746775780742047e-08\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530037  85.64298895 123.42732202  78.81380778  98.80667312\n",
      "  44.8725316   68.34177967  47.71585843  53.39762918]\n",
      "46-th iteration, loss: 0.3399751099048646, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.685610549632e-08\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530098  85.64298887 123.42732191  78.81380786  98.80667312\n",
      "  44.87253163  68.34177968  47.71585847  53.39762918]\n",
      "47-th iteration, loss: 0.3399751099048623, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.624000848287457e-08\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[3.85301583e-01 8.56429888e+01 1.23427322e+02 0.00000000e+00\n",
      " 4.97379915e-14 7.88138079e+01 9.88066731e+01 4.48725317e+01\n",
      " 6.83417797e+01 4.77158585e+01 5.33976292e+01]\n",
      "48-th iteration, loss: 0.33997510990485996, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.34563925894494e-08\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530219  85.6429887  123.42732168  78.81380808  98.80667312\n",
      "  44.8725317   68.3417797   47.71585855  53.39762918]\n",
      "49-th iteration, loss: 0.33997510990485763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.293739394443388e-08\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38530279  85.64298862 123.42732156  78.81380816  98.80667312\n",
      "  44.87253173  68.34177971  47.71585859  53.39762918]\n",
      "0-th iteration, loss: 0.9545036783633585, 7 gd steps\n",
      "insert gradient: -2.536822723274191\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.09598192   0.         433.29095276]\n",
      "1-th iteration, loss: 0.7485404418159318, 11 gd steps\n",
      "insert gradient: -0.6305151656698674\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.67444096  62.33745299 237.78162041   0.         195.50933234]\n",
      "2-th iteration, loss: 0.6057575328448086, 13 gd steps\n",
      "insert gradient: -0.5951526425486448\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.27757689  77.13458233 222.50187546  40.5340884  119.69959123\n",
      "   0.          75.80974111]\n",
      "3-th iteration, loss: 0.46430423849049596, 48 gd steps\n",
      "insert gradient: -0.15281013657602754\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.         106.25721829 104.87726602   0.          78.65794951\n",
      "  55.76883993 101.6580866   59.85799745  75.80974111]\n",
      "4-th iteration, loss: 0.4116732561472047, 32 gd steps\n",
      "insert gradient: -0.22400001064941757\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          83.29301204 104.29835637  51.68607023  54.21545136\n",
      "  40.38776126  94.74159533  63.68652576  75.80974111]\n",
      "5-th iteration, loss: 0.35220868775281466, 51 gd steps\n",
      "insert gradient: -0.23585599632111348\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[1.05812659e+00 0.00000000e+00 6.24500451e-16 7.20396528e+01\n",
      " 1.31672766e+02 7.78711810e+01 9.88292216e+01 3.82086133e+01\n",
      " 6.80745709e+01 5.40575652e+01 7.58097411e+01]\n",
      "6-th iteration, loss: 0.3402160645326017, 20 gd steps\n",
      "insert gradient: -0.03318410160211695\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[1.44306237e+00 8.48449328e+01 1.25766874e+02 7.72866400e+01\n",
      " 9.86402323e+01 0.00000000e+00 2.57571742e-14 4.44894919e+01\n",
      " 6.74055565e+01 4.73379230e+01 7.58097411e+01]\n",
      "7-th iteration, loss: 0.33997584723409785, 51 gd steps\n",
      "insert gradient: -0.0008101660701794044\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[2.69405326e-01 8.57052745e+01 0.00000000e+00 2.57571742e-14\n",
      " 1.23260684e+02 7.88744611e+01 9.88610041e+01 4.48778666e+01\n",
      " 6.82841603e+01 4.77514991e+01 7.58097411e+01]\n",
      "8-th iteration, loss: 0.33997517409833095, 34 gd steps\n",
      "insert gradient: -4.2438665068458045e-05\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[3.20603925e-01 8.56804177e+01 1.23401352e+02 7.88233475e+01\n",
      " 9.88052025e+01 4.48823924e+01 0.00000000e+00 1.11022302e-14\n",
      " 6.83337990e+01 4.77183799e+01 7.58097411e+01]\n",
      "9-th iteration, loss: 0.3399751105950451, 43 gd steps\n",
      "insert gradient: -1.630004361220139e-05\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[3.86615300e-01 8.56444427e+01 1.23425178e+02 7.88147556e+01\n",
      " 9.88087742e+01 4.48713793e+01 6.83467222e+01 0.00000000e+00\n",
      " 8.88178420e-16 4.77138526e+01 7.58097411e+01]\n",
      "10-th iteration, loss: 0.33997510992359103, 25 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.392202179138017e-06\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572875  85.64327062 123.42660141  78.81422375  98.80667497\n",
      "  44.87250315  68.34239977  47.71570064  75.80974111]\n",
      "11-th iteration, loss: 0.33997510992335117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4173491076947585e-06\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[3.85727557e-01 8.56432676e+01 0.00000000e+00 1.68753900e-14\n",
      " 1.23426604e+02 7.88142208e+01 9.88066747e+01 4.48725020e+01\n",
      " 6.83423960e+01 4.77157009e+01 7.58097411e+01]\n",
      "12-th iteration, loss: 0.3399751099230829, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.405593778336409e-06\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[3.85726387e-01 8.56432647e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426609e+02 7.88142180e+01 9.88066744e+01 4.48725010e+01\n",
      " 6.83423923e+01 4.77157013e+01 7.58097411e+01]\n",
      "13-th iteration, loss: 0.3399751099228196, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.394099130685104e-06\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[3.85725225e-01 8.56432618e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426614e+02 7.88142152e+01 9.88066742e+01 4.48725000e+01\n",
      " 6.83423886e+01 4.77157016e+01 7.58097411e+01]\n",
      "14-th iteration, loss: 0.3399751099225611, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3828095086768742e-06\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[3.85724070e-01 8.56432589e+01 0.00000000e+00 6.03961325e-14\n",
      " 1.23426619e+02 7.88142123e+01 9.88066739e+01 4.48724991e+01\n",
      " 6.83423850e+01 4.77157020e+01 7.58097411e+01]\n",
      "15-th iteration, loss: 0.3399751099223072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3716763809144864e-06\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[3.85722923e-01 8.56432561e+01 0.00000000e+00 2.22044605e-14\n",
      " 1.23426623e+02 7.88142095e+01 9.88066737e+01 4.48724983e+01\n",
      " 6.83423815e+01 4.77157025e+01 7.58097411e+01]\n",
      "16-th iteration, loss: 0.33997510992205754, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3606575845949157e-06\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572178  85.64325323 123.42662828  78.81420676  98.80667355\n",
      "  44.87249749  68.34237794  47.71570295  75.80974111]\n",
      "17-th iteration, loss: 0.33997510992184476, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3844586544641448e-06\n",
      "17-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38572065  85.64325038 123.42663068  78.81420397  98.80667337\n",
      "  44.87249676  68.34237446  47.71570345  75.80974111]\n",
      "18-th iteration, loss: 0.33997510992163643, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4068350479358843e-06\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571952  85.64324759 123.4266331   78.81420122  98.80667322\n",
      "  44.87249607  68.342371    47.71570397  75.80974111]\n",
      "19-th iteration, loss: 0.33997510992143226, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.427836328028074e-06\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[3.85718412e-01 8.56432449e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426636e+02 7.88141985e+01 9.88066731e+01 4.48724954e+01\n",
      " 6.83423676e+01 4.77157045e+01 7.58097411e+01]\n",
      "20-th iteration, loss: 0.3399751099211976, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.411999303451084e-06\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[3.85717324e-01 8.56432422e+01 0.00000000e+00 1.15463195e-14\n",
      " 1.23426640e+02 7.88141959e+01 9.88066730e+01 4.48724948e+01\n",
      " 6.83423642e+01 4.77157051e+01 7.58097411e+01]\n",
      "21-th iteration, loss: 0.3399751099209666, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3964431960537777e-06\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571624  85.64323957 123.4266454   78.81419328  98.8066729\n",
      "  44.87249424  68.3423608   47.71570564  75.80974111]\n",
      "22-th iteration, loss: 0.3399751099207729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.416177311621318e-06\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571516  85.64323692 123.42664783  78.81419066  98.80667281\n",
      "  44.87249371  68.34235747  47.71570624  75.80974111]\n",
      "23-th iteration, loss: 0.3399751099205828, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.434586979419704e-06\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[3.85714096e-01 8.56432343e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426650e+02 7.88141881e+01 9.88066727e+01 4.48724932e+01\n",
      " 6.83423542e+01 4.77157068e+01 7.58097411e+01]\n",
      "24-th iteration, loss: 0.33997510992036134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4162497740306433e-06\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571305  85.6432318  123.42665522  78.81418557  98.8066727\n",
      "  44.87249275  68.34235088  47.71570747  75.80974111]\n",
      "25-th iteration, loss: 0.33997510992017754, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.433489723878338e-06\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[3.85712003e-01 8.56432293e+01 0.00000000e+00 3.46389584e-14\n",
      " 1.23426658e+02 7.88141830e+01 9.88066727e+01 4.48724923e+01\n",
      " 6.83423476e+01 4.77157081e+01 7.58097411e+01]\n",
      "26-th iteration, loss: 0.3399751099199622, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.414086692768499e-06\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38571098  85.64322682 123.42666261  78.81418058  98.80667264\n",
      "  44.87249191  68.3423444   47.71570875  75.80974111]\n",
      "27-th iteration, loss: 0.3399751099197843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4302130531728945e-06\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[3.85709950e-01 8.56432243e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23426665e+02 7.88141781e+01 9.88066726e+01 4.48724915e+01\n",
      " 6.83423412e+01 4.77157094e+01 7.58097411e+01]\n",
      "28-th iteration, loss: 0.33997510991957475, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.409829626916722e-06\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[3.85708942e-01 8.56432220e+01 0.00000000e+00 4.70734562e-14\n",
      " 1.23426670e+02 7.88141757e+01 9.88066726e+01 4.48724912e+01\n",
      " 6.83423380e+01 4.77157101e+01 7.58097411e+01]\n",
      "29-th iteration, loss: 0.3399751099193683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3898944726769827e-06\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570794  85.64321957 123.42667488  78.81417329  98.80667262\n",
      "  44.87249085  68.34233486  47.71571074  75.80974111]\n",
      "30-th iteration, loss: 0.3399751099191983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.40509140145545e-06\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[3.85706938e-01 8.56432172e+01 0.00000000e+00 2.39808173e-14\n",
      " 1.23426677e+02 7.88141709e+01 9.88066726e+01 4.48724905e+01\n",
      " 6.83423317e+01 4.77157114e+01 7.58097411e+01]\n",
      "31-th iteration, loss: 0.3399751099189973, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3842057124902796e-06\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[3.85705955e-01 8.56432149e+01 0.00000000e+00 3.64153152e-14\n",
      " 1.23426682e+02 7.88141685e+01 9.88066727e+01 4.48724903e+01\n",
      " 6.83423286e+01 4.77157121e+01 7.58097411e+01]\n",
      "32-th iteration, loss: 0.33997510991879915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3637760996028113e-06\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[3.85704978e-01 8.56432126e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426687e+02 7.88141662e+01 9.88066727e+01 4.48724900e+01\n",
      " 6.83423256e+01 4.77157128e+01 7.58097411e+01]\n",
      "33-th iteration, loss: 0.33997510991860386, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.343768363204252e-06\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[3.85704006e-01 8.56432103e+01 0.00000000e+00 4.70734562e-14\n",
      " 1.23426692e+02 7.88141638e+01 9.88066727e+01 4.48724898e+01\n",
      " 6.83423225e+01 4.77157135e+01 7.58097411e+01]\n",
      "34-th iteration, loss: 0.33997510991841134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.324151545093056e-06\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[3.85703040e-01 8.56432080e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23426697e+02 7.88141615e+01 9.88066727e+01 4.48724895e+01\n",
      " 6.83423195e+01 4.77157142e+01 7.58097411e+01]\n",
      "35-th iteration, loss: 0.33997510991822166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3048975916686194e-06\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[3.85702080e-01 8.56432057e+01 0.00000000e+00 1.68753900e-14\n",
      " 1.23426701e+02 7.88141592e+01 9.88066728e+01 4.48724893e+01\n",
      " 6.83423165e+01 4.77157149e+01 7.58097411e+01]\n",
      "36-th iteration, loss: 0.3399751099180346, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2859810965273435e-06\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570113  85.64320349 123.42670596  78.81415688  98.80667282\n",
      "  44.87248915  68.34231348  47.71571565  75.80974111]\n",
      "37-th iteration, loss: 0.3399751099178809, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.300557981356601e-06\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38570017  85.64320124 123.42670827  78.81415457  98.80667286\n",
      "  44.87248898  68.34231052  47.71571637  75.80974111]\n",
      "38-th iteration, loss: 0.3399751099177296, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3139238091585636e-06\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[3.85699232e-01 8.56431990e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23426711e+02 7.88141523e+01 9.88066729e+01 4.48724888e+01\n",
      " 6.83423076e+01 4.77157171e+01 7.58097411e+01]\n",
      "39-th iteration, loss: 0.33997510991754937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2926585147736343e-06\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569831  85.64319689 123.42671529  78.81415008  98.80667296\n",
      "  44.87248866  68.34230464  47.7157178   75.80974111]\n",
      "40-th iteration, loss: 0.3399751099174026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3050287486013405e-06\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569739  85.64319474 123.4267176   78.81414785  98.80667302\n",
      "  44.87248853  68.34230173  47.71571852  75.80974111]\n",
      "41-th iteration, loss: 0.33997510991725804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.31629468680708e-06\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[3.85696474e-01 8.56431926e+01 0.00000000e+00 3.46389584e-14\n",
      " 1.23426720e+02 7.88141457e+01 9.88066731e+01 4.48724884e+01\n",
      " 6.83422988e+01 4.77157192e+01 7.58097411e+01]\n",
      "42-th iteration, loss: 0.3399751099170842, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.293083548089638e-06\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[3.85695579e-01 8.56431906e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23426725e+02 7.88141435e+01 9.88066732e+01 4.48724883e+01\n",
      " 6.83422959e+01 4.77157200e+01 7.58097411e+01]\n",
      "43-th iteration, loss: 0.3399751099169129, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2704463922677438e-06\n",
      "43-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569469  85.64318853 123.42672926  78.81414139  98.80667324\n",
      "  44.87248816  68.34229308  47.71572068  75.80974111]\n",
      "44-th iteration, loss: 0.3399751099167742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2811325630950974e-06\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[3.85693797e-01 8.56431865e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426732e+02 7.88141392e+01 9.88066733e+01 4.48724881e+01\n",
      " 6.83422902e+01 4.77157214e+01 7.58097411e+01]\n",
      "45-th iteration, loss: 0.33997510991660707, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2578966028649445e-06\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569292  85.64318449 123.42673618  78.81413715  98.8066734\n",
      "  44.87248797  68.34228741  47.71572212  75.80974111]\n",
      "46-th iteration, loss: 0.3399751099164723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2678073679578747e-06\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[3.85692046e-01 8.56431825e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23426738e+02 7.88141351e+01 9.88066735e+01 4.48724879e+01\n",
      " 6.83422846e+01 4.77157228e+01 7.58097411e+01]\n",
      "47-th iteration, loss: 0.3399751099163093, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2440574350480163e-06\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569119  85.64318054 123.42674304  78.814133    98.80667357\n",
      "  44.87248782  68.34228181  47.71572355  75.80974111]\n",
      "48-th iteration, loss: 0.3399751099161782, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2532577272540796e-06\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.38569033  85.64317858 123.42674531  78.81413094  98.80667366\n",
      "  44.87248776  68.34227904  47.71572427  75.80974111]\n",
      "49-th iteration, loss: 0.33997510991604896, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.261504371371068e-06\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[3.85689475e-01 8.56431767e+01 0.00000000e+00 1.50990331e-14\n",
      " 1.23426748e+02 7.88141289e+01 9.88066738e+01 4.48724877e+01\n",
      " 6.83422763e+01 4.77157250e+01 7.58097411e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368538032355716\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         458.11386249]\n",
      "1-th iteration, loss: 0.7491078933829632, 11 gd steps\n",
      "insert gradient: -0.6332402534573919\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 234.64368566   0.         223.47017682]\n",
      "2-th iteration, loss: 0.6042697166674827, 13 gd steps\n",
      "insert gradient: -0.6019316850733866\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.31802897  77.4470074  219.9411972   41.67877133 118.57601219\n",
      "   0.         104.89416463]\n",
      "3-th iteration, loss: 0.4719680401577259, 21 gd steps\n",
      "insert gradient: -0.35512655994492237\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  1.87963554  96.63098772 192.52710059  48.95569079  97.59005985\n",
      "  58.46126112 104.89416463]\n",
      "4-th iteration, loss: 0.466673400776488, 12 gd steps\n",
      "insert gradient: -0.20699914579543405\n",
      "4-th iteration, new layer inserted. now 7 layers\n",
      "[  0.6389503  102.03439013 188.01922596  51.86324402  99.17266572\n",
      "  58.71586533 104.89416463]\n",
      "5-th iteration, loss: 0.46433767087633304, 24 gd steps\n",
      "insert gradient: -0.2053967455015048\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[0.00000000e+00 1.05973132e+02 1.84307424e+02 5.59292891e+01\n",
      " 1.01431941e+02 5.99757831e+01 1.04894165e+02 0.00000000e+00\n",
      " 4.70734562e-14]\n",
      "6-th iteration, loss: 0.3788398171858579, 33 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.82195774e+01 1.49620322e+02 7.43267769e+01\n",
      " 1.27052692e+02 5.15032708e+01 8.27675886e+01 3.99241296e+01\n",
      " 2.46870861e-15 0.00000000e+00 6.41864240e-14]\n",
      "7-th iteration, loss: 0.3758974718572075, 21 gd steps\n",
      "insert gradient: -0.0485685471983215\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.73047546e+01 1.45086744e+02 7.59220977e+01\n",
      " 1.25018289e+02 5.15241049e+01 7.99185473e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.23959088e+01 6.36517476e-14]\n",
      "8-th iteration, loss: 0.3735295973747247, 22 gd steps\n",
      "insert gradient: -0.0068695295057222515\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 9.79838847e+01 1.37311122e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.88182323e+01 1.18424300e+02 5.25155408e+01\n",
      " 7.96394203e+01 4.49157593e+01 6.07974760e-14]\n",
      "9-th iteration, loss: 0.3555454583293367, 231 gd steps\n",
      "insert gradient: -0.03340917834320511\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[6.86088664e-01 1.02637254e+02 1.08338029e+02 1.00135499e+02\n",
      " 1.04091270e+02 6.17951022e+01 8.39994482e+01 4.82610465e+01\n",
      " 3.52787221e-14]\n",
      "10-th iteration, loss: 0.3525947778472343, 24 gd steps\n",
      "insert gradient: -0.039372576770335\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[5.86174132e-01 1.06764491e+02 1.01699481e+02 1.04777738e+02\n",
      " 1.04864946e+02 0.00000000e+00 4.70734562e-14 6.25050955e+01\n",
      " 9.23133275e+01 4.86652743e+01 6.93639314e-15]\n",
      "11-th iteration, loss: 0.35213322071364755, 14 gd steps\n",
      "insert gradient: -0.019770866809445068\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[8.41360622e-01 5.99287439e+01 0.00000000e+00 4.79429951e+01\n",
      " 9.92052982e+01 1.05992209e+02 1.05830599e+02 6.30872859e+01\n",
      " 9.41917595e+01 4.83510808e+01 2.07571509e-15]\n",
      "12-th iteration, loss: 0.35180395185915386, 24 gd steps\n",
      "insert gradient: -0.012715300995478265\n",
      "12-th iteration, new layer inserted. now 10 layers\n",
      "[  0.88796347  59.14609861   2.42670544  47.70437855  98.98154609\n",
      " 106.43084817 106.67497053  63.86771827  94.89856091  48.03702842]\n",
      "13-th iteration, loss: 0.35161349155452737, 21 gd steps\n",
      "insert gradient: -0.013658366159006325\n",
      "13-th iteration, new layer inserted. now 12 layers\n",
      "[1.16402532e+00 5.79111422e+01 4.89796091e+00 4.73151003e+01\n",
      " 9.91955467e+01 1.06755902e+02 1.07539943e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.39778567e+01 9.58803178e+01 4.85588845e+01]\n",
      "14-th iteration, loss: 0.3514228580603139, 18 gd steps\n",
      "insert gradient: -0.013653897845594032\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[  1.33397661  57.62708058   7.22337958  46.21390541  99.08066778\n",
      " 107.1718039  108.81731239  64.516082    96.53934022  48.54554029]\n",
      "15-th iteration, loss: 0.3297155248738684, 62 gd steps\n",
      "insert gradient: -0.16341739847415246\n",
      "15-th iteration, new layer inserted. now 12 layers\n",
      "[7.86737758e-01 5.06571230e+01 0.00000000e+00 3.55271368e-14\n",
      " 2.69507368e+01 4.51860485e+01 9.83485984e+01 1.22583667e+02\n",
      " 1.18495649e+02 6.88568153e+01 1.13242577e+02 6.04486918e+01]\n",
      "16-th iteration, loss: 0.3152264823366797, 59 gd steps\n",
      "insert gradient: -0.0510729079717163\n",
      "16-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          39.47757729  50.53724698  47.79214502  95.26731738\n",
      "  51.84668674   0.          86.41114457 116.44525169  69.59067652\n",
      " 117.29321124  69.94711959]\n",
      "17-th iteration, loss: 0.30122614189842994, 34 gd steps\n",
      "insert gradient: -0.061608421047495254\n",
      "17-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          28.37405713  61.90738415  55.3021177   95.45136044\n",
      "  36.83504761  46.78862344  79.55611741 116.20615924  74.02696109\n",
      " 123.26385974  78.29322101]\n",
      "18-th iteration, loss: 0.2963558113875898, 15 gd steps\n",
      "insert gradient: -0.08886453257292203\n",
      "18-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 2.05892671e+01 7.78507692e+01 0.00000000e+00\n",
      " 2.13162821e-14 5.29889567e+01 9.62546603e+01 3.07541332e+01\n",
      " 7.22229612e+01 7.76859439e+01 1.19744445e+02 7.33850536e+01\n",
      " 1.25846904e+02 8.29425584e+01]\n",
      "19-th iteration, loss: 0.2939537916449943, 51 gd steps\n",
      "insert gradient: -0.009482311930317176\n",
      "19-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          19.51813893  86.60812471  52.64634078  92.28676594\n",
      "  32.4634488   76.99073252  76.93536147 118.6857536   77.0061749\n",
      " 126.14138106  82.78325197]\n",
      "20-th iteration, loss: 0.2935888668763615, 14 gd steps\n",
      "insert gradient: -0.02597026904472926\n",
      "20-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          16.90796888  92.94205405  51.94088746  88.91832246\n",
      "  32.58733624  85.58843516  75.72638591 119.11560707  78.07627593\n",
      " 126.50765937  83.66868388]\n",
      "21-th iteration, loss: 0.29345430911080456, 54 gd steps\n",
      "insert gradient: -0.000615239444112239\n",
      "21-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71983221e+01 0.00000000e+00 3.10862447e-15\n",
      " 9.34934922e+01 5.22199139e+01 8.90065374e+01 3.22012667e+01\n",
      " 8.47302622e+01 7.63683833e+01 1.18291442e+02 7.79000961e+01\n",
      " 1.26284936e+02 8.41467854e+01]\n",
      "22-th iteration, loss: 0.2934542145808781, 28 gd steps\n",
      "insert gradient: -0.0005870658972997845\n",
      "22-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71962192e+01 0.00000000e+00 1.11022302e-15\n",
      " 9.34996603e+01 5.22197030e+01 8.90074848e+01 3.22022464e+01\n",
      " 8.47249423e+01 7.63708190e+01 1.18288707e+02 7.79011573e+01\n",
      " 1.26283504e+02 8.41471838e+01]\n",
      "23-th iteration, loss: 0.29345412615822924, 27 gd steps\n",
      "insert gradient: -0.0005725642029702486\n",
      "23-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.19414884  93.50539753  52.21930364  89.00830047\n",
      "  32.20313227  84.71986346  76.37317621 118.28614086  77.90222422\n",
      " 126.28215838  84.14757531]\n",
      "24-th iteration, loss: 0.2934540627486072, 23 gd steps\n",
      "insert gradient: -0.0005896946216967619\n",
      "24-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71924786e+01 0.00000000e+00 3.77475828e-15\n",
      " 9.35078189e+01 5.22190083e+01 8.90089994e+01 3.22039349e+01\n",
      " 8.47156293e+01 7.63751396e+01 1.18284010e+02 7.79031055e+01\n",
      " 1.26281041e+02 8.41479043e+01]\n",
      "25-th iteration, loss: 0.29345398205217577, 26 gd steps\n",
      "insert gradient: -0.0005683658284092156\n",
      "25-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.190672    93.51328016  52.21868684  89.00977777\n",
      "  32.20486412  84.71089102  76.37731554 118.2816381   77.90408223\n",
      " 126.27980183  84.1482778 ]\n",
      "26-th iteration, loss: 0.29345392397058795, 22 gd steps\n",
      "insert gradient: -0.0005823035586364738\n",
      "26-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71891736e+01 0.00000000e+00 3.10862447e-15\n",
      " 9.35155806e+01 5.22184008e+01 8.90104234e+01 3.22056667e+01\n",
      " 8.47069056e+01 7.63791492e+01 1.18279659e+02 7.79049111e+01\n",
      " 1.26278768e+02 8.41485963e+01]\n",
      "27-th iteration, loss: 0.29345384993675944, 25 gd steps\n",
      "insert gradient: -0.000560922505428517\n",
      "27-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71875425e+01 0.00000000e+00 4.88498131e-15\n",
      " 9.35207400e+01 5.22180757e+01 8.90111336e+01 3.22065786e+01\n",
      " 8.47024548e+01 7.63811806e+01 1.18277464e+02 7.79058326e+01\n",
      " 1.26277624e+02 8.41489566e+01]\n",
      "28-th iteration, loss: 0.2934537796156989, 24 gd steps\n",
      "insert gradient: -0.0005484799363679335\n",
      "28-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71859151e+01 0.00000000e+00 1.77635684e-15\n",
      " 9.35256176e+01 5.22176393e+01 8.90117635e+01 3.22074284e+01\n",
      " 8.46981581e+01 7.63831676e+01 1.18275377e+02 7.79067584e+01\n",
      " 1.26276535e+02 8.41493097e+01]\n",
      "29-th iteration, loss: 0.29345371245815177, 23 gd steps\n",
      "insert gradient: -0.0005394332136835595\n",
      "29-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18432239  93.530292    52.21716755  89.01235216\n",
      "  32.20826222  84.69400348  76.38510736 118.27337998  77.90766977\n",
      " 126.27548841  84.14965307]\n",
      "30-th iteration, loss: 0.29345366269489037, 20 gd steps\n",
      "insert gradient: -0.0005552596937265622\n",
      "30-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71829959e+01 0.00000000e+00 5.55111512e-15\n",
      " 9.35323332e+01 5.22168163e+01 8.90128835e+01 3.22090359e+01\n",
      " 8.46904283e+01 7.63867739e+01 1.18271666e+02 7.79084419e+01\n",
      " 1.26274590e+02 8.41499482e+01]\n",
      "31-th iteration, loss: 0.29345360007724725, 23 gd steps\n",
      "insert gradient: -0.0005380155746782537\n",
      "31-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18158346  93.53687733  52.21646101  89.0134752\n",
      "  32.20991679  84.68648862  76.38859079 118.26978185  77.9092786\n",
      " 126.27360509  84.1502753 ]\n",
      "32-th iteration, loss: 0.2934535537051507, 19 gd steps\n",
      "insert gradient: -0.000550278514499966\n",
      "32-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.18037331  93.53884684  52.21614771  89.01398105\n",
      "  32.21069147  84.68307853  76.39016494 118.26816371  77.9100068\n",
      " 126.27275942  84.15056073]\n",
      "33-th iteration, loss: 0.29345350886557764, 19 gd steps\n",
      "insert gradient: -0.0005557456513626195\n",
      "33-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17926868  93.54082193  52.21592538  89.01450501\n",
      "  32.21149645  84.67974362  76.39167659 118.2665781   77.91069232\n",
      " 126.27193463  84.15084035]\n",
      "34-th iteration, loss: 0.29345346533891153, 19 gd steps\n",
      "insert gradient: -0.0005580098563727028\n",
      "34-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71782360e+01 0.00000000e+00 1.99840144e-15\n",
      " 9.35427876e+01 5.22157416e+01 8.90150243e+01 3.22123014e+01\n",
      " 8.46764698e+01 7.63931380e+01 1.18265026e+02 7.79113522e+01\n",
      " 1.26271131e+02 8.41511172e+01]\n",
      "35-th iteration, loss: 0.29345340973242456, 21 gd steps\n",
      "insert gradient: -0.0005349497812573328\n",
      "35-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17707597  93.54708987  52.21546557  89.01555616\n",
      "  32.21315195  84.67284365  76.39475175 118.2633272   77.91209782\n",
      " 126.27025339  84.15142795]\n",
      "36-th iteration, loss: 0.29345336864370297, 18 gd steps\n",
      "insert gradient: -0.0005435916939108208\n",
      "36-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17604783  93.54894893  52.21518071  89.01599689\n",
      "  32.21388184  84.66968299  76.39616909 118.2618639   77.91276242\n",
      " 126.26949591  84.1517006 ]\n",
      "37-th iteration, loss: 0.29345332872272273, 18 gd steps\n",
      "insert gradient: -0.0005472509179544911\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17509544  93.55080899  52.21496391  89.01645115\n",
      "  32.2146349   84.6665832   76.39754054 118.26042822  77.91339412\n",
      " 126.26875452  84.15196704]\n",
      "38-th iteration, loss: 0.2934532898387362, 18 gd steps\n",
      "insert gradient: -0.0005484915860287965\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17419565  93.5526591   52.21477759  89.01690226\n",
      "  32.21538876  84.66353473  76.3988741  118.25902096  77.91400498\n",
      " 126.26802978  84.15222958]\n",
      "39-th iteration, loss: 0.293453251924034, 18 gd steps\n",
      "insert gradient: -0.0005484968949674981\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17333627  93.55449398  52.21460485  89.01734341\n",
      "  32.21613433  84.66053323  76.40017462 118.25764235  77.91460059\n",
      " 126.26732151  84.15248891]\n",
      "40-th iteration, loss: 0.29345321492809834, 18 gd steps\n",
      "insert gradient: -0.0005478286798773133\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17251014  93.55631121  52.21443802  89.01777211\n",
      "  32.21686836  84.65757625  76.40144554 118.25629212  77.91518369\n",
      " 126.26662917  84.1527451 ]\n",
      "41-th iteration, loss: 0.29345317882182415, 17 gd steps\n",
      "insert gradient: -0.0005467631250306316\n",
      "41-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17171291  93.55810902  52.2142737   89.01818762\n",
      "  32.21758981  84.65466329  76.40268902 118.2549703   77.9157555\n",
      " 126.26595234  84.15299789]\n",
      "42-th iteration, loss: 0.2934531435502156, 17 gd steps\n",
      "insert gradient: -0.0005454447453401262\n",
      "42-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17094089  93.5598878   52.21411021  89.01859035\n",
      "  32.21829926  84.65179174  76.40390773 118.25367565  77.91631714\n",
      " 126.26529002  84.1532472 ]\n",
      "43-th iteration, loss: 0.293453109077604, 17 gd steps\n",
      "insert gradient: -0.0005439524391861601\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.17019149  93.56164752  52.21394688  89.01898082\n",
      "  32.21899737  84.64896044  76.40510343 118.25240748  77.91686916\n",
      " 126.26464154  84.15349283]\n",
      "44-th iteration, loss: 0.29345307537105075, 17 gd steps\n",
      "insert gradient: -0.0005423334675802315\n",
      "44-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71694626e+01 0.00000000e+00 1.11022302e-15\n",
      " 9.35633883e+01 5.22137834e+01 8.90193596e+01 3.22196849e+01\n",
      " 8.46461683e+01 7.64062776e+01 1.18251165e+02 7.79174120e+01\n",
      " 1.26264006e+02 8.41537346e+01]\n",
      "45-th iteration, loss: 0.29345303195296113, 19 gd steps\n",
      "insert gradient: -0.0005192183639500634\n",
      "45-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71686241e+01 0.00000000e+00 3.77475828e-15\n",
      " 9.35671490e+01 5.22135318e+01 8.90197388e+01 3.22204043e+01\n",
      " 8.46431031e+01 7.64075743e+01 1.18249819e+02 7.79180240e+01\n",
      " 1.26263316e+02 8.41540002e+01]\n",
      "46-th iteration, loss: 0.29345299016929965, 19 gd steps\n",
      "insert gradient: -0.0005050512464151216\n",
      "46-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.16772722  93.57072002  52.2131742   89.02006163\n",
      "  32.22106693  84.64010543  76.40887381 118.24852882  77.91865515\n",
      " 126.26264903  84.15425963]\n",
      "47-th iteration, loss: 0.29345295871297955, 16 gd steps\n",
      "insert gradient: -0.0005141253129220337\n",
      "47-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          17.16692337  93.57231235  52.2128612   89.02035439\n",
      "  32.22167924  84.63743695  76.41004327 118.24738731  77.9192181\n",
      " 126.26205498  84.15448747]\n",
      "48-th iteration, loss: 0.29345292803011175, 16 gd steps\n",
      "insert gradient: -0.0005181103187465647\n",
      "48-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71661785e+01 0.00000000e+00 1.99840144e-15\n",
      " 9.35739123e+01 5.22126225e+01 8.90206711e+01 3.22223259e+01\n",
      " 8.46348155e+01 7.64111807e+01 1.18246262e+02 7.79197501e+01\n",
      " 1.26261469e+02 8.41547078e+01]\n",
      "49-th iteration, loss: 0.2934528890124966, 18 gd steps\n",
      "insert gradient: -0.0004998587051557277\n",
      "49-th iteration, new layer inserted. now 14 layers\n",
      "[0.00000000e+00 1.71653596e+01 0.00000000e+00 5.55111512e-15\n",
      " 9.35773681e+01 5.22123407e+01 8.90210011e+01 3.22230201e+01\n",
      " 8.46319700e+01 7.64124150e+01 1.18245051e+02 7.79203300e+01\n",
      " 1.26260837e+02 8.41549461e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368028745168503\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.49960647   0.         483.13858449]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6338943430935724\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 235.67735829   0.         247.4612262 ]\n",
      "2-th iteration, loss: 0.6045644704212222, 13 gd steps\n",
      "insert gradient: -0.6805507104959237\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.24069056  77.30193988 220.75829298  41.38070239 242.4109971\n",
      "   0.           5.05022911]\n",
      "3-th iteration, loss: 0.46851108920036433, 20 gd steps\n",
      "insert gradient: -0.4296688299100961\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.6303661  116.47503403   0.         110.00419881\n",
      "  54.87991758 163.36276674  51.36288895   5.05022911]\n",
      "4-th iteration, loss: 0.3783689467939926, 30 gd steps\n",
      "insert gradient: -0.19633619877402855\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[1.52614328e+00 5.05294567e+01 6.45915769e+01 0.00000000e+00\n",
      " 4.44089210e-14 3.13629838e+01 1.04615486e+02 7.32626193e+01\n",
      " 1.21601817e+02 7.67153597e+01 5.05022911e+00]\n",
      "5-th iteration, loss: 0.3634809333146551, 14 gd steps\n",
      "insert gradient: -0.01038262492074788\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.39602121e+01 5.45561220e+01 4.95591278e+01\n",
      " 1.08433560e+02 7.62961852e+01 1.24070625e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.91907133e+01 5.05022911e+00]\n",
      "6-th iteration, loss: 0.3632811427912788, 32 gd steps\n",
      "insert gradient: -0.0007205060379669465\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37764057  55.31898834  48.88350137 106.88120874\n",
      "  76.74975691 123.29501113  80.12932721   5.05022911]\n",
      "7-th iteration, loss: 0.36328100493128834, 59 gd steps\n",
      "insert gradient: -0.0005192819345988708\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37087424  55.32226269  48.88386141 106.88376234\n",
      "  76.76062247 123.28082203  80.12171978   5.05022911]\n",
      "8-th iteration, loss: 0.36328096720883873, 33 gd steps\n",
      "insert gradient: -0.0004637030737957822\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43688764e+01 5.53236957e+01 4.88840715e+01\n",
      " 1.06884536e+02 0.00000000e+00 4.35207426e-14 7.67641280e+01\n",
      " 1.23275740e+02 8.01197143e+01 5.05022911e+00]\n",
      "9-th iteration, loss: 0.3632809318984339, 30 gd steps\n",
      "insert gradient: -0.0003275944205994616\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43672549e+01 5.53251403e+01 4.88842395e+01\n",
      " 1.06885062e+02 2.67818019e-03 3.42675863e-04 7.67668374e+01\n",
      " 1.23271063e+02 8.01181655e+01 5.05022911e+00]\n",
      "10-th iteration, loss: 0.36328091150025127, 24 gd steps\n",
      "insert gradient: -0.00027074351297176445\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43662489e+01 5.53262414e+01 4.88843516e+01\n",
      " 1.06885301e+02 4.12979579e-03 1.93509308e-04 7.67683264e+01\n",
      " 1.23267772e+02 8.01172349e+01 5.05022911e+00]\n",
      "11-th iteration, loss: 0.36328089609032865, 20 gd steps\n",
      "insert gradient: -0.00023526625267307905\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43654570e+01 5.53271922e+01 4.88844371e+01\n",
      " 1.06885458e+02 0.00000000e+00 4.35207426e-14 7.67746209e+01\n",
      " 1.23265026e+02 8.01165718e+01 5.05022911e+00]\n",
      "12-th iteration, loss: 0.36328088376496376, 18 gd steps\n",
      "insert gradient: -0.0002146431993933982\n",
      "12-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43647872e+01 0.00000000e+00 2.13162821e-14\n",
      " 5.53280340e+01 4.88844926e+01 1.06885566e+02 8.52706978e-04\n",
      " 8.21782816e-05 7.67754767e+01 1.23262634e+02 8.01160819e+01\n",
      " 5.05022911e+00]\n",
      "13-th iteration, loss: 0.3632808719973363, 18 gd steps\n",
      "insert gradient: -0.00019605072956433787\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43641356e+01 5.53295950e+01 4.88844988e+01\n",
      " 1.06885641e+02 1.60270296e-03 5.76254312e-05 0.00000000e+00\n",
      " 1.77876919e-20 7.67762340e+01 1.23260377e+02 8.01157038e+01\n",
      " 5.05022911e+00]\n",
      "14-th iteration, loss: 0.3632808622054664, 16 gd steps\n",
      "insert gradient: -0.00020047929633290127\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36354432  55.33029017  48.88447808 106.88568839\n",
      "  76.77970162 123.25835123  80.11543093   5.05022911]\n",
      "15-th iteration, loss: 0.36328085511424824, 14 gd steps\n",
      "insert gradient: -0.0001910677969942886\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36305558  55.33090712  48.88446774 106.88571951\n",
      "  76.78019143 123.25660096  80.11524121   5.05022911]\n",
      "16-th iteration, loss: 0.36328084870713284, 14 gd steps\n",
      "insert gradient: -0.00018370657181601525\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36259218  55.33147867  48.88444995 106.88575854\n",
      "  76.78068774 123.25497169  80.11511848   5.05022911]\n",
      "17-th iteration, loss: 0.3632808428502493, 13 gd steps\n",
      "insert gradient: -0.00017776122150228257\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43621535e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53320148e+01 4.88844285e+01 1.06885803e+02 7.67811845e+01\n",
      " 1.23253442e+02 8.01150492e+01 5.05022911e+00]\n",
      "18-th iteration, loss: 0.3632808367978078, 14 gd steps\n",
      "insert gradient: -0.0001824797972871759\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36170966  55.33303797  48.88439044 106.88585177\n",
      "  76.78169959 123.2519515   80.11502713   5.05022911]\n",
      "19-th iteration, loss: 0.36328083180198784, 13 gd steps\n",
      "insert gradient: -0.00018187185610436048\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36128907  55.33349694  48.88434224 106.88590213\n",
      "  76.78220085 123.25058841  80.11504821   5.05022911]\n",
      "20-th iteration, loss: 0.36328082712695065, 13 gd steps\n",
      "insert gradient: -0.00018072138711598034\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36089564  55.33394225  48.8843019  106.88595491\n",
      "  76.78269122 123.24928417  80.11509671   5.05022911]\n",
      "21-th iteration, loss: 0.3632808227364164, 12 gd steps\n",
      "insert gradient: -0.00017914200952425888\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43605261e+01 5.53343746e+01 4.88842675e+01\n",
      " 1.06886009e+02 0.00000000e+00 4.52970994e-14 7.67831701e+01\n",
      " 1.23248033e+02 8.01151688e+01 5.05022911e+00]\n",
      "22-th iteration, loss: 0.3632808180294299, 13 gd steps\n",
      "insert gradient: -0.00015964777911818342\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43601687e+01 5.53348127e+01 4.88842358e+01\n",
      " 1.06886060e+02 4.64789759e-04 4.30869022e-05 0.00000000e+00\n",
      " 1.43995601e-20 7.67836358e+01 1.23246784e+02 8.01152598e+01\n",
      " 5.05022911e+00]\n",
      "23-th iteration, loss: 0.36328081329769063, 13 gd steps\n",
      "insert gradient: -0.00016449867906003933\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43598372e+01 5.53352609e+01 4.88842114e+01\n",
      " 1.06886095e+02 8.65027601e-04 4.01954748e-05 7.67844423e+01\n",
      " 1.23245544e+02 8.01153571e+01 5.05022911e+00]\n",
      "24-th iteration, loss: 0.36328080929836504, 12 gd steps\n",
      "insert gradient: -0.00016295474651101477\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43595422e+01 5.53356993e+01 4.88841963e+01\n",
      " 1.06886116e+02 1.19421951e-03 9.65219770e-07 7.67847735e+01\n",
      " 1.23244366e+02 8.01154581e+01 5.05022911e+00]\n",
      "25-th iteration, loss: 0.3632808055320263, 12 gd steps\n",
      "insert gradient: -0.00016100227308573354\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43592625e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.53361276e+01 4.88841818e+01 1.06886133e+02 7.67865808e+01\n",
      " 1.23243226e+02 8.01155728e+01 5.05022911e+00]\n",
      "26-th iteration, loss: 0.3632808017759372, 12 gd steps\n",
      "insert gradient: -0.00014736404443637822\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35898005  55.33694883  48.88415536 106.88614893\n",
      "  76.78688147 123.24211404  80.11570686   5.05022911]\n",
      "27-th iteration, loss: 0.36328079863135654, 11 gd steps\n",
      "insert gradient: -0.00014452921294667487\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43587007e+01 0.00000000e+00 2.17603713e-14\n",
      " 5.53373234e+01 4.88841178e+01 1.06886168e+02 7.67871879e+01\n",
      " 1.23241075e+02 8.01158593e+01 5.05022911e+00]\n",
      "28-th iteration, loss: 0.363280795297109, 11 gd steps\n",
      "insert gradient: -0.00013451740112199119\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43584186e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53380505e+01 4.88840737e+01 1.06886190e+02 7.67875046e+01\n",
      " 1.23240052e+02 8.01160301e+01 5.05022911e+00]\n",
      "29-th iteration, loss: 0.36328079214862685, 11 gd steps\n",
      "insert gradient: -0.00013404393524031418\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35813118  55.33872547  48.88401689 106.8862144\n",
      "  76.78782925 123.23906922  80.11621776   5.05022911]\n",
      "30-th iteration, loss: 0.3632807894163954, 11 gd steps\n",
      "insert gradient: -0.00013272399803940837\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43578542e+01 5.53390439e+01 4.88839604e+01\n",
      " 1.06886241e+02 0.00000000e+00 5.41788836e-14 7.67881511e+01\n",
      " 1.23238134e+02 8.01164137e+01 5.05022911e+00]\n",
      "31-th iteration, loss: 0.363280786540881, 11 gd steps\n",
      "insert gradient: -0.00012981689638658347\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43575910e+01 0.00000000e+00 7.99360578e-15\n",
      " 5.53393668e+01 4.88839119e+01 1.06886267e+02 3.11070510e-04\n",
      " 2.21343571e-05 7.67884625e+01 1.23237205e+02 8.01166153e+01\n",
      " 5.05022911e+00]\n",
      "32-th iteration, loss: 0.36328078354983756, 11 gd steps\n",
      "insert gradient: -0.00012492825934199477\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43573371e+01 0.00000000e+00 1.50990331e-14\n",
      " 5.53400155e+01 4.88838653e+01 1.06886287e+02 5.95679133e-04\n",
      " 1.95056967e-05 7.67887485e+01 1.23236280e+02 8.01168203e+01\n",
      " 5.05022911e+00]\n",
      "33-th iteration, loss: 0.3632807807101571, 11 gd steps\n",
      "insert gradient: -0.00012058444338132232\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35708474  55.34063653  48.88381221 106.88630273\n",
      "  76.78987834 123.23538004  80.1170306    5.05022911]\n",
      "34-th iteration, loss: 0.36328077840566536, 11 gd steps\n",
      "insert gradient: -0.00011991257648972015\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35684643  55.34093178  48.8837625  106.88631763\n",
      "  76.79012904 123.23452544  80.11724073   5.05022911]\n",
      "35-th iteration, loss: 0.3632807761914422, 10 gd steps\n",
      "insert gradient: -0.0001190217677062339\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35662054  55.34122318  48.88371983 106.88633556\n",
      "  76.79037894 123.23369294  80.11745496   5.05022911]\n",
      "36-th iteration, loss: 0.3632807740589924, 10 gd steps\n",
      "insert gradient: -0.00011797385192530675\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35640534  55.34151073  48.88368267 106.8863559\n",
      "  76.79062782 123.23288054  80.11767294   5.05022911]\n",
      "37-th iteration, loss: 0.3632807720029435, 10 gd steps\n",
      "insert gradient: -0.00011681438804260915\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43561996e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53417943e+01 4.88836499e+01 1.06886378e+02 7.67908753e+01\n",
      " 1.23232087e+02 8.01178941e+01 5.05022911e+00]\n",
      "38-th iteration, loss: 0.363280769822111, 10 gd steps\n",
      "insert gradient: -0.00010998843017129755\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43559938e+01 5.53423485e+01 4.88836140e+01\n",
      " 1.06886401e+02 0.00000000e+00 3.64153152e-14 7.67911259e+01\n",
      " 1.23231303e+02 8.01181224e+01 5.05022911e+00]\n",
      "39-th iteration, loss: 0.36328076774772916, 10 gd steps\n",
      "insert gradient: -0.00011237730284447946\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43557903e+01 5.53426161e+01 4.88835748e+01\n",
      " 1.06886423e+02 2.45261409e-04 1.92625535e-05 7.67913714e+01\n",
      " 1.23230539e+02 8.01183533e+01 5.05022911e+00]\n",
      "40-th iteration, loss: 0.36328076577080876, 10 gd steps\n",
      "insert gradient: -0.00011371430811655429\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43556015e+01 5.53428867e+01 4.88835430e+01\n",
      " 1.06886441e+02 4.65776511e-04 2.04954007e-05 7.67915931e+01\n",
      " 1.23229786e+02 8.01185798e+01 5.05022911e+00]\n",
      "41-th iteration, loss: 0.36328076387526786, 10 gd steps\n",
      "insert gradient: -0.00011424563277366837\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43554243e+01 5.53431583e+01 4.88835166e+01\n",
      " 1.06886456e+02 6.67657192e-04 6.51261528e-06 7.67917960e+01\n",
      " 1.23229044e+02 8.01188034e+01 5.05022911e+00]\n",
      "42-th iteration, loss: 0.3632807620528317, 10 gd steps\n",
      "insert gradient: -0.0001141683408697548\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43552565e+01 0.00000000e+00 1.90958360e-14\n",
      " 5.53434293e+01 4.88834941e+01 1.06886468e+02 7.67928390e+01\n",
      " 1.23228314e+02 8.01190254e+01 5.05022911e+00]\n",
      "43-th iteration, loss: 0.36328076020566685, 10 gd steps\n",
      "insert gradient: -0.00010630256051144528\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43550885e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53439576e+01 4.88834677e+01 1.06886479e+02 7.67930215e+01\n",
      " 1.23227595e+02 8.01192505e+01 5.05022911e+00]\n",
      "44-th iteration, loss: 0.36328075844037255, 10 gd steps\n",
      "insert gradient: -0.00010493100366740441\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43549124e+01 5.53444511e+01 4.88834305e+01\n",
      " 1.06886491e+02 7.67932129e+01 1.23226898e+02 0.00000000e+00\n",
      " 8.52651283e-14 8.01194823e+01 5.05022911e+00]\n",
      "45-th iteration, loss: 0.36328075674213667, 10 gd steps\n",
      "insert gradient: -0.00010146641478336992\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43547376e+01 0.00000000e+00 1.28785871e-14\n",
      " 5.53446892e+01 4.88833925e+01 1.06886506e+02 7.67934084e+01\n",
      " 1.23226219e+02 8.01199462e+01 5.05022911e+00]\n",
      "46-th iteration, loss: 0.36328075509517604, 10 gd steps\n",
      "insert gradient: -0.00010198602649129783\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43545676e+01 5.53451582e+01 4.88833560e+01\n",
      " 1.06886521e+02 7.67936003e+01 1.23225549e+02 0.00000000e+00\n",
      " 4.08562073e-14 8.01201700e+01 5.05022911e+00]\n",
      "47-th iteration, loss: 0.3632807535049975, 10 gd steps\n",
      "insert gradient: -9.722862993347218e-05\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3543991   55.34538501  48.88331898 106.8865377\n",
      "  76.79379536 123.22489513  80.12061757   5.05022911]\n",
      "48-th iteration, loss: 0.36328075209334354, 10 gd steps\n",
      "insert gradient: -9.715369650583903e-05\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.35424175  55.34561133  48.8832887  106.8865547\n",
      "  76.79398325 123.22425433  80.12083021   5.05022911]\n",
      "49-th iteration, loss: 0.3632807507255397, 10 gd steps\n",
      "insert gradient: -9.766242522435842e-05\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43540905e+01 5.53458350e+01 4.88832614e+01\n",
      " 1.06886573e+02 7.67941705e+01 1.23223626e+02 0.00000000e+00\n",
      " 4.97379915e-14 8.01210435e+01 5.05022911e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235571\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         508.36511877]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6335178126710227\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 235.58383553   0.         272.78128324]\n",
      "2-th iteration, loss: 0.6046018105977554, 13 gd steps\n",
      "insert gradient: -0.6750448388891083\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.27488554  77.33265715 220.69625589  41.38811104 239.37949346\n",
      "   0.          33.40178978]\n",
      "3-th iteration, loss: 0.469464367429951, 20 gd steps\n",
      "insert gradient: -0.40056111866799093\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.98728814 116.07442618   0.         109.62584695\n",
      "  56.16811469 163.82440226  48.76198286  33.40178978]\n",
      "4-th iteration, loss: 0.3633640786604565, 49 gd steps\n",
      "insert gradient: -0.023210391172775693\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.30462367  54.10872534  49.50299064 106.36302326\n",
      "  76.20978909 122.81082842  79.97055351  33.40178978]\n",
      "5-th iteration, loss: 0.3632917438234116, 12 gd steps\n",
      "insert gradient: -0.002551324147334875\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.77041751  54.560786    49.00136925 106.64648411\n",
      "  76.77975805 122.98106822  80.03220491  33.40178978]\n",
      "6-th iteration, loss: 0.36328076903538614, 26 gd steps\n",
      "insert gradient: -0.00021594096083199152\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43725718e+01 0.00000000e+00 1.90958360e-14\n",
      " 5.52990742e+01 4.88901147e+01 1.06893953e+02 7.68021605e+01\n",
      " 1.23147445e+02 8.01430454e+01 3.34017898e+01]\n",
      "7-th iteration, loss: 0.3632807666778427, 11 gd steps\n",
      "insert gradient: -0.00021381336063476666\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43722807e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53001044e+01 4.88899264e+01 1.06893738e+02 7.68024841e+01\n",
      " 1.23147873e+02 8.01432125e+01 3.34017898e+01]\n",
      "8-th iteration, loss: 0.36328076444221574, 10 gd steps\n",
      "insert gradient: -0.0002112605722009454\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37199366  55.30111547  48.88974663 106.89352694\n",
      "  76.80278471 123.14828354  80.14336387  33.40178978]\n",
      "9-th iteration, loss: 0.3632807629646177, 10 gd steps\n",
      "insert gradient: -0.00021823015321411097\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37173189  55.30160106  48.88959179 106.89332932\n",
      "  76.80305146 123.14866113  80.14349364  33.40178978]\n",
      "10-th iteration, loss: 0.3632807615843888, 10 gd steps\n",
      "insert gradient: -0.00022314532251840638\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43714936e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53020968e+01 4.88894622e+01 1.06893136e+02 7.68032911e+01\n",
      " 1.23149020e+02 8.01436047e+01 3.34017898e+01]\n",
      "11-th iteration, loss: 0.36328075958900075, 10 gd steps\n",
      "insert gradient: -0.00021619155271480743\n",
      "11-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37125051  55.30312571  48.88933451 106.89293548\n",
      "  76.80352103 123.14937988  80.14370713  33.40178978]\n",
      "12-th iteration, loss: 0.3632807583286205, 10 gd steps\n",
      "insert gradient: -0.00022006588398676684\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37102047  55.30361216  48.88921831 106.89274804\n",
      "  76.8037304  123.14971524  80.14379725  33.40178978]\n",
      "13-th iteration, loss: 0.3632807571265141, 10 gd steps\n",
      "insert gradient: -0.00022261416856180668\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37080554  55.30410382  48.88911849 106.89256424\n",
      "  76.80392126 123.15003707  80.14387453  33.40178978]\n",
      "14-th iteration, loss: 0.3632807559717493, 10 gd steps\n",
      "insert gradient: -0.00022413325161744474\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37060213  55.30459844  48.88903131 106.89238376\n",
      "  76.8040965  123.15034717  80.14394099  33.40178978]\n",
      "15-th iteration, loss: 0.36328075485698885, 10 gd steps\n",
      "insert gradient: -0.00022485222501985732\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37040752  55.30509423  48.88895389 106.89220637\n",
      "  76.80425843 123.15064702  80.14399825  33.40178978]\n",
      "16-th iteration, loss: 0.36328075377715696, 10 gd steps\n",
      "insert gradient: -0.00022494892873947125\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43702197e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53055898e+01 4.88888840e+01 1.06892032e+02 7.68044090e+01\n",
      " 1.23150938e+02 8.01440477e+01 3.34017898e+01]\n",
      "17-th iteration, loss: 0.36328075207245136, 10 gd steps\n",
      "insert gradient: -0.0002143152273664303\n",
      "17-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37001629  55.30660029  48.88880493 106.89185091\n",
      "  76.80455871 123.15123519  80.1440948   33.40178978]\n",
      "18-th iteration, loss: 0.3632807510542596, 9 gd steps\n",
      "insert gradient: -0.00021528879437769546\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36981554  55.30707187  48.88872559 106.89168183\n",
      "  76.80470161 123.15151676  80.14413917  33.40178978]\n",
      "19-th iteration, loss: 0.363280750066939, 9 gd steps\n",
      "insert gradient: -0.0002156101659537288\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36962259  55.30754392  48.88865479 106.89151617\n",
      "  76.80483464 123.15179031  80.14417672  33.40178978]\n",
      "20-th iteration, loss: 0.36328074910716646, 9 gd steps\n",
      "insert gradient: -0.00021542346628551946\n",
      "20-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36943573  55.30801528  48.88859067 106.89135375\n",
      "  76.80495919 123.1520567   80.14420845  33.40178978]\n",
      "21-th iteration, loss: 0.3632807481725466, 9 gd steps\n",
      "insert gradient: -0.0002148415465887287\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43692537e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53084851e+01 4.88885318e+01 1.06891194e+02 7.68050764e+01\n",
      " 1.23152317e+02 8.01442352e+01 3.34017898e+01]\n",
      "22-th iteration, loss: 0.3632807466767522, 10 gd steps\n",
      "insert gradient: -0.00020425988667995852\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36905647  55.3094377   48.88846348 106.89103009\n",
      "  76.80519433 123.15258312  80.14426096  33.40178978]\n",
      "23-th iteration, loss: 0.3632807457896679, 9 gd steps\n",
      "insert gradient: -0.00020476605203184888\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43688604e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.53098835e+01 4.88883932e+01 1.06890876e+02 7.68053098e+01\n",
      " 1.23152838e+02 8.01442869e+01 3.34017898e+01]\n",
      "24-th iteration, loss: 0.3632807443958091, 10 gd steps\n",
      "insert gradient: -0.0001955534551930872\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43686533e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53107887e+01 4.88883173e+01 1.06890719e+02 7.68054247e+01\n",
      " 1.23153098e+02 8.01443110e+01 3.34017898e+01]\n",
      "25-th iteration, loss: 0.3632807430662715, 10 gd steps\n",
      "insert gradient: -0.0001879084886012101\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43684328e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53116535e+01 4.88882297e+01 1.06890565e+02 7.68055428e+01\n",
      " 1.23153357e+02 8.01443377e+01 3.34017898e+01]\n",
      "26-th iteration, loss: 0.3632807417915767, 10 gd steps\n",
      "insert gradient: -0.00018145067331735428\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36820278  55.31248452  48.88813441 106.89041423\n",
      "  76.80566257 123.1536149   80.14436582  33.40178978]\n",
      "27-th iteration, loss: 0.3632807409889863, 9 gd steps\n",
      "insert gradient: -0.00018402597558445053\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43679815e+01 0.00000000e+00 6.66133815e-15\n",
      " 5.53128818e+01 4.88880457e+01 1.06890273e+02 7.68057772e+01\n",
      " 1.23153862e+02 8.01443917e+01 3.34017898e+01]\n",
      "28-th iteration, loss: 0.36328073978510456, 10 gd steps\n",
      "insert gradient: -0.00017746227613958201\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43677573e+01 0.00000000e+00 2.17603713e-14\n",
      " 5.53136919e+01 4.88879583e+01 1.06890130e+02 7.68058882e+01\n",
      " 1.23154111e+02 8.01444142e+01 3.34017898e+01]\n",
      "29-th iteration, loss: 0.36328073862736476, 10 gd steps\n",
      "insert gradient: -0.00017184873784831833\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43675260e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53144733e+01 4.88878654e+01 1.06889991e+02 7.68060002e+01\n",
      " 1.23154358e+02 8.01444376e+01 3.34017898e+01]\n",
      "30-th iteration, loss: 0.3632807375115516, 10 gd steps\n",
      "insert gradient: -0.0001669642714047948\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43672901e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53152301e+01 4.88877692e+01 1.06889856e+02 7.68061123e+01\n",
      " 1.23154604e+02 8.01444614e+01 3.34017898e+01]\n",
      "31-th iteration, loss: 0.36328073643465597, 10 gd steps\n",
      "insert gradient: -0.00016264304452092622\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36705113  55.31596514  48.88767127 106.88972436\n",
      "  76.80622364 123.15484675  80.14448501  33.40178978]\n",
      "32-th iteration, loss: 0.36328073573340497, 9 gd steps\n",
      "insert gradient: -0.0001660252410611977\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36682336  55.31632115  48.8875824  106.88960042\n",
      "  76.80632942 123.15508044  80.14450588  33.40178978]\n",
      "33-th iteration, loss: 0.3632807350607044, 9 gd steps\n",
      "insert gradient: -0.00016836512409810488\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43666100e+01 0.00000000e+00 6.66133815e-15\n",
      " 5.53166825e+01 4.88875077e+01 1.06889480e+02 7.68064260e+01\n",
      " 1.23155307e+02 8.01445206e+01 3.34017898e+01]\n",
      "34-th iteration, loss: 0.3632807340586268, 9 gd steps\n",
      "insert gradient: -0.00016242131945610225\n",
      "34-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36639547  55.3174155   48.88743458 106.88935997\n",
      "  76.80651895 123.15553431  80.14453235  33.40178978]\n",
      "35-th iteration, loss: 0.36328073342268846, 9 gd steps\n",
      "insert gradient: -0.0001645395820149867\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43661871e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53177682e+01 4.88873651e+01 1.06889246e+02 7.68066094e+01\n",
      " 1.23155754e+02 8.01445436e+01 3.34017898e+01]\n",
      "36-th iteration, loss: 0.36328073247326237, 9 gd steps\n",
      "insert gradient: -0.0001586183182808521\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36597735  55.31848238  48.88729689 106.88913195\n",
      "  76.80669658 123.15597485  80.14455208  33.40178978]\n",
      "37-th iteration, loss: 0.3632807318709317, 9 gd steps\n",
      "insert gradient: -0.00016058720819878854\n",
      "37-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36577314  55.31882619  48.88723158 106.88902393\n",
      "  76.80678181 123.15618877  80.14456042  33.40178978]\n",
      "38-th iteration, loss: 0.36328073128775473, 9 gd steps\n",
      "insert gradient: -0.0001618388116074239\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43655793e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53191729e+01 4.88871759e+01 1.06888919e+02 7.68068604e+01\n",
      " 1.23156397e+02 8.01445645e+01 3.34017898e+01]\n",
      "39-th iteration, loss: 0.36328073040200975, 9 gd steps\n",
      "insert gradient: -0.0001553953947526078\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36538229  55.31987138  48.88711905 106.88881364\n",
      "  76.80693683 123.15660692  80.1445669   33.40178978]\n",
      "40-th iteration, loss: 0.3632807298464106, 9 gd steps\n",
      "insert gradient: -0.00015681895127494554\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43651887e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.53202069e+01 4.88870630e+01 1.06888714e+02 7.68070127e+01\n",
      " 1.23156811e+02 8.01445701e+01 3.34017898e+01]\n",
      "41-th iteration, loss: 0.3632807290079443, 9 gd steps\n",
      "insert gradient: -0.0001507509553434828\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36499282  55.32088271  48.88700648 106.88861357\n",
      "  76.80708623 123.1570159   80.14457136  33.40178978]\n",
      "42-th iteration, loss: 0.36328072847891435, 9 gd steps\n",
      "insert gradient: -0.00015226300777102178\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43648006e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.53212081e+01 4.88869510e+01 1.06888519e+02 7.68071592e+01\n",
      " 1.23157216e+02 8.01445734e+01 3.34017898e+01]\n",
      "43-th iteration, loss: 0.3632807276842549, 9 gd steps\n",
      "insert gradient: -0.00014648278944985748\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43646068e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.53218633e+01 4.88868956e+01 1.06888424e+02 7.68072296e+01\n",
      " 1.23157416e+02 8.01445736e+01 3.34017898e+01]\n",
      "44-th iteration, loss: 0.3632807269185948, 9 gd steps\n",
      "insert gradient: -0.00014158010992284162\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43644067e+01 0.00000000e+00 9.32587341e-15\n",
      " 5.53224950e+01 4.88868339e+01 1.06888331e+02 7.68073024e+01\n",
      " 1.23157616e+02 8.01445759e+01 3.34017898e+01]\n",
      "45-th iteration, loss: 0.3632807261788969, 9 gd steps\n",
      "insert gradient: -0.00013735424531462545\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43642023e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.53231064e+01 4.88867678e+01 1.06888240e+02 7.68073765e+01\n",
      " 1.23157815e+02 8.01445797e+01 3.34017898e+01]\n",
      "46-th iteration, loss: 0.36328072546311496, 9 gd steps\n",
      "insert gradient: -0.00013365299946282208\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43639953e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.53237004e+01 4.88866991e+01 1.06888152e+02 7.68074513e+01\n",
      " 1.23158014e+02 8.01445845e+01 3.34017898e+01]\n",
      "47-th iteration, loss: 0.3632807247697769, 9 gd steps\n",
      "insert gradient: -0.00013036067141335543\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43637867e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.53242788e+01 4.88866288e+01 1.06888067e+02 7.68075260e+01\n",
      " 1.23158211e+02 8.01445897e+01 3.34017898e+01]\n",
      "48-th iteration, loss: 0.36328072409774714, 9 gd steps\n",
      "insert gradient: -0.00012738911903423234\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36357757  55.32484336  48.8865578  106.88798388\n",
      "  76.80760038 123.15840635  80.1445952   33.40178978]\n",
      "49-th iteration, loss: 0.36328072364766073, 9 gd steps\n",
      "insert gradient: -0.00013032127304911056\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36337662  55.32512031  48.886493   106.88790527\n",
      "  76.80767169 123.15859661  80.14459934  33.40178978]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536822723274187\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.09598192   0.         533.79346532]\n",
      "1-th iteration, loss: 0.7485404418159318, 11 gd steps\n",
      "insert gradient: -0.6320778488893469\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.67444096  62.33745299 234.34835063   0.         299.44511469]\n",
      "2-th iteration, loss: 0.6043688060073337, 13 gd steps\n",
      "insert gradient: -0.6651681320057647\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.41754116  77.5322004  219.74073292  41.69768811 244.44499158\n",
      "   0.          55.00012311]\n",
      "3-th iteration, loss: 0.4744786338070808, 20 gd steps\n",
      "insert gradient: -0.38358074035489387\n",
      "3-th iteration, new layer inserted. now 7 layers\n",
      "[  0.          58.91200117 227.21459818  54.34099352 162.55212847\n",
      "  39.79964427  55.00012311]\n",
      "4-th iteration, loss: 0.4327268984789332, 27 gd steps\n",
      "insert gradient: -0.42694671369981374\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  2.16084314  45.38787432 103.0800309    0.         109.14356213\n",
      "  58.65432342 134.10306458  58.60436403  55.00012311]\n",
      "5-th iteration, loss: 0.3636004250564142, 44 gd steps\n",
      "insert gradient: -0.017160766387702326\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          45.8918658   52.60982744  49.13185397 107.38323304\n",
      "  77.73819609 122.77476021  79.37330352  55.00012311]\n",
      "6-th iteration, loss: 0.36328118383301444, 25 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.30572675  55.33093059  48.88471244 106.93338137\n",
      "  76.79115984 123.15566191  80.0881066   55.00012311]\n",
      "7-th iteration, loss: 0.3632807594935392, 25 gd steps\n",
      "insert gradient: -0.00035353534534524555\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.33948658  55.34351604  48.88420855 106.92079118\n",
      "  76.79820106 123.17280434  80.12516806  55.00012311]\n",
      "8-th iteration, loss: 0.363280754566274, 13 gd steps\n",
      "insert gradient: -0.00033068395036597035\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.34019634  55.34396033  48.88385126 106.92001463\n",
      "  76.79834135 123.17318158  80.12612734  55.00012311]\n",
      "9-th iteration, loss: 0.3632807503049763, 12 gd steps\n",
      "insert gradient: -0.0003102383718605218\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.34083232  55.34437373  48.88352348 106.91928551\n",
      "  76.79847112 123.17351771  80.12699074  55.00012311]\n",
      "10-th iteration, loss: 0.3632807465720773, 12 gd steps\n",
      "insert gradient: -0.00029175406906093706\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43414082e+01 5.53447621e+01 4.88832210e+01\n",
      " 1.06918596e+02 7.67985931e+01 1.23173820e+02 0.00000000e+00\n",
      " 6.57252031e-14 8.01277759e+01 5.50001231e+01]\n",
      "11-th iteration, loss: 0.36328074178519826, 12 gd steps\n",
      "insert gradient: -0.00025163872991262756\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43419851e+01 5.53451698e+01 4.88829197e+01\n",
      " 1.06917877e+02 7.67987077e+01 1.23174108e+02 7.58395520e-04\n",
      " 2.77442043e-04 8.01285380e+01 5.50001231e+01]\n",
      "12-th iteration, loss: 0.3632807380844695, 12 gd steps\n",
      "insert gradient: -0.00022090460170673672\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43425014e+01 5.53455574e+01 4.88826628e+01\n",
      " 1.06917213e+02 7.67987911e+01 1.23174331e+02 1.36355668e-03\n",
      " 4.57131178e-04 8.01291587e+01 5.50001231e+01]\n",
      "13-th iteration, loss: 0.3632807350720144, 11 gd steps\n",
      "insert gradient: -0.00019637373554209822\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43429696e+01 5.53459268e+01 4.88824345e+01\n",
      " 1.06916590e+02 7.67988588e+01 1.23174513e+02 1.86513644e-03\n",
      " 5.70269618e-04 8.01296828e+01 5.50001231e+01]\n",
      "14-th iteration, loss: 0.3632807325337406, 11 gd steps\n",
      "insert gradient: -0.00017622728708258385\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43433975e+01 5.53462798e+01 4.88822261e+01\n",
      " 1.06915998e+02 7.67989192e+01 1.23174666e+02 2.29199060e-03\n",
      " 6.34822357e-04 0.00000000e+00 1.55854062e-19 8.01301360e+01\n",
      " 5.50001231e+01]\n",
      "15-th iteration, loss: 0.36328072996538613, 11 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43438019e+01 5.53466286e+01 4.88820299e+01\n",
      " 1.06915416e+02 7.67989746e+01 1.23174797e+02 2.66254235e-03\n",
      " 6.58604534e-04 3.99505687e-04 1.98525926e-05 8.01305358e+01\n",
      " 5.50001231e+01]\n",
      "16-th iteration, loss: 0.3632807278474141, 10 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43441736e+01 5.53469644e+01 4.88818532e+01\n",
      " 1.06914863e+02 7.67990233e+01 1.23174901e+02 2.96304333e-03\n",
      " 6.43941823e-04 8.01315945e+01 5.50001231e+01]\n",
      "17-th iteration, loss: 0.36328072623289775, 10 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43445108e+01 5.53472814e+01 4.88816909e+01\n",
      " 1.06914341e+02 7.67990729e+01 1.23174990e+02 3.21636175e-03\n",
      " 6.05045568e-04 8.01318747e+01 5.50001231e+01]\n",
      "18-th iteration, loss: 0.36328072477029816, 10 gd steps\n",
      "insert gradient: -0.00012852327025058823\n",
      "18-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43448213e+01 0.00000000e+00 7.99360578e-15\n",
      " 5.53475838e+01 4.88815349e+01 1.06913836e+02 7.67991298e+01\n",
      " 1.23175073e+02 3.44747100e-03 5.51165389e-04 8.01321307e+01\n",
      " 5.50001231e+01]\n",
      "19-th iteration, loss: 0.3632807230718496, 10 gd steps\n",
      "insert gradient: -0.0001119943018418353\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43450951e+01 2.79046693e-04 2.71851541e-04\n",
      " 0.00000000e+00 7.11507676e-20 5.53478648e+01 4.88813643e+01\n",
      " 1.06913332e+02 7.67991982e+01 1.23175154e+02 3.66785002e-03\n",
      " 4.84358044e-04 8.01323740e+01 5.50001231e+01]\n",
      "20-th iteration, loss: 0.363280721439458, 10 gd steps\n",
      "insert gradient: -0.00010188992624850231\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43453004e+01 5.04006954e-04 4.65869531e-04\n",
      " 2.35830548e-04 1.92334619e-04 5.53481020e+01 4.88811639e+01\n",
      " 1.06912839e+02 7.67992862e+01 1.23175239e+02 3.88093877e-03\n",
      " 4.12145796e-04 8.01326070e+01 5.50001231e+01]\n",
      "21-th iteration, loss: 0.3632807201098867, 10 gd steps\n",
      "insert gradient: -9.668604792076754e-05\n",
      "21-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43454456e+01 6.78705904e-04 5.92752057e-04\n",
      " 4.27259765e-04 3.09906785e-04 5.53483009e+01 4.88809481e+01\n",
      " 1.06912365e+02 7.67993915e+01 1.23175326e+02 4.08427806e-03\n",
      " 3.36482142e-04 8.01328268e+01 5.50001231e+01]\n",
      "22-th iteration, loss: 0.3632807189486653, 10 gd steps\n",
      "insert gradient: -9.066542620647246e-05\n",
      "22-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43455563e+01 8.24275703e-04 6.79703997e-04\n",
      " 5.93337886e-04 3.81422890e-04 5.53484779e+01 4.88807326e+01\n",
      " 1.06911908e+02 7.67995067e+01 1.23175412e+02 4.27644914e-03\n",
      " 2.53687213e-04 0.00000000e+00 4.74338450e-20 8.01330321e+01\n",
      " 5.50001231e+01]\n",
      "23-th iteration, loss: 0.36328071781295657, 10 gd steps\n",
      "insert gradient: -7.984227716176272e-05\n",
      "23-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43456473e+01 9.53514101e-04 7.42192894e-04\n",
      " 7.45791497e-04 4.23038651e-04 5.53486434e+01 4.88805246e+01\n",
      " 1.06911464e+02 7.67996271e+01 1.23175495e+02 4.45541197e-03\n",
      " 1.60602984e-04 0.00000000e+00 3.38813179e-21 8.01334094e+01\n",
      " 5.50001231e+01]\n",
      "24-th iteration, loss: 0.36328071680044843, 9 gd steps\n",
      "insert gradient: -7.051683303874378e-05\n",
      "24-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43457262e+01 1.07292546e-03 7.88699097e-04\n",
      " 8.90162487e-04 4.43876485e-04 5.53488020e+01 4.88803310e+01\n",
      " 1.06911035e+02 7.67997476e+01 1.23175571e+02 4.61433406e-03\n",
      " 5.56015092e-05 8.01337387e+01 5.50001231e+01]\n",
      "25-th iteration, loss: 0.3632807159551344, 9 gd steps\n",
      "insert gradient: -6.540098052394057e-05\n",
      "25-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43457960e+01 1.18413617e-03 8.22551269e-04\n",
      " 1.02748230e-03 4.47683904e-04 5.53489539e+01 4.88801524e+01\n",
      " 1.06910623e+02 7.67998689e+01 1.23175643e+02 8.01386431e+01\n",
      " 5.50001231e+01]\n",
      "26-th iteration, loss: 0.363280715239094, 9 gd steps\n",
      "insert gradient: -6.331802648027066e-05\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43458584e+01 1.28827158e-03 8.45853555e-04\n",
      " 1.15852038e-03 4.36802552e-04 5.53490995e+01 4.88799872e+01\n",
      " 1.06910225e+02 7.67999916e+01 1.23175712e+02 8.01387786e+01\n",
      " 5.50001231e+01]\n",
      "27-th iteration, loss: 0.36328071456801236, 9 gd steps\n",
      "insert gradient: -6.110273731478598e-05\n",
      "27-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43459155e+01 1.38706140e-03 8.60694298e-04\n",
      " 1.28482831e-03 4.13358564e-04 5.53492398e+01 4.88798339e+01\n",
      " 1.06909840e+02 7.68001167e+01 1.23175781e+02 0.00000000e+00\n",
      " 1.42108547e-14 8.01389091e+01 5.50001231e+01]\n",
      "28-th iteration, loss: 0.36328071388579486, 9 gd steps\n",
      "insert gradient: -6.06470261932821e-05\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43459701e+01 1.48349450e-03 8.69739811e-04\n",
      " 1.40924540e-03 3.80008816e-04 0.00000000e+00 1.35525272e-19\n",
      " 5.53493775e+01 4.88796927e+01 1.06909464e+02 7.68002421e+01\n",
      " 1.23175847e+02 1.22995011e-04 6.51678773e-05 8.01390326e+01\n",
      " 5.50001231e+01]\n",
      "29-th iteration, loss: 0.3632807132084558, 9 gd steps\n",
      "insert gradient: -5.953918858412758e-05\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43460211e+01 1.57735289e-03 8.72098104e-04\n",
      " 1.53133696e-03 3.35964997e-04 5.53496459e+01 4.88795617e+01\n",
      " 1.06909098e+02 0.00000000e+00 2.66453526e-14 7.68003654e+01\n",
      " 1.23175908e+02 2.31972454e-04 1.20499227e-04 8.01391445e+01\n",
      " 5.50001231e+01]\n",
      "30-th iteration, loss: 0.3632807125805538, 9 gd steps\n",
      "insert gradient: -5.879494489921981e-05\n",
      "30-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43460706e+01 1.67054616e-03 8.70113299e-04\n",
      " 1.65270177e-03 2.83824220e-04 5.53497778e+01 4.88794428e+01\n",
      " 1.06908741e+02 7.68006068e+01 1.23175964e+02 3.28522218e-04\n",
      " 1.67156230e-04 8.01392462e+01 5.50001231e+01]\n",
      "31-th iteration, loss: 0.36328071202785583, 9 gd steps\n",
      "insert gradient: -5.787716904038953e-05\n",
      "31-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43461211e+01 1.76558081e-03 8.66405822e-04\n",
      " 1.77568897e-03 2.26318263e-04 5.53499096e+01 4.88793359e+01\n",
      " 1.06908394e+02 7.68007182e+01 1.23176014e+02 4.12045165e-04\n",
      " 2.03665859e-04 8.01393365e+01 5.50001231e+01]\n",
      "32-th iteration, loss: 0.36328071150498886, 9 gd steps\n",
      "insert gradient: -5.6834749195474314e-05\n",
      "32-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43461712e+01 1.86045910e-03 8.59293741e-04\n",
      " 1.89832118e-03 1.61586935e-04 0.00000000e+00 3.38813179e-21\n",
      " 5.53500392e+01 4.88792393e+01 1.06908055e+02 7.68008293e+01\n",
      " 1.23176062e+02 4.86674288e-04 2.34209180e-04 8.01394193e+01\n",
      " 5.50001231e+01]\n",
      "33-th iteration, loss: 0.3632807109685633, 9 gd steps\n",
      "insert gradient: -5.520326844665313e-05\n",
      "33-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 4.43462185e+01 1.95329138e-03 8.46392867e-04\n",
      " 2.01867803e-03 8.70858332e-05 5.53502895e+01 4.88791494e+01\n",
      " 1.06907725e+02 0.00000000e+00 2.66453526e-14 7.68009411e+01\n",
      " 1.23176107e+02 5.54383955e-04 2.60159968e-04 8.01394964e+01\n",
      " 5.50001231e+01]\n",
      "34-th iteration, loss: 0.363280710460357, 9 gd steps\n",
      "insert gradient: -5.406105533289147e-05\n",
      "34-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43462648e+01 2.04576466e-03 8.29626089e-04\n",
      " 2.13817416e-03 5.01382859e-06 5.53504113e+01 4.88790676e+01\n",
      " 1.06907403e+02 7.68011640e+01 1.23176151e+02 6.15769315e-04\n",
      " 2.81887644e-04 8.01395681e+01 5.50001231e+01]\n",
      "35-th iteration, loss: 0.36328071002923984, 9 gd steps\n",
      "insert gradient: -5.179546087929864e-05\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43463110e+01 2.13900919e-03 8.10192627e-04\n",
      " 5.53527888e+01 4.88789936e+01 1.06907089e+02 0.00000000e+00\n",
      " 1.24344979e-14 7.68012685e+01 1.23176191e+02 6.69499512e-04\n",
      " 2.97448738e-04 8.01396328e+01 5.50001231e+01]\n",
      "36-th iteration, loss: 0.36328070962688075, 9 gd steps\n",
      "insert gradient: -5.186917874100472e-05\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43463545e+01 2.23019818e-03 7.85206223e-04\n",
      " 0.00000000e+00 2.16840434e-19 5.53529056e+01 4.88789264e+01\n",
      " 1.06906782e+02 7.68014760e+01 1.23176229e+02 7.18085516e-04\n",
      " 3.09389974e-04 8.01396930e+01 5.50001231e+01]\n",
      "37-th iteration, loss: 0.3632807092396791, 9 gd steps\n",
      "insert gradient: -4.8834526280265624e-05\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43463955e+01 2.31982335e-03 7.54910047e-04\n",
      " 5.53531344e+01 4.88788641e+01 1.06906481e+02 7.68015737e+01\n",
      " 1.23176264e+02 7.60740712e-04 3.16173384e-04 8.01397477e+01\n",
      " 5.50001231e+01]\n",
      "38-th iteration, loss: 0.36328070890153824, 9 gd steps\n",
      "insert gradient: -4.86495924196919e-05\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43464336e+01 2.40684707e-03 7.19192924e-04\n",
      " 5.53532451e+01 4.88788073e+01 1.06906188e+02 7.68016731e+01\n",
      " 1.23176298e+02 8.00407856e-04 3.21215239e-04 8.01397996e+01\n",
      " 5.50001231e+01]\n",
      "39-th iteration, loss: 0.363280708578623, 9 gd steps\n",
      "insert gradient: -4.854650420931303e-05\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43464720e+01 2.49396837e-03 6.81007541e-04\n",
      " 5.53533548e+01 4.88787579e+01 1.06905902e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68017721e+01 1.23176332e+02 8.36143651e-04\n",
      " 3.23478736e-04 8.01398477e+01 5.50001231e+01]\n",
      "40-th iteration, loss: 0.3632807082441444, 9 gd steps\n",
      "insert gradient: -4.819329709240638e-05\n",
      "40-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465111e+01 2.58197904e-03 6.40776548e-04\n",
      " 0.00000000e+00 5.42101086e-20 5.53534641e+01 4.88787159e+01\n",
      " 1.06905622e+02 7.68019661e+01 1.23176363e+02 8.68039728e-04\n",
      " 3.22710649e-04 8.01398921e+01 5.50001231e+01]\n",
      "41-th iteration, loss: 0.3632807079196444, 9 gd steps\n",
      "insert gradient: -4.6407149921869005e-05\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465478e+01 2.66847380e-03 5.95442080e-04\n",
      " 5.53536772e+01 4.88786767e+01 1.06905346e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68020582e+01 1.23176393e+02 8.96560686e-04\n",
      " 3.18632939e-04 8.01399330e+01 5.50001231e+01]\n",
      "42-th iteration, loss: 0.36328070761140924, 9 gd steps\n",
      "insert gradient: -4.5466067103646514e-05\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43465826e+01 2.75346859e-03 5.45596083e-04\n",
      " 0.00000000e+00 4.74338450e-20 5.53537807e+01 4.88786418e+01\n",
      " 1.06905077e+02 7.68022434e+01 1.23176422e+02 9.23624228e-04\n",
      " 3.13532571e-04 8.01399723e+01 5.50001231e+01]\n",
      "43-th iteration, loss: 0.363280707311927, 9 gd steps\n",
      "insert gradient: -4.44671530807825e-05\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43466160e+01 2.83768504e-03 4.91715929e-04\n",
      " 5.53539830e+01 4.88786100e+01 1.06904813e+02 7.68023314e+01\n",
      " 1.23176450e+02 9.47979913e-04 3.05619433e-04 8.01400087e+01\n",
      " 5.50001231e+01]\n",
      "44-th iteration, loss: 0.36328070704712434, 9 gd steps\n",
      "insert gradient: -4.451023231870737e-05\n",
      "44-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43466473e+01 2.91992891e-03 4.33258952e-04\n",
      " 5.53540804e+01 4.88785813e+01 1.06904555e+02 0.00000000e+00\n",
      " 3.55271368e-14 7.68024215e+01 1.23176479e+02 9.71922958e-04\n",
      " 2.97656737e-04 8.01400443e+01 5.50001231e+01]\n",
      "45-th iteration, loss: 0.36328070677029495, 9 gd steps\n",
      "insert gradient: -4.241424665442912e-05\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43466802e+01 3.00381243e-03 3.73844813e-04\n",
      " 5.53541776e+01 4.88785587e+01 1.06904302e+02 7.68025988e+01\n",
      " 1.23176506e+02 9.93735459e-04 2.87823485e-04 8.01400774e+01\n",
      " 5.50001231e+01]\n",
      "46-th iteration, loss: 0.3632807065228354, 9 gd steps\n",
      "insert gradient: -4.17196026720187e-05\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43467140e+01 3.08890473e-03 3.12782847e-04\n",
      " 5.53542742e+01 4.88785404e+01 1.06904054e+02 7.68026826e+01\n",
      " 1.23176532e+02 1.01321662e-03 2.75425052e-04 8.01401079e+01\n",
      " 5.50001231e+01]\n",
      "47-th iteration, loss: 0.3632807062829413, 9 gd steps\n",
      "insert gradient: -4.2075211262081945e-05\n",
      "47-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43467476e+01 3.17391762e-03 2.48948849e-04\n",
      " 5.53543687e+01 4.88785257e+01 1.06903812e+02 0.00000000e+00\n",
      " 2.66453526e-14 7.68027671e+01 1.23176558e+02 1.03210454e-03\n",
      " 2.62532350e-04 8.01401372e+01 5.50001231e+01]\n",
      "48-th iteration, loss: 0.3632807060310395, 9 gd steps\n",
      "insert gradient: -4.043766130770622e-05\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.43467819e+01 3.25985961e-03 1.83061311e-04\n",
      " 5.53544621e+01 4.88785152e+01 1.06903574e+02 7.68029346e+01\n",
      " 1.23176584e+02 1.04997596e-03 2.48537504e-04 8.01401651e+01\n",
      " 5.50001231e+01]\n",
      "49-th iteration, loss: 0.3632807058044003, 9 gd steps\n",
      "insert gradient: -3.979847784464311e-05\n",
      "49-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 4.43468164e+01 3.34663333e-03 1.14868433e-04\n",
      " 5.53545543e+01 4.88785074e+01 1.06903341e+02 0.00000000e+00\n",
      " 3.73034936e-14 7.68030141e+01 1.23176608e+02 1.06643503e-03\n",
      " 2.32623493e-04 8.01401909e+01 5.50001231e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536156841022466\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         559.42362414]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6275936673118951\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 231.95613684   0.         327.4674873 ]\n",
      "2-th iteration, loss: 0.603911243450414, 13 gd steps\n",
      "insert gradient: -0.6477050526781667\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.66084701  77.86244622 217.86303052  42.20336436 240.58835802\n",
      "   0.          86.87912928]\n",
      "3-th iteration, loss: 0.4421809060638743, 40 gd steps\n",
      "insert gradient: -0.3189308620588181\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.34781212  44.15352399 102.77448888   0.         108.82004705\n",
      "  66.83844686 131.81653144  48.32776688  86.87912928]\n",
      "4-th iteration, loss: 0.36856492606656277, 35 gd steps\n",
      "insert gradient: -0.24451322888032895\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.83525779e+01 4.87161030e+01 5.01821371e+01\n",
      " 1.03820541e+02 0.00000000e+00 3.46389584e-14 7.18575569e+01\n",
      " 1.19813475e+02 7.88336367e+01 8.68791293e+01]\n",
      "5-th iteration, loss: 0.36434708536548227, 9 gd steps\n",
      "insert gradient: -0.047786446270797665\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 4.74520434e+01 0.00000000e+00 6.21724894e-15\n",
      " 4.81949361e+01 4.95128836e+01 1.04489400e+02 2.35286478e+00\n",
      " 6.13019658e-01 7.42247603e+01 1.20926606e+02 7.96985737e+01\n",
      " 8.68791293e+01]\n",
      "6-th iteration, loss: 0.3633293463627884, 30 gd steps\n",
      "insert gradient: -0.002811968722227069\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.51358182e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.38492351e+01 4.93389830e+01 1.06115864e+02 7.70990222e+01\n",
      " 1.22385819e+02 8.02946170e+01 8.68791293e+01]\n",
      "7-th iteration, loss: 0.36328131040019135, 27 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 2.44345776827212e-16\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43384875e+01 5.55028241e+01 4.88273666e+01\n",
      " 1.06943274e+02 7.68411778e+01 1.23155766e+02 8.02015908e+01\n",
      " 0.00000000e+00 1.86517468e-14 8.68791293e+01]\n",
      "8-th iteration, loss: 0.36328103950629637, 59 gd steps\n",
      "insert gradient: -0.0003359099819763673\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43229448e+01 5.54862200e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88314066e+01 1.06934212e+02 7.68283652e+01\n",
      " 1.23154182e+02 8.01843375e+01 8.68791293e+01]\n",
      "9-th iteration, loss: 0.36328099884917386, 32 gd steps\n",
      "insert gradient: -0.0002532482454171819\n",
      "9-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32057437  55.48205155  48.83579025 106.93254259\n",
      "  76.82626662 123.15464357  80.18082396  86.87912928]\n",
      "10-th iteration, loss: 0.36328097728964837, 24 gd steps\n",
      "insert gradient: -0.00028256754447840687\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31919188  55.47920685  48.83719492 106.93146264\n",
      "  76.82507279 123.15511724  80.17867405  86.87912928]\n",
      "11-th iteration, loss: 0.36328096109120184, 21 gd steps\n",
      "insert gradient: -0.000292519314638274\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43182561e+01 5.54768776e+01 0.00000000e+00\n",
      " 1.33226763e-14 4.88384863e+01 1.06930616e+02 7.68241682e+01\n",
      " 1.23155585e+02 8.01769767e+01 8.68791293e+01]\n",
      "12-th iteration, loss: 0.3632809452097667, 20 gd steps\n",
      "insert gradient: -0.00023596956171920605\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43174636e+01 5.54746539e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88408338e+01 1.06929819e+02 7.68233880e+01\n",
      " 1.23156088e+02 8.01754621e+01 8.68791293e+01]\n",
      "13-th iteration, loss: 0.3632809324276927, 18 gd steps\n",
      "insert gradient: -0.00020390670549601242\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31679073  55.47265098  48.84261881 106.9290962\n",
      "  76.82276554 123.15657891  80.1742091   86.87912928]\n",
      "14-th iteration, loss: 0.36328092268900075, 16 gd steps\n",
      "insert gradient: -0.00022050747282701498\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43162642e+01 5.54708791e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88433833e+01 1.06928459e+02 7.68222512e+01\n",
      " 1.23157043e+02 8.01731473e+01 8.68791293e+01]\n",
      "15-th iteration, loss: 0.3632809129020029, 16 gd steps\n",
      "insert gradient: -0.0001905551635727278\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43158323e+01 5.54691750e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88448826e+01 1.06927847e+02 7.68217732e+01\n",
      " 1.23157512e+02 8.01721515e+01 8.68791293e+01]\n",
      "16-th iteration, loss: 0.36328090427873916, 15 gd steps\n",
      "insert gradient: -0.00017046702602091515\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43154551e+01 5.54675668e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88461479e+01 1.06927263e+02 7.68213529e+01\n",
      " 1.23157970e+02 8.01712597e+01 8.68791293e+01]\n",
      "17-th iteration, loss: 0.3632808965260592, 15 gd steps\n",
      "insert gradient: -0.0001559253758885402\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43151324e+01 5.54660401e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88472573e+01 1.06926703e+02 7.68209741e+01\n",
      " 1.23158418e+02 8.01704459e+01 8.68791293e+01]\n",
      "18-th iteration, loss: 0.36328088947477316, 14 gd steps\n",
      "insert gradient: -0.00014603112327446558\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43148612e+01 5.54645861e+01 4.88482528e+01\n",
      " 1.06926165e+02 7.68206276e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23158856e+02 8.01696952e+01 8.68791293e+01]\n",
      "19-th iteration, loss: 0.3632808829831059, 14 gd steps\n",
      "insert gradient: -0.0001611621120849146\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3146465   55.46319136  48.8487349  106.92564602\n",
      "  76.82029844 123.15970983  80.168983    86.87912928]\n",
      "20-th iteration, loss: 0.36328087744844206, 13 gd steps\n",
      "insert gradient: -0.00016964194126953096\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43145116e+01 5.54619125e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88492307e+01 1.06925168e+02 7.68199842e+01\n",
      " 1.23160103e+02 8.01683172e+01 8.68791293e+01]\n",
      "21-th iteration, loss: 0.3632808717316926, 13 gd steps\n",
      "insert gradient: -0.00014845715852859726\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31441526  55.46065799  48.85021379 106.92469835\n",
      "  76.81968261 123.16049733  80.16767567  86.87912928]\n",
      "22-th iteration, loss: 0.36328086685378946, 13 gd steps\n",
      "insert gradient: -0.00015631309812795958\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31433975  55.45946148  48.85065655 106.92424969\n",
      "  76.81940945 123.16088036  80.16708458  86.87912928]\n",
      "23-th iteration, loss: 0.36328086226341977, 13 gd steps\n",
      "insert gradient: -0.00016092268960109148\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43143075e+01 5.54583144e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88511093e+01 1.06923821e+02 7.68191496e+01\n",
      " 1.23161255e+02 8.01665207e+01 8.68791293e+01]\n",
      "24-th iteration, loss: 0.36328085745999283, 13 gd steps\n",
      "insert gradient: -0.00013954800671630324\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43142995e+01 5.54571803e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88519981e+01 1.06923396e+02 7.68188978e+01\n",
      " 1.23161631e+02 8.01659736e+01 8.68791293e+01]\n",
      "25-th iteration, loss: 0.3632808529719748, 12 gd steps\n",
      "insert gradient: -0.00013945748221981023\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43142917e+01 5.54560694e+01 4.88527727e+01\n",
      " 1.06922975e+02 7.68186646e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23162001e+02 8.01654598e+01 8.68791293e+01]\n",
      "26-th iteration, loss: 0.36328084866666666, 12 gd steps\n",
      "insert gradient: -0.00013314414990381757\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31429825  55.45497783  48.85313725 106.92255856\n",
      "  76.81843966 123.16272898  80.16496395  86.87912928]\n",
      "27-th iteration, loss: 0.36328084491301615, 12 gd steps\n",
      "insert gradient: -0.00013734674927798582\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43143399e+01 5.54539506e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88535081e+01 1.06922164e+02 7.68182189e+01\n",
      " 1.23163069e+02 8.01644890e+01 8.68791293e+01]\n",
      "28-th iteration, loss: 0.36328084101485253, 12 gd steps\n",
      "insert gradient: -0.00013203559606121473\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43144000e+01 5.54529364e+01 4.88542400e+01\n",
      " 1.06921773e+02 7.68180048e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23163408e+02 8.01640272e+01 8.68791293e+01]\n",
      "29-th iteration, loss: 0.3632808372721075, 12 gd steps\n",
      "insert gradient: -0.00012606855344675114\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43144640e+01 5.54519347e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88545780e+01 1.06921385e+02 7.68178001e+01\n",
      " 1.23164076e+02 8.01635822e+01 8.68791293e+01]\n",
      "30-th iteration, loss: 0.3632808337198407, 12 gd steps\n",
      "insert gradient: -0.00012624227376866192\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43145473e+01 5.54509692e+01 4.88552423e+01\n",
      " 1.06921007e+02 7.68175968e+01 0.00000000e+00 2.57571742e-14\n",
      " 1.23164394e+02 8.01631503e+01 8.68791293e+01]\n",
      "31-th iteration, loss: 0.3632808302818741, 12 gd steps\n",
      "insert gradient: -0.00012061063502990328\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31463316  55.45001353  48.8555504  106.92063091\n",
      "  76.81740238 123.1650223   80.16273321  86.87912928]\n",
      "32-th iteration, loss: 0.3632808272397968, 11 gd steps\n",
      "insert gradient: -0.00011985729418248936\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43147398e+01 5.54491035e+01 4.88558624e+01\n",
      " 1.06920271e+02 7.68172117e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23165319e+02 8.01623310e+01 8.68791293e+01]\n",
      "33-th iteration, loss: 0.3632808240808627, 11 gd steps\n",
      "insert gradient: -0.00012155212906336102\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43148639e+01 5.54482044e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88561821e+01 1.06919917e+02 7.68170243e+01\n",
      " 1.23165906e+02 8.01619349e+01 8.68791293e+01]\n",
      "34-th iteration, loss: 0.3632808210356492, 11 gd steps\n",
      "insert gradient: -0.00011557247981400184\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43149966e+01 5.54473281e+01 4.88567999e+01\n",
      " 1.06919568e+02 7.68168396e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.23166187e+02 8.01615497e+01 8.68791293e+01]\n",
      "35-th iteration, loss: 0.3632808180974856, 11 gd steps\n",
      "insert gradient: -0.00011074783500830144\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43151243e+01 5.54464573e+01 4.88570818e+01\n",
      " 1.06919220e+02 7.68166645e+01 0.00000000e+00 2.22044605e-14\n",
      " 1.23166746e+02 8.01611788e+01 8.68791293e+01]\n",
      "36-th iteration, loss: 0.3632808152762058, 11 gd steps\n",
      "insert gradient: -0.00011084780381019345\n",
      "36-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31526753  55.4456119   48.8573689  106.91888092\n",
      "  76.8164887  123.16727742  80.16081317  86.87912928]\n",
      "37-th iteration, loss: 0.3632808127337167, 11 gd steps\n",
      "insert gradient: -0.00011106254103097924\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43154202e+01 5.54447961e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88576545e+01 1.06918553e+02 7.68163175e+01\n",
      " 1.23167531e+02 8.01604596e+01 8.68791293e+01]\n",
      "38-th iteration, loss: 0.3632808100931959, 11 gd steps\n",
      "insert gradient: -0.00010666803409737156\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43155770e+01 5.54439850e+01 4.88582085e+01\n",
      " 1.06918227e+02 7.68161535e+01 0.00000000e+00 4.61852778e-14\n",
      " 1.23167785e+02 8.01601161e+01 8.68791293e+01]\n",
      "39-th iteration, loss: 0.36328080753572206, 11 gd steps\n",
      "insert gradient: -0.00010227269653319789\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43157270e+01 5.54431774e+01 4.88584606e+01\n",
      " 1.06917902e+02 7.68159981e+01 0.00000000e+00 3.28626015e-14\n",
      " 1.23168288e+02 8.01597849e+01 8.68791293e+01]\n",
      "40-th iteration, loss: 0.3632808050714135, 11 gd steps\n",
      "insert gradient: -0.000100572997621823\n",
      "40-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31588861  55.44239084  48.85871709 106.91758465\n",
      "  76.81584188 123.1687688   80.1594579   86.87912928]\n",
      "41-th iteration, loss: 0.3632808028342358, 11 gd steps\n",
      "insert gradient: -0.00010068984572209167\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43160570e+01 5.54416287e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88589726e+01 1.06917276e+02 7.68156893e+01\n",
      " 1.23168999e+02 8.01591404e+01 8.68791293e+01]\n",
      "42-th iteration, loss: 0.3632808005196922, 11 gd steps\n",
      "insert gradient: -9.838514779509691e-05\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31622821  55.44087134  48.85946885 106.91697026\n",
      "  76.8155431  123.16922822  80.15883184  86.87912928]\n",
      "43-th iteration, loss: 0.36328079840998445, 10 gd steps\n",
      "insert gradient: -9.750694866231722e-05\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43163897e+01 5.54401226e+01 4.88596921e+01\n",
      " 1.06916669e+02 7.68154077e+01 0.00000000e+00 1.33226763e-14\n",
      " 1.23169456e+02 8.01585385e+01 8.68791293e+01]\n",
      "44-th iteration, loss: 0.36328079622687326, 10 gd steps\n",
      "insert gradient: -9.350715353499429e-05\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43165606e+01 5.54393824e+01 4.88599205e+01\n",
      " 1.06916372e+02 7.68152744e+01 0.00000000e+00 9.76996262e-15\n",
      " 1.23169905e+02 8.01582493e+01 8.68791293e+01]\n",
      "45-th iteration, loss: 0.36328079411576086, 10 gd steps\n",
      "insert gradient: -9.187327418637054e-05\n",
      "45-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31673925  55.43865899  48.86015135 106.91607933\n",
      "  76.81514017 123.17033349  80.15796303  86.87912928]\n",
      "46-th iteration, loss: 0.3632807921850481, 10 gd steps\n",
      "insert gradient: -9.158407037620192e-05\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3169217   55.43795488  48.86038086 106.91579453\n",
      "  76.81500848 123.17053923  80.15768404  86.87912928]\n",
      "47-th iteration, loss: 0.3632807903064654, 10 gd steps\n",
      "insert gradient: -9.107837941036803e-05\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43171067e+01 5.54372607e+01 0.00000000e+00\n",
      " 3.64153152e-14 4.88606086e+01 1.06915516e+02 7.68148831e+01\n",
      " 1.23170743e+02 8.01574138e+01 8.68791293e+01]\n",
      "48-th iteration, loss: 0.3632807883647237, 10 gd steps\n",
      "insert gradient: -8.911686293079565e-05\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31729232  55.43657047  48.86104932 106.91523828\n",
      "  76.81476243 123.17094635  80.15715069  86.87912928]\n",
      "49-th iteration, loss: 0.3632807865815689, 10 gd steps\n",
      "insert gradient: -8.814876160511429e-05\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31746731  55.43588528  48.86124728 106.91496379\n",
      "  76.81465014 123.17114785  80.15689937  86.87912928]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010547\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.4077617    0.         582.73294182]\n",
      "1-th iteration, loss: 0.7509319290812664, 11 gd steps\n",
      "insert gradient: -0.6370669541234621\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 234.51447659   0.         348.21846523]\n",
      "2-th iteration, loss: 0.6035005154945375, 13 gd steps\n",
      "insert gradient: -0.6775126096940864\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.02206677  77.25687273 219.71899216  41.94158814 241.62097588\n",
      "   0.         106.59748936]\n",
      "3-th iteration, loss: 0.4681790661645014, 20 gd steps\n",
      "insert gradient: -0.44409795988461637\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.65848109 116.21597545   0.         109.75953237\n",
      "  54.25635415 164.46527626  50.93482395 106.59748936]\n",
      "4-th iteration, loss: 0.37942240653188947, 31 gd steps\n",
      "insert gradient: -0.24890456913373143\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  2.13705981  50.44139368  65.25563306  29.79026524 106.15113396\n",
      "  75.05760259 122.6094457   75.0391451  106.59748936]\n",
      "5-th iteration, loss: 0.36328092285526026, 59 gd steps\n",
      "insert gradient: -0.0003611358955531168\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.38212133  55.25753827  48.90214603 106.91508801\n",
      "  76.78386622 123.13573449  80.15092318 106.59748936]\n",
      "6-th iteration, loss: 0.363280909689953, 18 gd steps\n",
      "insert gradient: -0.00038154703327109165\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43814448e+01 0.00000000e+00 1.28785871e-14\n",
      " 5.52589664e+01 4.89005062e+01 1.06913783e+02 7.67852167e+01\n",
      " 1.23136544e+02 8.01506446e+01 1.06597489e+02]\n",
      "7-th iteration, loss: 0.36328089429655797, 21 gd steps\n",
      "insert gradient: -0.0003787511737662588\n",
      "7-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.38088835  55.26223083  48.89906118 106.91246978\n",
      "  76.78652871 123.13735957  80.15030699 106.59748936]\n",
      "8-th iteration, loss: 0.36328088628398597, 15 gd steps\n",
      "insert gradient: -0.00039661279396421476\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43805375e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.52634851e+01 4.88981380e+01 1.06911543e+02 7.67874554e+01\n",
      " 1.23137954e+02 8.01500523e+01 1.06597489e+02]\n",
      "9-th iteration, loss: 0.3632808746659955, 18 gd steps\n",
      "insert gradient: -0.00038013491610884565\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43801661e+01 0.00000000e+00 2.39808173e-14\n",
      " 5.52664028e+01 4.88971882e+01 1.06910512e+02 7.67884622e+01\n",
      " 1.23138621e+02 8.01497546e+01 1.06597489e+02]\n",
      "10-th iteration, loss: 0.36328086453067665, 17 gd steps\n",
      "insert gradient: -0.0003660814884395469\n",
      "10-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37978939  55.26904526  48.89635136 106.90958784\n",
      "  76.78938332 123.13924898  80.14949982 106.59748936]\n",
      "11-th iteration, loss: 0.363280858956111, 13 gd steps\n",
      "insert gradient: -0.00037585113975702097\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43795026e+01 0.00000000e+00 1.11022302e-14\n",
      " 5.52701095e+01 4.88957467e+01 1.06908875e+02 7.67900979e+01\n",
      " 1.23139749e+02 8.01493016e+01 1.06597489e+02]\n",
      "12-th iteration, loss: 0.3632808505199352, 16 gd steps\n",
      "insert gradient: -0.00035842888732787203\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43791805e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.52725273e+01 4.88951092e+01 1.06908082e+02 7.67908738e+01\n",
      " 1.23140308e+02 8.01490757e+01 1.06597489e+02]\n",
      "13-th iteration, loss: 0.3632808429302424, 15 gd steps\n",
      "insert gradient: -0.0003441943999034031\n",
      "13-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43788384e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.52747528e+01 4.88945159e+01 1.06907349e+02 7.67916045e+01\n",
      " 1.23140848e+02 8.01488798e+01 1.06597489e+02]\n",
      "14-th iteration, loss: 0.3632808360316779, 14 gd steps\n",
      "insert gradient: -0.00033210001403580913\n",
      "14-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37848524  55.27682536  48.89396465 106.90666525\n",
      "  76.79229343 123.14136777  80.14870648 106.59748936]\n",
      "15-th iteration, loss: 0.36328083200846994, 12 gd steps\n",
      "insert gradient: -0.00033984396838173547\n",
      "15-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37819364  55.27771063  48.89353514 106.90610064\n",
      "  76.79286342 123.14180792  80.14856238 106.59748936]\n",
      "16-th iteration, loss: 0.36328082827404673, 12 gd steps\n",
      "insert gradient: -0.0003443096435642445\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43779396e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52785971e+01 4.88931686e+01 1.06905563e+02 7.67933879e+01\n",
      " 1.23142225e+02 8.01484137e+01 1.06597489e+02]\n",
      "17-th iteration, loss: 0.36328082249542887, 13 gd steps\n",
      "insert gradient: -0.00032747517560017994\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43776502e+01 0.00000000e+00 9.32587341e-15\n",
      " 5.52805437e+01 4.88927787e+01 1.06904975e+02 7.67939479e+01\n",
      " 1.23142683e+02 8.01482507e+01 1.06597489e+02]\n",
      "18-th iteration, loss: 0.36328081717290844, 13 gd steps\n",
      "insert gradient: -0.00031386097879327425\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37733617  55.28235929  48.89239496 106.90441827\n",
      "  76.7944888  123.14313249  80.14810836 106.59748936]\n",
      "19-th iteration, loss: 0.36328081403880863, 11 gd steps\n",
      "insert gradient: -0.0003189867354448534\n",
      "19-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37706144  55.28315064  48.8920803  106.90394315\n",
      "  76.79495339 123.14352579  80.147989   106.59748936]\n",
      "20-th iteration, loss: 0.36328081108375054, 11 gd steps\n",
      "insert gradient: -0.0003218245690864562\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43768134e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52839433e+01 4.88918081e+01 1.06903486e+02 7.67953855e+01\n",
      " 1.23143903e+02 8.01478660e+01 1.06597489e+02]\n",
      "21-th iteration, loss: 0.36328080648965116, 12 gd steps\n",
      "insert gradient: -0.00030611156990458037\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43765337e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.52856522e+01 4.88915185e+01 1.06902994e+02 7.67958411e+01\n",
      " 1.23144310e+02 8.01477339e+01 1.06597489e+02]\n",
      "22-th iteration, loss: 0.36328080221674086, 12 gd steps\n",
      "insert gradient: -0.0002933623397028238\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3762308   55.28725594  48.89122519 106.90252244\n",
      "  76.79628639 123.14471368  80.14761845 106.59748936]\n",
      "23-th iteration, loss: 0.36328079966008814, 11 gd steps\n",
      "insert gradient: -0.0002975626444224255\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43759582e+01 0.00000000e+00 1.37667655e-14\n",
      " 5.52879698e+01 4.88909767e+01 1.06902110e+02 7.67966792e+01\n",
      " 1.23145075e+02 8.01475198e+01 1.06597489e+02]\n",
      "24-th iteration, loss: 0.3632807957837121, 12 gd steps\n",
      "insert gradient: -0.0002848478982583319\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37566401  55.2894941   48.89072124 106.90167304\n",
      "  76.79708472 123.14545887  80.14741323 106.59748936]\n",
      "25-th iteration, loss: 0.3632807934578036, 11 gd steps\n",
      "insert gradient: -0.0002884826077077707\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37539576  55.29017712  48.89050133 106.90128717\n",
      "  76.79744648 123.14580603  80.14732187 106.59748936]\n",
      "26-th iteration, loss: 0.3632807912426675, 10 gd steps\n",
      "insert gradient: -0.0002904581434127024\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37514803  55.29086146  48.89031047 106.90091409\n",
      "  76.79778499 123.14614084  80.14722736 106.59748936]\n",
      "27-th iteration, loss: 0.3632807891184137, 10 gd steps\n",
      "insert gradient: -0.00029119011602484905\n",
      "27-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37491535  55.29154507  48.89014225 106.90055206\n",
      "  76.79810407 123.14646556  80.14713119 106.59748936]\n",
      "28-th iteration, loss: 0.36328078707336914, 10 gd steps\n",
      "insert gradient: -0.00029098616293491954\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43746939e+01 0.00000000e+00 1.55431223e-14\n",
      " 5.52922259e+01 4.88899920e+01 1.06900200e+02 7.67984064e+01\n",
      " 1.23146782e+02 8.01470345e+01 1.06597489e+02]\n",
      "29-th iteration, loss: 0.3632807838264558, 11 gd steps\n",
      "insert gradient: -0.0002756922099598996\n",
      "29-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37444316  55.29365526  48.88982589 106.89982629\n",
      "  76.79872252 123.14711924  80.14693478 106.59748936]\n",
      "30-th iteration, loss: 0.36328078192711766, 10 gd steps\n",
      "insert gradient: -0.0002767943287334709\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43742039e+01 0.00000000e+00 3.10862447e-15\n",
      " 5.52942958e+01 4.88896733e+01 1.06899492e+02 7.67990126e+01\n",
      " 1.23147430e+02 8.01468514e+01 1.06597489e+02]\n",
      "31-th iteration, loss: 0.3632807789647823, 11 gd steps\n",
      "insert gradient: -0.0002634787127179372\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37394172  55.29563683  48.88951046 106.89914065\n",
      "  76.79931201 123.14775924  80.14676457 106.59748936]\n",
      "32-th iteration, loss: 0.36328077719859614, 10 gd steps\n",
      "insert gradient: -0.0002652637292893304\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37369335  55.29624504  48.88936213 106.89882387\n",
      "  76.7995879  123.14806404  80.1466907  106.59748936]\n",
      "33-th iteration, loss: 0.3632807754978782, 10 gd steps\n",
      "insert gradient: -0.0002659853890456593\n",
      "33-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37345871  55.29685352  48.88923199 106.89851586\n",
      "  76.79984848 123.14836048  80.14661488 106.59748936]\n",
      "34-th iteration, loss: 0.3632807738545636, 10 gd steps\n",
      "insert gradient: -0.00026589665916621147\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43732346e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.52974605e+01 4.88891163e+01 1.06898216e+02 7.68000958e+01\n",
      " 1.23148650e+02 8.01465381e+01 1.06597489e+02]\n",
      "35-th iteration, loss: 0.363280771261845, 11 gd steps\n",
      "insert gradient: -0.0002524726552690741\n",
      "35-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37298755  55.29871946  48.88898914 106.89790155\n",
      "  76.80035114 123.14895439  80.14645971 106.59748936]\n",
      "36-th iteration, loss: 0.363280769723204, 10 gd steps\n",
      "insert gradient: -0.00025350690493325395\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43727497e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.52992934e+01 4.88888692e+01 1.06897616e+02 7.68005903e+01\n",
      " 1.23149240e+02 8.01463933e+01 1.06597489e+02]\n",
      "37-th iteration, loss: 0.3632807673360172, 11 gd steps\n",
      "insert gradient: -0.00024172312946656768\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43724941e+01 0.00000000e+00 3.19744231e-14\n",
      " 5.53004822e+01 4.88887423e+01 1.06897318e+02 7.68008342e+01\n",
      " 1.23149538e+02 8.01463248e+01 1.06597489e+02]\n",
      "38-th iteration, loss: 0.36328076507553203, 11 gd steps\n",
      "insert gradient: -0.000232014774693723\n",
      "38-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37222218  55.30161166  48.88860396 106.89702966\n",
      "  76.80107898 123.14983624  80.14626573 106.59748936]\n",
      "39-th iteration, loss: 0.36328076367824264, 10 gd steps\n",
      "insert gradient: -0.0002346788512861433\n",
      "39-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37196519  55.30213694  48.88847836 106.89676446\n",
      "  76.80130768 123.15011697  80.14621361 106.59748936]\n",
      "40-th iteration, loss: 0.3632807623336028, 10 gd steps\n",
      "insert gradient: -0.00023620760557765188\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43717236e+01 0.00000000e+00 3.99680289e-15\n",
      " 5.53026650e+01 4.88883704e+01 1.06896507e+02 7.68015227e+01\n",
      " 1.23150390e+02 8.01461583e+01 1.06597489e+02]\n",
      "41-th iteration, loss: 0.3632807602750606, 10 gd steps\n",
      "insert gradient: -0.00022587857943892368\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37146917  55.30375253  48.88825857 106.89624089\n",
      "  76.8017395  123.15067169  80.14610105 106.59748936]\n",
      "42-th iteration, loss: 0.36328075901266044, 10 gd steps\n",
      "insert gradient: -0.00022773975013899757\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43712251e+01 0.00000000e+00 2.84217094e-14\n",
      " 5.53042589e+01 4.88881539e+01 1.06895995e+02 7.68019446e+01\n",
      " 1.23150939e+02 8.01460511e+01 1.06597489e+02]\n",
      "43-th iteration, loss: 0.3632807570951755, 10 gd steps\n",
      "insert gradient: -0.00021816394868032977\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43709705e+01 0.00000000e+00 2.53130850e-14\n",
      " 5.53052999e+01 4.88880470e+01 1.06895743e+02 7.68021501e+01\n",
      " 1.23151214e+02 8.01459990e+01 1.06597489e+02]\n",
      "44-th iteration, loss: 0.36328075526727543, 10 gd steps\n",
      "insert gradient: -0.0002101545428290628\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37070426  55.30629531  48.8879314  106.89549808\n",
      "  76.80235626 123.15148891  80.14595377 106.59748936]\n",
      "45-th iteration, loss: 0.3632807541148529, 10 gd steps\n",
      "insert gradient: -0.00021296461277742416\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43704512e+01 0.00000000e+00 1.73194792e-14\n",
      " 5.53067649e+01 4.88878258e+01 1.06895270e+02 7.68025509e+01\n",
      " 1.23151750e+02 8.01459131e+01 1.06597489e+02]\n",
      "46-th iteration, loss: 0.36328075240167473, 10 gd steps\n",
      "insert gradient: -0.00020494588090819865\n",
      "46-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.37019245  55.30772958  48.88772163 106.89503757\n",
      "  76.8027435  123.15201698  80.14586962 106.59748936]\n",
      "47-th iteration, loss: 0.3632807513221036, 10 gd steps\n",
      "insert gradient: -0.00020749039323518658\n",
      "47-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36994528  55.30818537  48.8876255  106.89482087\n",
      "  76.80292627 123.15227138  80.14583058 106.59748936]\n",
      "48-th iteration, loss: 0.36328075028217216, 9 gd steps\n",
      "insert gradient: -0.00020903221353455102\n",
      "48-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36971249  55.30864398  48.88754409 106.89461018\n",
      "  76.80309768 123.15251872  80.14578838 106.59748936]\n",
      "49-th iteration, loss: 0.36328074927504095, 9 gd steps\n",
      "insert gradient: -0.00020979681612795566\n",
      "49-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.36949094  55.30910407  48.88747415 106.89440476\n",
      "  76.80325952 123.15276018  80.14574394 106.59748936]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536721336560601\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         608.66581906]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6330534235696021\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 237.52812451   0.         371.13769455]\n",
      "2-th iteration, loss: 0.6052498022827035, 13 gd steps\n",
      "insert gradient: -0.6944425690737154\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1304576   77.05949885 222.22394736  40.75088166 242.37563725\n",
      "   0.         128.76205729]\n",
      "3-th iteration, loss: 0.46911332129562944, 22 gd steps\n",
      "insert gradient: -0.3357625049874109\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.55369955 119.14539481   0.         112.52620621\n",
      "  53.57306873 164.48102793  50.29191915 128.76205729]\n",
      "4-th iteration, loss: 0.3632811676756671, 79 gd steps\n",
      "insert gradient: -0.001045869753638507\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.31821539  55.44412022  48.8250639  106.98668368\n",
      "  76.7936252  123.251158    80.14677908 128.76205729]\n",
      "5-th iteration, loss: 0.36328095895853124, 32 gd steps\n",
      "insert gradient: -0.000260819867540851\n",
      "5-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32235387  55.44523473  48.83623859 106.97327039\n",
      "  76.78348298 123.23764499  80.13662481 128.76205729]\n",
      "6-th iteration, loss: 0.3632809463812268, 18 gd steps\n",
      "insert gradient: -0.00024329956744818855\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43221806e+01 5.54444674e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88372524e+01 1.06971214e+02 7.67831938e+01\n",
      " 1.23236105e+02 8.01358787e+01 1.28762057e+02]\n",
      "7-th iteration, loss: 0.3632809347537218, 18 gd steps\n",
      "insert gradient: -0.0001904324579898063\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43219562e+01 5.54436357e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88389517e+01 1.06969319e+02 7.67830998e+01\n",
      " 1.23234759e+02 8.01353135e+01 1.28762057e+02]\n",
      "8-th iteration, loss: 0.36328092532183903, 16 gd steps\n",
      "insert gradient: -0.00016221863891061636\n",
      "8-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32170219  55.44280468  48.84022957 106.9676378\n",
      "  76.78314232 123.23362637  80.13490878 128.76205729]\n",
      "9-th iteration, loss: 0.3632809178031234, 15 gd steps\n",
      "insert gradient: -0.00017247843400800033\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43214794e+01 5.54420213e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88407809e+01 1.06966142e+02 7.67832614e+01\n",
      " 1.23232649e+02 8.01346064e+01 1.28762057e+02]\n",
      "10-th iteration, loss: 0.3632809102873282, 15 gd steps\n",
      "insert gradient: -0.0001515829327609019\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43212827e+01 5.54412351e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88418640e+01 1.06964700e+02 7.67834341e+01\n",
      " 1.23231723e+02 8.01343570e+01 1.28762057e+02]\n",
      "11-th iteration, loss: 0.36328090346148617, 14 gd steps\n",
      "insert gradient: -0.00013803204654763254\n",
      "11-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32109372  55.44045757  48.84280636 106.96333682\n",
      "  76.78364873 123.23087356  80.13416441 128.76205729]\n",
      "12-th iteration, loss: 0.36328089761553506, 14 gd steps\n",
      "insert gradient: -0.00015061696157679438\n",
      "12-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32093682  55.43971763  48.84324619 106.9620712\n",
      "  76.78388701 123.23009644  80.13401516 128.76205729]\n",
      "13-th iteration, loss: 0.36328089217927384, 13 gd steps\n",
      "insert gradient: -0.00015916718994664732\n",
      "13-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32082623  55.43901183  48.84370647 106.96087238\n",
      "  76.78414059 123.22936166  80.13389248 128.76205729]\n",
      "14-th iteration, loss: 0.36328088707002976, 13 gd steps\n",
      "insert gradient: -0.000164830667210141\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43207512e+01 5.54383321e+01 0.00000000e+00\n",
      " 4.26325641e-14 4.88441786e+01 1.06959728e+02 7.67844068e+01\n",
      " 1.23228663e+02 8.01337929e+01 1.28762057e+02]\n",
      "15-th iteration, loss: 0.3632808816967494, 13 gd steps\n",
      "insert gradient: -0.00014420688143190212\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43206885e+01 5.54376444e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.88451084e+01 1.06958595e+02 7.67846909e+01\n",
      " 1.23227977e+02 8.01337138e+01 1.28762057e+02]\n",
      "16-th iteration, loss: 0.3632808766598895, 13 gd steps\n",
      "insert gradient: -0.00013008256049442792\n",
      "16-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32062097  55.43695808  48.84592456 106.95749665\n",
      "  76.78498642 123.22732782  80.13365987 128.76205729]\n",
      "17-th iteration, loss: 0.36328087224522615, 12 gd steps\n",
      "insert gradient: -0.00013930792944375072\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205681e+01 5.54362973e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88463049e+01 1.06956454e+02 7.67852832e+01\n",
      " 1.23226717e+02 8.01336242e+01 1.28762057e+02]\n",
      "18-th iteration, loss: 0.36328086766942763, 12 gd steps\n",
      "insert gradient: -0.0001254473331996197\n",
      "18-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32053522  55.43564073  48.84707422 106.95542584\n",
      "  76.78558563 123.22611656  80.1335997  128.76205729]\n",
      "19-th iteration, loss: 0.3632808636283633, 12 gd steps\n",
      "insert gradient: -0.00013378495738768513\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205128e+01 5.54350056e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88474331e+01 1.06954444e+02 7.67858866e+01\n",
      " 1.23225548e+02 8.01335889e+01 1.28762057e+02]\n",
      "20-th iteration, loss: 0.3632808594323253, 12 gd steps\n",
      "insert gradient: -0.00012043183187371358\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43205078e+01 5.54343746e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88481580e+01 1.06953473e+02 7.67861905e+01\n",
      " 1.23224986e+02 8.01335865e+01 1.28762057e+02]\n",
      "21-th iteration, loss: 0.3632808554285574, 12 gd steps\n",
      "insert gradient: -0.0001179083630951493\n",
      "21-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3205022   55.43374743  48.84881422 106.95252447\n",
      "  76.78649539 123.22444384  80.13359584 128.76205729]\n",
      "22-th iteration, loss: 0.3632808518284468, 12 gd steps\n",
      "insert gradient: -0.00011960280595445942\n",
      "22-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32050747  55.43313948  48.84912687 106.95161219\n",
      "  76.78679585 123.22392458  80.13361317 128.76205729]\n",
      "23-th iteration, loss: 0.36328084837676683, 12 gd steps\n",
      "insert gradient: -0.00012579327539290758\n",
      "23-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32053774  55.43255279  48.8494559  106.95072847\n",
      "  76.78709126 123.22341768  80.13363382 128.76205729]\n",
      "24-th iteration, loss: 0.3632808450568925, 11 gd steps\n",
      "insert gradient: -0.00013001479570521387\n",
      "24-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32058733  55.43198381  48.84979527 106.94987089\n",
      "  76.78738231 123.22292302  80.13365816 128.76205729]\n",
      "25-th iteration, loss: 0.3632808418521493, 11 gd steps\n",
      "insert gradient: -0.00013276291086184255\n",
      "25-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32065206  55.43142903  48.85014104 106.94903621\n",
      "  76.78766983 123.22243963  80.13368638 128.76205729]\n",
      "26-th iteration, loss: 0.3632808387526886, 11 gd steps\n",
      "insert gradient: -0.00013439970906708253\n",
      "26-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32072873  55.43088628  48.85049007 106.94822252\n",
      "  76.7879541  123.22196701  80.13371843 128.76205729]\n",
      "27-th iteration, loss: 0.3632808357508914, 11 gd steps\n",
      "insert gradient: -0.00013519664143120624\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43208149e+01 5.54303539e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.88508400e+01 1.06947428e+02 7.67882352e+01\n",
      " 1.23221505e+02 8.01337542e+01 1.28762057e+02]\n",
      "28-th iteration, loss: 0.3632808325532538, 11 gd steps\n",
      "insert gradient: -0.00011735774199630009\n",
      "28-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32090163  55.42981544  48.85151851 106.94663486\n",
      "  76.78851774 123.22104483  80.13379531 128.76205729]\n",
      "29-th iteration, loss: 0.3632808297247703, 11 gd steps\n",
      "insert gradient: -0.00012055919874978542\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43209806e+01 5.54292854e+01 0.00000000e+00\n",
      " 1.33226763e-14 4.88518259e+01 1.06945866e+02 7.67887959e+01\n",
      " 1.23220604e+02 8.01338423e+01 1.28762057e+02]\n",
      "30-th iteration, loss: 0.3632808267525687, 11 gd steps\n",
      "insert gradient: -0.00011341507986484873\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43210647e+01 5.54287548e+01 4.88524317e+01\n",
      " 1.06945102e+02 0.00000000e+00 1.33226763e-14 7.67890727e+01\n",
      " 1.23220166e+02 8.01338923e+01 1.28762057e+02]\n",
      "31-th iteration, loss: 0.3632808239041918, 11 gd steps\n",
      "insert gradient: -0.00011180327091534093\n",
      "31-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32114847  55.42823137  48.85271608 106.94435182\n",
      "  76.78960752 123.21973613  80.13394467 128.76205729]\n",
      "32-th iteration, loss: 0.3632808213212364, 11 gd steps\n",
      "insert gradient: -0.00011422258538294065\n",
      "32-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32124541  55.42772864  48.85300453 106.94362076\n",
      "  76.78985112 123.2193125   80.13399339 128.76205729]\n",
      "33-th iteration, loss: 0.3632808188137057, 11 gd steps\n",
      "insert gradient: -0.00011576323700582436\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43213504e+01 5.54272351e+01 0.00000000e+00\n",
      " 3.55271368e-14 4.88532964e+01 1.06942907e+02 7.67900928e+01\n",
      " 1.23218897e+02 8.01340441e+01 1.28762057e+02]\n",
      "34-th iteration, loss: 0.36328081617400776, 11 gd steps\n",
      "insert gradient: -0.0001019553006720576\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43214564e+01 5.54267388e+01 4.88538658e+01\n",
      " 1.06942196e+02 0.00000000e+00 4.52970994e-14 7.67903352e+01\n",
      " 1.23218485e+02 8.01340982e+01 1.28762057e+02]\n",
      "35-th iteration, loss: 0.36328081365691484, 11 gd steps\n",
      "insert gradient: -0.00010556562686781038\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43215584e+01 5.54262470e+01 0.00000000e+00\n",
      " 2.04281037e-14 4.88541310e+01 1.06941499e+02 7.67908077e+01\n",
      " 1.23218081e+02 8.01341549e+01 1.28762057e+02]\n",
      "36-th iteration, loss: 0.3632808111814494, 11 gd steps\n",
      "insert gradient: -9.349786469307454e-05\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43216654e+01 5.54257625e+01 4.88546508e+01\n",
      " 1.06940807e+02 0.00000000e+00 4.52970994e-14 7.67910270e+01\n",
      " 1.23217679e+02 8.01342100e+01 1.28762057e+02]\n",
      "37-th iteration, loss: 0.3632808088140244, 11 gd steps\n",
      "insert gradient: -9.796895350658334e-05\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43217696e+01 5.54252825e+01 0.00000000e+00\n",
      " 3.55271368e-14 4.88548952e+01 1.06940129e+02 7.67914576e+01\n",
      " 1.23217286e+02 8.01342676e+01 1.28762057e+02]\n",
      "38-th iteration, loss: 0.3632808064830229, 11 gd steps\n",
      "insert gradient: -8.729224960743663e-05\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43218790e+01 5.54248096e+01 0.00000000e+00\n",
      " 2.22044605e-14 4.88553775e+01 1.06939457e+02 7.67916584e+01\n",
      " 1.23216894e+02 8.01343237e+01 1.28762057e+02]\n",
      "39-th iteration, loss: 0.36328080423041953, 11 gd steps\n",
      "insert gradient: -8.767848673032619e-05\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43219800e+01 5.54243346e+01 4.88558135e+01\n",
      " 1.06938795e+02 0.00000000e+00 4.79616347e-14 7.67918618e+01\n",
      " 1.23216512e+02 8.01343845e+01 1.28762057e+02]\n",
      "40-th iteration, loss: 0.36328080204913255, 10 gd steps\n",
      "insert gradient: -8.536298828140609e-05\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43220822e+01 5.54238658e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88560238e+01 1.06938146e+02 7.67922609e+01\n",
      " 1.23216138e+02 8.01344465e+01 1.28762057e+02]\n",
      "41-th iteration, loss: 0.3632807999178219, 10 gd steps\n",
      "insert gradient: -8.128298075343249e-05\n",
      "41-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32219176  55.42340604  48.8564465  106.93750535\n",
      "  76.79244686 123.21576422  80.13450621 128.76205729]\n",
      "42-th iteration, loss: 0.3632807979459629, 10 gd steps\n",
      "insert gradient: -8.197389465354624e-05\n",
      "42-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.3222984   55.42295064  48.85664643 106.93687983\n",
      "  76.7926343  123.21540137  80.1345687  128.76205729]\n",
      "43-th iteration, loss: 0.3632807960302251, 10 gd steps\n",
      "insert gradient: -8.539779231516946e-05\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43224137e+01 5.54225045e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88568549e+01 1.06936269e+02 7.67928211e+01\n",
      " 1.23215045e+02 8.01346312e+01 1.28762057e+02]\n",
      "44-th iteration, loss: 0.36328079405751973, 10 gd steps\n",
      "insert gradient: -8.237339918080535e-05\n",
      "44-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          44.32253242  55.42206051  48.85727138 106.93566586\n",
      "  76.79300828 123.21469119  80.134695   128.76205729]\n",
      "45-th iteration, loss: 0.3632807922345006, 10 gd steps\n",
      "insert gradient: -8.196244517301648e-05\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43226467e+01 5.54216199e+01 4.88574668e+01\n",
      " 1.06935076e+02 0.00000000e+00 5.24025268e-14 7.67931960e+01\n",
      " 1.23214347e+02 8.01347609e+01 1.28762057e+02]\n",
      "46-th iteration, loss: 0.36328079037413125, 10 gd steps\n",
      "insert gradient: -8.404286042156858e-05\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43227707e+01 5.54211877e+01 0.00000000e+00\n",
      " 2.93098879e-14 4.88576723e+01 1.06934496e+02 7.67935616e+01\n",
      " 1.23214005e+02 8.01348253e+01 1.28762057e+02]\n",
      "47-th iteration, loss: 0.3632807885433814, 10 gd steps\n",
      "insert gradient: -7.621429659945855e-05\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43228975e+01 5.54207613e+01 4.88580775e+01\n",
      " 1.06933921e+02 0.00000000e+00 1.33226763e-14 7.67937324e+01\n",
      " 1.23213664e+02 8.01348878e+01 1.28762057e+02]\n",
      "48-th iteration, loss: 0.3632807867781529, 10 gd steps\n",
      "insert gradient: -7.866242112844741e-05\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43230207e+01 5.54203371e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.88582691e+01 1.06933355e+02 7.67940710e+01\n",
      " 1.23213329e+02 8.01349517e+01 1.28762057e+02]\n",
      "49-th iteration, loss: 0.3632807850402543, 10 gd steps\n",
      "insert gradient: -7.145492730691954e-05\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[0.00000000e+00 4.43231470e+01 5.54199188e+01 4.88586494e+01\n",
      " 1.06932796e+02 0.00000000e+00 4.35207426e-14 7.67942298e+01\n",
      " 1.23212996e+02 8.01350138e+01 1.28762057e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536156841022463\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         634.80050857]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6281630689044189\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 232.2440885    0.         402.55642007]\n",
      "2-th iteration, loss: 0.6039950911279921, 13 gd steps\n",
      "insert gradient: -0.643099815252181\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.64739753  77.83171265 218.09742056  42.13716135 238.24767718\n",
      "   0.         164.30874288]\n",
      "3-th iteration, loss: 0.4300495220948163, 56 gd steps\n",
      "insert gradient: -0.3721084040102081\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.68777522  47.67161656 101.99671632   0.         107.99652316\n",
      "  57.94522617 129.13425111  58.84309205 164.30874288]\n",
      "4-th iteration, loss: 0.36336746337906706, 45 gd steps\n",
      "insert gradient: -0.35195103147027973\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          45.26953421  53.43851059  49.45655522 106.45919614\n",
      "  76.3168162  122.68222116  80.4464554  164.30874288   0.        ]\n",
      "5-th iteration, loss: 0.24140514220562592, 114 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.643787690430947e-07\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265141  47.86022265  75.4396542   41.95009194  94.54811515\n",
      "  79.28610187 118.50566582  82.28923353 128.25309466  82.83300291]\n",
      "6-th iteration, loss: 0.24140514220559056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.481194078507138e-07\n",
      "6-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265156  47.86022312  75.43965398  41.95009075  94.54811458\n",
      "  79.28610112 118.50566587  82.28923432 128.25309562  82.83300327]\n",
      "7-th iteration, loss: 0.24140514220555756, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.323334960957369e-07\n",
      "7-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265170e+00 4.78602236e+01 7.54396538e+01 4.19500896e+01\n",
      " 9.45481140e+01 7.92861004e+01 1.18505666e+02 8.22892351e+01\n",
      " 0.00000000e+00 1.50990331e-14 1.28253097e+02 8.28330036e+01]\n",
      "8-th iteration, loss: 0.24140514220551568, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.029501820142558e-07\n",
      "8-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265185e+00 4.78602241e+01 7.54396536e+01 4.19500886e+01\n",
      " 9.45481135e+01 7.92860996e+01 1.18505666e+02 8.22892358e+01\n",
      " 9.24954289e-07 7.23337676e-07 0.00000000e+00 1.19114008e-22\n",
      " 1.28253097e+02 8.28330039e+01]\n",
      "9-th iteration, loss: 0.24140514220546794, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.625210311605204e-07\n",
      "9-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265201e+00 4.78602247e+01 7.54396535e+01 4.19500876e+01\n",
      " 9.45481130e+01 7.92860989e+01 1.18505666e+02 8.22892365e+01\n",
      " 1.81026718e-06 1.37372542e-06 8.92114064e-07 6.50387748e-07\n",
      " 1.28253098e+02 8.28330042e+01]\n",
      "10-th iteration, loss: 0.24140514220542575, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.260563636148081e-07\n",
      "10-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265216e+00 4.78602252e+01 7.54396534e+01 4.19500866e+01\n",
      " 9.45481125e+01 7.92860982e+01 1.18505666e+02 8.22892371e+01\n",
      " 2.64410938e-06 1.94061849e-06 1.73887270e-06 1.20889272e-06\n",
      " 0.00000000e+00 1.85288457e-22 1.28253099e+02 8.28330045e+01]\n",
      "11-th iteration, loss: 0.24140514220538137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.824879740558872e-07\n",
      "11-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265232e+00 4.78602257e+01 7.54396533e+01 4.19500858e+01\n",
      " 9.45481120e+01 7.92860974e+01 1.18505666e+02 8.22892376e+01\n",
      " 3.42873470e-06 2.42703806e-06 2.54174467e-06 1.67896256e-06\n",
      " 8.14238570e-07 4.70069845e-07 1.28253100e+02 8.28330047e+01]\n",
      "12-th iteration, loss: 0.24140514220534273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.443636748688129e-07\n",
      "12-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265247e+00 4.78602263e+01 7.54396532e+01 4.19500849e+01\n",
      " 9.45481116e+01 7.92860966e+01 1.18505666e+02 8.22892380e+01\n",
      " 4.15805352e-06 2.82958228e-06 3.29388369e-06 2.05760805e-06\n",
      " 1.58216402e-06 8.41059465e-07 0.00000000e+00 2.77932686e-22\n",
      " 1.28253101e+02 8.28330049e+01]\n",
      "13-th iteration, loss: 0.24140514220530387, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.027232868463308e-07\n",
      "13-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265263e+00 4.78602268e+01 7.54396532e+01 4.19500842e+01\n",
      " 9.45481111e+01 7.92860959e+01 1.18505666e+02 8.22892384e+01\n",
      " 4.83741513e-06 3.15598257e-06 3.99985041e-06 2.35303763e-06\n",
      " 2.30747739e-06 1.12161276e-06 7.33221421e-07 2.80553294e-07\n",
      " 1.28253102e+02 8.28330050e+01]\n",
      "14-th iteration, loss: 0.24140514220526985, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.672233987280434e-07\n",
      "14-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265279e+00 4.78602274e+01 7.54396532e+01 4.19500834e+01\n",
      " 9.45481107e+01 7.92860951e+01 1.18505666e+02 8.22892387e+01\n",
      " 5.46592185e-06 3.40886214e-06 4.65803124e-06 2.56830865e-06\n",
      " 2.98778262e-06 1.31518774e-06 1.42407261e-06 4.67234168e-07\n",
      " 0.00000000e+00 1.05879118e-22 1.28253102e+02 8.28330051e+01]\n",
      "15-th iteration, loss: 0.24140514220523587, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.308491934666855e-07\n",
      "15-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265294e+00 4.78602279e+01 7.54396531e+01 4.19500827e+01\n",
      " 9.45481103e+01 7.92860943e+01 1.18505666e+02 8.22892389e+01\n",
      " 6.05081709e-06 3.59876227e-06 5.27497827e-06 2.71441167e-06\n",
      " 3.62887816e-06 1.43319816e-06 2.07753419e-06 5.71854760e-07\n",
      " 6.57854743e-07 1.04620592e-07 1.28253103e+02 8.28330052e+01]\n",
      "16-th iteration, loss: 0.24140514220520531, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.00415189967178e-07\n",
      "16-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265310e+00 4.78602284e+01 7.54396531e+01 4.19500821e+01\n",
      " 9.45481099e+01 7.92860936e+01 1.18505665e+02 8.22892391e+01\n",
      " 6.59495370e-06 3.73221522e-06 5.85295222e-06 2.79826669e-06\n",
      " 4.23237434e-06 1.48293268e-06 2.69450601e-06 6.02055275e-07\n",
      " 1.28020342e-06 1.28635633e-07 1.28253104e+02 8.28330052e+01]\n",
      "17-th iteration, loss: 0.24140514220517711, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.747152263028717e-07\n",
      "17-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265325e+00 4.78602289e+01 7.54396532e+01 4.19500815e+01\n",
      " 9.45481094e+01 7.92860928e+01 1.18505665e+02 8.22892392e+01\n",
      " 7.10710387e-06 3.82189502e-06 6.40019450e-06 2.83291415e-06\n",
      " 4.80592731e-06 1.47778527e-06 3.28200224e-06 5.71572795e-07\n",
      " 1.87336047e-06 8.61160489e-08 1.28253104e+02 8.28330052e+01]\n",
      "18-th iteration, loss: 0.24140514220515072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.52675904445892e-07\n",
      "18-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265340e+00 4.78602294e+01 7.54396532e+01 4.19500809e+01\n",
      " 9.45481091e+01 7.92860920e+01 1.18505665e+02 8.22892394e+01\n",
      " 7.59428306e-06 3.87788224e-06 6.92330904e-06 2.82872358e-06\n",
      " 5.35567830e-06 1.42840701e-06 3.84564809e-06 4.91335528e-07\n",
      " 1.28253107e+02 8.28330051e+01]\n",
      "19-th iteration, loss: 0.2414051422051283, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.366423787456366e-07\n",
      "19-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265355e+00 4.78602299e+01 7.54396532e+01 4.19500803e+01\n",
      " 9.45481087e+01 7.92860913e+01 1.18505665e+02 8.22892395e+01\n",
      " 8.06259418e-06 3.90853250e-06 7.42808191e-06 2.79427748e-06\n",
      " 5.88704823e-06 1.34360418e-06 4.39044861e-06 3.70374010e-07\n",
      " 0.00000000e+00 9.26442286e-23 1.28253108e+02 8.28330051e+01]\n",
      "20-th iteration, loss: 0.24140514220510453, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.182131704005934e-07\n",
      "20-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265369e+00 4.78602304e+01 7.54396532e+01 4.19500798e+01\n",
      " 9.45481083e+01 7.92860905e+01 1.18505665e+02 8.22892396e+01\n",
      " 8.51672169e-06 3.92040529e-06 7.91895937e-06 2.73630778e-06\n",
      " 6.40419886e-06 1.23028154e-06 4.92023247e-06 2.15770212e-07\n",
      " 1.28253109e+02 8.28330050e+01]\n",
      "21-th iteration, loss: 0.241405142205084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.058142918355533e-07\n",
      "21-th iteration, new layer inserted. now 20 layers\n",
      "[1.88265384e+00 4.78602309e+01 7.54396533e+01 4.19500792e+01\n",
      " 9.45481079e+01 7.92860898e+01 1.18505665e+02 8.22892396e+01\n",
      " 8.95657352e-06 3.91350577e-06 8.39567279e-06 2.65495030e-06\n",
      " 6.90664039e-06 1.08870862e-06 5.43424172e-06 2.79348338e-08\n",
      " 0.00000000e+00 2.48154184e-24 1.28253109e+02 8.28330049e+01]\n",
      "22-th iteration, loss: 0.24140514220506246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.888168179085556e-07\n",
      "22-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265398e+00 4.78602314e+01 7.54396533e+01 4.19500787e+01\n",
      " 9.45481076e+01 7.92860891e+01 1.18505664e+02 8.22892397e+01\n",
      " 9.38788208e-06 3.89608718e-06 8.86377805e-06 2.55859145e-06\n",
      " 7.39970881e-06 9.27410132e-07 1.28253116e+02 8.28330048e+01]\n",
      "23-th iteration, loss: 0.24140514220504586, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.807670668211514e-07\n",
      "23-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265412e+00 4.78602319e+01 7.54396533e+01 4.19500782e+01\n",
      " 9.45481072e+01 7.92860884e+01 1.18505664e+02 8.22892398e+01\n",
      " 9.80621654e-06 3.86109337e-06 9.31874545e-06 2.44025603e-06\n",
      " 7.87873335e-06 7.39498991e-07 1.28253117e+02 8.28330047e+01]\n",
      "24-th iteration, loss: 0.24140514220502976, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.7319335917500296e-07\n",
      "24-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265426e+00 4.78602323e+01 7.54396534e+01 4.19500778e+01\n",
      " 9.45481069e+01 7.92860878e+01 1.18505664e+02 8.22892398e+01\n",
      " 1.02198271e-05 3.81987485e-06 9.76865995e-06 2.31141807e-06\n",
      " 8.35159234e-06 5.36581288e-07 1.28253117e+02 8.28330046e+01]\n",
      "25-th iteration, loss: 0.2414051422050142, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.660321088761262e-07\n",
      "25-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265440e+00 4.78602328e+01 7.54396534e+01 4.19500773e+01\n",
      " 9.45481066e+01 7.92860871e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.06294698e-05 3.77346431e-06 1.02142189e-05 2.17315777e-06\n",
      " 8.81888442e-06 3.19795191e-07 0.00000000e+00 1.32348898e-22\n",
      " 1.28253118e+02 8.28330045e+01]\n",
      "26-th iteration, loss: 0.24140514220499731, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.555694264649048e-07\n",
      "26-th iteration, new layer inserted. now 18 layers\n",
      "[1.88265454e+00 4.78602332e+01 7.54396535e+01 4.19500769e+01\n",
      " 9.45481063e+01 7.92860865e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.10355492e-05 3.72302037e-06 1.06557783e-05 2.02667470e-06\n",
      " 9.28087687e-06 9.03926143e-08 0.00000000e+00 3.63959470e-23\n",
      " 1.28253119e+02 8.28330044e+01]\n",
      "27-th iteration, loss: 0.24140514220498133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.443243254839823e-07\n",
      "27-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265467e+00 4.78602337e+01 7.54396535e+01 4.19500765e+01\n",
      " 9.45481059e+01 7.92860858e+01 1.18505664e+02 8.22892399e+01\n",
      " 1.14354220e-05 3.66488945e-06 1.10906568e-05 1.86835289e-06\n",
      " 1.28253129e+02 8.28330042e+01]\n",
      "28-th iteration, loss: 0.24140514220496936, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3993007105846506e-07\n",
      "28-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265480e+00 4.78602341e+01 7.54396536e+01 4.19500760e+01\n",
      " 9.45481056e+01 7.92860852e+01 1.18505663e+02 8.22892400e+01\n",
      " 1.18274534e-05 3.59541794e-06 1.15171473e-05 1.69460155e-06\n",
      " 0.00000000e+00 3.44107135e-22 1.28253130e+02 8.28330041e+01]\n",
      "29-th iteration, loss: 0.2414051422049562, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3218841867217145e-07\n",
      "29-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265493e+00 4.78602345e+01 7.54396537e+01 4.19500756e+01\n",
      " 9.45481054e+01 7.92860846e+01 1.18505663e+02 8.22892400e+01\n",
      " 1.22170211e-05 3.52283694e-06 1.19405209e-05 1.51373064e-06\n",
      " 1.28253131e+02 8.28330039e+01]\n",
      "30-th iteration, loss: 0.24140514220494505, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.282298392433078e-07\n",
      "30-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265506e+00 4.78602349e+01 7.54396537e+01 4.19500753e+01\n",
      " 9.45481051e+01 7.92860841e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.26016703e-05 3.44332429e-06 1.23582936e-05 1.32194733e-06\n",
      " 1.28253131e+02 8.28330038e+01]\n",
      "31-th iteration, loss: 0.2414051422049342, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.243283531025289e-07\n",
      "31-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265519e+00 4.78602353e+01 7.54396538e+01 4.19500749e+01\n",
      " 9.45481048e+01 7.92860835e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.29849047e-05 3.36162807e-06 1.27739038e-05 1.12405232e-06\n",
      " 0.00000000e+00 7.94093388e-23 1.28253131e+02 8.28330036e+01]\n",
      "32-th iteration, loss: 0.2414051422049222, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.171481021235514e-07\n",
      "32-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265531e+00 4.78602357e+01 7.54396538e+01 4.19500745e+01\n",
      " 9.45481045e+01 7.92860830e+01 1.18505663e+02 8.22892401e+01\n",
      " 1.33666057e-05 3.27807145e-06 1.31872124e-05 9.20389164e-07\n",
      " 1.28253132e+02 8.28330034e+01]\n",
      "33-th iteration, loss: 0.24140514220491205, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1359538788817783e-07\n",
      "33-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265544e+00 4.78602361e+01 7.54396539e+01 4.19500742e+01\n",
      " 9.45481043e+01 7.92860824e+01 1.18505663e+02 8.22892402e+01\n",
      " 1.37441511e-05 3.18839415e-06 1.35955799e-05 7.06719176e-07\n",
      " 1.28253133e+02 8.28330033e+01]\n",
      "34-th iteration, loss: 0.24140514220490214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1006705933774046e-07\n",
      "34-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265556e+00 4.78602365e+01 7.54396540e+01 4.19500738e+01\n",
      " 9.45481040e+01 7.92860819e+01 1.18505663e+02 8.22892402e+01\n",
      " 1.41209817e-05 3.09733406e-06 1.40023893e-05 4.87826736e-07\n",
      " 0.00000000e+00 9.92616735e-23 1.28253133e+02 8.28330031e+01]\n",
      "35-th iteration, loss: 0.2414051422048911, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.033321354614368e-07\n",
      "35-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265568e+00 4.78602369e+01 7.54396540e+01 4.19500735e+01\n",
      " 9.45481038e+01 7.92860814e+01 1.18505662e+02 8.22892402e+01\n",
      " 1.44970248e-05 3.00532612e-06 1.44075552e-05 2.64161409e-07\n",
      " 0.00000000e+00 7.94093388e-23 1.28253134e+02 8.28330029e+01]\n",
      "36-th iteration, loss: 0.24140514220488044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.9690597923283686e-07\n",
      "36-th iteration, new layer inserted. now 16 layers\n",
      "[1.88265579e+00 4.78602373e+01 7.54396541e+01 4.19500732e+01\n",
      " 9.45481035e+01 7.92860809e+01 1.18505662e+02 8.22892403e+01\n",
      " 1.48694060e-05 2.90824436e-06 1.48081939e-05 3.16127007e-08\n",
      " 0.00000000e+00 1.19941189e-23 1.28253135e+02 8.28330027e+01]\n",
      "37-th iteration, loss: 0.24140514220487044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8873990431614435e-07\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265591e+00 4.78602376e+01 7.54396541e+01 4.19500729e+01\n",
      " 9.45481033e+01 7.92860804e+01 1.18505662e+02 8.22892403e+01\n",
      " 1.52385546e-05 2.80663841e-06 1.28253151e+02 8.28330025e+01]\n",
      "38-th iteration, loss: 0.2414051422048634, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.868262197696537e-07\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265602e+00 4.78602380e+01 7.54396542e+01 4.19500726e+01\n",
      " 9.45481031e+01 7.92860799e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.56010153e-05 2.69437952e-06 1.28253151e+02 8.28330023e+01]\n",
      "39-th iteration, loss: 0.24140514220485657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8487581251324134e-07\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265613e+00 4.78602384e+01 7.54396543e+01 4.19500723e+01\n",
      " 9.45481029e+01 7.92860795e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.59625875e-05 2.57994458e-06 1.28253151e+02 8.28330021e+01]\n",
      "40-th iteration, loss: 0.24140514220484993, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8289460506604537e-07\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265624e+00 4.78602387e+01 7.54396543e+01 4.19500720e+01\n",
      " 9.45481026e+01 7.92860790e+01 1.18505662e+02 8.22892404e+01\n",
      " 1.63232577e-05 2.46325306e-06 1.28253152e+02 8.28330019e+01]\n",
      "41-th iteration, loss: 0.2414051422048435, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8088794964743793e-07\n",
      "41-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265635e+00 4.78602390e+01 7.54396544e+01 4.19500717e+01\n",
      " 9.45481024e+01 7.92860786e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.66830187e-05 2.34423366e-06 0.00000000e+00 3.44107135e-22\n",
      " 1.28253152e+02 8.28330017e+01]\n",
      "42-th iteration, loss: 0.24140514220483608, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7588474695580946e-07\n",
      "42-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265646e+00 4.78602394e+01 7.54396544e+01 4.19500714e+01\n",
      " 9.45481022e+01 7.92860782e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.70414453e-05 2.22254099e-06 0.00000000e+00 5.82335151e-22\n",
      " 1.28253153e+02 8.28330016e+01]\n",
      "43-th iteration, loss: 0.24140514220482884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.710619959686969e-07\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265656e+00 4.78602397e+01 7.54396545e+01 4.19500711e+01\n",
      " 9.45481020e+01 7.92860778e+01 1.18505662e+02 8.22892405e+01\n",
      " 1.73960726e-05 2.09492647e-06 1.28253154e+02 8.28330013e+01]\n",
      "44-th iteration, loss: 0.24140514220482298, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6931189836639376e-07\n",
      "44-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265666e+00 4.78602400e+01 7.54396546e+01 4.19500709e+01\n",
      " 9.45481018e+01 7.92860774e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.77474986e-05 1.96187281e-06 0.00000000e+00 5.82335151e-22\n",
      " 1.28253154e+02 8.28330011e+01]\n",
      "45-th iteration, loss: 0.24140514220481615, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.646388267456944e-07\n",
      "45-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265676e+00 4.78602403e+01 7.54396546e+01 4.19500706e+01\n",
      " 9.45481017e+01 7.92860770e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.80980444e-05 1.82677036e-06 1.28253155e+02 8.28330009e+01]\n",
      "46-th iteration, loss: 0.2414051422048106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.629846782254775e-07\n",
      "46-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265686e+00 4.78602406e+01 7.54396547e+01 4.19500704e+01\n",
      " 9.45481015e+01 7.92860766e+01 1.18505662e+02 8.22892406e+01\n",
      " 1.84455704e-05 1.68637606e-06 0.00000000e+00 3.44107135e-22\n",
      " 1.28253155e+02 8.28330007e+01]\n",
      "47-th iteration, loss: 0.2414051422048041, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.584520034405542e-07\n",
      "47-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265696e+00 4.78602409e+01 7.54396547e+01 4.19500701e+01\n",
      " 9.45481013e+01 7.92860762e+01 1.18505661e+02 8.22892406e+01\n",
      " 1.87924178e-05 1.54416477e-06 1.28253156e+02 8.28330005e+01]\n",
      "48-th iteration, loss: 0.2414051422047989, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.568866942756451e-07\n",
      "48-th iteration, new layer inserted. now 14 layers\n",
      "[1.88265706e+00 4.78602412e+01 7.54396548e+01 4.19500699e+01\n",
      " 9.45481011e+01 7.92860758e+01 1.18505661e+02 8.22892407e+01\n",
      " 1.91364159e-05 1.39678858e-06 0.00000000e+00 2.38228016e-22\n",
      " 1.28253156e+02 8.28330003e+01]\n",
      "49-th iteration, loss: 0.24140514220479273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5248590878714235e-07\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265715e+00 4.78602415e+01 7.54396549e+01 4.19500697e+01\n",
      " 9.45481009e+01 7.92860755e+01 1.18505661e+02 8.22892407e+01\n",
      " 1.94799248e-05 1.24780355e-06 1.28253157e+02 8.28330001e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535590565167533\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.20594943   0.         658.31163851]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6336159057824838\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.47534951  62.25781836 232.81753069   0.         425.49410782]\n",
      "2-th iteration, loss: 0.6031807630040061, 13 gd steps\n",
      "insert gradient: -0.6605279978497849\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1782722   77.52938313 218.37705095  42.34732975 243.13949018\n",
      "   0.         182.35461764]\n",
      "3-th iteration, loss: 0.46750685418285004, 20 gd steps\n",
      "insert gradient: -0.3581768149881661\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.49835606 115.67193958   0.         109.24572071\n",
      "  57.03947824 153.54515506  56.04003998 182.35461764]\n",
      "4-th iteration, loss: 0.36328095042507136, 81 gd steps\n",
      "insert gradient: -0.4484179934141403\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          44.40235846  55.23312461  48.91041178 106.85336532\n",
      "  76.81106462 123.13084914  80.14348952 182.35461764   0.        ]\n",
      "5-th iteration, loss: 0.24140518966994437, 73 gd steps\n",
      "insert gradient: -5.672060925629881e-05\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  1.92043892  47.85877217  75.41559678  41.95698206  94.56811181\n",
      "  79.27990746 118.49589996  82.28769316 128.27790383  82.81928341]\n",
      "6-th iteration, loss: 0.24140514249435457, 17 gd steps\n",
      "insert gradient: -1.1207861363264359e-05\n",
      "6-th iteration, new layer inserted. now 12 layers\n",
      "[1.88324536e+00 4.78599804e+01 0.00000000e+00 1.28785871e-14\n",
      " 7.54379846e+01 4.19501747e+01 9.45504300e+01 7.92854380e+01\n",
      " 1.18505066e+02 8.22887833e+01 1.28256866e+02 8.28313008e+01]\n",
      "7-th iteration, loss: 0.24140514221382287, 24 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1031860941848654e-06\n",
      "7-th iteration, new layer inserted. now 10 layers\n",
      "[  1.8826506   47.86011337  75.43998103  41.95006292  94.54779764\n",
      "  79.28608987 118.50581113  82.28903504 128.25414863  82.83251491]\n",
      "8-th iteration, loss: 0.2414051422137758, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1047587663157694e-06\n",
      "8-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265076  47.86011366  75.43997995  41.9500627   94.54779875\n",
      "  79.28608962 118.50581059  82.28903516 128.25414712  82.83251561]\n",
      "9-th iteration, loss: 0.2414051422137291, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.105786567377441e-06\n",
      "9-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265092e+00 4.78601140e+01 7.54399789e+01 4.19500625e+01\n",
      " 0.00000000e+00 1.77635684e-14 9.45477999e+01 7.92860894e+01\n",
      " 1.18505810e+02 8.22890353e+01 1.28254146e+02 8.28325163e+01]\n",
      "10-th iteration, loss: 0.24140514221367265, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0934054673671704e-06\n",
      "10-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265108e+00 4.78601143e+01 7.54399778e+01 4.19500623e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478021e+01 7.92860892e+01\n",
      " 1.18505810e+02 8.22890354e+01 1.28254144e+02 8.28325170e+01]\n",
      "11-th iteration, loss: 0.24140514221361675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0815026771459908e-06\n",
      "11-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265125  47.86011457  75.43997677  41.95006208  94.54780425\n",
      "  79.28608893 118.505809    82.28903556 128.25414262  82.83251772]\n",
      "12-th iteration, loss: 0.24140514221357104, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0826851813332346e-06\n",
      "12-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265141  47.86011487  75.43997571  41.95006186  94.54780534\n",
      "  79.28608869 118.50580848  82.2890357  128.25414113  82.83251842]\n",
      "13-th iteration, loss: 0.2414051422135257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0833684657964817e-06\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265157  47.86011517  75.43997466  41.95006166  94.54780642\n",
      "  79.28608847 118.50580796  82.28903585 128.25413964  82.83251913]\n",
      "14-th iteration, loss: 0.2414051422134806, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0836015674685633e-06\n",
      "14-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265173e+00 4.78601155e+01 7.54399736e+01 4.19500615e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478075e+01 7.92860882e+01\n",
      " 1.18505807e+02 8.22890360e+01 1.28254138e+02 8.28325198e+01]\n",
      "15-th iteration, loss: 0.24140514221342618, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.070783019799729e-06\n",
      "15-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265189e+00 4.78601158e+01 7.54399726e+01 4.19500613e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478097e+01 7.92860880e+01\n",
      " 1.18505807e+02 8.22890362e+01 1.28254137e+02 8.28325205e+01]\n",
      "16-th iteration, loss: 0.24140514221337228, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0585073422822943e-06\n",
      "16-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265205e+00 4.78601161e+01 7.54399716e+01 4.19500611e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478118e+01 7.92860878e+01\n",
      " 1.18505806e+02 8.22890363e+01 1.28254135e+02 8.28325213e+01]\n",
      "17-th iteration, loss: 0.2414051422133189, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.046725369659041e-06\n",
      "17-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265221e+00 4.78601164e+01 7.54399705e+01 4.19500609e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478139e+01 7.92860876e+01\n",
      " 1.18505806e+02 8.22890365e+01 1.28254134e+02 8.28325220e+01]\n",
      "18-th iteration, loss: 0.24140514221326603, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0353930238674971e-06\n",
      "18-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265237  47.86011672  75.43996948  41.95006068  94.54781603\n",
      "  79.28608736 118.50580544  82.28903667 128.25413229  82.83252267]\n",
      "19-th iteration, loss: 0.24140514221322248, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.036585315963505e-06\n",
      "19-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265253  47.86011702  75.43996845  41.95006047  94.54781707\n",
      "  79.28608713 118.50580494  82.28903684 128.25413083  82.83252338]\n",
      "20-th iteration, loss: 0.24140514221317924, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.037293370171485e-06\n",
      "20-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265268e+00 4.78601173e+01 7.54399674e+01 4.19500603e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478181e+01 7.92860869e+01\n",
      " 1.18505804e+02 8.22890370e+01 1.28254129e+02 8.28325241e+01]\n",
      "21-th iteration, loss: 0.24140514221312742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.025448152224915e-06\n",
      "21-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265284e+00 4.78601176e+01 7.54399664e+01 4.19500601e+01\n",
      " 0.00000000e+00 5.77315973e-15 9.45478202e+01 7.92860867e+01\n",
      " 1.18505804e+02 8.22890372e+01 1.28254128e+02 8.28325248e+01]\n",
      "22-th iteration, loss: 0.24140514221307607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0140769800985037e-06\n",
      "22-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265300e+00 4.78601179e+01 7.54399654e+01 4.19500599e+01\n",
      " 0.00000000e+00 1.28785871e-14 9.45478222e+01 7.92860865e+01\n",
      " 1.18505803e+02 8.22890374e+01 1.28254126e+02 8.28325255e+01]\n",
      "23-th iteration, loss: 0.24140514221302523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0031377159724911e-06\n",
      "23-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265315e+00 4.78601182e+01 7.54399644e+01 4.19500597e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478243e+01 7.92860863e+01\n",
      " 1.18505803e+02 8.22890376e+01 1.28254125e+02 8.28325262e+01]\n",
      "24-th iteration, loss: 0.2414051422129748, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.925925674159987e-07\n",
      "24-th iteration, new layer inserted. now 10 layers\n",
      "[  1.8826533   47.86011854  75.43996336  41.95005947  94.54782627\n",
      "  79.28608603 118.5058025   82.28903778 128.2541236   82.83252692]\n",
      "25-th iteration, loss: 0.24140514221293302, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.940289280117475e-07\n",
      "25-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265346e+00 4.78601188e+01 7.54399624e+01 4.19500593e+01\n",
      " 0.00000000e+00 1.33226763e-15 9.45478273e+01 7.92860858e+01\n",
      " 1.18505802e+02 8.22890380e+01 1.28254122e+02 8.28325276e+01]\n",
      "26-th iteration, loss: 0.24140514221288334, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.833462814648996e-07\n",
      "26-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265361e+00 4.78601191e+01 7.54399613e+01 4.19500591e+01\n",
      " 0.00000000e+00 1.46549439e-14 9.45478293e+01 7.92860856e+01\n",
      " 1.18505802e+02 8.22890382e+01 1.28254121e+02 8.28325283e+01]\n",
      "27-th iteration, loss: 0.24140514221283407, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.73046910537481e-07\n",
      "27-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265376  47.86011943  75.43996035  41.95005885  94.54783122\n",
      "  79.28608536 118.50580106  82.28903838 128.25411931  82.83252905]\n",
      "28-th iteration, loss: 0.24140514221279313, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.744913041360582e-07\n",
      "28-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265391  47.86011973  75.43995935  41.95005864  94.5478322\n",
      "  79.28608513 118.50580058  82.28903858 128.25411788  82.83252975]\n",
      "29-th iteration, loss: 0.24140514221275242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.754514055103986e-07\n",
      "29-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265406e+00 4.78601200e+01 7.54399584e+01 4.19500584e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478332e+01 7.92860849e+01\n",
      " 1.18505800e+02 8.22890388e+01 1.28254116e+02 8.28325305e+01]\n",
      "30-th iteration, loss: 0.24140514221270415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.645740756343162e-07\n",
      "30-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265421e+00 4.78601203e+01 7.54399574e+01 4.19500583e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478351e+01 7.92860847e+01\n",
      " 1.18505800e+02 8.22890390e+01 1.28254115e+02 8.28325312e+01]\n",
      "31-th iteration, loss: 0.24140514221265633, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.541175145759823e-07\n",
      "31-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265437  47.86012064  75.43995639  41.95005806  94.54783706\n",
      "  79.28608448 118.50579918  82.28903922 128.25411364  82.83253187]\n",
      "32-th iteration, loss: 0.2414051422126164, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.552092664087812e-07\n",
      "32-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265451  47.86012094  75.43995541  41.95005787  94.54783801\n",
      "  79.28608426 118.50579871  82.28903944 128.25411223  82.83253258]\n",
      "33-th iteration, loss: 0.2414051422125768, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.558598130445614e-07\n",
      "33-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265466  47.86012125  75.43995443  41.95005768  94.54783897\n",
      "  79.28608405 118.50579825  82.28903966 128.25411082  82.83253328]\n",
      "34-th iteration, loss: 0.24140514221253742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.561128746069293e-07\n",
      "34-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265482e+00 4.78601216e+01 7.54399535e+01 4.19500575e+01\n",
      " 0.00000000e+00 1.33226763e-15 9.45478399e+01 7.92860838e+01\n",
      " 1.18505798e+02 8.22890399e+01 1.28254109e+02 8.28325340e+01]\n",
      "35-th iteration, loss: 0.24140514221249076, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.448467179546584e-07\n",
      "35-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265497  47.86012187  75.43995251  41.95005734  94.54784184\n",
      "  79.28608363 118.50579735  82.2890401  128.25410803  82.83253469]\n",
      "36-th iteration, loss: 0.2414051422124519, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.450976204234664e-07\n",
      "36-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265512e+00 4.78601222e+01 7.54399516e+01 4.19500572e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478428e+01 7.92860834e+01\n",
      " 1.18505797e+02 8.22890403e+01 1.28254107e+02 8.28325354e+01]\n",
      "37-th iteration, loss: 0.24140514221240592, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.339623652931234e-07\n",
      "37-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265527  47.8601225   75.4399506   41.950057    94.54784468\n",
      "  79.28608322 118.50579646  82.28904056 128.25410524  82.8325361 ]\n",
      "38-th iteration, loss: 0.24140514221236753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.342114845919566e-07\n",
      "38-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265541e+00 4.78601228e+01 7.54399497e+01 4.19500568e+01\n",
      " 0.00000000e+00 1.46549439e-14 9.45478456e+01 7.92860830e+01\n",
      " 1.18505796e+02 8.22890408e+01 1.28254104e+02 8.28325368e+01]\n",
      "39-th iteration, loss: 0.2414051422123222, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.232060103726748e-07\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265556e+00 4.78601231e+01 7.54399487e+01 4.19500567e+01\n",
      " 0.00000000e+00 7.54951657e-15 9.45478475e+01 7.92860828e+01\n",
      " 1.18505796e+02 8.22890410e+01 1.28254102e+02 8.28325375e+01]\n",
      "40-th iteration, loss: 0.24140514221227724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.126695967874049e-07\n",
      "40-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265571e+00 4.78601234e+01 7.54399478e+01 4.19500565e+01\n",
      " 0.00000000e+00 6.66133815e-15 9.45478493e+01 7.92860826e+01\n",
      " 1.18505795e+02 8.22890413e+01 1.28254101e+02 8.28325382e+01]\n",
      "41-th iteration, loss: 0.24140514221223272, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.025599927835758e-07\n",
      "41-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265585  47.86012373  75.43994682  41.95005632  94.54785115\n",
      "  79.28608239 118.50579469  82.2890415  128.25409971  82.83253891]\n",
      "42-th iteration, loss: 0.24140514221219525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.033949965036798e-07\n",
      "42-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265600e+00 4.78601240e+01 7.54399459e+01 4.19500561e+01\n",
      " 0.00000000e+00 1.19904087e-14 9.45478521e+01 7.92860822e+01\n",
      " 1.18505794e+02 8.22890417e+01 1.28254098e+02 8.28325396e+01]\n",
      "43-th iteration, loss: 0.24140514221215134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.932779025061263e-07\n",
      "43-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265614e+00 4.78601243e+01 7.54399449e+01 4.19500560e+01\n",
      " 0.00000000e+00 2.48689958e-14 9.45478539e+01 7.92860820e+01\n",
      " 1.18505794e+02 8.22890420e+01 1.28254097e+02 8.28325403e+01]\n",
      "44-th iteration, loss: 0.2414051422121078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.835585394604122e-07\n",
      "44-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265629e+00 4.78601246e+01 7.54399440e+01 4.19500558e+01\n",
      " 0.00000000e+00 1.77635684e-14 9.45478557e+01 7.92860818e+01\n",
      " 1.18505793e+02 8.22890422e+01 1.28254096e+02 8.28325410e+01]\n",
      "45-th iteration, loss: 0.24140514221206463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.74201891284129e-07\n",
      "45-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265643  47.86012493  75.43994307  41.95005562  94.54785742\n",
      "  79.28608155 118.50579294  82.28904246 128.25409423  82.83254169]\n",
      "46-th iteration, loss: 0.24140514221202813, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.754084717768302e-07\n",
      "46-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265657  47.86012522  75.43994214  41.95005544  94.5478583\n",
      "  79.28608134 118.50579251  82.2890427  128.25409286  82.83254238]\n",
      "47-th iteration, loss: 0.24140514221199186, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.761942037699671e-07\n",
      "47-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265671  47.86012552  75.43994122  41.95005527  94.54785918\n",
      "  79.28608113 118.50579208  82.28904295 128.2540915   82.83254308]\n",
      "48-th iteration, loss: 0.24140514221195583, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.766008381182126e-07\n",
      "48-th iteration, new layer inserted. now 10 layers\n",
      "[  1.88265685  47.86012582  75.4399403   41.95005511  94.54786005\n",
      "  79.28608093 118.50579165  82.28904319 128.25409014  82.83254377]\n",
      "49-th iteration, loss: 0.24140514221192005, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.766659806181383e-07\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[1.88265699e+00 4.78601261e+01 7.54399394e+01 4.19500550e+01\n",
      " 0.00000000e+00 8.88178420e-16 9.45478609e+01 7.92860807e+01\n",
      " 1.18505791e+02 8.22890434e+01 1.28254089e+02 8.28325445e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5366485788750763\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.89416964   0.         684.74904644]\n",
      "1-th iteration, loss: 0.7481568929324234, 11 gd steps\n",
      "insert gradient: -0.6310653355680163\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.52886631  62.34663952 233.81674756   0.         450.93229887]\n",
      "2-th iteration, loss: 0.6043164652064028, 13 gd steps\n",
      "insert gradient: -0.659365031891441\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.49693009  77.62285849 219.33640905  41.80284804 239.2701994\n",
      "   0.         211.66209947]\n",
      "3-th iteration, loss: 0.46878047382198124, 20 gd steps\n",
      "insert gradient: -0.5568536542324254\n",
      "3-th iteration, new layer inserted. now 8 layers\n",
      "[  0.          59.8620548  225.09722016  55.80320243 163.84414506\n",
      "  49.23316264 211.66209947   0.        ]\n",
      "4-th iteration, loss: 0.3597929758806964, 26 gd steps\n",
      "insert gradient: -0.7683586951214818\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  0.66761927  50.51309446 123.84820524   0.         123.84820524\n",
      "  49.86983697 155.619805    57.10989663 159.16918478  54.6692517 ]\n",
      "5-th iteration, loss: 0.29029072086957297, 44 gd steps\n",
      "insert gradient: -0.028145158874571283\n",
      "5-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          69.34583452  79.07569712  36.12030387  85.87215004\n",
      "  52.69203691 161.56273294  59.67488701 155.60686627  64.09215298]\n",
      "6-th iteration, loss: 0.29007742387168195, 14 gd steps\n",
      "insert gradient: -0.03291539413738879\n",
      "6-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          70.38408758  76.07630773  36.97525194  85.98465784\n",
      "  52.45888038 162.16818581  59.71670699 155.02021876  65.12840444]\n",
      "7-th iteration, loss: 0.28991751824293, 91 gd steps\n",
      "insert gradient: -0.000979156291795891\n",
      "7-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.41376887  74.41261033  37.23538252  85.97519144\n",
      "  53.20137863 162.67559588  59.67509375 154.48535557  65.58153995]\n",
      "8-th iteration, loss: 0.28991730437844515, 43 gd steps\n",
      "insert gradient: -0.0009629694405827478\n",
      "8-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.41895155  74.40244837  37.24028478  85.9749576\n",
      "  53.20055986 162.67998358  59.67520036 154.48399461  65.58855849]\n",
      "9-th iteration, loss: 0.2899171067387849, 40 gd steps\n",
      "insert gradient: -0.0009487504016090227\n",
      "9-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14237982e+01 7.43928992e+01 3.72448160e+01\n",
      " 8.59747040e+01 5.31997359e+01 1.62684091e+02 5.96752532e+01\n",
      " 1.54482677e+02 0.00000000e+00 6.75015599e-14 6.55951135e+01]\n",
      "10-th iteration, loss: 0.2899167940439816, 54 gd steps\n",
      "insert gradient: -0.0008461240581427001\n",
      "10-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.43076885  74.38014542  37.25099986  85.9742859\n",
      "  53.1983459  162.68956042  59.67497608 154.48067068  65.61185958]\n",
      "11-th iteration, loss: 0.2899166275583249, 36 gd steps\n",
      "insert gradient: -0.0008539716515982768\n",
      "11-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14354118e+01 7.43718909e+01 3.72550556e+01\n",
      " 8.59740111e+01 5.31974558e+01 1.62693171e+02 5.96747764e+01\n",
      " 1.54479276e+02 0.00000000e+00 9.94759830e-14 6.56170891e+01]\n",
      "12-th iteration, loss: 0.289916385279708, 46 gd steps\n",
      "insert gradient: -0.0007857104582283194\n",
      "12-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14412652e+01 7.43614867e+01 3.72601307e+01\n",
      " 8.59736401e+01 5.31963245e+01 1.62697748e+02 5.96744987e+01\n",
      " 1.54477434e+02 0.00000000e+00 1.95399252e-14 6.56299117e+01]\n",
      "13-th iteration, loss: 0.2899161730768097, 42 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.44678255  74.35212675  37.26479033  85.97328125\n",
      "  53.19523824 162.70193075  59.67414784 154.47562779  65.64083741]\n",
      "14-th iteration, loss: 0.2899160384369114, 31 gd steps\n",
      "insert gradient: -0.0007534645247587004\n",
      "14-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45080894  74.34511922  37.26823894  85.97302267\n",
      "  53.19451749 162.7051176   59.67398052 154.47426476  65.64486254]\n",
      "15-th iteration, loss: 0.2899159114761373, 30 gd steps\n",
      "insert gradient: -0.000758744703003412\n",
      "15-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45445156  74.33835413  37.27145523  85.97278173\n",
      "  53.19392454 162.70820712  59.67394297 154.47299126  65.64879398]\n",
      "16-th iteration, loss: 0.2899157906203835, 29 gd steps\n",
      "insert gradient: -0.0007597046059790246\n",
      "16-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.45784316  74.33180541  37.2745015   85.97254915\n",
      "  53.19338893 162.71119423  59.67396116 154.47177703  65.65262675]\n",
      "17-th iteration, loss: 0.2899156750565602, 28 gd steps\n",
      "insert gradient: -0.0007583931429034879\n",
      "17-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.46105102  74.32545833  37.27741237  85.97232238\n",
      "  53.19288148 162.71407952  59.67400196 154.47060698  65.65635962]\n",
      "18-th iteration, loss: 0.28991556420668974, 28 gd steps\n",
      "insert gradient: -0.0007558439554638061\n",
      "18-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14641144e+01 7.43192987e+01 3.72802106e+01\n",
      " 8.59721010e+01 5.31923899e+01 1.62716868e+02 5.96740503e+01\n",
      " 1.54469472e+02 0.00000000e+00 6.75015599e-14 6.56599956e+01]\n",
      "19-th iteration, loss: 0.28991541354236106, 33 gd steps\n",
      "insert gradient: -0.0007062920674571224\n",
      "19-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.46784985  74.31204718  37.28354317  85.97181861\n",
      "  53.191714   162.72012701  59.67400216 154.46807147  65.66837078]\n",
      "20-th iteration, loss: 0.28991531168828566, 26 gd steps\n",
      "insert gradient: -0.0007118509839249001\n",
      "20-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.47088603  74.30631274  37.28622368  85.97159727\n",
      "  53.19116377 162.72271975  59.67394048 154.46692575  65.67162199]\n",
      "21-th iteration, loss: 0.2899152137830325, 26 gd steps\n",
      "insert gradient: -0.0007136867119771247\n",
      "21-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14737416e+01 7.43007195e+01 3.72887935e+01\n",
      " 8.59713885e+01 5.31906745e+01 1.62725251e+02 5.96739350e+01\n",
      " 1.54465825e+02 0.00000000e+00 2.66453526e-14 6.56748191e+01]\n",
      "22-th iteration, loss: 0.28991508458237675, 30 gd steps\n",
      "insert gradient: -0.0006733700961770494\n",
      "22-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14770880e+01 7.42942730e+01 3.72917661e+01\n",
      " 8.59711347e+01 5.31900617e+01 1.62728155e+02 5.96738769e+01\n",
      " 1.54464519e+02 0.00000000e+00 9.94759830e-14 6.56820649e+01]\n",
      "23-th iteration, loss: 0.2899149636731344, 29 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48043214  74.28815861  37.29465877  85.97088299\n",
      "  53.18941444 162.73091999  59.67374173 154.46320776  65.6886904 ]\n",
      "24-th iteration, loss: 0.28991487611257305, 24 gd steps\n",
      "insert gradient: -0.0006528994457853435\n",
      "24-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14832132e+01 7.42830495e+01 3.72970797e+01\n",
      " 8.59706805e+01 5.31889060e+01 1.62733252e+02 5.96736606e+01\n",
      " 1.54462104e+02 0.00000000e+00 1.77635684e-15 6.56914320e+01]\n",
      "25-th iteration, loss: 0.28991476562231194, 28 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "25-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48629208  74.27731249  37.29976968  85.97044852\n",
      "  53.18834318 162.73586947  59.67358203 154.46085648  65.69753414]\n",
      "26-th iteration, loss: 0.2899146840115599, 23 gd steps\n",
      "insert gradient: -0.0006327152517430554\n",
      "26-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.48890621  74.27244676  37.30205886  85.97025617\n",
      "  53.18788098 162.73810389  59.67353194 154.45978859  65.70009739]\n",
      "27-th iteration, loss: 0.2899146051249046, 23 gd steps\n",
      "insert gradient: -0.0006373191958709217\n",
      "27-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.49136725  74.26767107  37.3042597   85.97007299\n",
      "  53.18747128 162.74029824  59.67353722 154.45876262  65.70264262]\n",
      "28-th iteration, loss: 0.28991452864874, 22 gd steps\n",
      "insert gradient: -0.000639630778912058\n",
      "28-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14937156e+01 7.42629849e+01 3.73063884e+01\n",
      " 8.59698956e+01 5.31870901e+01 1.62742447e+02 5.96735723e+01\n",
      " 1.54457769e+02 0.00000000e+00 5.15143483e-14 6.57051620e+01]\n",
      "29-th iteration, loss: 0.28991443240454823, 25 gd steps\n",
      "insert gradient: -0.0006095841641735134\n",
      "29-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.49635898  74.25777814  37.30875694  85.96968764\n",
      "  53.18662646 162.74482015  59.67357304 154.45664314  65.71069096]\n",
      "30-th iteration, loss: 0.289914360218427, 22 gd steps\n",
      "insert gradient: -0.0006150501113203594\n",
      "30-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.14986756e+01 7.42533006e+01 3.73108185e+01\n",
      " 8.59695105e+01 5.31862186e+01 1.62746867e+02 5.96735633e+01\n",
      " 1.54455657e+02 0.00000000e+00 9.94759830e-14 6.57130414e+01]\n",
      "31-th iteration, loss: 0.289914270819755, 24 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "31-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50122286  74.24837057  37.31307907  85.96930973\n",
      "  53.18575646 162.7491139   59.67354156 154.45456017  65.71817076]\n",
      "32-th iteration, loss: 0.2899142025819352, 21 gd steps\n",
      "insert gradient: -0.000595914234683489\n",
      "32-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50346571  74.24407373  37.31506557  85.96913807\n",
      "  53.18535618 162.75108025  59.67352454 154.45359247  65.72038596]\n",
      "33-th iteration, loss: 0.2899141362368082, 21 gd steps\n",
      "insert gradient: -0.0005997026492926028\n",
      "33-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15056013e+01 7.42398424e+01 3.73169906e+01\n",
      " 8.59689737e+01 5.31849931e+01 1.62753016e+02 5.96735459e+01\n",
      " 1.54452656e+02 0.00000000e+00 6.75015599e-14 6.57225910e+01]\n",
      "34-th iteration, loss: 0.28991405463183817, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "34-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.50794696  74.23521846  37.31909088  85.96878714\n",
      "  53.18457391 162.75512131  59.67354883 154.45161903  65.7273653 ]\n",
      "35-th iteration, loss: 0.28991399160897574, 20 gd steps\n",
      "insert gradient: -0.0005805571051419331\n",
      "35-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15100423e+01 7.42311485e+01 3.73209585e+01\n",
      " 8.59686253e+01 5.31842014e+01 1.62756980e+02 5.96735470e+01\n",
      " 1.54450694e+02 0.00000000e+00 1.95399252e-14 6.57294473e+01]\n",
      "36-th iteration, loss: 0.2899139149670645, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "36-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.51230951  74.2267291   37.3229762   85.96844502\n",
      "  53.18378849 162.75899216  59.67353895 154.44968272  65.73393666]\n",
      "37-th iteration, loss: 0.2899138549949614, 20 gd steps\n",
      "insert gradient: -0.0005648798401899938\n",
      "37-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15143438e+01 7.42228004e+01 3.73247837e+01\n",
      " 8.59682880e+01 5.31834242e+01 1.62760787e+02 5.96735342e+01\n",
      " 1.54448775e+02 0.00000000e+00 5.68434189e-14 6.57359171e+01]\n",
      "38-th iteration, loss: 0.28991378269340795, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.51652588  74.21855776  37.32672296  85.96811455\n",
      "  53.18302651 162.76272056  59.67352751 154.44778999  65.74016942]\n",
      "39-th iteration, loss: 0.28991372554644945, 19 gd steps\n",
      "insert gradient: -0.0005511453554406479\n",
      "39-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15184941e+01 7.42147583e+01 3.73284711e+01\n",
      " 8.59679624e+01 5.31826746e+01 1.62764458e+02 5.96735258e+01\n",
      " 1.54446901e+02 0.00000000e+00 5.68434189e-14 6.57420608e+01]\n",
      "40-th iteration, loss: 0.2899136571387945, 21 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52059277  74.2106747   37.33033646  85.96779544\n",
      "  53.18229341 162.76632037  59.67352409 154.44594255  65.74610602]\n",
      "41-th iteration, loss: 0.28991360256849386, 19 gd steps\n",
      "insert gradient: -0.0005387161956462358\n",
      "41-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15224972e+01 7.42069924e+01 3.73320287e+01\n",
      " 8.59676479e+01 5.31819539e+01 1.62768005e+02 5.96735271e+01\n",
      " 1.54445072e+02 0.00000000e+00 5.86197757e-14 6.57479189e+01]\n",
      "42-th iteration, loss: 0.28991353766751554, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52451741  74.20305287  37.33382577  85.96748668\n",
      "  53.18158814 162.7698028   59.67353084 154.44413908  65.75178047]\n",
      "43-th iteration, loss: 0.28991348548892004, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "43-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.52636059  74.19948004  37.33546501  85.96734336\n",
      "  53.18126032 162.77143819  59.67353859 154.44328735  65.75352181]\n",
      "44-th iteration, loss: 0.2899134344890953, 18 gd steps\n",
      "insert gradient: -0.0005313977774290897\n",
      "44-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15281274e+01 7.41959471e+01 3.73370619e+01\n",
      " 8.59672051e+01 5.31809593e+01 1.62773054e+02 5.96735751e+01\n",
      " 1.54442459e+02 0.00000000e+00 9.94759830e-14 6.57552641e+01]\n",
      "45-th iteration, loss: 0.28991337387168153, 20 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53001242  74.19218071  37.33875831  85.96705287\n",
      "  53.18062452 162.77476862  59.67360254 154.44157008  65.7589565 ]\n",
      "46-th iteration, loss: 0.28991332492329974, 18 gd steps\n",
      "insert gradient: -0.0005183337349436476\n",
      "46-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15317509e+01 7.41887507e+01 3.73403170e+01\n",
      " 8.59669162e+01 5.31803169e+01 1.62776333e+02 5.96736246e+01\n",
      " 1.54440753e+02 0.00000000e+00 6.75015599e-14 6.57606268e+01]\n",
      "47-th iteration, loss: 0.2899132671519165, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53358862  74.18510825  37.34196311  85.96676763\n",
      "  53.17998378 162.7779887   59.67364383 154.43988192  65.76415823]\n",
      "48-th iteration, loss: 0.2899132201350472, 18 gd steps\n",
      "insert gradient: -0.000507242573518087\n",
      "48-th iteration, new layer inserted. now 10 layers\n",
      "[  0.          71.53528719  74.18177175  37.34348206  85.96663408\n",
      "  53.17967984 162.77950908  59.67366243 154.43907833  65.7657667 ]\n",
      "49-th iteration, loss: 0.28991317410656886, 17 gd steps\n",
      "insert gradient: -0.000510973106950792\n",
      "49-th iteration, new layer inserted. now 12 layers\n",
      "[0.00000000e+00 7.15369215e+01 7.41784703e+01 3.73449653e+01\n",
      " 8.59665050e+01 5.31793985e+01 1.62781012e+02 5.96737047e+01\n",
      " 1.54438295e+02 0.00000000e+00 3.37507799e-14 6.57673764e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010627\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.4077617    0.         708.36108252]\n",
      "1-th iteration, loss: 0.7509319290812663, 11 gd steps\n",
      "insert gradient: -0.6352343139406681\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 233.24084424   0.         475.12023828]\n",
      "2-th iteration, loss: 0.6031459279500627, 13 gd steps\n",
      "insert gradient: -0.6664488763860885\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.09379575  77.43082016 218.69660091  42.28660271 242.40828483\n",
      "   0.         232.71195344]\n",
      "3-th iteration, loss: 0.46727127496117205, 20 gd steps\n",
      "insert gradient: -0.572051658172892\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.24241481 225.52287286  54.91248034 160.82793181\n",
      "  53.04839126 212.76521458   0.          19.94673887]\n",
      "4-th iteration, loss: 0.35822815703124083, 37 gd steps\n",
      "insert gradient: -0.683665323532189\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.1875882   52.47210031 127.65090025   0.         118.53297881\n",
      "  53.08820655 154.52721102  55.29760568 160.94566795  53.20751706\n",
      "  19.94673887]\n",
      "5-th iteration, loss: 0.2899159316695374, 121 gd steps\n",
      "insert gradient: -0.00113836250367192\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14640629e+01 7.42092404e+01 3.72926482e+01\n",
      " 8.59428423e+01 5.32237253e+01 1.62606793e+02 5.97056863e+01\n",
      " 1.54453956e+02 0.00000000e+00 4.79616347e-14 6.55717193e+01\n",
      " 1.99467389e+01]\n",
      "6-th iteration, loss: 0.28991558963596376, 57 gd steps\n",
      "insert gradient: -0.0010061164957668308\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47127299  74.20110682  37.29922314  85.94355674\n",
      "  53.22131295 162.61669893  59.70439993 154.45379677  65.59306045\n",
      "  19.94673887]\n",
      "7-th iteration, loss: 0.2899154498443298, 32 gd steps\n",
      "insert gradient: -0.0010144997349880683\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14753248e+01 7.41966033e+01 3.73027449e+01\n",
      " 8.59438789e+01 5.32199235e+01 1.62622123e+02 5.97036212e+01\n",
      " 1.54453574e+02 0.00000000e+00 1.77635684e-15 6.55986490e+01\n",
      " 1.99467389e+01]\n",
      "8-th iteration, loss: 0.2899152086593886, 45 gd steps\n",
      "insert gradient: -0.0009619649648282179\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.48094406  74.19019993  37.30753246  85.94425787\n",
      "  53.21794116 162.62967988  59.7024731  154.45311374  65.61384348\n",
      "  19.94673887]\n",
      "9-th iteration, loss: 0.28991509102036983, 29 gd steps\n",
      "insert gradient: -0.0009607747828860331\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14844957e+01 7.41861029e+01 3.73105309e+01\n",
      " 8.59444839e+01 5.32167230e+01 0.00000000e+00 1.15463195e-14\n",
      " 1.62634480e+02 5.97017665e+01 1.54452747e+02 6.56185244e+01\n",
      " 1.99467389e+01]\n",
      "10-th iteration, loss: 0.28991490103814493, 38 gd steps\n",
      "insert gradient: -0.0009383143065006878\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14890056e+01 7.41806028e+01 3.73143961e+01\n",
      " 8.59447661e+01 5.32150485e+01 1.62646726e+02 5.97007781e+01\n",
      " 1.54452201e+02 0.00000000e+00 1.77635684e-15 6.56247562e+01\n",
      " 1.99467389e+01]\n",
      "11-th iteration, loss: 0.28991472623401005, 37 gd steps\n",
      "insert gradient: -0.0008835416739513316\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14933437e+01 7.41754372e+01 3.73180945e+01\n",
      " 8.59450326e+01 5.32133279e+01 1.62652247e+02 5.96996752e+01\n",
      " 1.54451570e+02 0.00000000e+00 7.28306304e-14 6.56362547e+01\n",
      " 1.99467389e+01]\n",
      "12-th iteration, loss: 0.28991457176741847, 34 gd steps\n",
      "insert gradient: -0.0008740087490279489\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49741009  74.17068148  37.32148744  85.94526496\n",
      "  53.21179893 162.65734376  59.6986716  154.45088082  65.64632969\n",
      "  19.94673887]\n",
      "13-th iteration, loss: 0.2899144830946842, 24 gd steps\n",
      "insert gradient: -0.0008767872270387542\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15002908e+01 7.41671852e+01 3.73239178e+01\n",
      " 8.59454407e+01 5.32107865e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.62661086e+02 5.96980359e+01 1.54450360e+02 6.56499475e+01\n",
      " 1.99467389e+01]\n",
      "14-th iteration, loss: 0.2899143485440195, 31 gd steps\n",
      "insert gradient: -0.0008527564265786557\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15037237e+01 7.41627639e+01 3.73268667e+01\n",
      " 8.59456537e+01 5.32095306e+01 1.62670190e+02 5.96972655e+01\n",
      " 1.54449691e+02 0.00000000e+00 1.59872116e-14 6.56545047e+01\n",
      " 1.99467389e+01]\n",
      "15-th iteration, loss: 0.2899142226529973, 30 gd steps\n",
      "insert gradient: -0.0008158320440366761\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50705427  74.15854605  37.32972063  85.94585897\n",
      "  53.20824603 162.67439302  59.69643877 154.44899327  65.66310229\n",
      "  19.94673887]\n",
      "16-th iteration, loss: 0.2899141470589896, 22 gd steps\n",
      "insert gradient: -0.000822713885717546\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50955072  74.15532099  37.33186914  85.94602127\n",
      "  53.2073455  162.67761424  59.69588017 154.44844656  65.6663252\n",
      "  19.94673887]\n",
      "17-th iteration, loss: 0.28991407454186113, 22 gd steps\n",
      "insert gradient: -0.0008265187087684173\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15118514e+01 7.41521304e+01 3.73339004e+01\n",
      " 8.59461832e+01 5.32065512e+01 1.62680771e+02 5.96954294e+01\n",
      " 1.54447928e+02 0.00000000e+00 5.32907052e-15 6.56695075e+01\n",
      " 1.99467389e+01]\n",
      "18-th iteration, loss: 0.28991396664366065, 27 gd steps\n",
      "insert gradient: -0.0008046477874752342\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51464617  74.1482377   37.33633539  85.94636232\n",
      "  53.20557681 162.68458569  59.69487032 154.44726106  65.67710922\n",
      "  19.94673887]\n",
      "19-th iteration, loss: 0.2899138992420893, 21 gd steps\n",
      "insert gradient: -0.0008029476641587885\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51687032  74.14517766  37.33825413  85.94650295\n",
      "  53.20482593 162.68758623  59.6944413  154.44671238  65.68004398\n",
      "  19.94673887]\n",
      "20-th iteration, loss: 0.289913834062906, 21 gd steps\n",
      "insert gradient: -0.0007981589463985573\n",
      "20-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51896028  74.14214839  37.34009094  85.94664197\n",
      "  53.20413941 162.69052842  59.69407864 154.44618225  65.68294649\n",
      "  19.94673887]\n",
      "21-th iteration, loss: 0.2899137708305173, 20 gd steps\n",
      "insert gradient: -0.0007915959216129823\n",
      "21-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15209519e+01 7.41391538e+01 3.73418611e+01\n",
      " 8.59467773e+01 5.32034937e+01 0.00000000e+00 6.21724894e-15\n",
      " 1.62693409e+02 5.96937575e+01 1.54445664e+02 6.56858114e+01\n",
      " 1.99467389e+01]\n",
      "22-th iteration, loss: 0.28991367967189263, 24 gd steps\n",
      "insert gradient: -0.0007847460787396109\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52330442  74.13560478  37.34393666  85.9469259\n",
      "  53.20267204 162.7000344   59.69332515 154.44502325  65.68919468\n",
      "  19.94673887]\n",
      "23-th iteration, loss: 0.2899136205550636, 20 gd steps\n",
      "insert gradient: -0.000786985562284491\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52523571  74.13273244  37.34565357  85.94705666\n",
      "  53.20196539 162.70264546  59.69293635 154.44449143  65.69195707\n",
      "  19.94673887]\n",
      "24-th iteration, loss: 0.2899135630781503, 19 gd steps\n",
      "insert gradient: -0.0007867501936979235\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52707584  74.12988892  37.34731243  85.94718829\n",
      "  53.2013151  162.70521538  59.69260076 154.44397279  65.69468848\n",
      "  19.94673887]\n",
      "25-th iteration, loss: 0.2899135070536776, 19 gd steps\n",
      "insert gradient: -0.0007849541962407556\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52884896  74.1270754   37.34892309  85.94731861\n",
      "  53.20070276 162.70774215  59.69230038 154.44346252  65.69738655\n",
      "  19.94673887]\n",
      "26-th iteration, loss: 0.2899134523763635, 19 gd steps\n",
      "insert gradient: -0.0007821695321155699\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15305699e+01 7.41242933e+01 3.73504921e+01\n",
      " 8.59474465e+01 5.32001174e+01 1.62710224e+02 5.96920241e+01\n",
      " 1.54442958e+02 0.00000000e+00 4.79616347e-14 6.57000493e+01\n",
      " 1.99467389e+01]\n",
      "27-th iteration, loss: 0.28991337270531475, 23 gd steps\n",
      "insert gradient: -0.0007433691394065065\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53264725  74.12103013  37.35233693  85.94758336\n",
      "  53.19939293 162.71311493  59.69165604 154.44233024  65.70618327\n",
      "  19.94673887]\n",
      "28-th iteration, loss: 0.28991332084672894, 18 gd steps\n",
      "insert gradient: -0.0007451400455752477\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.5344146   74.11835618  37.35387785  85.94769713\n",
      "  53.19879337 162.71549107  59.69134208 154.44179057  65.70865858\n",
      "  19.94673887]\n",
      "29-th iteration, loss: 0.2899132702287764, 18 gd steps\n",
      "insert gradient: -0.0007450124076006166\n",
      "29-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53610996  74.11570469  37.35537486  85.94781117\n",
      "  53.19823329 162.7178332   59.69106707 154.44126193  65.71111224\n",
      "  19.94673887]\n",
      "30-th iteration, loss: 0.28991322074758274, 18 gd steps\n",
      "insert gradient: -0.0007436427744253974\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53775023  74.11307772  37.35683442  85.94792399\n",
      "  53.1977003  162.72013899  59.69081846 154.44074099  65.71354124\n",
      "  19.94673887]\n",
      "31-th iteration, loss: 0.289913172333216, 18 gd steps\n",
      "insert gradient: -0.0007414558693200992\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15393465e+01 7.41104766e+01 3.73582612e+01\n",
      " 8.59480348e+01 5.31971867e+01 1.62722407e+02 5.96905883e+01\n",
      " 1.54440226e+02 0.00000000e+00 3.01980663e-14 6.57159438e+01\n",
      " 1.99467389e+01]\n",
      "32-th iteration, loss: 0.2899131036970446, 21 gd steps\n",
      "insert gradient: -0.0007073901759010153\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54122468  74.10749157  37.35990281  85.94815084\n",
      "  53.19656322 162.72499126  59.6902864  154.43960561  65.72136831\n",
      "  19.94673887]\n",
      "33-th iteration, loss: 0.2899130575012752, 17 gd steps\n",
      "insert gradient: -0.0007091106827890051\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54286188  74.10498049  37.3613113   85.94825003\n",
      "  53.19603116 162.72717078  59.69002024 154.43906228  65.72361898\n",
      "  19.94673887]\n",
      "34-th iteration, loss: 0.28991301230257505, 17 gd steps\n",
      "insert gradient: -0.0007092210426579731\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15444380e+01 7.41024880e+01 3.73626843e+01\n",
      " 8.59483496e+01 5.31955307e+01 1.62729322e+02 5.96897861e+01\n",
      " 1.54438529e+02 0.00000000e+00 1.59872116e-14 6.57258539e+01\n",
      " 1.99467389e+01]\n",
      "35-th iteration, loss: 0.2899129496502923, 20 gd steps\n",
      "insert gradient: -0.0006795687709264483\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54624424  74.09966404  37.36423574  85.9484536\n",
      "  53.19494444 162.73174164  59.68949967 154.43790479  65.73084399\n",
      "  19.94673887]\n",
      "36-th iteration, loss: 0.28991290638150147, 17 gd steps\n",
      "insert gradient: -0.0006821759487287941\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54783326  74.09724436  37.3655862   85.94854469\n",
      "  53.19444063 162.73382015  59.68924957 154.43735281  65.73295556\n",
      "  19.94673887]\n",
      "37-th iteration, loss: 0.2899128640183478, 17 gd steps\n",
      "insert gradient: -0.0006830752432603486\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54936045  74.09484003  37.36690257  85.94863641\n",
      "  53.19396792 162.73587377  59.68903209 154.436812    65.73505617\n",
      "  19.94673887]\n",
      "38-th iteration, loss: 0.28991282248827366, 17 gd steps\n",
      "insert gradient: -0.0006828102986294323\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15508397e+01 7.40924531e+01 3.73681898e+01\n",
      " 8.59487275e+01 5.31935164e+01 1.62737900e+02 5.96888370e+01\n",
      " 1.54436280e+02 0.00000000e+00 1.42108547e-14 6.57371428e+01\n",
      " 1.99467389e+01]\n",
      "39-th iteration, loss: 0.28991276584175235, 19 gd steps\n",
      "insert gradient: -0.0006552215679299371\n",
      "39-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55251651  74.0897841   37.36962734  85.94882069\n",
      "  53.19298909 162.74014961  59.68859566 154.43566568  65.74174497\n",
      "  19.94673887]\n",
      "40-th iteration, loss: 0.28991272596650036, 16 gd steps\n",
      "insert gradient: -0.0006575227891569441\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55401515  74.08746451  37.37089799  85.94890323\n",
      "  53.19252597 162.74210849  59.68837858 154.43511618  65.74372156\n",
      "  19.94673887]\n",
      "41-th iteration, loss: 0.28991268686588767, 16 gd steps\n",
      "insert gradient: -0.000658362507267293\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.55546026  74.08515875  37.37213942  85.94898623\n",
      "  53.19208854 162.74404482  59.68818885 154.43457685  65.74568941\n",
      "  19.94673887]\n",
      "42-th iteration, loss: 0.2899126484849987, 16 gd steps\n",
      "insert gradient: -0.0006581897179609029\n",
      "42-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15568633e+01 7.40828687e+01 3.73733557e+01\n",
      " 8.59490687e+01 5.31916688e+01 1.62745957e+02 5.96880181e+01\n",
      " 1.54434045e+02 0.00000000e+00 7.28306304e-14 6.57476458e+01\n",
      " 1.99467389e+01]\n",
      "43-th iteration, loss: 0.2899125968649508, 18 gd steps\n",
      "insert gradient: -0.0006326527306614531\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15584360e+01 7.40803351e+01 3.73747000e+01\n",
      " 8.59491520e+01 5.31911832e+01 1.62748056e+02 5.96878076e+01\n",
      " 1.54433441e+02 0.00000000e+00 4.79616347e-14 6.57519188e+01\n",
      " 1.99467389e+01]\n",
      "44-th iteration, loss: 0.28991254716896214, 18 gd steps\n",
      "insert gradient: -0.0006245925297911742\n",
      "44-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56005242  74.07786907  37.37604311  85.949228\n",
      "  53.19067565 162.75010168  59.68756242 154.43281957  65.75597767\n",
      "  19.94673887]\n",
      "45-th iteration, loss: 0.2899125108929783, 16 gd steps\n",
      "insert gradient: -0.0006234986598519709\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56150527  74.07567338  37.37724932  85.94929868\n",
      "  53.19023324 162.75192689  59.68735224 154.4322579   65.75776786\n",
      "  19.94673887]\n",
      "46-th iteration, loss: 0.2899124753187014, 16 gd steps\n",
      "insert gradient: -0.0006208236994948888\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56289801  74.07348683  37.37842575  85.94937074\n",
      "  53.18982078 162.75373518  59.68717477 154.43170879  65.75955621\n",
      "  19.94673887]\n",
      "47-th iteration, loss: 0.28991244038061376, 16 gd steps\n",
      "insert gradient: -0.0006194395228302146\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.5642443   74.07131172  37.37957698  85.94944285\n",
      "  53.18942856 162.75552387  59.68701993 154.43116935  65.7613393\n",
      "  19.94673887]\n",
      "48-th iteration, loss: 0.28991240605078056, 15 gd steps\n",
      "insert gradient: -0.0006196660282405624\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15655531e+01 7.40691506e+01 3.73807060e+01\n",
      " 8.59495143e+01 5.31890504e+01 1.62757291e+02 5.96868810e+01\n",
      " 1.54430638e+02 0.00000000e+00 7.28306304e-14 6.57631140e+01\n",
      " 1.99467389e+01]\n",
      "49-th iteration, loss: 0.2899123608965793, 17 gd steps\n",
      "insert gradient: -0.0006044731180738825\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.56699565  74.0667945   37.38193493  85.94958501\n",
      "  53.18861895 162.75920255  59.68671085 154.43004551  65.76693852\n",
      "  19.94673887]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5365154209643013\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.79326351   0.         735.10120886]\n",
      "1-th iteration, loss: 0.7479635372963357, 11 gd steps\n",
      "insert gradient: -0.629935558820551\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.45607731  62.35062607 233.0808711    0.         502.02033775]\n",
      "2-th iteration, loss: 0.6041408570093462, 13 gd steps\n",
      "insert gradient: -0.6488079377536331\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.55728014  77.7190542  218.75271497  41.97575678 245.88751237\n",
      "   0.         256.13282539]\n",
      "3-th iteration, loss: 0.4533194909358523, 20 gd steps\n",
      "insert gradient: -0.47971318921656037\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          50.34666105 227.57940403  55.93163505 143.23116118\n",
      "  48.73260395 190.27009886   0.          65.86272653]\n",
      "4-th iteration, loss: 0.3679040355946287, 21 gd steps\n",
      "insert gradient: -0.6945722474153982\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.79507152  51.21420814 111.33479806   0.         119.89901329\n",
      "  62.65974693 133.56601204  58.82764426 157.17153119  49.014536\n",
      "  65.86272653]\n",
      "5-th iteration, loss: 0.2414051425610868, 82 gd steps\n",
      "insert gradient: -1.373488081248325e-05\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[1.88380728e+00 4.78618775e+01 7.54371478e+01 4.19509101e+01\n",
      " 9.45463203e+01 7.92869192e+01 1.18505120e+02 0.00000000e+00\n",
      " 3.73034936e-14 8.22885996e+01 1.28252212e+02 8.28340307e+01\n",
      " 6.58627265e+01]\n",
      "6-th iteration, loss: 0.2414051422211473, 11 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.264054523245517e-06\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259297  47.86035475  75.43960072  41.95000832  94.54812601\n",
      "  79.28597337 118.50565336  82.28962274 128.25280265  82.83328733\n",
      "  65.86272653]\n",
      "7-th iteration, loss: 0.24140514222014317, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.037815172144931e-06\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259307  47.86035393  75.43960215  41.95001154  94.54812622\n",
      "  79.28597217 118.5056501   82.2896144  128.25280016  82.83328269\n",
      "  65.86272653]\n",
      "8-th iteration, loss: 0.24140514221923257, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8299276514707934e-06\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259315e+00 4.78603531e+01 7.54396035e+01 0.00000000e+00\n",
      " 2.39808173e-14 4.19500145e+01 9.45481264e+01 7.92859711e+01\n",
      " 1.18505647e+02 8.22896064e+01 1.28252798e+02 8.28332782e+01\n",
      " 6.58627265e+01]\n",
      "9-th iteration, loss: 0.24140514221833842, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4609811677014146e-06\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259321e+00 4.78603521e+01 7.54396047e+01 2.73631659e-06\n",
      " 1.18242317e-06 4.19500173e+01 9.45481266e+01 7.92859703e+01\n",
      " 1.18505644e+02 8.22895988e+01 1.28252796e+02 8.28332739e+01\n",
      " 6.58627265e+01]\n",
      "10-th iteration, loss: 0.24140514221754575, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.149114522272456e-06\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259323e+00 4.78603510e+01 7.54396057e+01 5.10382014e-06\n",
      " 2.14812083e-06 4.19500197e+01 9.45481267e+01 7.92859695e+01\n",
      " 1.18505641e+02 8.22895915e+01 1.28252794e+02 8.28332698e+01\n",
      " 6.58627265e+01]\n",
      "11-th iteration, loss: 0.24140514221683826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8850007849657438e-06\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259323e+00 4.78603498e+01 7.54396065e+01 7.15974066e-06\n",
      " 2.93110493e-06 0.00000000e+00 1.11173074e-21 4.19500217e+01\n",
      " 9.45481268e+01 7.92859689e+01 1.18505639e+02 8.22895845e+01\n",
      " 1.28252792e+02 8.28332659e+01 6.58627265e+01]\n",
      "12-th iteration, loss: 0.2414051422161775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.550565290489033e-06\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259321e+00 4.78603485e+01 7.54396072e+01 8.91699602e-06\n",
      " 3.54005067e-06 1.79953094e-06 6.08945747e-07 4.19500235e+01\n",
      " 9.45481268e+01 7.92859684e+01 1.18505636e+02 8.22895778e+01\n",
      " 1.28252790e+02 8.28332621e+01 6.58627265e+01]\n",
      "13-th iteration, loss: 0.24140514221559362, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2891401681792642e-06\n",
      "13-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259316e+00 4.78603471e+01 7.54396078e+01 1.03447445e-05\n",
      " 3.97194784e-06 3.27833791e-06 1.01488813e-06 4.19500250e+01\n",
      " 9.45481268e+01 7.92859681e+01 1.18505634e+02 8.22895714e+01\n",
      " 1.28252788e+02 8.28332584e+01 6.58627265e+01]\n",
      "14-th iteration, loss: 0.24140514221507126, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.083972506648409e-06\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259309e+00 4.78603456e+01 7.54396082e+01 1.15160885e-05\n",
      " 4.26681928e-06 4.50696960e-06 1.26247585e-06 4.19500263e+01\n",
      " 9.45481267e+01 7.92859678e+01 1.18505632e+02 8.22895653e+01\n",
      " 1.28252787e+02 8.28332549e+01 6.58627265e+01]\n",
      "15-th iteration, loss: 0.2414051422145999, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.221788860705804e-07\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259302e+00 4.78603440e+01 7.54396086e+01 1.24874135e-05\n",
      " 4.45551704e-06 5.53983522e-06 1.38616925e-06 4.19500273e+01\n",
      " 9.45481266e+01 7.92859677e+01 1.18505630e+02 8.22895594e+01\n",
      " 1.28252785e+02 8.28332515e+01 6.58627265e+01]\n",
      "16-th iteration, loss: 0.2414051422141719, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.938506348008796e-07\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259293e+00 4.78603425e+01 7.54396089e+01 1.33022455e-05\n",
      " 4.56183708e-06 6.41892949e-06 1.41258785e-06 0.00000000e+00\n",
      " 1.85288457e-22 4.19500282e+01 9.45481266e+01 7.92859676e+01\n",
      " 1.18505628e+02 8.22895538e+01 1.28252784e+02 8.28332483e+01\n",
      " 6.58627265e+01]\n",
      "17-th iteration, loss: 0.24140514221377724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.514032215275355e-07\n",
      "17-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259285e+00 4.78603410e+01 7.54396091e+01 1.39824485e-05\n",
      " 4.59761576e-06 7.16492810e-06 1.35578589e-06 4.19500298e+01\n",
      " 9.45481265e+01 7.92859677e+01 1.18505626e+02 8.22895484e+01\n",
      " 1.28252783e+02 8.28332451e+01 6.58627265e+01]\n",
      "18-th iteration, loss: 0.2414051422134207, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.77352108480204e-07\n",
      "18-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259277e+00 4.78603394e+01 7.54396093e+01 1.45405374e-05\n",
      " 4.57342884e-06 7.78932877e-06 1.22825876e-06 4.19500304e+01\n",
      " 9.45481264e+01 7.92859678e+01 1.18505624e+02 8.22895432e+01\n",
      " 1.28252782e+02 8.28332421e+01 6.58627265e+01]\n",
      "19-th iteration, loss: 0.24140514221309314, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.168338680697711e-07\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259269e+00 4.78603379e+01 7.54396094e+01 1.50302437e-05\n",
      " 4.51493520e-06 8.34499785e-06 1.05741918e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.19500310e+01 9.45481263e+01 7.92859680e+01\n",
      " 1.18505623e+02 8.22895382e+01 1.28252781e+02 8.28332392e+01\n",
      " 6.58627265e+01]\n",
      "20-th iteration, loss: 0.24140514221278972, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.398974254666863e-07\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259261e+00 4.78603365e+01 7.54396096e+01 1.54589961e-05\n",
      " 4.42616771e-06 8.83886939e-06 8.48291327e-07 4.19500320e+01\n",
      " 9.45481262e+01 7.92859683e+01 1.18505621e+02 8.22895334e+01\n",
      " 1.28252780e+02 8.28332364e+01 6.58627265e+01]\n",
      "21-th iteration, loss: 0.24140514221251197, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.0376852838059454e-07\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259255e+00 4.78603350e+01 7.54396097e+01 1.58250756e-05\n",
      " 4.30841438e-06 9.26878778e-06 6.03054533e-07 0.00000000e+00\n",
      " 1.32348898e-23 4.19500324e+01 9.45481261e+01 7.92859687e+01\n",
      " 1.18505620e+02 8.22895288e+01 1.28252779e+02 8.28332337e+01\n",
      " 6.58627265e+01]\n",
      "22-th iteration, loss: 0.24140514221225404, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.615960606120244e-07\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259249e+00 4.78603336e+01 7.54396098e+01 1.61570625e-05\n",
      " 4.17497390e-06 9.66291511e-06 3.35929899e-07 4.19500332e+01\n",
      " 9.45481261e+01 7.92859691e+01 1.18505619e+02 8.22895244e+01\n",
      " 1.28252779e+02 8.28332311e+01 6.58627265e+01]\n",
      "23-th iteration, loss: 0.24140514221201662, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.185170929336474e-07\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259243e+00 4.78603323e+01 7.54396098e+01 1.64483515e-05\n",
      " 4.02384598e-06 1.00144199e-05 4.54333310e-08 4.19500336e+01\n",
      " 9.45481260e+01 7.92859696e+01 1.18505618e+02 8.22895201e+01\n",
      " 1.28252778e+02 8.28332286e+01 6.58627265e+01]\n",
      "24-th iteration, loss: 0.24140514221179715, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.709020333684209e-07\n",
      "24-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259239e+00 4.78603310e+01 7.54396099e+01 1.67235069e-05\n",
      " 3.86657798e-06 4.19500443e+01 9.45481260e+01 7.92859701e+01\n",
      " 1.18505617e+02 8.22895160e+01 1.28252777e+02 8.28332261e+01\n",
      " 6.58627265e+01]\n",
      "25-th iteration, loss: 0.24140514221159434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.198102721098876e-07\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259235e+00 4.78603297e+01 7.54396100e+01 1.69772400e-05\n",
      " 3.70107695e-06 4.19500446e+01 9.45481260e+01 0.00000000e+00\n",
      " 1.24344979e-14 7.92859707e+01 1.18505615e+02 8.22895120e+01\n",
      " 1.28252777e+02 8.28332238e+01 6.58627265e+01]\n",
      "26-th iteration, loss: 0.2414051422114022, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.495740140253613e-07\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259233e+00 4.78603285e+01 7.54396101e+01 1.72224015e-05\n",
      " 3.53356696e-06 4.19500449e+01 9.45481260e+01 6.32795722e-07\n",
      " 3.29715765e-10 7.92859713e+01 1.18505615e+02 8.22895082e+01\n",
      " 1.28252776e+02 8.28332215e+01 6.58627265e+01]\n",
      "27-th iteration, loss: 0.24140514221122233, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.753092500541655e-07\n",
      "27-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603274e+01 7.54396102e+01 1.74590538e-05\n",
      " 3.36465011e-06 4.19500452e+01 9.45481260e+01 1.29399215e-06\n",
      " 2.47072027e-09 7.92859720e+01 1.18505614e+02 8.22895045e+01\n",
      " 1.28252776e+02 8.28332193e+01 6.58627265e+01]\n",
      "28-th iteration, loss: 0.2414051422110536, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.97367724503152e-07\n",
      "28-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603263e+01 7.54396102e+01 1.76868870e-05\n",
      " 3.19415012e-06 4.19500454e+01 9.45481260e+01 1.97965379e-06\n",
      " 4.53413430e-09 7.92859727e+01 1.18505613e+02 8.22895009e+01\n",
      " 1.28252776e+02 8.28332172e+01 6.58627265e+01]\n",
      "29-th iteration, loss: 0.2414051422108952, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.160717212651321e-07\n",
      "29-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603252e+01 7.54396103e+01 1.79057692e-05\n",
      " 3.02197945e-06 4.19500457e+01 9.45481261e+01 2.68621242e-06\n",
      " 4.86148000e-09 7.92859734e+01 1.18505612e+02 8.22894974e+01\n",
      " 1.28252776e+02 8.28332151e+01 6.58627265e+01]\n",
      "30-th iteration, loss: 0.24140514221074624, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.317166744588034e-07\n",
      "30-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259231e+00 4.78603242e+01 7.54396104e+01 1.81157081e-05\n",
      " 2.84811842e-06 4.19500460e+01 9.45481261e+01 3.41043555e-06\n",
      " 1.99774262e-09 7.92859741e+01 1.18505611e+02 8.22894941e+01\n",
      " 1.28252775e+02 8.28332132e+01 6.58627265e+01]\n",
      "31-th iteration, loss: 0.24140514221060594, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.445339955114505e-07\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259233e+00 4.78603232e+01 7.54396105e+01 1.83168198e-05\n",
      " 2.67259830e-06 4.19500462e+01 9.45481261e+01 7.92859790e+01\n",
      " 1.18505611e+02 8.22894908e+01 1.28252775e+02 8.28332112e+01\n",
      " 6.58627265e+01]\n",
      "32-th iteration, loss: 0.2414051422104785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.725297704408164e-07\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259235e+00 4.78603222e+01 7.54396106e+01 1.85114946e-05\n",
      " 2.49626514e-06 4.19500464e+01 9.45481262e+01 7.92859797e+01\n",
      " 1.18505610e+02 8.22894876e+01 1.28252775e+02 8.28332094e+01\n",
      " 6.58627265e+01]\n",
      "33-th iteration, loss: 0.24140514221035883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.977318732636531e-07\n",
      "33-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259238e+00 4.78603213e+01 7.54396107e+01 1.86989560e-05\n",
      " 2.31797023e-06 4.19500467e+01 9.45481262e+01 7.92859805e+01\n",
      " 1.18505609e+02 8.22894846e+01 1.28252775e+02 8.28332076e+01\n",
      " 6.58627265e+01]\n",
      "34-th iteration, loss: 0.2414051422102463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.203188588626633e-07\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259242e+00 4.78603204e+01 7.54396108e+01 1.88791837e-05\n",
      " 2.13767898e-06 4.19500469e+01 9.45481263e+01 7.92859813e+01\n",
      " 1.18505609e+02 8.22894816e+01 1.28252775e+02 8.28332058e+01\n",
      " 6.58627265e+01]\n",
      "35-th iteration, loss: 0.2414051422101403, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.404585563749972e-07\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259246e+00 4.78603196e+01 7.54396108e+01 1.90522299e-05\n",
      " 1.95539528e-06 4.19500471e+01 9.45481264e+01 0.00000000e+00\n",
      " 3.55271368e-14 7.92859822e+01 1.18505608e+02 8.22894787e+01\n",
      " 1.28252775e+02 8.28332041e+01 6.58627265e+01]\n",
      "36-th iteration, loss: 0.24140514221003434, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.378341641290647e-07\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259251e+00 4.78603188e+01 7.54396109e+01 1.92151056e-05\n",
      " 1.76995987e-06 4.19500473e+01 9.45481264e+01 8.42529911e-07\n",
      " 7.45849818e-08 7.92859830e+01 1.18505608e+02 8.22894759e+01\n",
      " 1.28252775e+02 8.28332024e+01 6.58627265e+01]\n",
      "37-th iteration, loss: 0.24140514220993367, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.340421285901981e-07\n",
      "37-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259256e+00 4.78603180e+01 7.54396110e+01 1.93684731e-05\n",
      " 1.58256061e-06 4.19500475e+01 9.45481265e+01 1.68099089e-06\n",
      " 1.36212682e-07 7.92859838e+01 1.18505608e+02 8.22894732e+01\n",
      " 1.28252775e+02 8.28332008e+01 6.58627265e+01]\n",
      "38-th iteration, loss: 0.24140514220983805, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.291965498671701e-07\n",
      "38-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259262e+00 4.78603172e+01 7.54396111e+01 1.95132244e-05\n",
      " 1.39359948e-06 4.19500476e+01 9.45481266e+01 2.51442879e-06\n",
      " 1.84800351e-07 7.92859847e+01 1.18505607e+02 8.22894706e+01\n",
      " 1.28252775e+02 8.28331993e+01 6.58627265e+01]\n",
      "39-th iteration, loss: 0.24140514220974713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.234027710764139e-07\n",
      "39-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259268e+00 4.78603165e+01 7.54396112e+01 1.96501586e-05\n",
      " 1.20342140e-06 4.19500478e+01 9.45481267e+01 3.34200254e-06\n",
      " 2.20289732e-07 0.00000000e+00 4.30133919e-23 7.92859855e+01\n",
      " 1.18505607e+02 8.22894680e+01 1.28252776e+02 8.28331978e+01\n",
      " 6.58627265e+01]\n",
      "40-th iteration, loss: 0.2414051422096551, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.974026542471645e-07\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259275e+00 4.78603158e+01 7.54396113e+01 1.97773349e-05\n",
      " 1.01133913e-06 4.19500479e+01 9.45481267e+01 4.15728766e-06\n",
      " 2.40297098e-07 8.18349804e-07 2.00073662e-08 7.92859863e+01\n",
      " 1.18505607e+02 8.22894655e+01 1.28252776e+02 8.28331963e+01\n",
      " 6.58627265e+01]\n",
      "41-th iteration, loss: 0.24140514220956757, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.726934523263347e-07\n",
      "41-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259282e+00 4.78603151e+01 7.54396114e+01 1.98966813e-05\n",
      " 8.19031379e-07 4.19500481e+01 9.45481268e+01 4.94637728e-06\n",
      " 2.41581537e-07 1.61078245e-06 9.90687729e-09 7.92859871e+01\n",
      " 1.18505606e+02 8.22894631e+01 1.28252776e+02 8.28331948e+01\n",
      " 6.58627265e+01]\n",
      "42-th iteration, loss: 0.2414051422094843, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.489346163995599e-07\n",
      "42-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259289e+00 4.78603145e+01 7.54396115e+01 2.00097232e-05\n",
      " 6.27076391e-07 4.19500482e+01 9.45481268e+01 5.71119421e-06\n",
      " 2.25447134e-07 0.00000000e+00 2.31610572e-23 7.92859903e+01\n",
      " 1.18505606e+02 8.22894607e+01 1.28252776e+02 8.28331934e+01\n",
      " 6.58627265e+01]\n",
      "43-th iteration, loss: 0.24140514220940498, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.260059834946482e-07\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259297e+00 4.78603138e+01 7.54396116e+01 2.01170449e-05\n",
      " 4.35591644e-07 4.19500483e+01 9.45481269e+01 6.45281346e-06\n",
      " 1.92626607e-07 0.00000000e+00 7.27918939e-23 7.92859918e+01\n",
      " 1.18505606e+02 8.22894584e+01 1.28252777e+02 8.28331921e+01\n",
      " 6.58627265e+01]\n",
      "44-th iteration, loss: 0.2414051422093294, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.039339342378633e-07\n",
      "44-th iteration, new layer inserted. now 15 layers\n",
      "[1.88259306e+00 4.78603132e+01 7.54396117e+01 2.02193310e-05\n",
      " 2.44756171e-07 4.19500484e+01 9.45481269e+01 7.17220719e-06\n",
      " 1.43848418e-07 7.92859932e+01 1.18505605e+02 8.22894561e+01\n",
      " 1.28252777e+02 8.28331907e+01 6.58627265e+01]\n",
      "45-th iteration, loss: 0.24140514220926138, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.991226630206466e-07\n",
      "45-th iteration, new layer inserted. now 17 layers\n",
      "[1.88259315e+00 4.78603127e+01 7.54396118e+01 2.03189851e-05\n",
      " 5.52995122e-08 4.19500485e+01 9.45481270e+01 7.87507469e-06\n",
      " 8.16139864e-08 0.00000000e+00 2.15066959e-23 7.92859939e+01\n",
      " 1.18505605e+02 8.22894539e+01 1.28252777e+02 8.28331894e+01\n",
      " 6.58627265e+01]\n",
      "46-th iteration, loss: 0.24140514220919262, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.772199115005901e-07\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[1.88259324e+00 4.78603121e+01 7.54396119e+01 4.19500690e+01\n",
      " 9.45481270e+01 8.56912208e-06 7.72061969e-09 7.92859953e+01\n",
      " 1.18505605e+02 8.22894517e+01 1.28252778e+02 8.28331881e+01\n",
      " 6.58627265e+01]\n",
      "47-th iteration, loss: 0.24140514220913087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.716673465306043e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259334  47.86031155  75.43961199  41.95006911  94.54812702\n",
      "  79.28600522 118.50560476  82.28944959 128.25277811  82.83318688\n",
      "  65.86272653]\n",
      "48-th iteration, loss: 0.24140514220907555, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.81273381878295e-07\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259344  47.86031103  75.4396121   41.95006919  94.54812706\n",
      "  79.2860059  118.50560457  82.28944752 128.25277853  82.83318567\n",
      "  65.86272653]\n",
      "49-th iteration, loss: 0.24140514220902273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.895065718779149e-07\n",
      "49-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88259354  47.86031052  75.4396122   41.95006927  94.54812709\n",
      "  79.28600658 118.5056044   82.28944551 128.25277897  82.83318449\n",
      "  65.86272653]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535590565167538\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.20594943   0.         758.81415108]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6298257046951171\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.47534951  62.25781836 231.34577777   0.         527.46837331]\n",
      "2-th iteration, loss: 0.6028357090092895, 13 gd steps\n",
      "insert gradient: -0.640428788151959\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.26457513  77.72398141 217.18307533  42.67576272 236.82253496\n",
      "   0.         290.64583835]\n",
      "3-th iteration, loss: 0.4680449105790495, 20 gd steps\n",
      "insert gradient: -0.5340526710477517\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          58.98204479 223.57435155  56.64604459 162.34065052\n",
      "  48.42104135 207.60417025   0.          83.0416681 ]\n",
      "4-th iteration, loss: 0.3637895354020658, 18 gd steps\n",
      "insert gradient: -0.7955197611880737\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.7281548   50.80577965 123.05263203   0.         114.26315831\n",
      "  60.7481694  141.170654    57.7880987  162.7244079   50.24138847\n",
      "  83.0416681 ]\n",
      "5-th iteration, loss: 0.30548107477298586, 15 gd steps\n",
      "insert gradient: -0.18879097559312943\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  2.25223407  68.0327811   87.34479274  27.92415681  81.59003782\n",
      "  61.42249859 161.07959783  50.49366898 148.33443595  67.22317974\n",
      "  83.0416681 ]\n",
      "6-th iteration, loss: 0.29146927491542046, 16 gd steps\n",
      "insert gradient: -0.03201606158015947\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  0.71241323  67.66859156  82.77964067  33.56202263  83.68691251\n",
      "  55.09552583 159.87988901  59.90477305 150.84290069  63.2906816\n",
      "  83.0416681 ]\n",
      "7-th iteration, loss: 0.2905474449430984, 14 gd steps\n",
      "insert gradient: -0.01527595102285939\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 6.98699132e+01 7.76802943e+01 0.00000000e+00\n",
      " 7.99360578e-15 3.48750371e+01 8.54454574e+01 5.51435256e+01\n",
      " 1.60686190e+02 5.92378827e+01 1.53303942e+02 6.34323660e+01\n",
      " 8.30416681e+01]\n",
      "8-th iteration, loss: 0.2899473158174556, 58 gd steps\n",
      "insert gradient: -0.0021037439461637987\n",
      "8-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          70.99180622  75.34360543  36.92132222  85.77370893\n",
      "  53.32604567 162.19987325  59.75455541 154.53873511  65.0733081\n",
      "  83.0416681 ]\n",
      "9-th iteration, loss: 0.2899162529362811, 77 gd steps\n",
      "insert gradient: -0.0008946947704866726\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14448165e+01 7.43428006e+01 3.72719331e+01\n",
      " 8.59389093e+01 5.32074753e+01 1.62681003e+02 5.96822982e+01\n",
      " 1.54458874e+02 0.00000000e+00 5.32907052e-15 6.56242248e+01\n",
      " 8.30416681e+01]\n",
      "10-th iteration, loss: 0.2899160267531113, 44 gd steps\n",
      "insert gradient: -0.0008136837581369243\n",
      "10-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.45009695  74.3334231   37.27659057  85.93973387\n",
      "  53.20620391 162.68586498  59.68217487 154.45776828  65.63695619\n",
      "  83.0416681 ]\n",
      "11-th iteration, loss: 0.2899158961675684, 31 gd steps\n",
      "insert gradient: -0.0008189830161628703\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14539221e+01 7.43268473e+01 3.72798542e+01\n",
      " 8.59402663e+01 5.32052493e+01 1.62689295e+02 5.96820174e+01\n",
      " 1.54456923e+02 0.00000000e+00 3.01980663e-14 6.56412796e+01\n",
      " 8.30416681e+01]\n",
      "12-th iteration, loss: 0.28991570977744796, 38 gd steps\n",
      "insert gradient: -0.0007626610963282614\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.45863454  74.31874124  37.28381025  85.94086714\n",
      "  53.20403228 162.69351358  59.68176819 154.45581601  65.65166857\n",
      "  83.0416681 ]\n",
      "13-th iteration, loss: 0.28991559319944266, 29 gd steps\n",
      "insert gradient: -0.0007708487123257304\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14621759e+01 7.43126779e+01 3.72867622e+01\n",
      " 8.59412998e+01 5.32031404e+01 1.62696697e+02 5.96815862e+01\n",
      " 1.54454949e+02 0.00000000e+00 4.08562073e-14 6.56554681e+01\n",
      " 8.30416681e+01]\n",
      "14-th iteration, loss: 0.28991543306952317, 35 gd steps\n",
      "insert gradient: -0.000724896272019905\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.46636886  74.30541935  37.29024095  85.94178403\n",
      "  53.2020714  162.70050187  59.68135408 154.45387108  65.6643877\n",
      "  83.0416681 ]\n",
      "15-th iteration, loss: 0.2899153275839849, 27 gd steps\n",
      "insert gradient: -0.0007334302026060037\n",
      "15-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14696377e+01 7.42997642e+01 3.72929497e+01\n",
      " 8.59421526e+01 5.32012627e+01 1.62703488e+02 5.96811862e+01\n",
      " 1.54453003e+02 0.00000000e+00 4.79616347e-14 6.56678007e+01\n",
      " 8.30416681e+01]\n",
      "16-th iteration, loss: 0.289915186647475, 32 gd steps\n",
      "insert gradient: -0.00069390446043002\n",
      "16-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47341839  74.29314026  37.29607854  85.94255962\n",
      "  53.20031776 162.70697825  59.68098344 154.45195689  65.67566752\n",
      "  83.0416681 ]\n",
      "17-th iteration, loss: 0.2899150901126489, 26 gd steps\n",
      "insert gradient: -0.0007023116613443414\n",
      "17-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.47645603  74.28782069  37.29859406  85.94288127\n",
      "  53.19957954 162.70979777  59.68083303 154.45109448  65.6787791\n",
      "  83.0416681 ]\n",
      "18-th iteration, loss: 0.2899149974707947, 25 gd steps\n",
      "insert gradient: -0.0007059960307738558\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.4792854   74.2826204   37.30098849  85.94319118\n",
      "  53.19891861 162.71255079  59.68075074 154.45027101  65.68184722\n",
      "  83.0416681 ]\n",
      "19-th iteration, loss: 0.28991490816232, 24 gd steps\n",
      "insert gradient: -0.0007069689192529621\n",
      "19-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14819657e+01 7.42775345e+01 3.73032876e+01\n",
      " 8.59434877e+01 5.31983013e+01 1.62715234e+02 5.96807022e+01\n",
      " 1.54449474e+02 0.00000000e+00 4.08562073e-14 6.56848654e+01\n",
      " 8.30416681e+01]\n",
      "20-th iteration, loss: 0.2899147903621449, 29 gd steps\n",
      "insert gradient: -0.0006684142022091772\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14850947e+01 7.42716929e+01 3.73059333e+01\n",
      " 8.59438068e+01 5.31975396e+01 1.62718294e+02 5.96805860e+01\n",
      " 1.54448520e+02 0.00000000e+00 4.08562073e-14 6.56916790e+01\n",
      " 8.30416681e+01]\n",
      "21-th iteration, loss: 0.28991467972108953, 28 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.48823003  74.26613058  37.30851618  85.94409536\n",
      "  53.19674878 162.72121286  59.68039179 154.44754241  65.69793873\n",
      "  83.0416681 ]\n",
      "22-th iteration, loss: 0.28991459928119845, 23 gd steps\n",
      "insert gradient: -0.0006481394727698176\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.14908593e+01 7.42614477e+01 3.73106944e+01\n",
      " 8.59443395e+01 5.31961114e+01 1.62723684e+02 5.96802511e+01\n",
      " 1.54446708e+02 0.00000000e+00 3.01980663e-14 6.57005472e+01\n",
      " 8.30416681e+01]\n",
      "23-th iteration, loss: 0.28991449769703276, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49376151  74.25620114  37.31310674  85.94460129\n",
      "  53.19540599 162.72644557  59.68010031 154.44576249  65.70633654\n",
      "  83.0416681 ]\n",
      "24-th iteration, loss: 0.28991442243174265, 22 gd steps\n",
      "insert gradient: -0.0006285144375772933\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49624152  74.25172131  37.31517361  85.9448246\n",
      "  53.19481939 162.7288131   59.67998404 154.44494328  65.70878427\n",
      "  83.0416681 ]\n",
      "25-th iteration, loss: 0.2899143495491773, 22 gd steps\n",
      "insert gradient: -0.000632815180395195\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.49858143  74.24731521  37.31716339  85.94504319\n",
      "  53.19428603 162.73113779  59.67991849 154.4441548   65.71121678\n",
      "  83.0416681 ]\n",
      "26-th iteration, loss: 0.28991427878794507, 21 gd steps\n",
      "insert gradient: -0.0006349685672988097\n",
      "26-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50081698  74.24298359  37.31908999  85.94525517\n",
      "  53.1937842  162.73341509  59.67988099 154.4433895   65.71362679\n",
      "  83.0416681 ]\n",
      "27-th iteration, loss: 0.2899142099454693, 21 gd steps\n",
      "insert gradient: -0.0006357531722864722\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50297152  74.23872427  37.32096384  85.94546012\n",
      "  53.19330188 162.73564436  59.67985862 154.44264231  65.71601128\n",
      "  83.0416681 ]\n",
      "28-th iteration, loss: 0.2899141428941931, 21 gd steps\n",
      "insert gradient: -0.0006356440688609367\n",
      "28-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15050597e+01 7.42345362e+01 3.73227918e+01\n",
      " 8.59456582e+01 5.31928326e+01 1.62737826e+02 5.96798440e+01\n",
      " 1.54441910e+02 0.00000000e+00 3.01980663e-14 6.57183681e+01\n",
      " 8.30416681e+01]\n",
      "29-th iteration, loss: 0.2899140581659976, 24 gd steps\n",
      "insert gradient: -0.000605837168624865\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15074190e+01 7.42299022e+01 3.73248258e+01\n",
      " 8.59458642e+01 5.31922703e+01 1.62740221e+02 5.96797807e+01\n",
      " 1.54441073e+02 0.00000000e+00 2.30926389e-14 6.57235117e+01\n",
      " 8.30416681e+01]\n",
      "30-th iteration, loss: 0.2899139772170939, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.50981814  74.22543169  37.32684065  85.94605415\n",
      "  53.19167521 162.74253186  59.67965592 154.44021956  65.72832731\n",
      "  83.0416681 ]\n",
      "31-th iteration, loss: 0.2899139151901269, 20 gd steps\n",
      "insert gradient: -0.0005893594411983714\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51193568  74.22150459  37.32862284  85.9462234\n",
      "  53.19116626 162.74457025  59.67955624 154.43945999  65.73042534\n",
      "  83.0416681 ]\n",
      "32-th iteration, loss: 0.28991385480065235, 20 gd steps\n",
      "insert gradient: -0.000593215997597732\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51395125  74.2176295   37.33035108  85.9463914\n",
      "  53.19070031 162.74657956  59.67949762 154.43872462  65.73251672\n",
      "  83.0416681 ]\n",
      "33-th iteration, loss: 0.2899137958825479, 20 gd steps\n",
      "insert gradient: -0.0005953462508193789\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51588988  74.21380705  37.33203445  85.94655626\n",
      "  53.19026103 162.74855616  59.67946345 154.43800802  65.73459608\n",
      "  83.0416681 ]\n",
      "34-th iteration, loss: 0.2899137383490367, 19 gd steps\n",
      "insert gradient: -0.0005963336092520295\n",
      "34-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15177670e+01 7.42100388e+01 3.73336787e+01\n",
      " 8.59467171e+01 5.31898391e+01 1.62750498e+02 5.96794437e+01\n",
      " 1.54437307e+02 0.00000000e+00 2.30926389e-14 6.57366592e+01\n",
      " 8.30416681e+01]\n",
      "35-th iteration, loss: 0.2899136671724674, 21 gd steps\n",
      "insert gradient: -0.0005713352866261891\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.51984132  74.2059365   37.33547393  85.94688214\n",
      "  53.18934986 162.75259726  59.67939047 154.43652556  65.74110043\n",
      "  83.0416681 ]\n",
      "36-th iteration, loss: 0.2899136121952745, 19 gd steps\n",
      "insert gradient: -0.0005751557226816843\n",
      "36-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52172156  74.20230104  37.33708857  85.94702866\n",
      "  53.18890839 162.7544615   59.67933176 154.43581799  65.74304795\n",
      "  83.0416681 ]\n",
      "37-th iteration, loss: 0.2899135584611548, 19 gd steps\n",
      "insert gradient: -0.0005773452562294527\n",
      "37-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52353321  74.19871017  37.33866465  85.94717356\n",
      "  53.18849343 162.75629903  59.67929736 154.43512767  65.74498709\n",
      "  83.0416681 ]\n",
      "38-th iteration, loss: 0.28991350588434533, 19 gd steps\n",
      "insert gradient: -0.0005784403372581874\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52529066  74.19516447  37.34020738  85.94731582\n",
      "  53.18809579 162.7581078   59.67927779 154.43445148  65.74691446\n",
      "  83.0416681 ]\n",
      "39-th iteration, loss: 0.2899134544199846, 18 gd steps\n",
      "insert gradient: -0.0005787887951359445\n",
      "39-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15270030e+01 7.41916653e+01 3.73417201e+01\n",
      " 8.59474549e+01 5.31877101e+01 1.62759886e+02 5.96792672e+01\n",
      " 1.54433788e+02 0.00000000e+00 3.01980663e-14 6.57488273e+01\n",
      " 8.30416681e+01]\n",
      "40-th iteration, loss: 0.2899133911483969, 20 gd steps\n",
      "insert gradient: -0.0005552915579641917\n",
      "40-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.52888495  74.18788544  37.34336096  85.94759649\n",
      "  53.18726547 162.76179504  59.67922569 154.43305362  65.75291406\n",
      "  83.0416681 ]\n",
      "41-th iteration, loss: 0.28991334177065603, 18 gd steps\n",
      "insert gradient: -0.0005584638304820247\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53061281  74.18450099  37.34485407  85.94772388\n",
      "  53.18685901 162.76350747  59.67917656 154.43238224  65.75472516\n",
      "  83.0416681 ]\n",
      "42-th iteration, loss: 0.2899132934088341, 18 gd steps\n",
      "insert gradient: -0.0005602821918182003\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53228434  74.18115412  37.34631552  85.94785028\n",
      "  53.18647521 162.7651978   59.67914807 154.43172558  65.7565296\n",
      "  83.0416681 ]\n",
      "43-th iteration, loss: 0.2899132460015321, 18 gd steps\n",
      "insert gradient: -0.0005611755803928601\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15339109e+01 7.41778455e+01 3.73477493e+01\n",
      " 8.59479748e+01 5.31861066e+01 1.62766864e+02 5.96791326e+01\n",
      " 1.54431081e+02 0.00000000e+00 1.42108547e-14 6.57583245e+01\n",
      " 8.30416681e+01]\n",
      "44-th iteration, loss: 0.28991318817547373, 19 gd steps\n",
      "insert gradient: -0.0005396296857333447\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15356805e+01 7.41742947e+01 3.73492920e+01\n",
      " 8.59481009e+01 5.31856891e+01 1.62768642e+02 5.96790925e+01\n",
      " 1.54430376e+02 0.00000000e+00 3.01980663e-14 6.57621388e+01\n",
      " 8.30416681e+01]\n",
      "45-th iteration, loss: 0.2899131321879292, 19 gd steps\n",
      "insert gradient: -0.0005222507315353482\n",
      "45-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53748744  74.17083096  37.35083141  85.94821871\n",
      "  53.18524892 162.77037671  59.67901435 154.42965944  65.76576925\n",
      "  83.0416681 ]\n",
      "46-th iteration, loss: 0.28991308745317146, 17 gd steps\n",
      "insert gradient: -0.0005273053264491935\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.53914627  74.16767554  37.35224511  85.94832836\n",
      "  53.18485642 162.77196325  59.67894936 154.42900033  65.76741221\n",
      "  83.0416681 ]\n",
      "47-th iteration, loss: 0.2899130436210481, 17 gd steps\n",
      "insert gradient: -0.000530568694680669\n",
      "47-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15407413e+01 7.41645490e+01 3.73536259e+01\n",
      " 8.59484382e+01 5.31844926e+01 1.62773535e+02 5.96789129e+01\n",
      " 1.54428358e+02 0.00000000e+00 1.77635684e-15 6.57690557e+01\n",
      " 8.30416681e+01]\n",
      "48-th iteration, loss: 0.28991299105208357, 18 gd steps\n",
      "insert gradient: -0.0005128064281306106\n",
      "48-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          71.54244042  74.16122079  37.35509077  85.94854913\n",
      "  53.18409661 162.77520014  59.67886574 154.42766737  65.77253026\n",
      "  83.0416681 ]\n",
      "49-th iteration, loss: 0.2899129488151653, 17 gd steps\n",
      "insert gradient: -0.0005169769284391468\n",
      "49-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.15440170e+01 7.41581748e+01 3.73564451e+01\n",
      " 8.59486517e+01 5.31837345e+01 1.62776729e+02 5.96788214e+01\n",
      " 1.54427028e+02 0.00000000e+00 2.30926389e-14 6.57741086e+01\n",
      " 8.30416681e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5353880465751923\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.28873282   0.         785.85699582]\n",
      "1-th iteration, loss: 0.7469808567750643, 11 gd steps\n",
      "insert gradient: -0.6228908420477111\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.09207687  62.36451321 239.59054751   0.         546.26644832]\n",
      "2-th iteration, loss: 0.6073428866251602, 13 gd steps\n",
      "insert gradient: -0.7003321457777142\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.46951943  77.07216638 224.12065477  39.56862861 245.262487\n",
      "   0.         301.00396132]\n",
      "3-th iteration, loss: 0.45896331676869556, 18 gd steps\n",
      "insert gradient: -0.5174613944140584\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          35.30152593 230.8995637   63.40011918 137.39758525\n",
      "  51.1576633  172.00226361   0.         129.00169771]\n",
      "4-th iteration, loss: 0.38329758343224213, 14 gd steps\n",
      "insert gradient: -0.6453517420931537\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          43.85258274 117.55103545   0.         109.15453292\n",
      "  70.42379358 132.86630111  47.88472485 148.32266905  57.51868436\n",
      " 129.00169771]\n",
      "5-th iteration, loss: 0.241405142221727, 94 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.917490668006998e-06\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88266746  47.85989307  75.43979642  41.95022222  94.54812524\n",
      "  79.28616465 118.50575615  82.28921178 128.25339733  82.83259567\n",
      " 129.00169771]\n",
      "6-th iteration, loss: 0.2414051422210046, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.667039411447256e-06\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88266893  47.85989994  75.43979711  41.95021985  94.54812416\n",
      "  79.28616185 118.50575597  82.28921359 128.25339956  82.83259958\n",
      " 129.00169771]\n",
      "7-th iteration, loss: 0.24140514222032075, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.432972199702665e-06\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[1.88267035e+00 0.00000000e+00 9.15933995e-16 4.78599066e+01\n",
      " 7.54397977e+01 4.19502174e+01 9.45481230e+01 7.92861591e+01\n",
      " 1.18505756e+02 8.22892153e+01 1.28253402e+02 8.28326034e+01\n",
      " 1.29001698e+02]\n",
      "8-th iteration, loss: 0.24140514221934206, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.9354788511280365e-06\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267170e+00 6.32966921e-06 1.35100398e-06 0.00000000e+00\n",
      " 5.02925812e-22 4.78599129e+01 7.54397982e+01 4.19502148e+01\n",
      " 9.45481219e+01 7.92861563e+01 1.18505755e+02 8.22892170e+01\n",
      " 1.28253404e+02 8.28326071e+01 1.29001698e+02]\n",
      "9-th iteration, loss: 0.24140514221820467, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.245596458467507e-06\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267293e+00 1.21026107e-05 2.49659034e-06 5.79044493e-06\n",
      " 1.14558636e-06 4.78599187e+01 7.54397984e+01 4.19502120e+01\n",
      " 9.45481206e+01 7.92861536e+01 1.18505755e+02 8.22892185e+01\n",
      " 1.28253406e+02 8.28326107e+01 1.29001698e+02]\n",
      "10-th iteration, loss: 0.24140514221724657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.669298204339548e-06\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[1.88267400e+00 1.71841978e-05 3.41605092e-06 1.09043776e-05\n",
      " 1.99002645e-06 4.78599238e+01 7.54397983e+01 4.19502087e+01\n",
      " 9.45481193e+01 7.92861509e+01 1.18505755e+02 8.22892200e+01\n",
      " 1.28253408e+02 8.28326142e+01 1.29001698e+02]\n",
      "11-th iteration, loss: 0.24140514221642018, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.186423430468257e-06\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267495e+00 2.16896959e-05 4.14345894e-06 1.54541338e-05\n",
      " 2.57615829e-06 0.00000000e+00 1.58818678e-22 4.78599284e+01\n",
      " 7.54397981e+01 4.19502053e+01 9.45481178e+01 7.92861483e+01\n",
      " 1.18505754e+02 8.22892214e+01 1.28253409e+02 8.28326176e+01\n",
      " 1.29001698e+02]\n",
      "12-th iteration, loss: 0.24140514221556972, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6093684020535283e-06\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267579e+00 2.56755442e-05 4.69822590e-06 1.94936644e-05\n",
      " 2.93070282e-06 4.07290705e-06 3.54544531e-07 0.00000000e+00\n",
      " 7.27918939e-23 4.78599325e+01 7.54397976e+01 4.19502015e+01\n",
      " 9.45481162e+01 7.92861457e+01 1.18505754e+02 8.22892227e+01\n",
      " 1.28253411e+02 8.28326208e+01 1.29001698e+02]\n",
      "13-th iteration, loss: 0.24140514221475942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.097966093563517e-06\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[1.88267649e+00 2.90640279e-05 5.07083753e-06 2.29430180e-05\n",
      " 3.05075620e-06 7.56023055e-06 4.21829665e-07 3.49191695e-06\n",
      " 6.72851342e-08 4.78599359e+01 7.54397968e+01 4.19501976e+01\n",
      " 9.45481146e+01 7.92861431e+01 1.18505753e+02 8.22892240e+01\n",
      " 1.28253413e+02 0.00000000e+00 2.48689958e-14 8.28326240e+01\n",
      " 1.29001698e+02]\n",
      "14-th iteration, loss: 0.241405142213987, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9526783533959085e-06\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267707e+00 3.18714003e-05 5.27356638e-06 2.58160877e-05\n",
      " 2.95623727e-06 1.04728256e-05 2.29361011e-07 4.78599453e+01\n",
      " 7.54397959e+01 4.19501933e+01 9.45481129e+01 7.92861405e+01\n",
      " 1.18505752e+02 8.22892251e+01 1.28253414e+02 3.05795360e-06\n",
      " 1.53796719e-06 8.28326271e+01 1.29001698e+02]\n",
      "15-th iteration, loss: 0.24140514221336137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.8287151981032474e-06\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267756e+00 3.42589315e-05 5.34973022e-06 2.82719427e-05\n",
      " 2.69793011e-06 4.78599607e+01 7.54397948e+01 4.19501890e+01\n",
      " 9.45481111e+01 7.92861380e+01 1.18505752e+02 8.22892262e+01\n",
      " 1.28253416e+02 5.97059765e-06 2.95593145e-06 8.28326300e+01\n",
      " 1.29001698e+02]\n",
      "16-th iteration, loss: 0.2414051422128309, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7203413368490455e-06\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[1.88267800e+00 3.64219528e-05 5.34670221e-06 3.05042747e-05\n",
      " 2.32861325e-06 4.78599630e+01 7.54397936e+01 4.19501847e+01\n",
      " 9.45481094e+01 7.92861355e+01 1.18505751e+02 8.22892272e+01\n",
      " 1.28253417e+02 8.75758990e-06 4.27191406e-06 8.28326328e+01\n",
      " 1.29001698e+02]\n",
      "17-th iteration, loss: 0.2414051422123368, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.6182176346775023e-06\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267842e+00 3.84695348e-05 5.29053903e-06 3.26211280e-05\n",
      " 1.87723933e-06 4.78599652e+01 7.54397925e+01 4.19501805e+01\n",
      " 9.45481077e+01 7.92861331e+01 1.18505750e+02 8.22892280e+01\n",
      " 1.28253418e+02 1.14314760e-05 5.49715777e-06 0.00000000e+00\n",
      " 1.16467030e-21 8.28326355e+01 1.29001698e+02]\n",
      "18-th iteration, loss: 0.24140514221181295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.478289738731962e-06\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[1.88267881e+00 4.04126891e-05 5.18526268e-06 3.46328261e-05\n",
      " 1.34932648e-06 4.78599672e+01 7.54397914e+01 4.19501764e+01\n",
      " 9.45481060e+01 7.92861307e+01 1.18505749e+02 8.22892288e+01\n",
      " 1.28253420e+02 1.39868883e-05 6.62968924e-06 2.58261069e-06\n",
      " 1.13253147e-06 8.28326381e+01 1.29001698e+02]\n",
      "19-th iteration, loss: 0.24140514221133166, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.34881703176118e-06\n",
      "19-th iteration, new layer inserted. now 21 layers\n",
      "[1.88267918e+00 4.22586680e-05 5.03426495e-06 3.65459847e-05\n",
      " 7.49628882e-07 4.78599691e+01 7.54397903e+01 4.19501724e+01\n",
      " 9.45481044e+01 7.92861283e+01 1.18505748e+02 8.22892295e+01\n",
      " 1.28253421e+02 1.63940771e-05 7.65443649e-06 5.02260140e-06\n",
      " 2.14450068e-06 0.00000000e+00 6.88214270e-22 8.28326405e+01\n",
      " 1.29001698e+02]\n",
      "20-th iteration, loss: 0.24140514221084072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1910362338360028e-06\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[1.88267954e+00 4.40172822e-05 4.84096731e-06 3.83698225e-05\n",
      " 8.28446689e-08 4.78599710e+01 7.54397892e+01 4.19501686e+01\n",
      " 9.45481028e+01 7.92861260e+01 1.18505748e+02 8.22892300e+01\n",
      " 1.28253422e+02 1.86539882e-05 8.57359800e-06 7.32038457e-06\n",
      " 3.03881175e-06 2.30839357e-06 8.94311067e-07 0.00000000e+00\n",
      " 7.94093388e-23 8.28326428e+01 1.29001698e+02]\n",
      "21-th iteration, loss: 0.24140514221036072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0131501670817546e-06\n",
      "21-th iteration, new layer inserted. now 23 layers\n",
      "[1.88267988e+00 4.56919543e-05 4.60768941e-06 4.78600128e+01\n",
      " 7.54397881e+01 4.19501648e+01 9.45481012e+01 7.92861237e+01\n",
      " 1.18505747e+02 8.22892304e+01 1.28253423e+02 2.07373101e-05\n",
      " 9.37228010e-06 9.44612625e-06 3.80127460e-06 4.44917044e-06\n",
      " 1.64535263e-06 2.14520168e-06 7.51041563e-07 0.00000000e+00\n",
      " 7.94093388e-23 8.28326450e+01 1.29001698e+02]\n",
      "22-th iteration, loss: 0.24140514220992496, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8291693921538931e-06\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268020e+00 4.72876110e-05 4.33697397e-06 4.78600145e+01\n",
      " 7.54397871e+01 4.19501612e+01 9.45480997e+01 7.92861214e+01\n",
      " 1.18505746e+02 8.22892307e+01 1.28253423e+02 2.26263084e-05\n",
      " 1.00433011e-05 1.13814961e-05 4.42555881e-06 6.40334791e-06\n",
      " 2.24762357e-06 4.10751989e-06 1.34269864e-06 1.96603416e-06\n",
      " 5.91657073e-07 8.28326469e+01 1.29001698e+02]\n",
      "23-th iteration, loss: 0.24140514220954254, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6682993531828372e-06\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268051e+00 4.88636148e-05 4.04246653e-06 4.78600161e+01\n",
      " 7.54397861e+01 4.19501577e+01 9.45480982e+01 7.92861191e+01\n",
      " 1.18505745e+02 8.22892308e+01 1.28253424e+02 2.43222977e-05\n",
      " 1.05913434e-05 1.31271768e-05 4.91728850e-06 8.17092510e-06\n",
      " 2.70767126e-06 5.88621771e-06 1.78242345e-06 3.75137527e-06\n",
      " 1.02165449e-06 8.28326487e+01 1.29001698e+02]\n",
      "24-th iteration, loss: 0.241405142209203, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5792395732948866e-06\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268083e+00 5.04214415e-05 3.72440193e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.78600177e+01 7.54397852e+01 4.19501544e+01\n",
      " 9.45480967e+01 7.92861169e+01 1.18505744e+02 8.22892308e+01\n",
      " 1.28253425e+02 2.58510396e-05 1.10335046e-05 1.47083219e-05\n",
      " 5.29450002e-06 9.77639957e-06 3.04445527e-06 7.50508899e-06\n",
      " 2.09008408e-06 5.37906550e-06 1.31075435e-06 8.28326504e+01\n",
      " 1.29001698e+02]\n",
      "25-th iteration, loss: 0.2414051422088783, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4925993876837029e-06\n",
      "25-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268114e+00 5.19480450e-05 3.37975803e-06 4.78600208e+01\n",
      " 7.54397843e+01 4.19501512e+01 9.45480954e+01 7.92861147e+01\n",
      " 1.18505743e+02 8.22892308e+01 1.28253425e+02 2.72331151e-05\n",
      " 1.13828233e-05 1.61449881e-05 5.57104596e-06 1.12392616e-05\n",
      " 3.27263026e-06 8.98301413e-06 2.28112594e-06 6.86733180e-06\n",
      " 1.47518208e-06 8.28326518e+01 1.29001698e+02]\n",
      "26-th iteration, loss: 0.2414051422086027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4799652868421827e-06\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268143e+00 5.34086733e-05 3.00158744e-06 4.78600224e+01\n",
      " 7.54397834e+01 4.19501481e+01 9.45480940e+01 7.92861125e+01\n",
      " 1.18505741e+02 8.22892306e+01 1.28253425e+02 2.84870171e-05\n",
      " 1.16511603e-05 1.74552093e-05 5.75950191e-06 1.25770468e-05\n",
      " 3.40547740e-06 1.03369914e-05 2.36952759e-06 8.23259545e-06\n",
      " 1.52960605e-06 8.28326532e+01 1.29001698e+02]\n",
      "27-th iteration, loss: 0.24140514220835102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.466129573075931e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268173e+00 5.48611557e-05 2.60279788e-06 0.00000000e+00\n",
      " 4.23516474e-22 4.78600238e+01 7.54397826e+01 4.19501451e+01\n",
      " 9.45480927e+01 7.92861104e+01 1.18505740e+02 8.22892304e+01\n",
      " 1.28253426e+02 2.96328580e-05 1.18527738e-05 1.86586971e-05\n",
      " 5.87475169e-06 1.38090310e-05 3.45849934e-06 1.15858250e-05\n",
      " 2.37140491e-06 9.49315285e-06 1.49075074e-06 8.28326545e+01\n",
      " 1.29001698e+02]\n",
      "28-th iteration, loss: 0.24140514220810216, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.390059746687982e-06\n",
      "28-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268201e+00 5.62917005e-05 2.18020312e-06 4.78600268e+01\n",
      " 7.54397818e+01 4.19501423e+01 9.45480914e+01 7.92861083e+01\n",
      " 1.18505739e+02 8.22892301e+01 1.28253426e+02 3.06832881e-05\n",
      " 1.19953876e-05 1.97677715e-05 5.92504721e-06 1.49471722e-05\n",
      " 3.44047149e-06 1.27410779e-05 2.29605353e-06 1.06601388e-05\n",
      " 1.36842982e-06 8.28326557e+01 1.29001698e+02]\n",
      "29-th iteration, loss: 0.24140514220788817, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3815638785810581e-06\n",
      "29-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268229e+00 5.76660291e-05 1.72689994e-06 4.78600282e+01\n",
      " 7.54397810e+01 4.19501395e+01 9.45480902e+01 7.92861063e+01\n",
      " 1.18505738e+02 8.22892298e+01 1.28253426e+02 3.16498636e-05\n",
      " 1.20861874e-05 2.07936970e-05 5.91804136e-06 1.60024132e-05\n",
      " 3.35951103e-06 1.38133415e-05 2.15205364e-06 1.17437626e-05\n",
      " 1.17168646e-06 8.28326568e+01 1.29001698e+02]\n",
      "30-th iteration, loss: 0.24140514220768913, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3709420510654484e-06\n",
      "30-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268257e+00 5.90369745e-05 1.25469346e-06 4.78600296e+01\n",
      " 7.54397802e+01 4.19501368e+01 9.45480890e+01 7.92861043e+01\n",
      " 1.18505737e+02 8.22892295e+01 1.28253427e+02 3.25465006e-05\n",
      " 1.21351904e-05 2.17501334e-05 5.86416260e-06 1.69881305e-05\n",
      " 3.22645658e-06 1.48156807e-05 1.95065448e-06 1.27567496e-05\n",
      " 9.12182320e-07 8.28326578e+01 1.29001698e+02]\n",
      "31-th iteration, loss: 0.2414051422075031, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.358428276640836e-06\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268284e+00 6.04026984e-05 7.63127066e-07 0.00000000e+00\n",
      " 5.29395592e-23 4.78600310e+01 7.54397795e+01 4.19501343e+01\n",
      " 9.45480879e+01 7.92861024e+01 1.18505736e+02 8.22892291e+01\n",
      " 1.28253427e+02 3.33822522e-05 1.21482127e-05 2.26459268e-05\n",
      " 5.76957086e-06 1.79129382e-05 3.04781206e-06 1.57564521e-05\n",
      " 1.69870591e-06 1.37071723e-05 5.97116771e-07 8.28326587e+01\n",
      " 1.29001698e+02]\n",
      "32-th iteration, loss: 0.24140514220731363, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2876167106340908e-06\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[1.88268311e+00 6.17499773e-05 2.49202833e-07 0.00000000e+00\n",
      " 3.30872245e-23 4.78600337e+01 7.54397788e+01 4.19501318e+01\n",
      " 9.45480868e+01 7.92861006e+01 1.18505735e+02 8.22892287e+01\n",
      " 1.28253427e+02 3.41635867e-05 1.21288659e-05 2.34873675e-05\n",
      " 5.63817796e-06 1.87829253e-05 2.82779064e-06 1.66415189e-05\n",
      " 1.40072573e-06 1.46006439e-05 2.31317163e-07 8.28326596e+01\n",
      " 1.29001698e+02]\n",
      "33-th iteration, loss: 0.2414051422071382, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.221996257742535e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268336e+00 4.78600993e+01 7.54397781e+01 4.19501294e+01\n",
      " 9.45480858e+01 7.92860988e+01 1.18505734e+02 8.22892284e+01\n",
      " 1.28253427e+02 3.48952966e-05 1.20793550e-05 2.42790879e-05\n",
      " 5.47245757e-06 1.96025419e-05 2.56913723e-06 1.74751267e-05\n",
      " 1.05973448e-06 8.28326759e+01 1.29001698e+02]\n",
      "34-th iteration, loss: 0.24140514220700623, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.26624624054199e-06\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268361e+00 4.78601005e+01 7.54397774e+01 4.19501270e+01\n",
      " 9.45480847e+01 7.92860970e+01 1.18505733e+02 8.22892279e+01\n",
      " 1.28253427e+02 3.55866402e-05 1.20058781e-05 2.50301971e-05\n",
      " 5.27885399e-06 2.03807274e-05 2.27854537e-06 1.82660236e-05\n",
      " 6.82680262e-07 8.28326767e+01 1.29001698e+02]\n",
      "35-th iteration, loss: 0.241405142206883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3032632126614188e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268386e+00 4.78601018e+01 7.54397768e+01 4.19501248e+01\n",
      " 9.45480838e+01 7.92860953e+01 1.18505732e+02 8.22892275e+01\n",
      " 1.28253427e+02 3.62525104e-05 1.19196836e-05 2.57554694e-05\n",
      " 5.06881639e-06 2.11321181e-05 1.96766917e-06 1.90286880e-05\n",
      " 2.81428532e-07 8.28326775e+01 1.29001698e+02]\n",
      "36-th iteration, loss: 0.24140514220676812, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.333385286225315e-06\n",
      "36-th iteration, new layer inserted. now 17 layers\n",
      "[1.88268412e+00 4.78601031e+01 7.54397762e+01 4.19501227e+01\n",
      " 9.45480829e+01 7.92860937e+01 1.18505731e+02 8.22892271e+01\n",
      " 1.28253427e+02 3.68950888e-05 1.18220217e-05 2.64570234e-05\n",
      " 4.84372295e-06 2.18587513e-05 1.63801941e-06 8.28326980e+01\n",
      " 1.29001698e+02]\n",
      "37-th iteration, loss: 0.24140514220666548, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3590181633906655e-06\n",
      "37-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268439e+00 0.00000000e+00 2.77555756e-16 4.78601045e+01\n",
      " 7.54397757e+01 4.19501207e+01 9.45480820e+01 7.92860921e+01\n",
      " 1.18505730e+02 8.22892267e+01 1.28253427e+02 3.75163859e-05\n",
      " 1.17133443e-05 2.71368128e-05 4.60414281e-06 2.25625063e-05\n",
      " 1.29028771e-06 8.28326987e+01 1.29001698e+02]\n",
      "38-th iteration, loss: 0.24140514220655282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3196272387778386e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268465e+00 1.36137226e-06 2.66891718e-07 4.78601058e+01\n",
      " 7.54397751e+01 4.19501188e+01 9.45480812e+01 7.92860906e+01\n",
      " 1.18505729e+02 8.22892263e+01 1.28253427e+02 3.81204673e-05\n",
      " 1.15956912e-05 2.77988490e-05 4.35222354e-06 2.32473226e-05\n",
      " 9.26734845e-07 8.28326994e+01 1.29001698e+02]\n",
      "39-th iteration, loss: 0.24140514220644632, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2823872307243689e-06\n",
      "39-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268491e+00 2.68014305e-06 5.05993369e-07 0.00000000e+00\n",
      " 9.26442286e-23 4.78601072e+01 7.54397746e+01 4.19501169e+01\n",
      " 9.45480804e+01 7.92860891e+01 1.18505728e+02 8.22892259e+01\n",
      " 1.28253427e+02 3.87058887e-05 1.14671471e-05 2.84416431e-05\n",
      " 4.08613764e-06 2.39116504e-05 5.45627024e-07 8.28327000e+01\n",
      " 1.29001698e+02]\n",
      "40-th iteration, loss: 0.24140514220633238, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.192238207267924e-06\n",
      "40-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268516e+00 3.94639049e-06 7.15596070e-07 1.27280303e-06\n",
      " 2.09602701e-07 0.00000000e+00 5.29395592e-23 4.78601084e+01\n",
      " 7.54397741e+01 4.19501151e+01 9.45480796e+01 7.92860877e+01\n",
      " 1.18505728e+02 8.22892255e+01 1.28253427e+02 3.92726819e-05\n",
      " 1.13271752e-05 2.90651729e-05 3.80544349e-06 2.45553974e-05\n",
      " 1.46623999e-07 8.28327007e+01 1.29001698e+02]\n",
      "41-th iteration, loss: 0.24140514220621667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0644188558423273e-06\n",
      "41-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268538e+00 5.10876470e-06 8.85394463e-07 2.44444842e-06\n",
      " 3.62910807e-07 1.17436097e-06 1.53308106e-07 4.78601096e+01\n",
      " 7.54397735e+01 4.19501132e+01 9.45480789e+01 7.92860863e+01\n",
      " 1.18505727e+02 8.22892252e+01 1.28253428e+02 3.98189760e-05\n",
      " 1.11733250e-05 2.96675111e-05 3.50778594e-06 8.28327265e+01\n",
      " 1.29001698e+02]\n",
      "42-th iteration, loss: 0.2414051422061148, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.628936749791983e-07\n",
      "42-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268557e+00 6.14317394e-06 1.01181097e-06 3.49032872e-06\n",
      " 4.57657330e-07 2.22494311e-06 2.32839745e-07 0.00000000e+00\n",
      " 5.95570041e-23 4.78601107e+01 7.54397729e+01 4.19501114e+01\n",
      " 9.45480781e+01 7.92860850e+01 1.18505726e+02 8.22892248e+01\n",
      " 1.28253428e+02 4.03433439e-05 1.10036439e-05 3.02471619e-05\n",
      " 3.19131719e-06 8.28327271e+01 1.29001698e+02]\n",
      "43-th iteration, loss: 0.24140514220601425, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.416413025217139e-07\n",
      "43-th iteration, new layer inserted. now 23 layers\n",
      "[1.88268574e+00 7.06776489e-06 1.10029306e-06 4.42802858e-06\n",
      " 5.00919129e-07 3.16857233e-06 2.47275441e-07 9.46645860e-07\n",
      " 1.44356954e-08 4.78601116e+01 7.54397723e+01 4.19501095e+01\n",
      " 9.45480774e+01 7.92860837e+01 1.18505725e+02 8.22892244e+01\n",
      " 1.28253428e+02 4.08523679e-05 1.08225598e-05 3.08106293e-05\n",
      " 2.86057752e-06 8.28327277e+01 1.29001698e+02]\n",
      "44-th iteration, loss: 0.24140514220592268, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.481816157805908e-07\n",
      "44-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268588e+00 7.87553062e-06 1.15097051e-06 5.25004958e-06\n",
      " 4.94227561e-07 3.99708317e-06 1.99532221e-07 4.78601142e+01\n",
      " 7.54397716e+01 4.19501077e+01 9.45480766e+01 7.92860824e+01\n",
      " 1.18505724e+02 8.22892241e+01 1.28253428e+02 4.13465715e-05\n",
      " 1.06302454e-05 3.13583803e-05 2.51581961e-06 8.28327282e+01\n",
      " 1.29001698e+02]\n",
      "45-th iteration, loss: 0.2414051422058425, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.053431157369079e-07\n",
      "45-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268600e+00 8.60138452e-06 1.17300720e-06 5.99081531e-06\n",
      " 4.48245229e-07 4.74425206e-06 1.01764147e-07 4.78601150e+01\n",
      " 7.54397710e+01 4.19501058e+01 9.45480759e+01 7.92860812e+01\n",
      " 1.18505724e+02 8.22892237e+01 1.28253428e+02 4.18290931e-05\n",
      " 1.04294881e-05 3.18934979e-05 2.15990864e-06 8.28327288e+01\n",
      " 1.29001698e+02]\n",
      "46-th iteration, loss: 0.24140514220576703, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.683176993267606e-07\n",
      "46-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268612e+00 9.28712830e-06 1.17639186e-06 6.69175644e-06\n",
      " 3.74013599e-07 0.00000000e+00 1.05879118e-22 4.78601211e+01\n",
      " 7.54397703e+01 4.19501040e+01 9.45480752e+01 7.92860801e+01\n",
      " 1.18505723e+02 8.22892234e+01 1.28253428e+02 4.23027328e-05\n",
      " 1.02228136e-05 3.24187402e-05 1.79543278e-06 8.28327293e+01\n",
      " 1.29001698e+02]\n",
      "47-th iteration, loss: 0.2414051422056962, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.34976635388225e-07\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268623e+00 9.93879637e-06 1.16297293e-06 7.35866570e-06\n",
      " 2.73897068e-07 0.00000000e+00 5.95570041e-23 4.78601225e+01\n",
      " 7.54397696e+01 4.19501023e+01 9.45480745e+01 7.92860789e+01\n",
      " 1.18505722e+02 8.22892230e+01 1.28253428e+02 4.27681650e-05\n",
      " 1.00107101e-05 3.29347524e-05 1.42292916e-06 8.28327298e+01\n",
      " 1.29001698e+02]\n",
      "48-th iteration, loss: 0.24140514220562945, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.056665322885561e-07\n",
      "48-th iteration, new layer inserted. now 21 layers\n",
      "[1.88268633e+00 1.05592991e-05 1.13383969e-06 7.99423580e-06\n",
      " 1.49425827e-07 0.00000000e+00 1.65436123e-23 4.78601238e+01\n",
      " 7.54397690e+01 4.19501005e+01 9.45480738e+01 7.92860779e+01\n",
      " 1.18505721e+02 8.22892227e+01 1.28253428e+02 4.32259499e-05\n",
      " 9.79356640e-06 3.34420680e-05 1.04283212e-06 8.28327303e+01\n",
      " 1.29001698e+02]\n",
      "49-th iteration, loss: 0.24140514220556653, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.795885775647849e-07\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[1.88268642e+00 1.11529948e-05 1.09036200e-06 8.60262139e-06\n",
      " 2.37577393e-09 4.78601250e+01 7.54397683e+01 4.19500989e+01\n",
      " 9.45480732e+01 7.92860768e+01 1.18505721e+02 8.22892224e+01\n",
      " 1.28253428e+02 4.36766145e-05 9.57175001e-06 3.39411888e-05\n",
      " 6.55552314e-07 8.28327309e+01 1.29001698e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5367213365606043\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.6005126    0.         809.67084418]\n",
      "1-th iteration, loss: 0.74948097338804, 11 gd steps\n",
      "insert gradient: -0.6336695984730955\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.03840365  62.30738235 236.97683244   0.         572.69401174]\n",
      "2-th iteration, loss: 0.6050067963590977, 13 gd steps\n",
      "insert gradient: -0.6845947395742539\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.15317095  77.12431625 221.78396866  40.95625941 245.44029074\n",
      "   0.         327.25372099]\n",
      "3-th iteration, loss: 0.4625800514616977, 19 gd steps\n",
      "insert gradient: -0.5298361419073401\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          42.96282847 228.94543851  60.67538491 148.63482393\n",
      "  44.9059765  187.00212628   0.         140.25159471]\n",
      "4-th iteration, loss: 0.38015883925297916, 15 gd steps\n",
      "insert gradient: -0.693483076359026\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.79909379  49.14812795 117.08061311   0.         108.71771217\n",
      "  67.27680171 138.98726035  47.11641145 163.6666056   44.06578774\n",
      " 140.25159471]\n",
      "5-th iteration, loss: 0.3035307487711468, 14 gd steps\n",
      "insert gradient: -0.26943544901191385\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  4.26797872  54.43141506  88.32621319  27.77636276  85.89920685\n",
      "  57.95766486 148.01939362  57.16529279 145.1535385   63.88486531\n",
      " 140.25159471]\n",
      "6-th iteration, loss: 0.24140514501359497, 81 gd steps\n",
      "insert gradient: -2.3614363268371253e-05\n",
      "6-th iteration, new layer inserted. now 11 layers\n",
      "[  1.87652279  47.86500481  75.43559163  41.95021016  94.54756156\n",
      "  79.28674048 118.50129327  82.29064167 128.26221529  82.82816949\n",
      " 140.25159471]\n",
      "7-th iteration, loss: 0.24140514220493367, 52 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.174821488883429e-07\n",
      "7-th iteration, new layer inserted. now 11 layers\n",
      "[  1.88269159  47.86021354  75.43966172  41.9500714   94.54811081\n",
      "  79.28607021 118.50572012  82.28923339 128.25325195  82.83287596\n",
      " 140.25159471]\n",
      "8-th iteration, loss: 0.24140514220491607, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.917974088284352e-07\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269167e+00 0.00000000e+00 5.82867088e-16 4.78602142e+01\n",
      " 7.54396619e+01 4.19500712e+01 9.45481105e+01 7.92860695e+01\n",
      " 1.18505720e+02 8.22892334e+01 1.28253253e+02 8.28328767e+01\n",
      " 1.40251595e+02]\n",
      "9-th iteration, loss: 0.24140514220489556, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.721140921688558e-07\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269175e+00 6.81040505e-07 8.07656188e-08 4.78602149e+01\n",
      " 7.54396620e+01 4.19500709e+01 9.45481102e+01 7.92860687e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253253e+02 8.28328773e+01\n",
      " 1.40251595e+02]\n",
      "10-th iteration, loss: 0.2414051422048765, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.59797269865421e-07\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269182e+00 1.30906766e-06 1.41162236e-07 4.78602156e+01\n",
      " 7.54396622e+01 4.19500707e+01 9.45481099e+01 7.92860680e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253254e+02 8.28328780e+01\n",
      " 1.40251595e+02]\n",
      "11-th iteration, loss: 0.24140514220485876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.479629493788366e-07\n",
      "11-th iteration, new layer inserted. now 13 layers\n",
      "[1.88269188e+00 1.89031771e-06 1.83245631e-07 4.78602161e+01\n",
      " 7.54396623e+01 4.19500704e+01 9.45481096e+01 7.92860673e+01\n",
      " 1.18505719e+02 8.22892335e+01 1.28253254e+02 8.28328787e+01\n",
      " 1.40251595e+02]\n",
      "12-th iteration, loss: 0.2414051422048421, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.365626362147421e-07\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[1.88269193e+00 2.43020514e-06 2.08811036e-07 4.78602167e+01\n",
      " 7.54396624e+01 4.19500701e+01 9.45481093e+01 7.92860667e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253255e+02 0.00000000e+00\n",
      " 3.73034936e-14 8.28328793e+01 1.40251595e+02]\n",
      "13-th iteration, loss: 0.24140514220482112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.130513589522634e-07\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269197e+00 2.93213687e-06 2.19197287e-07 4.78602172e+01\n",
      " 7.54396625e+01 4.19500698e+01 9.45481090e+01 7.92860660e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253255e+02 6.30590750e-07\n",
      " 5.10328108e-07 0.00000000e+00 3.97046694e-23 8.28328799e+01\n",
      " 1.40251595e+02]\n",
      "14-th iteration, loss: 0.2414051422047968, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.788851114778665e-07\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269201e+00 3.39711916e-06 2.15327716e-07 4.78602177e+01\n",
      " 7.54396625e+01 4.19500695e+01 9.45481087e+01 7.92860654e+01\n",
      " 1.18505718e+02 8.22892336e+01 1.28253256e+02 1.23230152e-06\n",
      " 9.99393298e-07 6.04235774e-07 4.89065189e-07 8.28328805e+01\n",
      " 1.40251595e+02]\n",
      "15-th iteration, loss: 0.24140514220477463, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.472478747113012e-07\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[1.88269204e+00 3.82751722e-06 1.98334184e-07 4.78602181e+01\n",
      " 7.54396626e+01 4.19500691e+01 9.45481084e+01 7.92860647e+01\n",
      " 1.18505717e+02 8.22892336e+01 1.28253256e+02 1.79573136e-06\n",
      " 1.46234004e-06 1.17261042e-06 9.49022292e-07 8.28328811e+01\n",
      " 1.40251595e+02]\n",
      "16-th iteration, loss: 0.2414051422047543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.17921561987412e-07\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269206e+00 4.22748735e-06 1.69540674e-07 4.78602185e+01\n",
      " 7.54396626e+01 4.19500688e+01 9.45481081e+01 7.92860641e+01\n",
      " 1.18505717e+02 8.22892335e+01 1.28253257e+02 2.32361580e-06\n",
      " 1.90120069e-06 1.70773023e-06 1.38208110e-06 0.00000000e+00\n",
      " 3.44107135e-22 8.28328817e+01 1.40251595e+02]\n",
      "17-th iteration, loss: 0.2414051422047324, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.807697541036638e-07\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269207e+00 4.59960045e-06 1.29915684e-07 4.78602189e+01\n",
      " 7.54396627e+01 4.19500685e+01 9.45481078e+01 7.92860635e+01\n",
      " 1.18505717e+02 8.22892335e+01 1.28253257e+02 2.81577133e-06\n",
      " 2.31598045e-06 2.20929254e-06 1.78841134e-06 5.08400572e-07\n",
      " 4.06330243e-07 8.28328822e+01 1.40251595e+02]\n",
      "18-th iteration, loss: 0.2414051422047128, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.47065927642791e-07\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269208e+00 4.94559386e-06 8.02936605e-08 4.78602192e+01\n",
      " 7.54396627e+01 4.19500682e+01 9.45481075e+01 7.92860628e+01\n",
      " 1.18505716e+02 8.22892334e+01 1.28253257e+02 3.26566543e-06\n",
      " 2.70350839e-06 2.67064565e-06 2.16500813e-06 9.78602400e-07\n",
      " 7.80411564e-07 8.28328826e+01 1.40251595e+02]\n",
      "19-th iteration, loss: 0.24140514220469514, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1645873851015697e-07\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269209e+00 5.26877533e-06 2.16865590e-08 4.78602195e+01\n",
      " 7.54396627e+01 4.19500678e+01 9.45481072e+01 7.92860622e+01\n",
      " 1.18505716e+02 8.22892333e+01 1.28253258e+02 3.67708556e-06\n",
      " 3.06645043e-06 3.09544222e-06 2.51473633e-06 1.41411100e-06\n",
      " 1.12529784e-06 8.28328831e+01 1.40251595e+02]\n",
      "20-th iteration, loss: 0.24140514220467912, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8861274908512565e-07\n",
      "20-th iteration, new layer inserted. now 19 layers\n",
      "[1.88269209e+00 4.78602254e+01 7.54396628e+01 4.19500675e+01\n",
      " 9.45481069e+01 7.92860616e+01 1.18505716e+02 8.22892332e+01\n",
      " 1.28253258e+02 4.05343659e-06 3.40719519e-06 3.48696545e-06\n",
      " 2.84016544e-06 1.81807666e-06 1.44373020e-06 0.00000000e+00\n",
      " 5.82335151e-22 8.28328835e+01 1.40251595e+02]\n",
      "21-th iteration, loss: 0.2414051422046635, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5677151014204685e-07\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[1.88269209e+00 4.78602257e+01 7.54396628e+01 4.19500672e+01\n",
      " 9.45481066e+01 7.92860611e+01 1.18505715e+02 8.22892331e+01\n",
      " 1.28253258e+02 4.39608719e-06 3.72681775e-06 3.84647419e-06\n",
      " 3.14253518e-06 2.19163798e-06 1.73710444e-06 3.80704614e-07\n",
      " 2.93374242e-07 0.00000000e+00 1.98523347e-23 8.28328839e+01\n",
      " 1.40251595e+02]\n",
      "22-th iteration, loss: 0.2414051422046481, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.217289240832977e-07\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[1.88269208e+00 4.78602260e+01 7.54396628e+01 4.19500669e+01\n",
      " 9.45481063e+01 7.92860605e+01 1.18505715e+02 8.22892329e+01\n",
      " 1.28253259e+02 4.70004673e-06 4.02294813e-06 4.16887330e-06\n",
      " 3.41963395e-06 2.52958575e-06 2.00335942e-06 7.27247234e-07\n",
      " 5.57745564e-07 3.47994179e-07 2.64371322e-07 0.00000000e+00\n",
      " 3.30872245e-23 8.28328842e+01 1.40251595e+02]\n",
      "23-th iteration, loss: 0.24140514220463352, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.853012622632444e-07\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269208e+00 4.78602262e+01 7.54396629e+01 4.19500666e+01\n",
      " 9.45481061e+01 7.92860599e+01 1.18505715e+02 8.22892327e+01\n",
      " 1.28253259e+02 4.96181944e-06 4.29392061e-06 4.45055078e-06\n",
      " 3.66997965e-06 2.82818292e-06 2.24118923e-06 1.03575663e-06\n",
      " 7.91977095e-07 6.59263193e-07 4.96881046e-07 3.12577072e-07\n",
      " 2.32509724e-07 0.00000000e+00 1.98523347e-23 8.28328845e+01\n",
      " 1.40251595e+02]\n",
      "24-th iteration, loss: 0.2414051422046201, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.528020832827099e-07\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269207e+00 0.00000000e+00 2.22044605e-16 4.78602265e+01\n",
      " 7.54396629e+01 4.19500664e+01 9.45481058e+01 7.92860593e+01\n",
      " 1.18505714e+02 8.22892325e+01 1.28253259e+02 5.18002613e-06\n",
      " 4.53934971e-06 4.69000296e-06 3.89338829e-06 3.08579344e-06\n",
      " 2.45060457e-06 1.30445612e-06 9.96267716e-07 9.31881230e-07\n",
      " 6.97909761e-07 5.87653580e-07 4.31991869e-07 2.76226920e-07\n",
      " 1.99482145e-07 8.28328848e+01 1.40251595e+02]\n",
      "25-th iteration, loss: 0.2414051422046081, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3455412904486574e-07\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269207e+00 0.00000000e+00 3.33066907e-16 4.78602270e+01\n",
      " 7.54396629e+01 4.19500661e+01 9.45481055e+01 7.92860588e+01\n",
      " 1.18505714e+02 8.22892323e+01 1.28253260e+02 5.35612515e-06\n",
      " 4.76050942e-06 4.88856181e-06 4.09134277e-06 3.30361602e-06\n",
      " 2.63329114e-06 1.53440381e-06 1.17250009e-06 1.16675826e-06\n",
      " 8.69531373e-07 8.25983730e-07 6.00705886e-07 5.16694482e-07\n",
      " 3.66829446e-07 8.28328850e+01 1.40251595e+02]\n",
      "26-th iteration, loss: 0.24140514220459766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1908015688049584e-07\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269206e+00 4.78602275e+01 7.54396630e+01 4.19500658e+01\n",
      " 9.45481053e+01 7.92860582e+01 1.18505713e+02 8.22892321e+01\n",
      " 1.28253260e+02 5.49651675e-06 4.96134037e-06 5.05250749e-06\n",
      " 4.26798606e-06 3.48780487e-06 2.79358878e-06 1.73162168e-06\n",
      " 1.32520580e-06 1.36977743e-06 1.01646419e-06 1.03330517e-06\n",
      " 7.43551897e-07 7.26988099e-07 5.07118954e-07 8.28328853e+01\n",
      " 1.40251595e+02]\n",
      "27-th iteration, loss: 0.24140514220458878, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1498168409364778e-07\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269204e+00 0.00000000e+00 3.88578059e-16 4.78602277e+01\n",
      " 7.54396630e+01 4.19500656e+01 9.45481050e+01 7.92860577e+01\n",
      " 1.18505713e+02 8.22892318e+01 1.28253260e+02 5.60721592e-06\n",
      " 5.14578467e-06 5.18775441e-06 4.42743154e-06 3.64416896e-06\n",
      " 2.93577727e-06 1.90180790e-06 1.45882657e-06 1.54652051e-06\n",
      " 1.14330756e-06 1.21507752e-06 8.65282679e-07 9.12439397e-07\n",
      " 6.25252740e-07 8.28328854e+01 1.40251595e+02]\n",
      "28-th iteration, loss: 0.2414051422045804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0245389616705633e-07\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269203e+00 0.00000000e+00 2.22044605e-16 4.78602281e+01\n",
      " 7.54396631e+01 4.19500654e+01 9.45481047e+01 7.92860572e+01\n",
      " 1.18505713e+02 8.22892316e+01 1.28253260e+02 5.69304938e-06\n",
      " 5.31696543e-06 5.29904823e-06 4.57294428e-06 3.77736885e-06\n",
      " 3.06325938e-06 2.04953345e-06 1.57689890e-06 1.70146405e-06\n",
      " 1.25372801e-06 1.37567792e-06 9.69691163e-07 1.07732104e-06\n",
      " 7.25146652e-07 8.28328856e+01 1.40251595e+02]\n",
      "29-th iteration, loss: 0.24140514220457276, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.915677693586851e-07\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269201e+00 0.00000000e+00 3.33066907e-16 4.78602285e+01\n",
      " 7.54396631e+01 4.19500651e+01 9.45481045e+01 7.92860567e+01\n",
      " 1.18505712e+02 8.22892313e+01 1.28253260e+02 5.75744251e-06\n",
      " 5.47690406e-06 5.38974868e-06 4.70666425e-06 3.89069535e-06\n",
      " 3.17828966e-06 2.17801636e-06 1.68178849e-06 1.83774915e-06\n",
      " 1.35019909e-06 1.51816622e-06 1.05935565e-06 1.22460719e-06\n",
      " 8.09480764e-07 8.28328858e+01 1.40251595e+02]\n",
      "30-th iteration, loss: 0.24140514220456574, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8202524323115624e-07\n",
      "30-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269199e+00 4.78602289e+01 7.54396631e+01 4.19500649e+01\n",
      " 9.45481043e+01 7.92860562e+01 1.18505712e+02 8.22892311e+01\n",
      " 1.28253261e+02 5.80365316e-06 5.62765654e-06 5.46305800e-06\n",
      " 4.83074929e-06 3.98729233e-06 3.28312429e-06 2.29033891e-06\n",
      " 1.77584672e-06 1.95839286e-06 1.43516449e-06 1.64549045e-06\n",
      " 1.13680946e-06 1.35717290e-06 8.80875460e-07 8.28328859e+01\n",
      " 1.40251595e+02]\n",
      "31-th iteration, loss: 0.24140514220455955, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8121194441769932e-07\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269197e+00 4.78602291e+01 7.54396632e+01 4.19500647e+01\n",
      " 9.45481040e+01 7.92860557e+01 1.18505711e+02 8.22892308e+01\n",
      " 1.28253261e+02 5.83456152e-06 5.77110318e-06 5.52181093e-06\n",
      " 4.94716577e-06 4.06994688e-06 3.37981242e-06 2.38923771e-06\n",
      " 1.86120269e-06 2.06607821e-06 1.51083071e-06 1.76027672e-06\n",
      " 1.20433412e-06 1.47758388e-06 9.41685091e-07 8.28328860e+01\n",
      " 1.40251595e+02]\n",
      "32-th iteration, loss: 0.24140514220455375, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8032302389926632e-07\n",
      "32-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269195e+00 0.00000000e+00 2.22044605e-16 4.78602293e+01\n",
      " 7.54396632e+01 4.19500645e+01 9.45481038e+01 7.92860552e+01\n",
      " 1.18505711e+02 8.22892306e+01 1.28253261e+02 5.85264311e-06\n",
      " 5.90889833e-06 5.56844683e-06 5.05764005e-06 4.14106041e-06\n",
      " 3.47014939e-06 2.47707390e-06 1.93971818e-06 2.16312327e-06\n",
      " 1.57912365e-06 1.86479707e-06 1.26391755e-06 1.58806305e-06\n",
      " 9.93957721e-07 8.28328861e+01 1.40251595e+02]\n",
      "33-th iteration, loss: 0.2414051422045481, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7177701829865442e-07\n",
      "33-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269193e+00 4.78602296e+01 7.54396632e+01 4.19500643e+01\n",
      " 9.45481036e+01 7.92860548e+01 1.18505711e+02 8.22892303e+01\n",
      " 1.28253261e+02 5.85938446e-06 6.04186177e-06 5.60442429e-06\n",
      " 5.16305188e-06 4.20206209e-06 3.55507205e-06 2.55524522e-06\n",
      " 2.01238475e-06 2.25089193e-06 1.64108751e-06 1.96037894e-06\n",
      " 1.31665475e-06 1.68989853e-06 1.03883750e-06 8.28328862e+01\n",
      " 1.40251595e+02]\n",
      "34-th iteration, loss: 0.241405142204543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7129504181494238e-07\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269191e+00 0.00000000e+00 3.88578059e-16 4.78602298e+01\n",
      " 7.54396633e+01 4.19500641e+01 9.45481033e+01 7.92860544e+01\n",
      " 1.18505710e+02 8.22892301e+01 1.28253261e+02 5.85610701e-06\n",
      " 6.17073757e-06 5.63104082e-06 5.26419807e-06 4.25422441e-06\n",
      " 3.63542724e-06 2.62499735e-06 2.08009708e-06 2.33060096e-06\n",
      " 1.69766286e-06 2.04820774e-06 1.36353053e-06 1.78424189e-06\n",
      " 1.07735199e-06 8.28328863e+01 1.40251595e+02]\n",
      "35-th iteration, loss: 0.24140514220453801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6343703168247077e-07\n",
      "35-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269188e+00 4.78602301e+01 7.54396633e+01 4.19500639e+01\n",
      " 9.45481031e+01 7.92860539e+01 1.18505710e+02 8.22892298e+01\n",
      " 1.28253261e+02 5.84418242e-06 6.29639414e-06 5.64964786e-06\n",
      " 5.36199332e-06 4.29887770e-06 3.71217341e-06 2.68763803e-06\n",
      " 2.14385527e-06 2.40353356e-06 1.74988968e-06 2.12954005e-06\n",
      " 1.40562322e-06 1.87232069e-06 1.11061659e-06 8.28328864e+01\n",
      " 1.40251595e+02]\n",
      "36-th iteration, loss: 0.24140514220453355, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6316856463470246e-07\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269186e+00 4.78602303e+01 7.54396634e+01 4.19500637e+01\n",
      " 9.45481029e+01 7.92860535e+01 1.18505709e+02 8.22892296e+01\n",
      " 1.28253262e+02 5.82448235e-06 6.41928863e-06 5.66110116e-06\n",
      " 5.45693443e-06 4.33686112e-06 3.78584449e-06 2.74398857e-06\n",
      " 2.20422844e-06 2.47049148e-06 1.79837063e-06 2.20515609e-06\n",
      " 1.44356762e-06 1.95489148e-06 1.13929711e-06 8.28328865e+01\n",
      " 1.40251595e+02]\n",
      "37-th iteration, loss: 0.2414051422045293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.626545287026264e-07\n",
      "37-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269184e+00 0.00000000e+00 3.88578059e-16 4.78602305e+01\n",
      " 7.54396634e+01 4.19500635e+01 9.45481027e+01 7.92860531e+01\n",
      " 1.18505709e+02 8.22892293e+01 1.28253262e+02 5.79817124e-06\n",
      " 6.54022001e-06 5.66655148e-06 5.54985576e-06 4.36931131e-06\n",
      " 3.85730786e-06 2.79517040e-06 2.26211507e-06 2.53257940e-06\n",
      " 1.84403375e-06 2.27614200e-06 1.47832005e-06 2.03301988e-06\n",
      " 1.16437712e-06 8.28328866e+01 1.40251595e+02]\n",
      "38-th iteration, loss: 0.24140514220452508, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.550945785922936e-07\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269181e+00 0.00000000e+00 3.88578059e-16 4.78602308e+01\n",
      " 7.54396634e+01 4.19500634e+01 9.45481025e+01 7.92860528e+01\n",
      " 1.18505709e+02 8.22892291e+01 1.28253262e+02 5.76571286e-06\n",
      " 6.65936040e-06 5.66645287e-06 5.64095912e-06 4.39667233e-06\n",
      " 3.92679270e-06 2.84161664e-06 2.31776995e-06 2.59021815e-06\n",
      " 1.88715792e-06 2.34290468e-06 1.51018230e-06 2.10709700e-06\n",
      " 1.18618038e-06 8.28328867e+01 1.40251595e+02]\n",
      "39-th iteration, loss: 0.2414051422045211, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4820535579911038e-07\n",
      "39-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269179e+00 4.78602311e+01 7.54396635e+01 4.19500632e+01\n",
      " 9.45481024e+01 7.92860524e+01 1.18505708e+02 8.22892289e+01\n",
      " 1.28253262e+02 5.72740817e-06 6.77674402e-06 5.66109744e-06\n",
      " 5.73030620e-06 4.41922729e-06 3.99438589e-06 2.88360062e-06\n",
      " 2.37130336e-06 2.64367002e-06 1.92787543e-06 2.40569384e-06\n",
      " 1.53930757e-06 2.17735825e-06 1.20488014e-06 8.28328868e+01\n",
      " 1.40251595e+02]\n",
      "40-th iteration, loss: 0.24140514220451753, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4811595292771753e-07\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269176e+00 0.00000000e+00 2.22044605e-16 4.78602313e+01\n",
      " 7.54396635e+01 4.19500630e+01 9.45481022e+01 7.92860520e+01\n",
      " 1.18505708e+02 8.22892286e+01 1.28253262e+02 5.68399911e-06\n",
      " 6.89286027e-06 5.65121842e-06 5.81841242e-06 4.43770075e-06\n",
      " 4.06062661e-06 2.92183753e-06 2.42327658e-06 2.69363969e-06\n",
      " 1.96676827e-06 2.46520226e-06 1.56629750e-06 2.24448287e-06\n",
      " 1.22109692e-06 8.28328868e+01 1.40251595e+02]\n",
      "41-th iteration, loss: 0.24140514220451395, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4168548031942137e-07\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269173e+00 4.78602315e+01 7.54396636e+01 4.19500629e+01\n",
      " 9.45481020e+01 7.92860517e+01 1.18505708e+02 8.22892284e+01\n",
      " 0.00000000e+00 3.55271368e-15 1.28253262e+02 5.63603057e-06\n",
      " 7.00804686e-06 5.63735447e-06 5.90563785e-06 4.45262519e-06\n",
      " 4.12589514e-06 2.95685318e-06 2.47408841e-06 2.74064525e-06\n",
      " 2.00425249e-06 2.52193902e-06 1.59158438e-06 2.30896936e-06\n",
      " 1.23527853e-06 8.28328869e+01 1.40251595e+02]\n",
      "42-th iteration, loss: 0.2414051422045106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.409715453292349e-07\n",
      "42-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269170e+00 4.78602317e+01 7.54396636e+01 4.19500627e+01\n",
      " 9.45481018e+01 7.92860514e+01 1.18505708e+02 8.22892282e+01\n",
      " 1.28253263e+02 5.58407439e-06 7.12273186e-06 5.62007278e-06\n",
      " 5.99243028e-06 4.46456348e-06 4.19065683e-06 2.98920562e-06\n",
      " 2.52422016e-06 2.78523900e-06 2.04082405e-06 2.57644943e-06\n",
      " 1.61567786e-06 2.37135463e-06 1.24794767e-06 8.28328870e+01\n",
      " 1.40251595e+02]\n",
      "43-th iteration, loss: 0.2414051422045075, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4066828952745595e-07\n",
      "43-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269168e+00 4.78602318e+01 7.54396636e+01 4.19500626e+01\n",
      " 9.45481017e+01 7.92860511e+01 1.18505707e+02 8.22892280e+01\n",
      " 1.28253263e+02 5.52685276e-06 7.23543451e-06 5.59809309e-06\n",
      " 6.07732587e-06 4.47223321e-06 4.25346260e-06 3.01760992e-06\n",
      " 2.57223591e-06 2.82613266e-06 2.07505897e-06 2.62744069e-06\n",
      " 1.63716500e-06 2.43033996e-06 1.25770181e-06 8.28328870e+01\n",
      " 1.40251595e+02]\n",
      "44-th iteration, loss: 0.24140514220450462, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4010307081608912e-07\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269165e+00 0.00000000e+00 2.22044605e-16 4.78602320e+01\n",
      " 7.54396637e+01 4.19500624e+01 9.45481015e+01 7.92860507e+01\n",
      " 1.18505707e+02 8.22892278e+01 1.28253263e+02 5.46630566e-06\n",
      " 7.34790762e-06 5.57334556e-06 6.16210067e-06 4.47755514e-06\n",
      " 4.31610964e-06 3.04397718e-06 2.61995239e-06 2.86522685e-06\n",
      " 2.10879228e-06 2.67680188e-06 1.65789824e-06 2.48780153e-06\n",
      " 1.26641019e-06 8.28328871e+01 1.40251595e+02]\n",
      "45-th iteration, loss: 0.24140514220450168, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3795296804451164e-07\n",
      "45-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269162e+00 4.78602323e+01 7.54396637e+01 4.19500623e+01\n",
      " 9.45481014e+01 7.92860505e+01 1.18505707e+02 8.22892276e+01\n",
      " 1.28253263e+02 5.40240853e-06 7.46003006e-06 5.54580451e-06\n",
      " 6.24664724e-06 4.48050300e-06 4.37850211e-06 3.06828033e-06\n",
      " 2.66728384e-06 2.90249303e-06 2.14194714e-06 2.72450196e-06\n",
      " 1.67780879e-06 2.54370457e-06 1.27401158e-06 8.28328872e+01\n",
      " 1.40251595e+02]\n",
      "46-th iteration, loss: 0.24140514220449907, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.375559958382451e-07\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269159e+00 4.78602324e+01 7.54396638e+01 4.19500622e+01\n",
      " 9.45481012e+01 7.92860502e+01 1.18505706e+02 8.22892274e+01\n",
      " 0.00000000e+00 2.84217094e-14 1.28253263e+02 5.33520886e-06\n",
      " 7.57175011e-06 5.51551568e-06 6.33092769e-06 4.48112140e-06\n",
      " 4.44061388e-06 3.09056273e-06 2.71421434e-06 2.93797264e-06\n",
      " 2.17451665e-06 2.77057952e-06 1.69689799e-06 2.59808361e-06\n",
      " 1.28051501e-06 8.28328872e+01 1.40251595e+02]\n",
      "47-th iteration, loss: 0.24140514220449638, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.360042334539792e-07\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[1.88269156e+00 4.78602325e+01 7.54396638e+01 4.19500620e+01\n",
      " 9.45481011e+01 7.92860499e+01 1.18505706e+02 8.22892272e+01\n",
      " 1.28253263e+02 5.26540658e-06 7.68368854e-06 5.48317700e-06\n",
      " 6.41557638e-06 4.48010697e-06 4.50309083e-06 3.11151962e-06\n",
      " 2.76139977e-06 2.97235895e-06 2.20716555e-06 2.81572492e-06\n",
      " 1.71583859e-06 2.65162493e-06 1.28660080e-06 8.28328873e+01\n",
      " 1.40251595e+02]\n",
      "48-th iteration, loss: 0.24140514220449397, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3573866510818808e-07\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269153e+00 4.78602327e+01 7.54396638e+01 4.19500619e+01\n",
      " 9.45481009e+01 7.92860496e+01 1.18505706e+02 8.22892270e+01\n",
      " 0.00000000e+00 2.48689958e-14 1.28253264e+02 5.19139032e-06\n",
      " 7.79412740e-06 5.44717820e-06 6.49888550e-06 4.47585123e-06\n",
      " 4.56423322e-06 3.12954432e-06 2.80714695e-06 3.00404653e-06\n",
      " 2.23820607e-06 2.85833314e-06 1.73294744e-06 2.70272280e-06\n",
      " 1.29058991e-06 8.28328873e+01 1.40251595e+02]\n",
      "49-th iteration, loss: 0.24140514220449152, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3430731677847682e-07\n",
      "49-th iteration, new layer inserted. now 27 layers\n",
      "[1.88269150e+00 4.78602328e+01 7.54396639e+01 4.19500618e+01\n",
      " 9.45481008e+01 7.92860494e+01 1.18505706e+02 8.22892268e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.28253264e+02 5.11512842e-06\n",
      " 7.90492215e-06 5.40948019e-06 6.58272864e-06 4.47030849e-06\n",
      " 4.62593069e-06 3.14658455e-06 2.85336002e-06 3.03497599e-06\n",
      " 2.26955569e-06 2.90033682e-06 1.75015457e-06 2.75330077e-06\n",
      " 1.29442448e-06 8.28328874e+01 1.40251595e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5328864240338573\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.91229239   0.         833.48469254]\n",
      "1-th iteration, loss: 0.7518055542540729, 11 gd steps\n",
      "insert gradient: -0.646337800987476\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.98565692  62.18124359 121.97336964   0.         711.5113229 ]\n",
      "2-th iteration, loss: 0.5091293589099802, 48 gd steps\n",
      "insert gradient: -0.6524902789050355\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.58216824  53.63831489  99.11799134  99.41280834 232.33022789\n",
      "   0.         479.18109501]\n",
      "3-th iteration, loss: 0.43122249789814926, 25 gd steps\n",
      "insert gradient: -0.2953960207596941\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  1.14560132  60.60140707  99.79272257  94.0744238  112.78673403\n",
      "   0.          94.97830234  46.82089987 479.18109501]\n",
      "4-th iteration, loss: 0.340824173917652, 38 gd steps\n",
      "insert gradient: -0.29264391299753206\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.05665697  83.10069653 126.49658851  77.14424388  98.47948316\n",
      "  43.43846085  66.72337203  47.22703745 124.23213574   0.\n",
      " 354.94895927]\n",
      "5-th iteration, loss: 0.2855948269010889, 17 gd steps\n",
      "insert gradient: -0.28387671389022034\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  2.8587922   93.47176748 108.63734981  82.1206607  104.08491935\n",
      "  52.68862047  66.83517465  37.08658234 121.40440077  53.4779588\n",
      " 354.94895927]\n",
      "6-th iteration, loss: 0.2739445789442682, 43 gd steps\n",
      "insert gradient: -0.13754287474005267\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[3.48154098e-01 9.02364580e+01 1.07989996e+02 9.18565791e+01\n",
      " 1.00792236e+02 4.84816857e+01 7.17554179e+01 4.20889856e+01\n",
      " 1.13636977e+02 5.39944622e+01 3.54948959e+02 0.00000000e+00\n",
      " 1.20792265e-13]\n",
      "7-th iteration, loss: 0.255161308450703, 17 gd steps\n",
      "insert gradient: -0.053879804999266596\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[0.00000000e+00 7.56292211e+01 1.30214084e+02 8.90450678e+01\n",
      " 9.84761846e+01 5.08451011e+01 8.20916515e+01 3.27093094e+01\n",
      " 1.10348914e+02 6.59950925e+01 3.15062364e+02 3.41559620e+01\n",
      " 1.66566837e-13]\n",
      "8-th iteration, loss: 0.22867122414575505, 142 gd steps\n",
      "insert gradient: -0.06563701124623803\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[2.14322226e+00 6.42583019e+01 1.35930430e+02 8.06347824e+01\n",
      " 1.10002145e+02 5.08690062e+01 7.11683958e+01 3.48720918e+01\n",
      " 8.56248949e+01 9.04642815e+01 1.28046017e+02 0.00000000e+00\n",
      " 1.02436814e+02 6.05413009e+01 4.21598384e-13]\n",
      "9-th iteration, loss: 0.21651916699451115, 34 gd steps\n",
      "insert gradient: -0.048360323002242224\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[3.80530721e+00 6.80197565e+01 1.22372458e+02 8.93101542e+01\n",
      " 1.15728734e+02 0.00000000e+00 1.24344979e-14 5.76541573e+01\n",
      " 7.60641948e+01 3.32016056e+01 8.37525189e+01 9.48265958e+01\n",
      " 9.75848335e+01 3.09971733e+01 5.87145383e+01 6.83443381e+01\n",
      " 5.29705559e-13]\n",
      "10-th iteration, loss: 0.21004691860549093, 22 gd steps\n",
      "insert gradient: -0.044182126740155155\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[2.82261188e+00 6.75532349e+01 1.22227350e+02 9.49524616e+01\n",
      " 1.14145154e+02 6.06304816e+01 8.19330234e+01 3.29049849e+01\n",
      " 8.14504617e+01 9.46910726e+01 1.02481881e+02 3.85661406e+01\n",
      " 5.65139728e+01 5.77063052e+01 5.46756487e-13]\n",
      "11-th iteration, loss: 0.2078690358616768, 20 gd steps\n",
      "insert gradient: -0.03594531383933382\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[1.79995766e+00 6.53721335e+01 1.21221567e+02 9.65309140e+01\n",
      " 1.14640924e+02 6.39627392e+01 8.64499426e+01 3.22069073e+01\n",
      " 7.85302922e+01 6.48959096e+01 0.00000000e+00 2.94981407e+01\n",
      " 1.06501062e+02 4.24984077e+01 5.48869992e+01 5.37791312e+01\n",
      " 5.57903997e-13]\n",
      "12-th iteration, loss: 0.20681553464731356, 13 gd steps\n",
      "insert gradient: -0.03902925815673713\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[2.15363783e+00 6.70066655e+01 1.21037693e+02 9.71707537e+01\n",
      " 1.14160665e+02 6.28758424e+01 8.86191921e+01 3.35492790e+01\n",
      " 7.68886737e+01 6.35174341e+01 0.00000000e+00 7.10542736e-15\n",
      " 3.43044476e+00 2.79195177e+01 1.07948935e+02 4.40064136e+01\n",
      " 5.42797743e+01 5.22682863e+01 5.69222186e-13]\n",
      "13-th iteration, loss: 0.20006093881945097, 19 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[2.64592283e+00 6.87446555e+01 1.24203413e+02 9.61568536e+01\n",
      " 1.16709358e+02 6.36012341e+01 9.17533073e+01 3.63395413e+01\n",
      " 7.92440295e+01 6.36032728e+01 2.62403611e+01 1.10546033e+01\n",
      " 1.63358488e+01 0.00000000e+00 9.80150926e+01 4.72356206e+01\n",
      " 4.98372621e+01 5.05494357e+01 6.00231081e-13]\n",
      "14-th iteration, loss: 0.1895096618561833, 57 gd steps\n",
      "insert gradient: -0.0038140869798465498\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[2.33801391e+00 7.13357566e+01 1.28563621e+02 9.62574911e+01\n",
      " 1.16131344e+02 6.61390196e+01 9.63360582e+01 3.98853371e+01\n",
      " 7.73028613e+01 5.27842840e+01 7.60927309e+01 1.10900978e+00\n",
      " 1.04557285e+02 5.05209968e+01 0.00000000e+00 1.15463195e-14\n",
      " 4.91438754e+01 4.80758054e+01 6.23619533e-13]\n",
      "15-th iteration, loss: 0.18881107442482548, 48 gd steps\n",
      "insert gradient: -0.001638374901153283\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[2.29821630e+00 7.04950493e+01 1.31651570e+02 9.67026875e+01\n",
      " 1.15649523e+02 6.62485472e+01 9.87952412e+01 4.05416642e+01\n",
      " 7.72733309e+01 5.00222285e+01 1.87170931e+02 5.08914994e+01\n",
      " 5.40676250e+01 4.45129258e+01 6.37954267e-13]\n",
      "16-th iteration, loss: 0.18869222571968178, 279 gd steps\n",
      "insert gradient: -0.0013744854382971214\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[2.53892398e+00 6.98829639e+01 1.33353546e+02 9.69498154e+01\n",
      " 1.15369673e+02 6.63337341e+01 1.00007347e+02 4.07507972e+01\n",
      " 7.70608267e+01 4.97486907e+01 9.38760239e+01 0.00000000e+00\n",
      " 9.38760239e+01 5.01111382e+01 5.99409458e+01 4.09900233e+01\n",
      " 9.23211497e-13]\n",
      "17-th iteration, loss: 0.18868935885361035, 51 gd steps\n",
      "insert gradient: -0.00033626143555386893\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[2.54151572e+00 6.98983659e+01 1.33549192e+02 9.69208762e+01\n",
      " 1.15382786e+02 0.00000000e+00 2.48689958e-14 6.63468034e+01\n",
      " 1.00064886e+02 4.07659539e+01 7.71108737e+01 4.98498876e+01\n",
      " 9.35295938e+01 3.60197452e-01 9.30502579e+01 5.02434306e+01\n",
      " 6.00168252e+01 4.09527578e+01 9.85853640e-13]\n",
      "18-th iteration, loss: 0.18868844766743298, 917 gd steps\n",
      "insert gradient: -4.0266121005492495e-05\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[2.54390787e+00 6.98924776e+01 1.33601893e+02 9.69149359e+01\n",
      " 1.15385523e+02 6.63755274e+01 1.00094211e+02 4.07762224e+01\n",
      " 7.71169754e+01 4.98422636e+01 9.42179205e+01 4.33727616e-01\n",
      " 9.21619599e+01 5.02667071e+01 6.00833898e+01 4.09228488e+01\n",
      " 1.11255925e-12]\n",
      "19-th iteration, loss: 0.1886874443258856, 126 gd steps\n",
      "insert gradient: -1.0288126279739883e-05\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[2.54579007e+00 6.98960266e+01 1.33645686e+02 9.69010534e+01\n",
      " 1.15387910e+02 6.63827980e+01 1.00106943e+02 4.07775866e+01\n",
      " 7.71386501e+01 4.98408601e+01 9.58772391e+01 5.06790354e-01\n",
      " 9.02776073e+01 5.03058876e+01 6.00866290e+01 4.09123616e+01\n",
      " 1.74493286e-12]\n",
      "20-th iteration, loss: 0.18868744353690303, 13 gd steps\n",
      "insert gradient: -2.5630256462051117e-05\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[2.54773032e+00 6.98963998e+01 1.33647530e+02 9.69008074e+01\n",
      " 1.15386974e+02 6.63798694e+01 1.00111409e+02 4.07784935e+01\n",
      " 7.71331714e+01 4.98428732e+01 9.58888577e+01 5.06450467e-01\n",
      " 9.02666534e+01 5.03045353e+01 6.00899981e+01 4.09104373e+01\n",
      " 1.76391637e-12]\n",
      "21-th iteration, loss: 0.18868743983634106, 64 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.261298683245403e-06\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[2.54629874e+00 0.00000000e+00 7.21644966e-16 6.98938365e+01\n",
      " 1.33646969e+02 9.69014523e+01 1.15388008e+02 6.63828314e+01\n",
      " 1.00109583e+02 4.07774370e+01 7.71358935e+01 4.98418669e+01\n",
      " 9.59212695e+01 5.06442994e-01 9.02334646e+01 5.03056766e+01\n",
      " 6.00913110e+01 4.09099403e+01 1.83931384e-12]\n",
      "22-th iteration, loss: 0.1886874398334359, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.056423888370797e-06\n",
      "22-th iteration, new layer inserted. now 19 layers\n",
      "[2.54630098e+00 8.20989079e-06 2.23811717e-06 6.98938447e+01\n",
      " 1.33646971e+02 9.69014532e+01 1.15388007e+02 6.63828260e+01\n",
      " 1.00109581e+02 4.07774353e+01 7.71358916e+01 4.98418634e+01\n",
      " 9.59212745e+01 5.06448587e-01 9.02334616e+01 5.03056774e+01\n",
      " 6.00913124e+01 4.09099425e+01 1.83931153e-12]\n",
      "23-th iteration, loss: 0.18868743983067965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.85591188297858e-06\n",
      "23-th iteration, new layer inserted. now 19 layers\n",
      "[2.54630316e+00 1.62030541e-05 4.37059151e-06 6.98938527e+01\n",
      " 1.33646973e+02 9.69014540e+01 1.15388005e+02 6.63828207e+01\n",
      " 1.00109579e+02 4.07774338e+01 7.71358898e+01 4.98418598e+01\n",
      " 9.59212794e+01 5.06453920e-01 9.02334586e+01 5.03056781e+01\n",
      " 6.00913138e+01 4.09099446e+01 1.83931722e-12]\n",
      "24-th iteration, loss: 0.18868743982806102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.660055478861386e-06\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[2.54630528e+00 2.39845193e-05 6.40003233e-06 0.00000000e+00\n",
      " 2.54109884e-21 6.98938605e+01 1.33646975e+02 9.69014547e+01\n",
      " 1.15388004e+02 6.63828156e+01 1.00109577e+02 4.07774324e+01\n",
      " 7.71358881e+01 4.98418563e+01 9.59212843e+01 5.06459016e-01\n",
      " 9.02334554e+01 5.03056787e+01 6.00913151e+01 4.09099467e+01\n",
      " 1.83932154e-12]\n",
      "25-th iteration, loss: 0.18868743982494496, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.2978848216022005e-06\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[2.54630734e+00 3.15145738e-05 8.31874704e-06 7.56817823e-06\n",
      " 1.91871472e-06 6.98938681e+01 1.33646977e+02 9.69014552e+01\n",
      " 1.15388003e+02 6.63828105e+01 1.00109575e+02 4.07774313e+01\n",
      " 7.71358864e+01 4.98418528e+01 9.59212891e+01 5.06463927e-01\n",
      " 9.02334522e+01 5.03056792e+01 6.00913164e+01 4.09099487e+01\n",
      " 1.83934321e-12]\n",
      "26-th iteration, loss: 0.1886874398220402, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.963505442698563e-06\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[2.54630931e+00 3.86675402e-05 1.00987817e-05 1.47706977e-05\n",
      " 3.65366720e-06 0.00000000e+00 3.17637355e-22 6.98938753e+01\n",
      " 1.33646979e+02 9.69014556e+01 1.15388001e+02 6.63828056e+01\n",
      " 1.00109574e+02 4.07774302e+01 7.71358848e+01 4.98418493e+01\n",
      " 9.59212938e+01 5.06468772e-01 9.02334490e+01 5.03056797e+01\n",
      " 6.00913177e+01 4.09099508e+01 1.83934496e-12]\n",
      "27-th iteration, loss: 0.1886874398188302, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.5010675133587164e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[2.54631118e+00 4.54320767e-05 1.17400838e-05 2.15953907e-05\n",
      " 5.20698316e-06 6.84645718e-06 1.55331596e-06 0.00000000e+00\n",
      " 2.11758237e-22 6.98938821e+01 1.33646980e+02 9.69014557e+01\n",
      " 1.15388000e+02 6.63828006e+01 1.00109572e+02 4.07774294e+01\n",
      " 7.71358833e+01 4.98418457e+01 9.59212985e+01 5.06473563e-01\n",
      " 9.02334457e+01 5.03056801e+01 6.00913190e+01 4.09099528e+01\n",
      " 1.83935887e-12]\n",
      "28-th iteration, loss: 0.1886874398154854, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.947459776834266e-06\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[2.54631293e+00 5.16827689e-05 1.32157872e-05 2.79160164e-05\n",
      " 6.55404692e-06 1.31980998e-05 2.85959666e-06 6.36089548e-06\n",
      " 1.30628070e-06 0.00000000e+00 1.58818678e-22 6.98938885e+01\n",
      " 1.33646982e+02 9.69014556e+01 1.15387998e+02 6.63827956e+01\n",
      " 1.00109571e+02 4.07774287e+01 7.71358818e+01 4.98418421e+01\n",
      " 9.59213032e+01 5.06478405e-01 9.02334424e+01 5.03056805e+01\n",
      " 6.00913203e+01 4.09099548e+01 1.83936473e-12]\n",
      "29-th iteration, loss: 0.188687439812165, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.341133630027295e-06\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[2.54631453e+00 5.73346662e-05 1.45092668e-05 3.36466377e-05\n",
      " 7.68123594e-06 1.89677624e-05 3.90816701e-06 1.21475921e-05\n",
      " 2.31696035e-06 5.79447793e-06 1.01067965e-06 0.00000000e+00\n",
      " 1.58818678e-22 6.98938943e+01 1.33646983e+02 9.69014553e+01\n",
      " 1.15387997e+02 6.63827905e+01 1.00109569e+02 4.07774280e+01\n",
      " 7.71358804e+01 4.98418384e+01 9.59213079e+01 5.06483362e-01\n",
      " 9.02334392e+01 5.03056807e+01 6.00913215e+01 4.09099568e+01\n",
      " 1.83937073e-12]\n",
      "30-th iteration, loss: 0.1886874398089898, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.4140305210701845e-06\n",
      "30-th iteration, new layer inserted. now 31 layers\n",
      "[2.54631598e+00 6.23443763e-05 1.56141517e-05 3.87427768e-05\n",
      " 8.58569405e-06 2.41096572e-05 4.69963756e-06 1.73127672e-05\n",
      " 3.03606983e-06 1.09734547e-05 1.69527244e-06 5.18499726e-06\n",
      " 6.84592793e-07 6.98938995e+01 1.33646984e+02 9.69014546e+01\n",
      " 1.15387995e+02 6.63827854e+01 1.00109568e+02 4.07774276e+01\n",
      " 7.71358790e+01 4.98418346e+01 9.59213125e+01 0.00000000e+00\n",
      " 3.55271368e-15 5.06488463e-01 9.02334359e+01 5.03056809e+01\n",
      " 6.00913228e+01 4.09099588e+01 1.83938115e-12]\n",
      "31-th iteration, loss: 0.18868743980575067, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.884472978843182e-06\n",
      "31-th iteration, new layer inserted. now 33 layers\n",
      "[2.54631728e+00 6.67513105e-05 1.65439109e-05 4.32427215e-05\n",
      " 9.28466945e-06 2.86607455e-05 5.25499595e-06 2.18918504e-05\n",
      " 3.48829905e-06 1.55706232e-05 2.08213465e-06 9.79226421e-06\n",
      " 1.04056885e-06 6.98939041e+01 1.33646984e+02 9.69014536e+01\n",
      " 1.15387993e+02 6.63827802e+01 1.00109567e+02 4.07774272e+01\n",
      " 7.71358776e+01 4.98418307e+01 9.59213171e+01 4.85230916e-06\n",
      " 4.55401594e-06 0.00000000e+00 2.11758237e-22 5.06493315e-01\n",
      " 9.02334325e+01 5.03056810e+01 6.00913240e+01 4.09099607e+01\n",
      " 1.83938933e-12]\n",
      "32-th iteration, loss: 0.1886874398025356, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.166387555924061e-06\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[2.54631848e+00 7.07536469e-05 1.73495254e-05 4.73436071e-05\n",
      " 9.83269469e-06 3.28169382e-05 5.63229405e-06 2.60793462e-05\n",
      " 3.73519108e-06 1.97788983e-05 2.23627505e-06 1.40129422e-05\n",
      " 1.13637833e-06 6.98939083e+01 1.33646985e+02 9.69014525e+01\n",
      " 1.15387990e+02 6.63827752e+01 1.00109565e+02 4.07774268e+01\n",
      " 7.71358762e+01 4.98418264e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.59213214e+01 8.94248658e-06 8.80613009e-06 4.09051651e-06\n",
      " 4.25211415e-06 5.06497405e-01 9.02334287e+01 5.03056806e+01\n",
      " 6.00913252e+01 4.09099626e+01 1.83940867e-12]\n",
      "33-th iteration, loss: 0.188687439799561, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.849194309405082e-06\n",
      "33-th iteration, new layer inserted. now 35 layers\n",
      "[2.54631962e+00 7.44700351e-05 1.80610948e-05 5.11633433e-05\n",
      " 1.02622465e-05 3.66952460e-05 5.86636094e-06 2.99912047e-05\n",
      " 3.81390759e-06 2.37130067e-05 2.19717185e-06 1.79603718e-05\n",
      " 1.01380225e-06 6.98939123e+01 1.33646986e+02 9.69014513e+01\n",
      " 1.15387988e+02 6.63827703e+01 1.00109564e+02 4.07774265e+01\n",
      " 7.71358747e+01 4.98418217e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.59213293e+01 1.22917292e-05 1.27748052e-05 7.44041483e-06\n",
      " 8.22048466e-06 5.06500756e-01 9.02334247e+01 5.03056800e+01\n",
      " 6.00913262e+01 4.09099644e+01 1.83942401e-12]\n",
      "34-th iteration, loss: 0.18868743979691552, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.620915569895495e-06\n",
      "34-th iteration, new layer inserted. now 35 layers\n",
      "[2.54632068e+00 7.79139349e-05 1.86839181e-05 5.47148298e-05\n",
      " 1.05802987e-05 4.03078630e-05 5.96582560e-06 3.36387666e-05\n",
      " 3.73471949e-06 2.73832875e-05 1.97672910e-06 2.16437407e-05\n",
      " 6.86372334e-07 6.98939160e+01 1.33646986e+02 9.69014500e+01\n",
      " 1.15387986e+02 6.63827656e+01 1.00109563e+02 4.07774264e+01\n",
      " 7.71358732e+01 4.98418168e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.59213367e+01 1.49866396e-05 1.64850678e-05 1.01362764e-05\n",
      " 1.19301933e-05 5.06503452e-01 9.02334205e+01 5.03056790e+01\n",
      " 6.00913272e+01 4.09099661e+01 1.83943775e-12]\n",
      "35-th iteration, loss: 0.1886874397945113, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.454418211085945e-06\n",
      "35-th iteration, new layer inserted. now 35 layers\n",
      "[2.54632168e+00 8.10964351e-05 1.92218627e-05 5.80086268e-05\n",
      " 1.07923164e-05 4.36646851e-05 5.93773598e-06 3.70311262e-05\n",
      " 3.50624905e-06 3.07978942e-05 1.58514063e-06 2.50701225e-05\n",
      " 1.65855356e-07 6.98939194e+01 1.33646987e+02 9.69014486e+01\n",
      " 1.15387984e+02 6.63827610e+01 1.00109561e+02 4.07774263e+01\n",
      " 7.71358718e+01 4.98418116e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.59213438e+01 1.72017935e-05 2.00088098e-05 1.23526577e-05\n",
      " 1.54531807e-05 5.06505669e-01 9.02334160e+01 5.03056778e+01\n",
      " 6.00913281e+01 4.09099678e+01 1.83943821e-12]\n",
      "36-th iteration, loss: 0.1886874397922964, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3319139480239545e-06\n",
      "36-th iteration, new layer inserted. now 33 layers\n",
      "[2.54632263e+00 8.40368219e-05 1.96808186e-05 6.10635149e-05\n",
      " 1.09057247e-05 4.67838613e-05 5.79104086e-06 4.01856725e-05\n",
      " 3.13896527e-06 3.39733267e-05 1.03439843e-06 6.98939508e+01\n",
      " 1.33646987e+02 9.69014471e+01 1.15387982e+02 6.63827566e+01\n",
      " 1.00109560e+02 4.07774264e+01 7.71358703e+01 4.98418064e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59213505e+01 1.90560712e-05\n",
      " 2.33961435e-05 1.42084252e-05 1.88395946e-05 5.06507526e-01\n",
      " 9.02334115e+01 5.03056765e+01 6.00913290e+01 4.09099695e+01\n",
      " 1.83944471e-12]\n",
      "37-th iteration, loss: 0.18868743979032013, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.230917778714139e-06\n",
      "37-th iteration, new layer inserted. now 33 layers\n",
      "[2.54632352e+00 8.67673479e-05 2.00696039e-05 6.39112762e-05\n",
      " 1.09307649e-05 4.96965862e-05 5.53739706e-06 4.31328937e-05\n",
      " 2.64594157e-06 3.69392463e-05 3.39000642e-07 6.98939538e+01\n",
      " 1.33646987e+02 9.69014456e+01 1.15387979e+02 6.63827522e+01\n",
      " 1.00109559e+02 4.07774265e+01 7.71358690e+01 4.98418011e+01\n",
      " 0.00000000e+00 1.77635684e-15 9.59213571e+01 2.06249046e-05\n",
      " 2.66819267e-05 1.57790006e-05 2.21243200e-05 5.06509098e-01\n",
      " 9.02334068e+01 5.03056751e+01 6.00913299e+01 4.09099711e+01\n",
      " 1.83944647e-12]\n",
      "38-th iteration, loss: 0.1886874397884501, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1591403283945553e-06\n",
      "38-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632437e+00 8.93510242e-05 2.04042292e-05 6.66145036e-05\n",
      " 1.08846813e-05 5.24649264e-05 5.19527874e-06 4.59342194e-05\n",
      " 2.04688712e-06 6.98939964e+01 1.33646988e+02 9.69014441e+01\n",
      " 1.15387977e+02 6.63827480e+01 1.00109558e+02 4.07774268e+01\n",
      " 7.71358676e+01 4.98417958e+01 9.59213635e+01 2.19344238e-05\n",
      " 2.98857673e-05 1.70905065e-05 2.53269857e-05 5.06510411e-01\n",
      " 9.02334020e+01 5.03056736e+01 6.00913307e+01 4.09099727e+01\n",
      " 1.83945188e-12]\n",
      "39-th iteration, loss: 0.1886874397868468, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.149620430466098e-06\n",
      "39-th iteration, new layer inserted. now 31 layers\n",
      "[2.54632518e+00 9.17940829e-05 2.06874409e-05 6.91791068e-05\n",
      " 1.07710814e-05 5.50943679e-05 4.76915345e-06 4.85946084e-05\n",
      " 1.34713852e-06 6.98939991e+01 1.33646988e+02 9.69014425e+01\n",
      " 1.15387975e+02 6.63827439e+01 1.00109558e+02 4.07774273e+01\n",
      " 7.71358663e+01 4.98417905e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.59213666e+01 2.30043760e-05 3.30086236e-05 1.81626839e-05\n",
      " 2.84485695e-05 5.06511486e-01 9.02333972e+01 5.03056720e+01\n",
      " 6.00913315e+01 4.09099743e+01 1.83945965e-12]\n",
      "40-th iteration, loss: 0.18868743978520136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0929156013786883e-06\n",
      "40-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632597e+00 9.41494764e-05 2.09311802e-05 7.16577320e-05\n",
      " 1.06027322e-05 5.76371547e-05 4.27261563e-06 5.11658043e-05\n",
      " 5.61129779e-07 6.98940016e+01 1.33646988e+02 9.69014410e+01\n",
      " 1.15387973e+02 6.63827400e+01 1.00109557e+02 4.07774279e+01\n",
      " 7.71358652e+01 4.98417854e+01 9.59213729e+01 2.40120781e-05\n",
      " 3.61461930e-05 1.91728440e-05 3.15847865e-05 5.06512498e-01\n",
      " 9.02333923e+01 5.03056705e+01 6.00913323e+01 4.09099759e+01\n",
      " 1.83946684e-12]\n",
      "41-th iteration, loss: 0.1886874397837152, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1042775151200533e-06\n",
      "41-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632674e+00 9.64149540e-05 2.11370081e-05 7.40478931e-05\n",
      " 1.03817067e-05 6.00904746e-05 3.70825438e-06 6.98940578e+01\n",
      " 1.33646989e+02 9.69014394e+01 1.15387970e+02 6.63827361e+01\n",
      " 1.00109556e+02 4.07774286e+01 7.71358640e+01 4.98417803e+01\n",
      " 0.00000000e+00 8.88178420e-15 9.59213760e+01 2.48082938e-05\n",
      " 3.92178313e-05 1.99717511e-05 3.46549973e-05 5.06513299e-01\n",
      " 9.02333875e+01 5.03056689e+01 6.00913331e+01 4.09099775e+01\n",
      " 1.83947974e-12]\n",
      "42-th iteration, loss: 0.18868743978224353, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0555219555516026e-06\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[2.54632748e+00 9.86062851e-05 2.13078483e-05 7.63651338e-05\n",
      " 1.01114556e-05 6.24695575e-05 3.08005355e-06 6.98940602e+01\n",
      " 1.33646989e+02 9.69014379e+01 1.15387968e+02 6.63827324e+01\n",
      " 1.00109555e+02 4.07774295e+01 7.71358630e+01 4.98417754e+01\n",
      " 9.59213822e+01 2.55867087e-05 4.23189264e-05 2.07530864e-05\n",
      " 3.77546053e-05 5.06514083e-01 9.02333826e+01 5.03056674e+01\n",
      " 6.00913338e+01 4.09099790e+01 1.83947890e-12]\n",
      "43-th iteration, loss: 0.18868743978091002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0731566195435804e-06\n",
      "43-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632822e+00 1.00753534e-04 2.14526776e-05 7.86393097e-05\n",
      " 9.80139016e-06 6.48039658e-05 2.39786637e-06 6.98940625e+01\n",
      " 1.33646989e+02 9.69014363e+01 1.15387966e+02 6.63827288e+01\n",
      " 1.00109555e+02 4.07774304e+01 7.71358620e+01 4.98417705e+01\n",
      " 0.00000000e+00 1.77635684e-15 9.59213853e+01 2.61657305e-05\n",
      " 4.53626439e-05 2.13352595e-05 4.07967775e-05 5.06514668e-01\n",
      " 9.02333777e+01 5.03056658e+01 6.00913346e+01 4.09099806e+01\n",
      " 1.83949179e-12]\n",
      "44-th iteration, loss: 0.18868743977951308, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.038594164364344e-06\n",
      "44-th iteration, new layer inserted. now 29 layers\n",
      "[2.54632893e+00 1.02856091e-04 2.15704365e-05 8.08696566e-05\n",
      " 9.45070733e-06 6.70926980e-05 1.66115606e-06 6.98940648e+01\n",
      " 1.33646990e+02 9.69014348e+01 1.15387964e+02 6.63827253e+01\n",
      " 1.00109554e+02 4.07774316e+01 7.71358612e+01 4.98417658e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59213914e+01 2.67383109e-05\n",
      " 4.84405798e-05 2.19112178e-05 4.38731247e-05 5.06515247e-01\n",
      " 9.02333728e+01 5.03056643e+01 6.00913353e+01 4.09099821e+01\n",
      " 1.83951340e-12]\n",
      "45-th iteration, loss: 0.18868743977814836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.016466567193415e-06\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[2.54632964e+00 1.04917462e-04 2.16632752e-05 8.30595194e-05\n",
      " 9.06181846e-06 6.93388573e-05 8.72605992e-07 6.98940671e+01\n",
      " 1.33646990e+02 9.69014334e+01 1.15387962e+02 6.63827219e+01\n",
      " 1.00109554e+02 4.07774328e+01 7.71358603e+01 4.98417612e+01\n",
      " 9.59213975e+01 2.71830678e-05 5.14913238e-05 2.23595820e-05\n",
      " 4.69222372e-05 5.06515699e-01 9.02333679e+01 5.03056627e+01\n",
      " 6.00913361e+01 4.09099836e+01 1.83953956e-12]\n",
      "46-th iteration, loss: 0.18868743977690858, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0500809780307986e-06\n",
      "46-th iteration, new layer inserted. now 29 layers\n",
      "[2.54633033e+00 1.06924800e-04 2.17289153e-05 8.51959022e-05\n",
      " 8.63268617e-06 7.15292199e-05 3.04323798e-08 6.98940693e+01\n",
      " 1.33646990e+02 9.69014319e+01 1.15387960e+02 6.63827186e+01\n",
      " 1.00109553e+02 4.07774341e+01 7.71358596e+01 4.98417566e+01\n",
      " 0.00000000e+00 5.32907052e-15 9.59214005e+01 2.75068033e-05\n",
      " 5.45078261e-05 2.26871520e-05 4.99370745e-05 5.06516030e-01\n",
      " 9.02333629e+01 5.03056612e+01 6.00913368e+01 4.09099852e+01\n",
      " 1.83954759e-12]\n",
      "47-th iteration, loss: 0.1886874397756068, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.027621959045647e-06\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633101e+00 1.08882638e-04 2.17676472e-05 8.72831755e-05\n",
      " 8.16391955e-06 6.98941451e+01 1.33646991e+02 9.69014305e+01\n",
      " 1.15387958e+02 6.63827154e+01 1.00109553e+02 4.07774355e+01\n",
      " 7.71358589e+01 4.98417522e+01 9.59214067e+01 2.78691028e-05\n",
      " 5.75689094e-05 2.30535107e-05 5.29964684e-05 5.06516400e-01\n",
      " 9.02333580e+01 5.03056597e+01 6.00913375e+01 4.09099867e+01\n",
      " 1.83955080e-12]\n",
      "48-th iteration, loss: 0.1886874397744734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0533095946512484e-06\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633167e+00 1.10791483e-04 2.17811676e-05 8.93216870e-05\n",
      " 7.65750784e-06 6.98941472e+01 1.33646991e+02 9.69014290e+01\n",
      " 1.15387957e+02 6.63827122e+01 1.00109553e+02 4.07774369e+01\n",
      " 7.71358582e+01 4.98417478e+01 9.59214097e+01 2.81175289e-05\n",
      " 6.05963631e-05 2.33062240e-05 5.60222054e-05 5.06516656e-01\n",
      " 9.02333531e+01 5.03056582e+01 6.00913382e+01 4.09099881e+01\n",
      " 1.83956286e-12]\n",
      "49-th iteration, loss: 0.18868743977336125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0735235351712152e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[2.54633232e+00 1.12679537e-04 2.17756657e-05 9.13394881e-05\n",
      " 7.11993067e-06 6.98941492e+01 1.33646991e+02 9.69014277e+01\n",
      " 1.15387955e+02 6.63827092e+01 1.00109552e+02 4.07774385e+01\n",
      " 7.71358576e+01 4.98417435e+01 9.59214127e+01 2.83599515e-05\n",
      " 6.36468073e-05 2.35531594e-05 5.90709141e-05 5.06516907e-01\n",
      " 9.02333481e+01 5.03056567e+01 6.00913389e+01 4.09099896e+01\n",
      " 1.83957898e-12]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.53615684102246\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.59145123   0.         860.93116183]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6252791532618974\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 230.98153122   0.         629.94963061]\n",
      "2-th iteration, loss: 0.6036407791272141, 13 gd steps\n",
      "insert gradient: -0.6377458424663993\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.70763905  77.96399303 217.06852198  42.4114489  244.2661833\n",
      "   0.         385.68344731]\n",
      "3-th iteration, loss: 0.46276138578816994, 28 gd steps\n",
      "insert gradient: -0.717599980057436\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.30497788 227.36794006  48.07019182 161.49137796\n",
      "  50.7806192  363.64439318   0.          22.03905413]\n",
      "4-th iteration, loss: 0.37871581962501016, 14 gd steps\n",
      "insert gradient: -0.2944730264569776\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.55375599  61.83953572 220.20870363  61.02240831 144.9868635\n",
      "  45.04806695 239.22456294   0.         100.72613176  49.4793047\n",
      "  22.03905413]\n",
      "5-th iteration, loss: 0.29593148364295324, 47 gd steps\n",
      "insert gradient: -0.03814060834413912\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          75.22731587 106.51180226   0.         106.51180226\n",
      "  66.30003284 115.28820819  58.16579857 179.38697473  47.09146704\n",
      "  79.16159589  45.18307761  22.03905413]\n",
      "6-th iteration, loss: 0.29565340077572283, 13 gd steps\n",
      "insert gradient: -0.01708315133704025\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.61887364e+01 1.02916928e+02 0.00000000e+00\n",
      " 3.73034936e-14 2.64179118e+00 1.02273035e+02 6.71466875e+01\n",
      " 1.15758171e+02 5.81074491e+01 1.78908912e+02 4.78335201e+01\n",
      " 7.95921103e+01 4.46798475e+01 2.20390541e+01]\n",
      "7-th iteration, loss: 0.29558542707475655, 17 gd steps\n",
      "insert gradient: -0.006591361450998393\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.61852674e+01 1.03144951e+02 1.69954739e-01\n",
      " 1.82461272e-01 2.83666741e+00 1.01926663e+02 6.67518458e+01\n",
      " 1.15875584e+02 5.82916550e+01 1.79368809e+02 4.76557240e+01\n",
      " 7.95091771e+01 4.51206707e+01 2.20390541e+01]\n",
      "8-th iteration, loss: 0.29555242688556965, 22 gd steps\n",
      "insert gradient: -0.0024413146805199208\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.62735683e+01 1.03889841e+02 7.11915437e-03\n",
      " 7.06507981e-01 3.18171202e+00 1.00613309e+02 6.67521846e+01\n",
      " 1.16258544e+02 5.82856518e+01 1.79795172e+02 4.76977617e+01\n",
      " 7.97128660e+01 4.51407801e+01 2.20390541e+01]\n",
      "9-th iteration, loss: 0.29554345420127753, 202 gd steps\n",
      "insert gradient: -0.00033291403312604105\n",
      "9-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.34528203 105.0516571    3.31887414 100.03738349\n",
      "  66.65532974 116.47552296  58.29158317 180.01840244  47.6955232\n",
      "  79.91282012  45.19232996  22.03905413]\n",
      "10-th iteration, loss: 0.29554326498722805, 150 gd steps\n",
      "insert gradient: -0.00036249992161231556\n",
      "10-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63478790e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05062699e+02 3.32329178e+00 1.00014076e+02 6.66549096e+01\n",
      " 1.16483318e+02 5.82888488e+01 1.80027655e+02 4.76943246e+01\n",
      " 7.99182122e+01 4.51924000e+01 2.20390541e+01]\n",
      "11-th iteration, loss: 0.29554299785239546, 199 gd steps\n",
      "insert gradient: -0.000333839699749244\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63498678e+01 1.40051353e-02 2.91857809e-06\n",
      " 1.05076823e+02 3.32758813e+00 9.99830666e+01 6.66542330e+01\n",
      " 1.16492485e+02 5.82857120e+01 1.80038493e+02 4.76934910e+01\n",
      " 7.99253585e+01 4.51927688e+01 2.20390541e+01]\n",
      "12-th iteration, loss: 0.29554281760039863, 147 gd steps\n",
      "insert gradient: -0.0003256983611748408\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63510192e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05110463e+02 3.33012817e+00 9.99610206e+01 6.66537626e+01\n",
      " 1.16498439e+02 5.82836675e+01 1.80045278e+02 4.76928709e+01\n",
      " 7.99301474e+01 4.51928368e+01 2.20390541e+01]\n",
      "13-th iteration, loss: 0.2955426756911953, 124 gd steps\n",
      "insert gradient: -0.00032278387830824904\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63516656e+01 7.90221114e-03 9.49702612e-06\n",
      " 0.00000000e+00 1.48230766e-21 1.05118387e+02 3.33208778e+00\n",
      " 9.99430705e+01 6.66534656e+01 1.16502961e+02 5.82821361e+01\n",
      " 1.80050309e+02 4.76923999e+01 7.99339025e+01 4.51928437e+01\n",
      " 2.20390541e+01]\n",
      "14-th iteration, loss: 0.2955424763018284, 154 gd steps\n",
      "insert gradient: -0.0002888962821284706\n",
      "14-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35181418 105.15525902   3.33324276  99.91912355\n",
      "  66.65265003 116.50841383  58.28039701 180.05645465  47.69209255\n",
      "  79.93874979  45.19282639  22.03905413]\n",
      "15-th iteration, loss: 0.2955424195409074, 60 gd steps\n",
      "insert gradient: -0.00032179398960825994\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63520166e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05158969e+02 3.33419609e+00 9.99105017e+01 6.66525831e+01\n",
      " 1.16510372e+02 5.82797426e+01 1.80058485e+02 4.76917840e+01\n",
      " 7.99403723e+01 4.51927082e+01 2.20390541e+01]\n",
      "16-th iteration, loss: 0.295542329505211, 87 gd steps\n",
      "insert gradient: -0.00031025698114949874\n",
      "16-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63522862e+01 0.00000000e+00 4.26325641e-14\n",
      " 1.05169637e+02 3.33566763e+00 9.98983730e+01 6.66526196e+01\n",
      " 1.16513016e+02 5.82788508e+01 1.80061155e+02 4.76913522e+01\n",
      " 7.99426143e+01 4.51925849e+01 2.20390541e+01]\n",
      "17-th iteration, loss: 0.2955422477852864, 81 gd steps\n",
      "insert gradient: -0.00030699926870489543\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35245476 105.17935702   3.3368628   99.8871523\n",
      "  66.65263084 116.51533772  58.27810896 180.06351537  47.69107035\n",
      "  79.94467543  45.19250822  22.03905413]\n",
      "18-th iteration, loss: 0.29554220336124304, 51 gd steps\n",
      "insert gradient: -0.00032333151699300727\n",
      "18-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35266104 105.18253661   3.33786934  99.88017299\n",
      "  66.65275603 116.51677363  58.27764446 180.06491082  47.69084001\n",
      "  79.94591585  45.19243497  22.03905413]\n",
      "19-th iteration, loss: 0.29554216131308103, 49 gd steps\n",
      "insert gradient: -0.00032941167817682513\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63529372e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05185693e+02 3.33903709e+00 9.98735761e+01 6.66529789e+01\n",
      " 1.16518115e+02 5.82771919e+01 1.80066159e+02 4.76905732e+01\n",
      " 7.99470566e+01 4.51923587e+01 2.20390541e+01]\n",
      "20-th iteration, loss: 0.2955420911942776, 72 gd steps\n",
      "insert gradient: -0.0003115892736571349\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63531575e+01 4.44781862e-03 2.36347011e-05\n",
      " 0.00000000e+00 8.47032947e-21 1.05190147e+02 3.34034324e+00\n",
      " 9.98638792e+01 6.66531761e+01 1.16519956e+02 5.82766017e+01\n",
      " 1.80067946e+02 4.76903538e+01 7.99487571e+01 4.51923190e+01\n",
      " 2.20390541e+01]\n",
      "21-th iteration, loss: 0.2955419864069977, 96 gd steps\n",
      "insert gradient: -0.0002825556058860012\n",
      "21-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63530528e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05211370e+02 3.34107164e+00 9.98503351e+01 6.66530289e+01\n",
      " 1.16522298e+02 5.82759256e+01 1.80070412e+02 4.76903531e+01\n",
      " 7.99511445e+01 4.51923112e+01 2.20390541e+01]\n",
      "22-th iteration, loss: 0.2955419270049164, 63 gd steps\n",
      "insert gradient: -0.00029404456373590104\n",
      "22-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35300367 105.21854952   3.34163366  99.84164646\n",
      "  66.6529644  116.52377695  58.27548605 180.07191713  47.69027201\n",
      "  79.95259738  45.19223083  22.03905413]\n",
      "23-th iteration, loss: 0.29554189219932203, 42 gd steps\n",
      "insert gradient: -0.0003107753561439937\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63530942e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05221121e+02 3.34234157e+00 9.98358723e+01 6.66530661e+01\n",
      " 1.16524779e+02 5.82751675e+01 1.80072851e+02 4.76901129e+01\n",
      " 7.99535106e+01 4.51921372e+01 2.20390541e+01]\n",
      "24-th iteration, loss: 0.29554183720290456, 60 gd steps\n",
      "insert gradient: -0.00030036881574818257\n",
      "24-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63531862e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05228234e+02 3.34328236e+00 9.98278923e+01 6.66532192e+01\n",
      " 1.16526108e+02 5.82747368e+01 1.80074077e+02 4.76899094e+01\n",
      " 7.99547576e+01 4.51920321e+01 2.20390541e+01]\n",
      "25-th iteration, loss: 0.2955417849136966, 58 gd steps\n",
      "insert gradient: -0.000296872569931073\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532163e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05234955e+02 3.34405118e+00 9.98201937e+01 6.66533291e+01\n",
      " 1.16527337e+02 5.82743678e+01 1.80075240e+02 4.76897924e+01\n",
      " 7.99559654e+01 4.51919582e+01 2.20390541e+01]\n",
      "26-th iteration, loss: 0.29554173482239077, 56 gd steps\n",
      "insert gradient: -0.00029490591576657583\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532262e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05241414e+02 3.34475847e+00 9.98127531e+01 6.66534301e+01\n",
      " 1.16528489e+02 5.82740270e+01 1.80076327e+02 4.76896959e+01\n",
      " 7.99571145e+01 4.51918874e+01 2.20390541e+01]\n",
      "27-th iteration, loss: 0.2955416866945056, 54 gd steps\n",
      "insert gradient: -0.00029333791965852437\n",
      "27-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35322527 105.24765203   3.34542987  99.80555123\n",
      "  66.6535313  116.52957213  58.27370779 180.07734303  47.68960764\n",
      "  79.95820621  45.19181588  22.03905413]\n",
      "28-th iteration, loss: 0.295541657533137, 38 gd steps\n",
      "insert gradient: -0.0003065271370194082\n",
      "28-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63532880e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05249913e+02 3.34606121e+00 9.98005472e+01 6.66536731e+01\n",
      " 1.16530331e+02 5.82734826e+01 1.80078020e+02 4.76895102e+01\n",
      " 7.99589406e+01 4.51917498e+01 2.20390541e+01]\n",
      "29-th iteration, loss: 0.29554161189231665, 52 gd steps\n",
      "insert gradient: -0.00029641494908436914\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3533433  105.25604516   3.34686427  99.79373111\n",
      "  66.65385622 116.53132089  58.27317365 180.07889695  47.68938284\n",
      "  79.95992581  45.1916713   22.03905413]\n",
      "30-th iteration, loss: 0.2955415843684699, 36 gd steps\n",
      "insert gradient: -0.00030735474082741707\n",
      "30-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.353413   105.25823431   3.34749949  99.78896745\n",
      "  66.65402055 116.53201113  58.27297236 180.07950012  47.68930082\n",
      "  79.96060763  45.19161624  22.03905413]\n",
      "31-th iteration, loss: 0.29554155763356943, 35 gd steps\n",
      "insert gradient: -0.00031252529397227536\n",
      "31-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35352906 105.26043638   3.34823788  99.7843554\n",
      "  66.65423466 116.53267572  58.27276308 180.08004904  47.68918328\n",
      "  79.96124537  45.19155315  22.03905413]\n",
      "32-th iteration, loss: 0.2955415314888767, 35 gd steps\n",
      "insert gradient: -0.00031535409766542864\n",
      "32-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35366306 105.26263377   3.34901103  99.7798466\n",
      "  66.65447236 116.53331484  58.27256011 180.08056548  47.68906674\n",
      "  79.96186103  45.19149608  22.03905413]\n",
      "33-th iteration, loss: 0.29554150589439493, 34 gd steps\n",
      "insert gradient: -0.0003171489020583961\n",
      "33-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63538044e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05264816e+02 3.34979086e+00 9.97754267e+01 6.66547211e+01\n",
      " 1.16533929e+02 5.82723668e+01 1.80081057e+02 4.76889617e+01\n",
      " 7.99624609e+01 4.51914477e+01 2.20390541e+01]\n",
      "34-th iteration, loss: 0.2955414646583072, 48 gd steps\n",
      "insert gradient: -0.00030169954566687946\n",
      "34-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63538947e+01 2.92003974e-03 7.94228014e-06\n",
      " 0.00000000e+00 1.48230766e-21 1.05267738e+02 3.35061959e+00\n",
      " 9.97692544e+01 6.66549720e+01 1.16534729e+02 5.82721249e+01\n",
      " 1.80081744e+02 4.76889006e+01 7.99633158e+01 4.51914154e+01\n",
      " 2.20390541e+01]\n",
      "35-th iteration, loss: 0.2955414038143116, 63 gd steps\n",
      "insert gradient: -0.0002763084014323868\n",
      "35-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35377204 105.28144541   3.35108847  99.76086595\n",
      "  66.65504751 116.53572493  58.27187168 180.08272634  47.6890055\n",
      "  79.96451677  45.19141588  22.03905413]\n",
      "36-th iteration, loss: 0.2955413791648044, 33 gd steps\n",
      "insert gradient: -0.0002955132661000742\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63537480e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05283381e+02 3.35144078e+00 9.97564474e+01 6.66551225e+01\n",
      " 1.16536264e+02 5.82717407e+01 1.80083239e+02 4.76890313e+01\n",
      " 7.99651255e+01 4.51913853e+01 2.20390541e+01]\n",
      "37-th iteration, loss: 0.295541342063183, 45 gd steps\n",
      "insert gradient: -0.0002904764925030565\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35374848 105.28854649   3.35199351  99.75068831\n",
      "  66.65526065 116.53695336  58.27153042 180.08384562  47.6889833\n",
      "  79.96587355  45.19131783  22.03905413]\n",
      "38-th iteration, loss: 0.2955413189277482, 32 gd steps\n",
      "insert gradient: -0.00030121896766729033\n",
      "38-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35378408 105.29046644   3.35248775  99.74652874\n",
      "  66.65540512 116.53745707  58.27138584 180.08427348  47.68893849\n",
      "  79.9664033   45.19126354  22.03905413]\n",
      "39-th iteration, loss: 0.2955412963660613, 32 gd steps\n",
      "insert gradient: -0.0003066163489523183\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63538622e+01 0.00000000e+00 3.55271368e-14\n",
      " 1.05292406e+02 3.35308312e+00 9.97424885e+01 6.66555971e+01\n",
      " 1.16537947e+02 5.82712318e+01 1.80084659e+02 4.76888569e+01\n",
      " 7.99668973e+01 4.51912010e+01 2.20390541e+01]\n",
      "40-th iteration, loss: 0.295541260985948, 43 gd steps\n",
      "insert gradient: -0.0002946970200741749\n",
      "40-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35390578 105.29752025   3.35374672  99.7370244\n",
      "  66.65580993 116.53857212  58.2710375  180.08517164  47.68879146\n",
      "  79.96757337  45.19114174  22.03905413]\n",
      "41-th iteration, loss: 0.2955412391575383, 31 gd steps\n",
      "insert gradient: -0.00030319530249062687\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63539502e+01 0.00000000e+00 4.26325641e-14\n",
      " 1.05299390e+02 3.35425484e+00 9.97330701e+01 6.66559796e+01\n",
      " 1.16539025e+02 5.82709140e+01 1.80085546e+02 4.76887623e+01\n",
      " 7.99680638e+01 4.51911027e+01 2.20390541e+01]\n",
      "42-th iteration, loss: 0.29554120520744925, 42 gd steps\n",
      "insert gradient: -0.00029322115024775963\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63539761e+01 0.00000000e+00 3.01980663e-14\n",
      " 1.05304326e+02 3.35485740e+00 9.97277806e+01 6.66561786e+01\n",
      " 1.16539603e+02 5.82707425e+01 1.80086025e+02 4.76887266e+01\n",
      " 7.99687107e+01 4.51910559e+01 2.20390541e+01]\n",
      "43-th iteration, loss: 0.2955411722545243, 41 gd steps\n",
      "insert gradient: -0.0002888652240478643\n",
      "43-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3539565  105.3090575    3.35533494  99.72257404\n",
      "  66.65633084 116.54015127  58.27059632 180.08650496  47.68873687\n",
      "  79.96935485  45.19102205  22.03905413]\n",
      "44-th iteration, loss: 0.29554115145120075, 30 gd steps\n",
      "insert gradient: -0.0002989661011359021\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35397687 105.31084293   3.35577323  99.71875273\n",
      "  66.65647887 116.5405606   58.27048917 180.08684774  47.68872629\n",
      "  79.96981424  45.19098596  22.03905413]\n",
      "45-th iteration, loss: 0.2955411311088864, 30 gd steps\n",
      "insert gradient: -0.0003041625884080799\n",
      "45-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63540374e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.05312648e+02 3.35630458e+00 9.97150315e+01 6.66566673e+01\n",
      " 1.16540960e+02 5.82703688e+01 1.80087154e+02 4.76886758e+01\n",
      " 7.99702404e+01 4.51909380e+01 2.20390541e+01]\n",
      "46-th iteration, loss: 0.2955410994035633, 40 gd steps\n",
      "insert gradient: -0.0002928984726349559\n",
      "46-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35406661 105.31736497   3.35689456  99.71005312\n",
      "  66.65687689 116.54146559  58.27021469 180.08755629  47.6886387\n",
      "  79.97081458  45.19089116  22.03905413]\n",
      "47-th iteration, loss: 0.29554107961943904, 29 gd steps\n",
      "insert gradient: -0.00030075842501889793\n",
      "47-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35409891 105.31911277   3.35735145  99.7063962\n",
      "  66.65704378 116.54183843  58.27011677 180.08785672  47.68862749\n",
      "  79.97123769  45.19085957  22.03905413]\n",
      "48-th iteration, loss: 0.295541060191884, 29 gd steps\n",
      "insert gradient: -0.00030492422050669443\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.3541617  105.3208753    3.35787743  99.70281767\n",
      "  66.65724051 116.54220267  58.27000887 180.08812921  47.68858685\n",
      "  79.97163501  45.19081925  22.03905413]\n",
      "49-th iteration, loss: 0.2955410410526909, 28 gd steps\n",
      "insert gradient: -0.00030733140606368966\n",
      "49-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          76.35423848 105.32263998   3.35843307  99.69929704\n",
      "  66.65745309 116.54255695  58.26990065 180.08838492  47.6885394\n",
      "  79.97201863  45.19077951  22.03905413]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5364665204182852\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.80232488   0.         884.84591633]\n",
      "1-th iteration, loss: 0.7498498981810913, 11 gd steps\n",
      "insert gradient: -0.6341293753643171\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.18401986  62.29250037 237.39768487   0.         647.44823146]\n",
      "2-th iteration, loss: 0.605030487631589, 13 gd steps\n",
      "insert gradient: -0.6831801842439986\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.07183426  77.02886188 222.09171429  40.84729674 237.83812584\n",
      "   0.         409.61010562]\n",
      "3-th iteration, loss: 0.47108647062245856, 19 gd steps\n",
      "insert gradient: -0.7362093262722418\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.61169508 225.60297918  57.50168636 163.41189533\n",
      "  47.12467507 374.50066799   0.          35.10943762]\n",
      "4-th iteration, loss: 0.3805374092409368, 13 gd steps\n",
      "insert gradient: -0.3933043536427769\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          63.37034187 225.04471606  59.39619166 149.60983497\n",
      "  49.83274757 222.7882043    0.         111.39410215  49.78118097\n",
      "  35.10943762]\n",
      "5-th iteration, loss: 0.3020543932246289, 20 gd steps\n",
      "insert gradient: -0.09631657393249264\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[2.68279657e+00 7.36594211e+01 2.06842814e+02 7.08577296e+01\n",
      " 1.07083282e+02 6.01659911e+01 1.87549877e+02 0.00000000e+00\n",
      " 8.52651283e-14 3.99889202e+01 7.30024322e+01 4.81089361e+01\n",
      " 3.51094376e+01]\n",
      "6-th iteration, loss: 0.2961292699571588, 73 gd steps\n",
      "insert gradient: -0.057260471855894786\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  0.61877635  75.87070827 105.61654563   0.         105.61654563\n",
      "  67.4304729  114.06513542  57.89492127 181.12828742  46.82435071\n",
      "  78.23722056  45.93127885  35.10943762]\n",
      "7-th iteration, loss: 0.29562043315980324, 18 gd steps\n",
      "insert gradient: -0.0076009753655646205\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.58660625e+01 1.04004284e+02 2.46538915e+00\n",
      " 1.03316045e+02 6.64637438e+01 1.16284683e+02 5.85518428e+01\n",
      " 1.79314874e+02 0.00000000e+00 2.48689958e-14 4.72008624e+01\n",
      " 7.98506165e+01 4.54460374e+01 3.51094376e+01]\n",
      "8-th iteration, loss: 0.29557367092777326, 18 gd steps\n",
      "insert gradient: -0.004963880092817772\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.62366322e+01 1.03886006e+02 0.00000000e+00\n",
      " 1.59872116e-14 2.86395431e+00 1.02279119e+02 6.65532635e+01\n",
      " 1.16248970e+02 5.81712316e+01 1.79845244e+02 1.31133099e-01\n",
      " 1.56960478e-01 4.76028083e+01 7.96990732e+01 4.50905010e+01\n",
      " 3.51094376e+01]\n",
      "9-th iteration, loss: 0.2955614174616655, 13 gd steps\n",
      "insert gradient: -0.001932618223352608\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63212550e+01 1.03935078e+02 9.58498104e-02\n",
      " 3.93692165e-02 2.96157797e+00 1.01872680e+02 6.65243862e+01\n",
      " 1.16395108e+02 5.82900855e+01 1.79962457e+02 1.39042688e-02\n",
      " 9.00318782e-02 4.75978071e+01 7.98319331e+01 4.51838895e+01\n",
      " 3.51094376e+01]\n",
      "10-th iteration, loss: 0.2955521811372716, 636 gd steps\n",
      "insert gradient: -0.000434035796886964\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63729582e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04162737e+02 1.43278161e-01 1.72374702e-01 3.07620249e+00\n",
      " 1.01085294e+02 6.65635455e+01 1.16540980e+02 5.82648403e+01\n",
      " 1.80123717e+02 4.76579082e+01 7.99509929e+01 4.51838188e+01\n",
      " 3.51094376e+01]\n",
      "11-th iteration, loss: 0.29554761154530934, 689 gd steps\n",
      "insert gradient: -0.00032055210745193623\n",
      "11-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63606724e+01 1.04482154e+02 1.01279446e-01\n",
      " 2.63633591e-01 3.14961817e+00 1.00609095e+02 6.65833559e+01\n",
      " 1.16557449e+02 5.82635167e+01 1.80122203e+02 4.76663440e+01\n",
      " 7.99735127e+01 4.51815596e+01 3.51094376e+01]\n",
      "12-th iteration, loss: 0.2955462691172362, 687 gd steps\n",
      "insert gradient: -0.0003480015795472855\n",
      "12-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63601141e+01 0.00000000e+00 3.19744231e-14\n",
      " 1.04541516e+02 8.28151213e-02 3.03265929e-01 3.18884892e+00\n",
      " 1.00444857e+02 6.65958543e+01 1.16559539e+02 5.82633272e+01\n",
      " 1.80119409e+02 4.76684210e+01 7.99781390e+01 4.51820617e+01\n",
      " 3.51094376e+01]\n",
      "13-th iteration, loss: 0.295545109693043, 556 gd steps\n",
      "insert gradient: -0.00029330225466350643\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63558150e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04634447e+02 5.76055062e-02 3.36847022e-01 3.21968098e+00\n",
      " 1.00309489e+02 6.66024506e+01 1.16560044e+02 5.82633979e+01\n",
      " 1.80118333e+02 4.76711236e+01 7.99819117e+01 4.51824437e+01\n",
      " 3.51094376e+01]\n",
      "14-th iteration, loss: 0.29554449019771584, 394 gd steps\n",
      "insert gradient: -0.00027583939602609034\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63527116e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.04683095e+02 4.20707508e-02 3.55953331e-01 3.23883281e+00\n",
      " 1.00234521e+02 6.66060521e+01 1.16560489e+02 5.82635253e+01\n",
      " 1.80117680e+02 4.76722107e+01 7.99838723e+01 4.51825423e+01\n",
      " 3.51094376e+01]\n",
      "15-th iteration, loss: 0.29554405028218217, 308 gd steps\n",
      "insert gradient: -0.0002637167527085539\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63504732e+01 1.04717289e+02 2.99874699e-02\n",
      " 3.70259468e-01 3.25365734e+00 1.00180327e+02 6.66087245e+01\n",
      " 1.16560823e+02 5.82636272e+01 1.80117153e+02 4.76729745e+01\n",
      " 7.99852790e+01 4.51826358e+01 3.51094376e+01]\n",
      "16-th iteration, loss: 0.29554382923111105, 182 gd steps\n",
      "insert gradient: -0.0002777617609503838\n",
      "16-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63497598e+01 1.04726984e+02 2.39087117e-02\n",
      " 3.78796731e-01 3.26300506e+00 1.00151160e+02 6.66107167e+01\n",
      " 1.16561063e+02 5.82636552e+01 1.80116678e+02 4.76731869e+01\n",
      " 7.99859082e+01 4.51826594e+01 3.51094376e+01]\n",
      "17-th iteration, loss: 0.2955436272978505, 169 gd steps\n",
      "insert gradient: -0.0002786159393790651\n",
      "17-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63493568e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.04736094e+02 1.81018003e-02 3.87065553e-01 3.27184909e+00\n",
      " 1.00124609e+02 6.66127207e+01 1.16561232e+02 5.82636991e+01\n",
      " 1.80116181e+02 4.76734891e+01 7.99864962e+01 4.51827780e+01\n",
      " 3.51094376e+01]\n",
      "18-th iteration, loss: 0.2955433324334386, 226 gd steps\n",
      "insert gradient: -0.0002552302415944117\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63482345e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.04759692e+02 8.54428946e-03 3.98123874e-01 3.28295123e+00\n",
      " 1.00088120e+02 6.66147265e+01 1.16561292e+02 5.82637616e+01\n",
      " 1.80115735e+02 4.76742533e+01 7.99874689e+01 4.51830134e+01\n",
      " 3.51094376e+01]\n",
      "19-th iteration, loss: 0.2955430808292688, 184 gd steps\n",
      "insert gradient: -0.0002472165762747389\n",
      "19-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63469996e+01 1.05187150e+02 0.00000000e+00\n",
      " 3.55271368e-14 3.29280578e+00 1.00056628e+02 6.66162586e+01\n",
      " 1.16561355e+02 5.82637916e+01 1.80115412e+02 4.76748080e+01\n",
      " 7.99882687e+01 4.51831102e+01 3.51094376e+01]\n",
      "20-th iteration, loss: 0.2955429682815254, 99 gd steps\n",
      "insert gradient: -0.00020754778029909588\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63460798e+01 1.05191454e+02 2.68048493e-03\n",
      " 4.27176505e-03 3.29553205e+00 1.00039939e+02 6.66168109e+01\n",
      " 1.16561547e+02 5.82641612e+01 1.80115476e+02 4.76755210e+01\n",
      " 7.99888598e+01 4.51832172e+01 3.51094376e+01]\n",
      "21-th iteration, loss: 0.2955428724939761, 91 gd steps\n",
      "insert gradient: -0.00021036107129608205\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63456268e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05195167e+02 4.15057833e-03 7.91679478e-03 3.29712121e+00\n",
      " 1.00025185e+02 6.66174917e+01 1.16561916e+02 5.82645038e+01\n",
      " 1.80115500e+02 4.76760371e+01 7.99893070e+01 4.51831780e+01\n",
      " 3.51094376e+01]\n",
      "22-th iteration, loss: 0.2955427594730731, 104 gd steps\n",
      "insert gradient: -0.00020122393792729832\n",
      "22-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63452450e+01 1.05203434e+02 5.18213637e-03\n",
      " 1.19433002e-02 3.29837666e+00 1.00008348e+02 6.66182704e+01\n",
      " 1.16562368e+02 5.82647831e+01 1.80115471e+02 4.76765544e+01\n",
      " 7.99898023e+01 4.51831159e+01 3.51094376e+01]\n",
      "23-th iteration, loss: 0.29554267749745033, 81 gd steps\n",
      "insert gradient: -0.00021202206922845876\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63451312e+01 1.05206736e+02 6.05353574e-03\n",
      " 1.51463755e-02 3.29948574e+00 9.99954390e+01 6.66190109e+01\n",
      " 1.16562740e+02 5.82649086e+01 1.80115376e+02 4.76768336e+01\n",
      " 7.99901366e+01 4.51830443e+01 3.51094376e+01]\n",
      "24-th iteration, loss: 0.2955426003876867, 77 gd steps\n",
      "insert gradient: -0.00021499144958593433\n",
      "24-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63451349e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05209967e+02 6.89277833e-03 1.82679474e-02 3.30060537e+00\n",
      " 9.99832614e+01 6.66198260e+01 1.16563089e+02 5.82649857e+01\n",
      " 1.80115227e+02 4.76770568e+01 7.99904366e+01 4.51829977e+01\n",
      " 3.51094376e+01]\n",
      "25-th iteration, loss: 0.2955425066598961, 90 gd steps\n",
      "insert gradient: -0.0002051711975141483\n",
      "25-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63450534e+01 1.05217278e+02 7.47912583e-03\n",
      " 2.17815271e-02 3.30158322e+00 9.99690888e+01 6.66206762e+01\n",
      " 1.16563444e+02 5.82650608e+01 1.80115076e+02 4.76774060e+01\n",
      " 7.99908388e+01 4.51830078e+01 3.51094376e+01]\n",
      "26-th iteration, loss: 0.2955424380029718, 71 gd steps\n",
      "insert gradient: -0.00021483560603985502\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63450602e+01 1.05220211e+02 8.04400483e-03\n",
      " 2.45946010e-02 3.30250423e+00 9.99580798e+01 6.66214059e+01\n",
      " 1.16563720e+02 5.82650850e+01 1.80114935e+02 4.76776286e+01\n",
      " 7.99911290e+01 4.51830053e+01 3.51094376e+01]\n",
      "27-th iteration, loss: 0.2955423726468191, 68 gd steps\n",
      "insert gradient: -0.00021721873663354098\n",
      "27-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63451244e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.05223098e+02 8.63124402e-03 2.73568536e-02 3.30347606e+00\n",
      " 9.99475848e+01 6.66221760e+01 1.16563974e+02 5.82650904e+01\n",
      " 1.80114766e+02 4.76778116e+01 7.99913893e+01 4.51830102e+01\n",
      " 3.51094376e+01]\n",
      "28-th iteration, loss: 0.29554229231754103, 80 gd steps\n",
      "insert gradient: -0.0002072033956689064\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63450944e+01 0.00000000e+00 3.19744231e-14\n",
      " 1.05229659e+02 9.00502142e-03 3.04831439e-02 3.30435087e+00\n",
      " 9.99352882e+01 6.66229740e+01 1.16564228e+02 5.82651050e+01\n",
      " 1.80114600e+02 4.76781081e+01 7.99917365e+01 4.51830604e+01\n",
      " 3.51094376e+01]\n",
      "29-th iteration, loss: 0.2955422165820008, 76 gd steps\n",
      "insert gradient: -0.0002052031240408257\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63450063e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.05235807e+02 9.23886528e-03 3.34051734e-02 3.30511269e+00\n",
      " 9.99235766e+01 6.66236697e+01 1.16564447e+02 5.82651087e+01\n",
      " 1.80114466e+02 4.76784051e+01 7.99920769e+01 4.51831099e+01\n",
      " 3.51094376e+01]\n",
      "30-th iteration, loss: 0.29554214467339396, 73 gd steps\n",
      "insert gradient: -0.00020381724823870378\n",
      "30-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63448976e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05241671e+02 9.43257852e-03 3.61877040e-02 3.30585853e+00\n",
      " 9.99123854e+01 6.66243223e+01 1.16564644e+02 5.82650994e+01\n",
      " 1.80114337e+02 4.76786684e+01 7.99923932e+01 4.51831487e+01\n",
      " 3.51094376e+01]\n",
      "31-th iteration, loss: 0.29554207615433037, 71 gd steps\n",
      "insert gradient: -0.00020260959247628282\n",
      "31-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63447746e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.05247289e+02 9.59515998e-03 3.88498143e-02 3.30659438e+00\n",
      " 9.99016599e+01 6.66249466e+01 1.16564824e+02 5.82650861e+01\n",
      " 1.80114214e+02 4.76789070e+01 7.99926900e+01 4.51831820e+01\n",
      " 3.51094376e+01]\n",
      "32-th iteration, loss: 0.29554201070582836, 68 gd steps\n",
      "insert gradient: -0.0002015039166157539\n",
      "32-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63446417e+01 1.05252684e+02 9.73029547e-03\n",
      " 4.14037262e-02 3.30732110e+00 9.98913590e+01 6.66255484e+01\n",
      " 1.16564990e+02 5.82650723e+01 1.80114094e+02 4.76791267e+01\n",
      " 7.99929705e+01 4.51832126e+01 3.51094376e+01]\n",
      "33-th iteration, loss: 0.2955419607468768, 56 gd steps\n",
      "insert gradient: -0.00021043001810471974\n",
      "33-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63445972e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.05254955e+02 9.98238084e-03 4.35543517e-02 3.30808561e+00\n",
      " 9.98830330e+01 6.66261150e+01 1.16565133e+02 5.82650546e+01\n",
      " 1.80113975e+02 4.76792597e+01 7.99931723e+01 4.51832232e+01\n",
      " 3.51094376e+01]\n",
      "34-th iteration, loss: 0.29554189983849816, 64 gd steps\n",
      "insert gradient: -0.00020246644622898483\n",
      "34-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63445083e+01 1.05260142e+02 1.01442268e-02\n",
      " 4.60060231e-02 3.30887246e+00 9.98734145e+01 6.66267452e+01\n",
      " 1.16565279e+02 5.82650399e+01 1.80113836e+02 4.76794374e+01\n",
      " 7.99934165e+01 4.51832556e+01 3.51094376e+01]\n",
      "35-th iteration, loss: 0.2955418532640853, 53 gd steps\n",
      "insert gradient: -0.00021050136674309806\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63444717e+01 1.05262304e+02 1.03474655e-02\n",
      " 4.80496664e-02 3.30961424e+00 9.98655905e+01 6.66272989e+01\n",
      " 1.16565402e+02 5.82650306e+01 1.80113720e+02 4.76795709e+01\n",
      " 7.99936071e+01 4.51832780e+01 3.51094376e+01]\n",
      "36-th iteration, loss: 0.29554180812684905, 52 gd steps\n",
      "insert gradient: -0.00021267825968345\n",
      "36-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63444758e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05264459e+02 1.05980540e-02 5.00867456e-02 3.31041435e+00\n",
      " 9.98580094e+01 6.66278838e+01 1.16565520e+02 5.82650138e+01\n",
      " 1.80113585e+02 4.76796754e+01 7.99937766e+01 4.51832975e+01\n",
      " 3.51094376e+01]\n",
      "37-th iteration, loss: 0.2955417527561651, 60 gd steps\n",
      "insert gradient: -0.0002037128132584577\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63444137e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.05269332e+02 1.06938523e-02 5.23834128e-02 3.31117539e+00\n",
      " 9.98491971e+01 6.66284953e+01 1.16565632e+02 5.82650075e+01\n",
      " 1.80113448e+02 4.76798520e+01 7.99940004e+01 4.51833467e+01\n",
      " 3.51094376e+01]\n",
      "38-th iteration, loss: 0.2955416995256612, 58 gd steps\n",
      "insert gradient: -0.00020158234954512663\n",
      "38-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63443100e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.05273970e+02 1.06906220e-02 5.45664542e-02 3.31184562e+00\n",
      " 9.98406474e+01 6.66290368e+01 1.16565729e+02 5.82650064e+01\n",
      " 1.80113340e+02 4.76800502e+01 7.99942311e+01 4.51833999e+01\n",
      " 3.51094376e+01]\n",
      "39-th iteration, loss: 0.2955416480922608, 57 gd steps\n",
      "insert gradient: -0.0002002589905841098\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63441937e+01 1.05278458e+02 1.06659634e-02\n",
      " 5.66781687e-02 3.31250344e+00 9.98323430e+01 6.66295466e+01\n",
      " 1.16565816e+02 5.82649986e+01 1.80113236e+02 4.76802329e+01\n",
      " 7.99944505e+01 4.51834445e+01 3.51094376e+01]\n",
      "40-th iteration, loss: 0.29554160811899405, 47 gd steps\n",
      "insert gradient: -0.0002082957263999071\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63441448e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.05280377e+02 1.07591743e-02 5.84864873e-02 3.31318609e+00\n",
      " 9.98254898e+01 6.66300228e+01 1.16565897e+02 5.82649852e+01\n",
      " 1.80113135e+02 4.76803466e+01 7.99946109e+01 4.51834663e+01\n",
      " 3.51094376e+01]\n",
      "41-th iteration, loss: 0.2955415594160548, 54 gd steps\n",
      "insert gradient: -0.00020099822413984766\n",
      "41-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440606e+01 1.05284746e+02 1.07775114e-02\n",
      " 6.05426636e-02 3.31390571e+00 9.98176095e+01 6.66305498e+01\n",
      " 1.16565976e+02 5.82649699e+01 1.80113015e+02 4.76804862e+01\n",
      " 7.99947994e+01 4.51835009e+01 3.51094376e+01]\n",
      "42-th iteration, loss: 0.29554152153726254, 45 gd steps\n",
      "insert gradient: -0.00020813874489576344\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440160e+01 1.05286596e+02 1.08467883e-02\n",
      " 6.22845248e-02 3.31457974e+00 9.98110734e+01 6.66310168e+01\n",
      " 1.16566048e+02 5.82649627e+01 1.80112916e+02 4.76805971e+01\n",
      " 7.99949511e+01 4.51835266e+01 3.51094376e+01]\n",
      "43-th iteration, loss: 0.29554148457415896, 45 gd steps\n",
      "insert gradient: -0.0002102154558900777\n",
      "43-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63440044e+01 1.05288448e+02 1.09640070e-02\n",
      " 6.40296820e-02 3.31530856e+00 9.98047003e+01 6.66315107e+01\n",
      " 1.16566117e+02 5.82649481e+01 1.80112800e+02 4.76806810e+01\n",
      " 7.99950849e+01 4.51835471e+01 3.51094376e+01]\n",
      "44-th iteration, loss: 0.29554144845764946, 44 gd steps\n",
      "insert gradient: -0.00021128454312232183\n",
      "44-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63440048e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.05290282e+02 1.10759202e-02 6.57583528e-02 3.31603875e+00\n",
      " 9.97984633e+01 6.66320098e+01 1.16566182e+02 5.82649374e+01\n",
      " 1.80112683e+02 4.76807668e+01 7.99952155e+01 4.51835726e+01\n",
      " 3.51094376e+01]\n",
      "45-th iteration, loss: 0.29554140416338176, 51 gd steps\n",
      "insert gradient: -0.00020262619002846117\n",
      "45-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439479e+01 1.05294407e+02 1.10464462e-02\n",
      " 6.76976500e-02 3.31674256e+00 9.97912433e+01 6.66325261e+01\n",
      " 1.16566239e+02 5.82649340e+01 1.80112564e+02 4.76809090e+01\n",
      " 7.99953880e+01 4.51836213e+01 3.51094376e+01]\n",
      "46-th iteration, loss: 0.2955413696928012, 42 gd steps\n",
      "insert gradient: -0.00020908714625392827\n",
      "46-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439167e+01 1.05296150e+02 1.10587973e-02\n",
      " 6.93379346e-02 3.31738602e+00 9.97852285e+01 6.66329695e+01\n",
      " 1.16566290e+02 5.82649334e+01 1.80112470e+02 4.76810252e+01\n",
      " 7.99955290e+01 4.51836573e+01 3.51094376e+01]\n",
      "47-th iteration, loss: 0.2955413359634496, 42 gd steps\n",
      "insert gradient: -0.00021095515670304198\n",
      "47-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439129e+01 1.05297896e+02 1.11169515e-02\n",
      " 7.09824157e-02 3.31808104e+00 9.97793481e+01 6.66334324e+01\n",
      " 1.16566339e+02 5.82649222e+01 1.80112361e+02 4.76811125e+01\n",
      " 7.99956521e+01 4.51836850e+01 3.51094376e+01]\n",
      "48-th iteration, loss: 0.2955413029255095, 41 gd steps\n",
      "insert gradient: -0.0002118504817316265\n",
      "48-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 7.63439180e+01 1.05299628e+02 1.11731305e-02\n",
      " 7.26134350e-02 3.31877963e+00 9.97735802e+01 6.66338980e+01\n",
      " 1.16566385e+02 5.82649126e+01 1.80112251e+02 4.76811983e+01\n",
      " 7.99957713e+01 4.51837147e+01 3.51094376e+01]\n",
      "49-th iteration, loss: 0.29554127053216095, 41 gd steps\n",
      "insert gradient: -0.00021248487653877085\n",
      "49-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 7.63439276e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.05301343e+02 1.12186207e-02 7.42285041e-02 3.31947322e+00\n",
      " 9.97679139e+01 6.66343610e+01 1.16566427e+02 5.82649051e+01\n",
      " 1.80112140e+02 4.76812862e+01 7.99958885e+01 4.51837471e+01\n",
      " 3.51094376e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.532378848404032\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 46.01319852   0.         908.76067082]\n",
      "1-th iteration, loss: 0.7519772466020608, 11 gd steps\n",
      "insert gradient: -0.648075865769649\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 44.05862423  62.16864421 121.90691926   0.         786.85375157]\n",
      "2-th iteration, loss: 0.509119037923581, 49 gd steps\n",
      "insert gradient: -0.6566803892666219\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.46710872  53.46604916  99.19547274  99.46009194 224.81535759\n",
      "   0.         562.03839398]\n",
      "3-th iteration, loss: 0.4357263470496145, 13 gd steps\n",
      "insert gradient: -0.5745348413125594\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[6.13030512e-02 6.65198261e+01 9.40230769e+01 9.23204106e+01\n",
      " 1.11309507e+02 0.00000000e+00 9.37343219e+01 5.00676913e+01\n",
      " 5.62038394e+02]\n",
      "4-th iteration, loss: 0.33997510990533286, 146 gd steps\n",
      "insert gradient: -0.3225109975132139\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[3.85517615e-01 8.56428906e+01 1.23427334e+02 7.88137866e+01\n",
      " 9.88066834e+01 4.48725766e+01 6.83417660e+01 4.77158866e+01\n",
      " 5.41222157e+02 0.00000000e+00 2.08162368e+01]\n",
      "5-th iteration, loss: 0.29281158670410534, 53 gd steps\n",
      "insert gradient: -0.10382560485710843\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.76985609  69.62385486 137.40869471  84.32270537 100.77443407\n",
      "  46.6315234   66.98117451  42.06431547 177.36634632   0.\n",
      " 310.39110605  58.69456809  20.81623681]\n",
      "6-th iteration, loss: 0.2287663694856964, 96 gd steps\n",
      "insert gradient: -0.05932295652486771\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.76605107  64.08058261 135.86834163  81.47620652 109.96584624\n",
      "  50.82870415  72.17044591  34.83596738  84.69416716  90.67370001\n",
      " 116.02735768   0.         116.02735768  61.68650946  20.81623681]\n",
      "7-th iteration, loss: 0.22345544121944608, 19 gd steps\n",
      "insert gradient: -0.023747538480940952\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[3.54218087e+00 6.45291270e+01 1.34832685e+02 8.15379423e+01\n",
      " 1.12136721e+02 0.00000000e+00 1.59872116e-14 5.30095862e+01\n",
      " 6.91878908e+01 3.39329204e+01 8.99534310e+01 9.27603730e+01\n",
      " 9.82427300e+01 1.53806590e+01 9.07416355e+01 6.41160260e+01\n",
      " 2.08162368e+01]\n",
      "8-th iteration, loss: 0.21711305029066127, 37 gd steps\n",
      "insert gradient: -0.045524501898154934\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  2.20771537  62.44714577 127.89998744  88.48987962 115.38085789\n",
      "  60.05570021  74.60964683  32.55769478  84.25194549  96.00857799\n",
      "  97.33841149  25.87612305  70.07844535  67.40791747  20.81623681]\n",
      "9-th iteration, loss: 0.20871990094694115, 37 gd steps\n",
      "insert gradient: -0.04153240493320586\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  2.09058409  65.29917251 120.0730366   96.13631275 114.26583525\n",
      "  62.73930342  86.56281819  31.78970875  80.27813895  63.18705932\n",
      "   0.          28.7213906  111.35817723  39.06511697  56.87487454\n",
      "  53.23048451  20.81623681]\n",
      "10-th iteration, loss: 0.2075391101716561, 14 gd steps\n",
      "insert gradient: -0.05026670156057329\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[1.69170077e+00 6.56635860e+01 1.20673806e+02 9.67994602e+01\n",
      " 1.14425985e+02 6.32356202e+01 8.82645222e+01 3.30817012e+01\n",
      " 7.69799936e+01 0.00000000e+00 1.24344979e-14 6.16958147e+01\n",
      " 4.24548084e+00 2.66729785e+01 1.12548703e+02 4.24980421e+01\n",
      " 5.46112208e+01 5.15945427e+01 2.08162368e+01]\n",
      "11-th iteration, loss: 0.20559941987418687, 14 gd steps\n",
      "insert gradient: -0.05229779555490137\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[  1.99455791  66.1947935  121.3350015   97.22905861 115.18847274\n",
      "  63.56543589  89.48001988  34.2340126   77.62656757  62.2678747\n",
      "   9.93842658  21.05452732 114.58971429  44.61407091  52.45487996\n",
      "  51.5703828   20.81623681]\n",
      "12-th iteration, loss: 0.18921854424132864, 62 gd steps\n",
      "insert gradient: -0.005434665316047452\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[  2.47169283  71.38516533 128.7821308   96.27768244 116.41883529\n",
      "  66.23497993  97.29131082  40.25026572  77.02762513  50.66461515\n",
      " 186.69367315  51.44310513  48.91253856  47.04490822  20.81623681]\n",
      "13-th iteration, loss: 0.18869222573378236, 251 gd steps\n",
      "insert gradient: -0.0013743373227520406\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[  2.53889121  69.88314107 133.35299874  96.94975411 115.36977581\n",
      "  66.33356724 100.0070211   40.7506943   77.06084685  49.74892913\n",
      "  93.87575267   0.          93.87575267  50.11145231  59.93929493\n",
      "  40.99087594  20.81623681]\n",
      "14-th iteration, loss: 0.18868816751566805, 153 gd steps\n",
      "insert gradient: -1.6241565914552696e-05\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[2.54632852e+00 6.98947636e+01 1.33610472e+02 9.69112410e+01\n",
      " 1.15383717e+02 6.63749159e+01 1.00096723e+02 4.07780670e+01\n",
      " 7.71175115e+01 4.98521536e+01 0.00000000e+00 1.33226763e-14\n",
      " 9.45044850e+01 4.51228991e-01 9.18165256e+01 5.02695376e+01\n",
      " 6.00784325e+01 4.09256284e+01 2.08162368e+01]\n",
      "15-th iteration, loss: 0.18868811077116085, 54 gd steps\n",
      "insert gradient: -2.1255487107328786e-05\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[  2.5441898   69.89332193 133.61470166  96.91156397 115.38606887\n",
      "  66.37739422 100.09745783  40.77771997  77.122838    49.83834637\n",
      "  94.57982      0.44897888  91.75846863  50.27605407  60.08167127\n",
      "  40.92191804  20.81623681]\n",
      "16-th iteration, loss: 0.18868790400222357, 16 gd steps\n",
      "insert gradient: -0.0002698629671659858\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[2.52573777e+00 0.00000000e+00 7.21644966e-16 6.98661571e+01\n",
      " 1.33616682e+02 9.69084263e+01 1.15413319e+02 6.64087827e+01\n",
      " 1.00072804e+02 4.07607469e+01 7.71773785e+01 4.98153326e+01\n",
      " 9.51356639e+01 4.68594789e-01 9.11163678e+01 5.03312586e+01\n",
      " 6.00179532e+01 4.09604400e+01 2.08162368e+01]\n",
      "17-th iteration, loss: 0.1886876531225761, 274 gd steps\n",
      "insert gradient: -2.067402555736575e-05\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[2.54503591e+00 6.98950585e+01 1.33630803e+02 9.69053981e+01\n",
      " 1.15387781e+02 6.63800251e+01 1.00102663e+02 4.07770828e+01\n",
      " 7.71297068e+01 4.98455857e+01 0.00000000e+00 1.15463195e-14\n",
      " 9.52134497e+01 4.86487547e-01 9.09996641e+01 5.02955580e+01\n",
      " 6.00754337e+01 4.09225701e+01 2.08162368e+01]\n",
      "18-th iteration, loss: 0.1886876314362251, 46 gd steps\n",
      "insert gradient: -2.143817125006503e-05\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54535865  69.89483218 133.6337581   96.90513522 115.387657\n",
      "  66.37990158 100.10393897  40.77690989  77.12969297  49.84295414\n",
      "  95.26865955   0.48373807  90.95951735  50.29233242  60.08251837\n",
      "  40.91754608  20.81623681]\n",
      "19-th iteration, loss: 0.18868744111675576, 190 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.389968480600635e-06\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[  2.5464745   69.89468498 133.64727864  96.90131943 115.38785989\n",
      "  66.3828346  100.10973312  40.77783355  77.13564798  49.84136985\n",
      "  95.90661202   0.50690411  90.2485133   50.30520575  60.09153688\n",
      "  40.91003644  20.81623681]\n",
      "20-th iteration, loss: 0.18868744111490082, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.732217769641752e-06\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54647461  69.89468575 133.64727938  96.90132082 115.38785934\n",
      "  66.38283052 100.10973292  40.77783749  77.13565118  49.84137604\n",
      "  95.90661684   0.50690126  90.24850925  50.30521069  60.09153993\n",
      "  40.9100398   20.81623681]\n",
      "21-th iteration, loss: 0.18868744111322008, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.164711398531601e-06\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[2.54647471e+00 6.98946865e+01 1.33647280e+02 9.69013222e+01\n",
      " 1.15387859e+02 6.63828265e+01 1.00109733e+02 4.07778410e+01\n",
      " 7.71356541e+01 0.00000000e+00 8.88178420e-15 4.98413816e+01\n",
      " 9.59066215e+01 5.06898176e-01 9.02485050e+01 5.03052152e+01\n",
      " 6.00915428e+01 4.09100430e+01 2.08162368e+01]\n",
      "22-th iteration, loss: 0.18868744111138241, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3492943271215275e-06\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647479e+00 6.98946871e+01 1.33647281e+02 9.69013235e+01\n",
      " 1.15387858e+02 6.63828224e+01 1.00109732e+02 4.07778441e+01\n",
      " 7.71356567e+01 4.92890127e-06 2.60341118e-06 0.00000000e+00\n",
      " 3.70576914e-22 4.98413865e+01 9.59066259e+01 5.06894809e-01\n",
      " 9.02485006e+01 5.03052194e+01 6.00915456e+01 4.09100461e+01\n",
      " 2.08162368e+01]\n",
      "23-th iteration, loss: 0.18868744110959493, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.965183899186987e-06\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647487e+00 6.98946876e+01 1.33647282e+02 9.69013248e+01\n",
      " 1.15387857e+02 6.63828183e+01 1.00109732e+02 4.07778465e+01\n",
      " 7.71356589e+01 9.00039763e-06 4.77768969e-06 4.08831152e-06\n",
      " 2.17427851e-06 4.98413906e+01 9.59066300e+01 5.06891093e-01\n",
      " 9.02484959e+01 5.03052230e+01 6.00915482e+01 4.09100490e+01\n",
      " 2.08162368e+01]\n",
      "24-th iteration, loss: 0.18868744110809213, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7688868031882896e-06\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647493e+00 6.98946881e+01 1.33647282e+02 9.69013260e+01\n",
      " 1.15387856e+02 6.63828141e+01 1.00109731e+02 4.07778485e+01\n",
      " 7.71356607e+01 1.22031731e-05 6.53142999e-06 7.32194572e-06\n",
      " 3.90161276e-06 4.98413939e+01 9.59066339e+01 5.06887113e-01\n",
      " 9.02484909e+01 5.03052262e+01 6.00915507e+01 4.09100517e+01\n",
      " 2.08162368e+01]\n",
      "25-th iteration, loss: 0.18868744110676372, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6394867562672904e-06\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647497e+00 6.98946884e+01 1.33647283e+02 9.69013271e+01\n",
      " 1.15387855e+02 6.63828100e+01 1.00109730e+02 4.07778499e+01\n",
      " 7.71356622e+01 1.47561585e-05 7.96459673e-06 9.91711711e-06\n",
      " 5.28748759e-06 4.98413965e+01 9.59066377e+01 5.06883004e-01\n",
      " 9.02484858e+01 5.03052290e+01 6.00915531e+01 4.09100543e+01\n",
      " 2.08162368e+01]\n",
      "26-th iteration, loss: 0.18868744110554805, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5578753534758213e-06\n",
      "26-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647500e+00 6.98946887e+01 1.33647284e+02 9.69013282e+01\n",
      " 1.15387854e+02 6.63828058e+01 1.00109729e+02 4.07778511e+01\n",
      " 7.71356635e+01 1.68209994e-05 9.15157844e-06 1.20334008e-05\n",
      " 6.41041527e-06 4.98413986e+01 9.59066413e+01 5.06878860e-01\n",
      " 9.02484806e+01 5.03052316e+01 6.00915553e+01 4.09100568e+01\n",
      " 2.08162368e+01]\n",
      "27-th iteration, loss: 0.1886874411044099, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.510177616196315e-06\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647502e+00 6.98946889e+01 1.33647285e+02 9.69013292e+01\n",
      " 1.15387853e+02 6.63828016e+01 1.00109728e+02 4.07778520e+01\n",
      " 7.71356646e+01 1.85172374e-05 1.01479061e-05 1.37887483e-05\n",
      " 7.32901990e-06 4.98414004e+01 9.59066449e+01 5.06874744e-01\n",
      " 9.02484753e+01 5.03052339e+01 6.00915575e+01 4.09100593e+01\n",
      " 2.08162368e+01]\n",
      "28-th iteration, loss: 0.18868744110332872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4863395570462414e-06\n",
      "28-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647502e+00 6.98946890e+01 1.33647286e+02 9.69013302e+01\n",
      " 1.15387852e+02 6.63827974e+01 1.00109727e+02 4.07778527e+01\n",
      " 7.71356656e+01 1.99334479e-05 1.09951888e-05 1.52705037e-05\n",
      " 8.08724172e-06 4.98414020e+01 9.59066484e+01 5.06870699e-01\n",
      " 9.02484700e+01 5.03052361e+01 6.00915596e+01 4.09100616e+01\n",
      " 2.08162368e+01]\n",
      "29-th iteration, loss: 0.188687441102292, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.479094734785635e-06\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647501e+00 6.98946891e+01 1.33647286e+02 9.69013312e+01\n",
      " 1.15387851e+02 6.63827932e+01 1.00109726e+02 4.07778532e+01\n",
      " 7.71356664e+01 2.11354133e-05 1.17247408e-05 1.65434865e-05\n",
      " 8.71816220e-06 4.98414033e+01 9.59066519e+01 5.06866751e-01\n",
      " 9.02484647e+01 5.03052382e+01 6.00915616e+01 4.09100638e+01\n",
      " 2.08162368e+01]\n",
      "30-th iteration, loss: 0.18868744110129185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.48320836437205e-06\n",
      "30-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647499e+00 6.98946892e+01 1.33647287e+02 9.69013321e+01\n",
      " 1.15387850e+02 6.63827891e+01 1.00109724e+02 4.07778535e+01\n",
      " 7.71356672e+01 2.21721228e-05 1.23602498e-05 1.76559256e-05\n",
      " 9.24681747e-06 4.98414045e+01 9.59066554e+01 5.06862917e-01\n",
      " 9.02484593e+01 5.03052402e+01 6.00915636e+01 4.09100660e+01\n",
      " 2.08162368e+01]\n",
      "31-th iteration, loss: 0.188687441100323, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4949246561756847e-06\n",
      "31-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647497e+00 6.98946891e+01 1.33647288e+02 9.69013330e+01\n",
      " 1.15387849e+02 6.63827850e+01 1.00109723e+02 4.07778538e+01\n",
      " 7.71356679e+01 2.30801789e-05 1.29197403e-05 1.86438161e-05\n",
      " 9.69226904e-06 4.98414055e+01 9.59066589e+01 5.06859205e-01\n",
      " 9.02484540e+01 5.03052421e+01 6.00915656e+01 4.09100682e+01\n",
      " 2.08162368e+01]\n",
      "32-th iteration, loss: 0.18868744109938176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.511562934762468e-06\n",
      "32-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647493e+00 6.98946891e+01 1.33647288e+02 9.69013339e+01\n",
      " 1.15387848e+02 6.63827809e+01 1.00109722e+02 4.07778540e+01\n",
      " 7.71356686e+01 2.38870345e-05 1.34170206e-05 1.95341198e-05\n",
      " 1.00691296e-05 4.98414065e+01 9.59066625e+01 5.06855618e-01\n",
      " 9.02484487e+01 5.03052440e+01 6.00915675e+01 4.09100703e+01\n",
      " 2.08162368e+01]\n",
      "33-th iteration, loss: 0.1886874410984654, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.531222648437379e-06\n",
      "33-th iteration, new layer inserted. now 21 layers\n",
      "[2.54647488e+00 6.98946890e+01 1.33647289e+02 9.69013348e+01\n",
      " 1.15387847e+02 6.63827768e+01 1.00109721e+02 4.07778541e+01\n",
      " 7.71356692e+01 2.46133730e-05 1.38627498e-05 2.03471183e-05\n",
      " 1.03886886e-05 4.98414073e+01 9.59066660e+01 5.06852156e-01\n",
      " 9.02484434e+01 5.03052458e+01 6.00915694e+01 4.09100723e+01\n",
      " 2.08162368e+01]\n",
      "34-th iteration, loss: 0.18868744109757185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5525680572217546e-06\n",
      "34-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647483e+00 6.98946888e+01 1.33647290e+02 9.69013356e+01\n",
      " 1.15387846e+02 6.63827728e+01 1.00109720e+02 4.07778542e+01\n",
      " 7.71356697e+01 2.52748589e-05 1.42652258e-05 2.10981434e-05\n",
      " 1.06597432e-05 4.98414082e+01 0.00000000e+00 7.10542736e-15\n",
      " 9.59066696e+01 5.06848816e-01 9.02484381e+01 5.03052476e+01\n",
      " 6.00915712e+01 4.09100743e+01 2.08162368e+01]\n",
      "35-th iteration, loss: 0.18868744109655908, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5167363407449975e-06\n",
      "35-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647477e+00 6.98946887e+01 1.33647290e+02 9.69013364e+01\n",
      " 1.15387845e+02 6.63827688e+01 1.00109718e+02 4.07778542e+01\n",
      " 7.71356702e+01 2.58425308e-05 1.46150504e-05 2.17579538e-05\n",
      " 1.08732956e-05 4.98414089e+01 3.55689889e-06 7.28661418e-07\n",
      " 9.59066731e+01 5.06845514e-01 9.02484329e+01 5.03052493e+01\n",
      " 6.00915730e+01 4.09100762e+01 2.08162368e+01]\n",
      "36-th iteration, loss: 0.18868744109556793, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.497928907200881e-06\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647471e+00 6.98946885e+01 1.33647291e+02 9.69013372e+01\n",
      " 1.15387843e+02 6.63827649e+01 1.00109717e+02 4.07778541e+01\n",
      " 7.71356707e+01 2.62759321e-05 1.49009395e-05 2.22857533e-05\n",
      " 1.10186507e-05 4.98414095e+01 7.08209733e-06 1.30872113e-06\n",
      " 0.00000000e+00 2.11758237e-22 9.59066767e+01 5.06842216e-01\n",
      " 9.02484276e+01 5.03052509e+01 6.00915748e+01 4.09100781e+01\n",
      " 2.08162368e+01]\n",
      "37-th iteration, loss: 0.1886874410944644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.439920151562162e-06\n",
      "37-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647465e+00 6.98946883e+01 1.33647292e+02 9.69013380e+01\n",
      " 1.15387842e+02 6.63827611e+01 1.00109716e+02 4.07778539e+01\n",
      " 7.71356711e+01 2.65798658e-05 1.51274789e-05 2.26859317e-05\n",
      " 1.11012470e-05 4.98414100e+01 1.05636807e-05 1.74428869e-06\n",
      " 3.48819525e-06 4.35567558e-07 9.59066801e+01 5.06838871e-01\n",
      " 9.02484222e+01 5.03052524e+01 6.00915764e+01 4.09100799e+01\n",
      " 2.08162368e+01]\n",
      "38-th iteration, loss: 0.18868744109338073, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.406671655527159e-06\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647459e+00 6.98946881e+01 1.33647292e+02 9.69013388e+01\n",
      " 1.15387841e+02 6.63827573e+01 1.00109715e+02 4.07778537e+01\n",
      " 7.71356714e+01 2.67453147e-05 1.52957496e-05 2.29490886e-05\n",
      " 1.11229899e-05 4.98414103e+01 1.39945586e-05 2.02577864e-06\n",
      " 6.92788558e-06 6.99434532e-07 0.00000000e+00 1.85288457e-22\n",
      " 9.59066836e+01 5.06835470e-01 9.02484168e+01 5.03052538e+01\n",
      " 6.00915781e+01 4.09100817e+01 2.08162368e+01]\n",
      "39-th iteration, loss: 0.18868744109219074, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.346504212132848e-06\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647453e+00 6.98946879e+01 1.33647293e+02 9.69013396e+01\n",
      " 1.15387840e+02 6.63827536e+01 1.00109713e+02 4.07778534e+01\n",
      " 7.71356717e+01 2.67953569e-05 1.54174875e-05 2.30979257e-05\n",
      " 1.10965002e-05 4.98414105e+01 1.73733984e-05 2.17575588e-06\n",
      " 1.03169600e-05 8.14410855e-07 3.39260806e-06 1.14976323e-07\n",
      " 9.59066870e+01 5.06831987e-01 9.02484114e+01 5.03052551e+01\n",
      " 6.00915797e+01 4.09100834e+01 2.08162368e+01]\n",
      "40-th iteration, loss: 0.18868744109101696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.311950856022234e-06\n",
      "40-th iteration, new layer inserted. now 29 layers\n",
      "[2.54647447e+00 6.98946877e+01 1.33647293e+02 9.69013404e+01\n",
      " 1.15387839e+02 6.63827499e+01 1.00109712e+02 4.07778531e+01\n",
      " 7.71356720e+01 2.67387297e-05 1.55001090e-05 2.31408796e-05\n",
      " 1.10299328e-05 4.98414106e+01 2.07023725e-05 2.20260833e-06\n",
      " 1.36569264e-05 7.89140183e-07 6.73668905e-06 7.25655908e-08\n",
      " 0.00000000e+00 9.92616735e-24 9.59066903e+01 5.06828424e-01\n",
      " 9.02484059e+01 5.03052563e+01 6.00915812e+01 4.09100850e+01\n",
      " 2.08162368e+01]\n",
      "41-th iteration, loss: 0.18868744108974106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2513240382224866e-06\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647441e+00 6.98946876e+01 1.33647294e+02 9.69013412e+01\n",
      " 1.15387838e+02 6.63827464e+01 1.00109711e+02 4.07778527e+01\n",
      " 7.71356722e+01 2.66020823e-05 1.55562557e-05 2.31043471e-05\n",
      " 1.09366130e-05 4.98414107e+01 2.39841271e-05 2.13272573e-06\n",
      " 1.69498090e-05 6.50260348e-07 0.00000000e+00 7.94093388e-23\n",
      " 9.59067070e+01 5.06824763e-01 9.02484004e+01 5.03052574e+01\n",
      " 6.00915827e+01 4.09100866e+01 2.08162368e+01]\n",
      "42-th iteration, loss: 0.18868744108859167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.248477986220215e-06\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647436e+00 6.98946874e+01 1.33647295e+02 9.69013420e+01\n",
      " 1.15387837e+02 6.63827429e+01 1.00109710e+02 4.07778524e+01\n",
      " 7.71356724e+01 2.64071068e-05 1.55960239e-05 2.30098491e-05\n",
      " 1.08271508e-05 4.98414106e+01 2.72371677e-05 1.98769498e-06\n",
      " 2.02136246e-05 4.19596132e-07 0.00000000e+00 6.61744490e-23\n",
      " 9.59067135e+01 5.06821059e-01 9.02483948e+01 5.03052585e+01\n",
      " 6.00915841e+01 4.09100882e+01 2.08162368e+01]\n",
      "43-th iteration, loss: 0.18868744108745242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2473986421672847e-06\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[2.54647430e+00 6.98946872e+01 1.33647295e+02 9.69013428e+01\n",
      " 1.15387836e+02 6.63827395e+01 1.00109709e+02 4.07778521e+01\n",
      " 7.71356726e+01 2.62109892e-05 1.56397895e-05 2.29144659e-05\n",
      " 1.07222962e-05 4.98414106e+01 3.04902278e-05 1.82463699e-06\n",
      " 2.34767268e-05 1.54415315e-07 0.00000000e+00 1.98523347e-23\n",
      " 9.59067200e+01 5.06817367e-01 9.02483893e+01 5.03052595e+01\n",
      " 6.00915855e+01 4.09100897e+01 2.08162368e+01]\n",
      "44-th iteration, loss: 0.18868744108632332, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2444417872021756e-06\n",
      "44-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647425e+00 6.98946871e+01 1.33647296e+02 9.69013436e+01\n",
      " 1.15387835e+02 6.63827362e+01 1.00109707e+02 4.07778518e+01\n",
      " 7.71356728e+01 2.60161488e-05 1.56882818e-05 2.28206424e-05\n",
      " 1.06227846e-05 4.98414106e+01 3.37450299e-05 1.64602653e-06\n",
      " 9.59067533e+01 5.06813691e-01 9.02483837e+01 5.03052604e+01\n",
      " 6.00915869e+01 4.09100912e+01 2.08162368e+01]\n",
      "45-th iteration, loss: 0.18868744108543456, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.320254008662244e-06\n",
      "45-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647419e+00 6.98946869e+01 1.33647297e+02 9.69013444e+01\n",
      " 1.15387834e+02 6.63827330e+01 1.00109706e+02 4.07778515e+01\n",
      " 7.71356731e+01 2.58598705e-05 1.57533578e-05 2.27656943e-05\n",
      " 1.05404626e-05 4.98414106e+01 3.70378714e-05 1.48920479e-06\n",
      " 0.00000000e+00 2.11758237e-22 9.59067566e+01 5.06810139e-01\n",
      " 9.02483782e+01 5.03052614e+01 6.00915883e+01 4.09100927e+01\n",
      " 2.08162368e+01]\n",
      "46-th iteration, loss: 0.1886874410844415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.336759492390121e-06\n",
      "46-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647413e+00 6.98946867e+01 1.33647297e+02 9.69013451e+01\n",
      " 1.15387833e+02 6.63827298e+01 1.00109705e+02 4.07778512e+01\n",
      " 7.71356733e+01 2.57718219e-05 1.58430885e-05 2.27793962e-05\n",
      " 1.04831503e-05 4.98414107e+01 4.03760929e-05 1.38386541e-06\n",
      " 9.59067633e+01 5.06806710e-01 9.02483727e+01 5.03052624e+01\n",
      " 6.00915896e+01 4.09100942e+01 2.08162368e+01]\n",
      "47-th iteration, loss: 0.18868744108357954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3914860859834686e-06\n",
      "47-th iteration, new layer inserted. now 23 layers\n",
      "[2.54647407e+00 6.98946865e+01 1.33647298e+02 9.69013459e+01\n",
      " 1.15387832e+02 6.63827266e+01 1.00109704e+02 4.07778510e+01\n",
      " 7.71356736e+01 2.57173208e-05 1.59446633e-05 2.28272252e-05\n",
      " 1.04375936e-05 4.98414108e+01 4.37512322e-05 1.29541759e-06\n",
      " 9.59067667e+01 5.06803402e-01 9.02483673e+01 5.03052634e+01\n",
      " 6.00915910e+01 4.09100956e+01 2.08162368e+01]\n",
      "48-th iteration, loss: 0.1886874410827303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.433900740537277e-06\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647400e+00 6.98946863e+01 1.33647298e+02 9.69013467e+01\n",
      " 1.15387831e+02 6.63827235e+01 1.00109703e+02 4.07778508e+01\n",
      " 7.71356739e+01 2.57199283e-05 1.60649178e-05 2.29328188e-05\n",
      " 1.04104076e-05 4.98414110e+01 4.71749341e-05 1.24738839e-06\n",
      " 0.00000000e+00 5.29395592e-23 9.59067701e+01 5.06800238e-01\n",
      " 9.02483619e+01 5.03052644e+01 6.00915923e+01 4.09100970e+01\n",
      " 2.08162368e+01]\n",
      "49-th iteration, loss: 0.18868744108176524, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4256886820189315e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[2.54647393e+00 6.98946861e+01 1.33647299e+02 9.69013474e+01\n",
      " 1.15387831e+02 6.63827205e+01 1.00109702e+02 4.07778506e+01\n",
      " 7.71356742e+01 2.57315651e-05 1.61857066e-05 2.30482185e-05\n",
      " 1.03830740e-05 4.98414112e+01 5.06138904e-05 1.19169256e-06\n",
      " 0.00000000e+00 1.58818678e-22 9.59067770e+01 5.06797129e-01\n",
      " 9.02483565e+01 5.03052655e+01 6.00915936e+01 4.09100984e+01\n",
      " 2.08162368e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535388046575191\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 43.28873282   0.         936.61076467]\n",
      "1-th iteration, loss: 0.7469808567750643, 11 gd steps\n",
      "insert gradient: -0.6220056359276027\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.09207687  62.36451321 239.86373241   0.         696.74703225]\n",
      "2-th iteration, loss: 0.607514803743586, 13 gd steps\n",
      "insert gradient: -0.7002161897951394\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.4635834   77.04469709 224.34700949  39.44269071 241.72856221\n",
      "   0.         455.01847004]\n",
      "3-th iteration, loss: 0.47567491598931855, 20 gd steps\n",
      "insert gradient: -0.7173363768445731\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          56.6490121  228.89137612  55.21989105 154.84319619\n",
      "  39.6159821  377.01530375   0.          78.00316629]\n",
      "4-th iteration, loss: 0.35587699484558405, 35 gd steps\n",
      "insert gradient: -0.19680346595430365\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          55.43792894 110.17216906   0.         102.30272841\n",
      "  53.70489687 138.79631748  60.53255303 322.37820145  39.4555736\n",
      "  78.00316629]\n",
      "5-th iteration, loss: 0.3239215196415179, 30 gd steps\n",
      "insert gradient: -0.06648194891291852\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  1.65701798  52.58100067  79.81016487  11.95695782  90.57886181\n",
      "  60.23249322 125.55406352  62.82146105 288.70787379  59.59830943\n",
      "  78.00316629]\n",
      "6-th iteration, loss: 0.3224880629792657, 50 gd steps\n",
      "insert gradient: -0.10893489127631974\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  2.00919157  53.25580821  79.12614285  10.51758422  92.17257836\n",
      "  58.98736724 122.77018052  63.29640148 127.26232904   0.\n",
      " 152.71479485  62.27060034  78.00316629]\n",
      "7-th iteration, loss: 0.3187450840779734, 21 gd steps\n",
      "insert gradient: -0.0303442776118737\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  2.4847488   52.71148459  84.5380406    8.50817749  91.79890231\n",
      "  57.59936115 128.21405917  61.40348084 117.72450762   8.72211358\n",
      " 146.49538428  58.65789785  78.00316629]\n",
      "8-th iteration, loss: 0.3156624104555752, 48 gd steps\n",
      "insert gradient: -0.05172544142028196\n",
      "8-th iteration, new layer inserted. now 13 layers\n",
      "[  2.35368225  52.25266688  88.16244666   5.90778929  91.15466014\n",
      "  56.93625964 125.34356933  63.914357    96.37303278  15.66139482\n",
      " 142.90519597  60.82247307  78.00316629]\n",
      "9-th iteration, loss: 0.28634125404641925, 136 gd steps\n",
      "insert gradient: -0.026568655408213487\n",
      "9-th iteration, new layer inserted. now 11 layers\n",
      "[  4.95170284  55.33727514 179.26854897  58.32924604 152.85071938\n",
      "  53.3493096   80.49340011  36.88844147  98.65504656  60.08686821\n",
      "  78.00316629]\n",
      "10-th iteration, loss: 0.27900915349824834, 18 gd steps\n",
      "insert gradient: -0.006092829206851561\n",
      "10-th iteration, new layer inserted. now 13 layers\n",
      "[8.13448838e+00 6.09600664e+01 1.61636792e+02 5.79014842e+01\n",
      " 1.60408933e+02 5.20191711e+01 8.09376487e+01 3.85054217e+01\n",
      " 8.94693862e+01 0.00000000e+00 3.55271368e-14 6.29977463e+01\n",
      " 7.80031663e+01]\n",
      "11-th iteration, loss: 0.2781876517562937, 41 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.7395663736314672e-17\n",
      "11-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.37512445  62.65207945 157.00042158  59.3869409  158.51827645\n",
      "  50.51244318  78.95410799  39.28344745  87.65208933  63.53397534\n",
      "  78.00316629]\n",
      "12-th iteration, loss: 0.27817860321462046, 29 gd steps\n",
      "insert gradient: -8.96286971273526e-05\n",
      "12-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.94131425  62.63501271 156.52981595  59.56184338 158.16797234\n",
      "  50.53688723  78.7607765   39.24158048  87.58439894  63.4604792\n",
      "  78.00316629]\n",
      "13-th iteration, loss: 0.2781782386753313, 21 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.471648042302706e-06\n",
      "13-th iteration, new layer inserted. now 13 layers\n",
      "[1.08906392e+01 6.26325129e+01 1.56486312e+02 5.95965856e+01\n",
      " 1.58091943e+02 5.05368217e+01 7.87058567e+01 3.92454649e+01\n",
      " 0.00000000e+00 5.77315973e-15 8.76097676e+01 6.34365234e+01\n",
      " 7.80031663e+01]\n",
      "14-th iteration, loss: 0.27817819337037736, 75 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9881849356194023e-06\n",
      "14-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86871803  62.64095689 156.46848365  59.60248616 158.05896446\n",
      "  50.53517705  78.69241248  39.24252443  87.61842741  63.42623912\n",
      "  78.00316629]\n",
      "15-th iteration, loss: 0.2781781930257328, 29 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5801514264834582e-07\n",
      "15-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457914  62.64162696 156.46679377  59.60372832 158.05638597\n",
      "  50.53547054  78.69069454  39.2423359   87.61913702  63.42566446\n",
      "  78.00316629]\n",
      "16-th iteration, loss: 0.2781781930254889, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5414018404575024e-07\n",
      "16-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645786e+01 6.26416261e+01 1.56466791e+02 5.96037251e+01\n",
      " 1.58056383e+02 5.05354707e+01 7.86906941e+01 0.00000000e+00\n",
      " 8.88178420e-16 3.92423361e+01 8.76191363e+01 6.34256618e+01\n",
      " 7.80031663e+01]\n",
      "17-th iteration, loss: 0.27817819302526337, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.321231943664684e-07\n",
      "17-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645780e+01 6.26416253e+01 1.56466789e+02 5.96037221e+01\n",
      " 1.58056381e+02 0.00000000e+00 5.50670620e-14 5.05354709e+01\n",
      " 7.86906936e+01 3.92423365e+01 8.76191355e+01 6.34256591e+01\n",
      " 7.80031663e+01]\n",
      "18-th iteration, loss: 0.2781781930250543, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.094101176054646e-07\n",
      "18-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457753  62.64162463 156.46678663  59.60371942 158.05637794\n",
      "  50.53547165  78.69069317  39.2423367   87.61913476  63.42565634\n",
      "  78.00316629]\n",
      "19-th iteration, loss: 0.27817819302485763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.959854222344822e-07\n",
      "19-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457703  62.64162405 156.46678439  59.60371686 158.05637539\n",
      "  50.53547207  78.69069272  39.24233686  87.61913403  63.42565362\n",
      "  78.00316629]\n",
      "20-th iteration, loss: 0.2781781930246729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.699015964884739e-07\n",
      "20-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645765e+01 6.26416236e+01 1.56466782e+02 5.96037145e+01\n",
      " 1.58056373e+02 0.00000000e+00 1.59872116e-14 5.05354726e+01\n",
      " 7.86906923e+01 3.92423370e+01 8.76191333e+01 6.34256509e+01\n",
      " 7.80031663e+01]\n",
      "21-th iteration, loss: 0.2781781930244971, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.015114339296286e-07\n",
      "21-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457608  62.64162318 156.46678011  59.60371232 158.05637048\n",
      "  50.53547372  78.69069182  39.24233716  87.61913261  63.42564822\n",
      "  78.00316629]\n",
      "22-th iteration, loss: 0.27817819302433117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.573527007959342e-07\n",
      "22-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457562  62.64162283 156.46677804  59.60371025 158.05636808\n",
      "  50.53547431  78.69069137  39.24233728  87.61913192  63.42564554\n",
      "  78.00316629]\n",
      "23-th iteration, loss: 0.2781781930241734, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.043179792853982e-07\n",
      "23-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457518  62.64162255 156.46677602  59.60370834 158.05636574\n",
      "  50.53547494  78.6906909   39.24233739  87.61913124  63.42564289\n",
      "  78.00316629]\n",
      "24-th iteration, loss: 0.2781781930240229, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.434987948260482e-07\n",
      "24-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457475  62.64162233 156.46677405  59.60370655 158.05636343\n",
      "  50.53547562  78.69069044  39.24233749  87.61913058  63.42564025\n",
      "  78.00316629]\n",
      "25-th iteration, loss: 0.278178193023879, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.75859296838513e-07\n",
      "25-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457433  62.64162216 156.46677213  59.60370489 158.05636117\n",
      "  50.53547634  78.69068997  39.24233758  87.61912993  63.42563763\n",
      "  78.00316629]\n",
      "26-th iteration, loss: 0.2781781930237409, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.022508795784448e-07\n",
      "26-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645739e+01 6.26416220e+01 1.56466770e+02 5.96037033e+01\n",
      " 1.58056359e+02 0.00000000e+00 1.59872116e-14 5.05354771e+01\n",
      " 7.86906895e+01 3.92423376e+01 8.76191293e+01 6.34256350e+01\n",
      " 7.80031663e+01]\n",
      "27-th iteration, loss: 0.2781781930236048, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.831820176737147e-07\n",
      "27-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457352  62.64162196 156.46676841  59.6037019  158.05635676\n",
      "  50.53547863  78.69068901  39.24233771  87.61912868  63.42563247\n",
      "  78.00316629]\n",
      "28-th iteration, loss: 0.27817819302347696, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.034053767341242e-07\n",
      "28-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457312  62.64162189 156.4667666   59.60370051 158.05635459\n",
      "  50.53547937  78.69068851  39.24233775  87.61912807  63.42562995\n",
      "  78.00316629]\n",
      "29-th iteration, loss: 0.2781781930233537, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.192749130884123e-07\n",
      "29-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645727e+01 6.26416219e+01 1.56466765e+02 5.96036992e+01\n",
      " 1.58056352e+02 0.00000000e+00 3.73034936e-14 5.05354801e+01\n",
      " 7.86906880e+01 3.92423378e+01 8.76191275e+01 6.34256274e+01\n",
      " 7.80031663e+01]\n",
      "30-th iteration, loss: 0.27817819302323127, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.908339519209588e-07\n",
      "30-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645723e+01 6.26416219e+01 1.56466763e+02 5.96036980e+01\n",
      " 1.58056350e+02 0.00000000e+00 2.30926389e-14 5.05354817e+01\n",
      " 7.86906875e+01 3.92423378e+01 8.76191269e+01 6.34256250e+01\n",
      " 7.80031663e+01]\n",
      "31-th iteration, loss: 0.278178193023113, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.641241600396788e-07\n",
      "31-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457197  62.64162189 156.46676139  59.60369687 158.05634827\n",
      "  50.5354832   78.69068696  39.24233778  87.61912633  63.42562256\n",
      "  78.00316629]\n",
      "32-th iteration, loss: 0.2781781930230016, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.767653397338992e-07\n",
      "32-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457159  62.64162191 156.4667597   59.60369577 158.05634619\n",
      "  50.53548391  78.69068641  39.24233776  87.61912577  63.42562017\n",
      "  78.00316629]\n",
      "33-th iteration, loss: 0.278178193022894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.862040162084937e-07\n",
      "33-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457122  62.64162196 156.46675804  59.60369473 158.05634414\n",
      "  50.53548464  78.69068585  39.24233772  87.61912523  63.42561782\n",
      "  78.00316629]\n",
      "34-th iteration, loss: 0.2781781930227897, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.928572420156045e-07\n",
      "34-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86457086  62.64162203 156.46675641  59.60369376 158.05634211\n",
      "  50.53548538  78.69068529  39.24233768  87.61912471  63.42561549\n",
      "  78.00316629]\n",
      "35-th iteration, loss: 0.2781781930226886, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.970926769436118e-07\n",
      "35-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.8645705   62.64162211 156.46675481  59.60369285 158.0563401\n",
      "  50.53548611  78.69068472  39.24233764  87.6191242   63.4256132\n",
      "  78.00316629]\n",
      "36-th iteration, loss: 0.27817819302259056, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.992342983047747e-07\n",
      "36-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645701e+01 6.26416222e+01 1.56466753e+02 5.96036920e+01\n",
      " 1.58056338e+02 0.00000000e+00 5.50670620e-14 5.05354869e+01\n",
      " 7.86906841e+01 3.92423376e+01 8.76191237e+01 6.34256109e+01\n",
      " 7.80031663e+01]\n",
      "37-th iteration, loss: 0.27817819302249197, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.608421433806709e-07\n",
      "37-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645698e+01 6.26416223e+01 1.56466752e+02 5.96036912e+01\n",
      " 1.58056336e+02 0.00000000e+00 3.73034936e-14 5.05354884e+01\n",
      " 7.86906836e+01 3.92423375e+01 8.76191232e+01 6.34256087e+01\n",
      " 7.80031663e+01]\n",
      "38-th iteration, loss: 0.2781781930223965, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.261672886782125e-07\n",
      "38-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456945  62.64162247 156.46675015  59.60369044 158.05633421\n",
      "  50.53548977  78.69068296  39.24233745  87.61912275  63.42560654\n",
      "  78.00316629]\n",
      "39-th iteration, loss: 0.27817819302230645, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.300011746307338e-07\n",
      "39-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645691e+01 6.26416226e+01 1.56466749e+02 5.96036897e+01\n",
      " 1.58056332e+02 0.00000000e+00 1.59872116e-14 5.05354904e+01\n",
      " 7.86906823e+01 3.92423374e+01 8.76191223e+01 6.34256044e+01\n",
      " 7.80031663e+01]\n",
      "40-th iteration, loss: 0.27817819302221625, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.966769457691546e-07\n",
      "40-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645688e+01 6.26416227e+01 1.56466747e+02 5.96036890e+01\n",
      " 1.58056330e+02 0.00000000e+00 2.30926389e-14 5.05354918e+01\n",
      " 7.86906817e+01 3.92423373e+01 8.76191218e+01 6.34256023e+01\n",
      " 7.80031663e+01]\n",
      "41-th iteration, loss: 0.2781781930221287, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.665185941728005e-07\n",
      "41-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456842  62.64162286 156.46674568  59.60368835 158.05632844\n",
      "  50.53549309  78.69068109  39.24233715  87.61912141  63.42560021\n",
      "  78.00316629]\n",
      "42-th iteration, loss: 0.27817819302204577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.714439330987864e-07\n",
      "42-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456807  62.64162299 156.46674422  59.60368771 158.05632653\n",
      "  50.5354937   78.69068044  39.24233703  87.61912099  63.42559818\n",
      "  78.00316629]\n",
      "43-th iteration, loss: 0.27817819302196506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.745769433952203e-07\n",
      "43-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456774  62.64162313 156.46674278  59.60368711 158.05632464\n",
      "  50.53549431  78.69067979  39.24233691  87.61912059  63.42559618\n",
      "  78.00316629]\n",
      "44-th iteration, loss: 0.2781781930218864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.761551448836682e-07\n",
      "44-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645674e+01 6.26416233e+01 1.56466741e+02 5.96036865e+01\n",
      " 1.58056323e+02 0.00000000e+00 1.59872116e-14 5.05354949e+01\n",
      " 7.86906791e+01 3.92423368e+01 8.76191202e+01 6.34255942e+01\n",
      " 7.80031663e+01]\n",
      "45-th iteration, loss: 0.27817819302180746, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.439084292588941e-07\n",
      "45-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645671e+01 6.26416234e+01 1.56466740e+02 5.96036860e+01\n",
      " 1.58056321e+02 0.00000000e+00 2.30926389e-14 5.05354962e+01\n",
      " 7.86906785e+01 3.92423367e+01 8.76191198e+01 6.34255923e+01\n",
      " 7.80031663e+01]\n",
      "46-th iteration, loss: 0.2781781930217307, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.149281428821911e-07\n",
      "46-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456674  62.64162361 156.46673858  59.60368555 158.05631908\n",
      "  50.53549735  78.6906778   39.24233654  87.61911944  63.42559036\n",
      "  78.00316629]\n",
      "47-th iteration, loss: 0.2781781930216577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.183811133770314e-07\n",
      "47-th iteration, new layer inserted. now 11 layers\n",
      "[ 10.86456641  62.64162376 156.4667372   59.60368507 158.05631723\n",
      "  50.5354979   78.69067711  39.2423364   87.61911909  63.42558849\n",
      "  78.00316629]\n",
      "48-th iteration, loss: 0.2781781930215864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.204049723395787e-07\n",
      "48-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645661e+01 6.26416239e+01 1.56466736e+02 5.96036846e+01\n",
      " 1.58056315e+02 0.00000000e+00 3.01980663e-14 5.05354985e+01\n",
      " 7.86906764e+01 3.92423363e+01 8.76191187e+01 6.34255866e+01\n",
      " 7.80031663e+01]\n",
      "49-th iteration, loss: 0.27817819302151503, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.915076004692615e-07\n",
      "49-th iteration, new layer inserted. now 13 layers\n",
      "[1.08645658e+01 6.26416241e+01 1.56466735e+02 5.96036842e+01\n",
      " 1.58056314e+02 0.00000000e+00 1.59872116e-14 5.05354996e+01\n",
      " 7.86906757e+01 3.92423361e+01 8.76191184e+01 6.34255848e+01\n",
      " 7.80031663e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235574\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 44.39870033   0.         960.6264253 ]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6329889765936314\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 234.29912812   0.         726.32729718]\n",
      "2-th iteration, loss: 0.6041559944835919, 13 gd steps\n",
      "insert gradient: -0.6571083733011073\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.33440721  77.48910228 219.66389467  41.77895758 237.16809704\n",
      "   0.         489.15920014]\n",
      "3-th iteration, loss: 0.46981081677476, 19 gd steps\n",
      "insert gradient: -0.7303705159496565\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.15464247 224.49921321  57.52224535 162.36804838\n",
      "  47.60324363 363.37540582   0.         125.78379432]\n",
      "4-th iteration, loss: 0.3747365599446947, 13 gd steps\n",
      "insert gradient: -0.16179366379662277\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.75195582  64.6872777  225.06554677  49.92275093 148.2489335\n",
      "  51.2158818  229.39072122   0.         114.69536061  47.89259111\n",
      " 125.78379432]\n",
      "5-th iteration, loss: 0.2963914246221837, 42 gd steps\n",
      "insert gradient: -0.058593708191371836\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          74.29088287 104.73124625   0.         104.73124625\n",
      "  67.3093137  113.93318474  58.31361667 176.29465854  46.88138594\n",
      "  77.62932715  45.76705458 125.78379432]\n",
      "6-th iteration, loss: 0.29595119098490497, 17 gd steps\n",
      "insert gradient: -0.03498148808544434\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          75.67524348 102.36254813   2.69774673 100.74146105\n",
      "  67.77238526 113.70236244  57.85180976 177.55068692  47.68902462\n",
      "  78.04953034  45.48344733 111.80781717   0.          13.97597715]\n",
      "7-th iteration, loss: 0.2076922575417709, 188 gd steps\n",
      "insert gradient: -0.022286618619509205\n",
      "7-th iteration, new layer inserted. now 13 layers\n",
      "[  0.21707208  94.80935469 169.7948007   61.15971823 155.09832927\n",
      "  59.55666273 164.43633308  50.50995593  86.82999604  47.39371736\n",
      "  93.16817057  51.11322412  13.97597715]\n",
      "8-th iteration, loss: 0.20590347091744116, 31 gd steps\n",
      "insert gradient: -0.008966194496936582\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          32.25088476   0.          64.50176952 162.38560727\n",
      "  61.63381092 162.59831829  57.50573695 162.71482026  51.38999945\n",
      "  86.72193744  48.13531784  90.39916272  52.20993409  13.97597715]\n",
      "9-th iteration, loss: 0.20551189224206112, 28 gd steps\n",
      "insert gradient: -0.003239145410883349\n",
      "9-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          28.55882865   6.42398842  63.29835057 162.87391485\n",
      "  62.05109809 163.00550479  57.67897655 163.38952747  51.65101955\n",
      "  86.90174612  48.39525566  90.66005903  52.55725756  13.97597715]\n",
      "10-th iteration, loss: 0.2054156922425025, 21 gd steps\n",
      "insert gradient: -0.0017695159032984427\n",
      "10-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.66051541e+01 8.69772546e+00 6.38324409e+01\n",
      " 1.63633469e+02 6.17857526e+01 1.63714874e+02 5.76858095e+01\n",
      " 1.63862811e+02 0.00000000e+00 1.77635684e-14 5.18050588e+01\n",
      " 8.71293537e+01 4.86275476e+01 9.08007434e+01 5.26510575e+01\n",
      " 1.39759771e+01]\n",
      "11-th iteration, loss: 0.2053940561917082, 131 gd steps\n",
      "insert gradient: -0.001202706491625631\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.62187301e+01 0.00000000e+00 4.88498131e-15\n",
      " 9.49601502e+00 6.37536101e+01 1.63899426e+02 6.16925974e+01\n",
      " 1.64020877e+02 5.76285636e+01 1.64225783e+02 5.19519647e+01\n",
      " 8.73323163e+01 4.87222214e+01 9.08006206e+01 5.26249730e+01\n",
      " 1.39759771e+01]\n",
      "12-th iteration, loss: 0.20539222588882777, 236 gd steps\n",
      "insert gradient: -0.0009312837079622736\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.17804974   9.60862092  63.73727762 163.92458017\n",
      "  61.68566108 164.04725432  57.63074124 164.23752232  51.96460945\n",
      "  87.34849631  48.73285859  90.80311446  52.62634382  13.97597715]\n",
      "13-th iteration, loss: 0.2053919901790878, 60 gd steps\n",
      "insert gradient: -0.000993453482472523\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.61700211e+01 0.00000000e+00 6.66133815e-15\n",
      " 9.61913820e+00 6.37339307e+01 1.63929737e+02 6.16844138e+01\n",
      " 1.64052505e+02 5.76312519e+01 1.64239955e+02 5.19668196e+01\n",
      " 8.73513629e+01 4.87344524e+01 9.08034279e+01 5.26264982e+01\n",
      " 1.39759771e+01]\n",
      "14-th iteration, loss: 0.2053911356092198, 151 gd steps\n",
      "insert gradient: -0.0008902868091985919\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.148379     9.6730845   63.72460489 163.94336737\n",
      "  61.68073977 164.06607469  57.6322758  164.24625005  51.97236209\n",
      "  87.35883209  48.73883762  90.80443975  52.62718496  13.97597715]\n",
      "15-th iteration, loss: 0.20539094828995472, 51 gd steps\n",
      "insert gradient: -0.0009421325446714834\n",
      "15-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.61416893e+01 0.00000000e+00 7.54951657e-15\n",
      " 9.68156703e+00 6.37217330e+01 1.63947793e+02 6.16796074e+01\n",
      " 1.64070374e+02 5.76325841e+01 1.64248271e+02 5.19739856e+01\n",
      " 8.73610761e+01 4.87400783e+01 9.08047281e+01 5.26274093e+01\n",
      " 1.39759771e+01]\n",
      "16-th iteration, loss: 0.20539034202615675, 119 gd steps\n",
      "insert gradient: -0.0008637727968657183\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.61257463e+01 0.00000000e+00 9.76996262e-15\n",
      " 9.72086526e+00 6.37147596e+01 1.63958235e+02 6.16767256e+01\n",
      " 1.64080333e+02 5.76331287e+01 1.64252956e+02 5.19776622e+01\n",
      " 8.73663104e+01 4.87430889e+01 9.08055031e+01 5.26280816e+01\n",
      " 1.39759771e+01]\n",
      "17-th iteration, loss: 0.20538986450417068, 101 gd steps\n",
      "insert gradient: -0.0008326327641722261\n",
      "17-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.61121944e+01 0.00000000e+00 6.66133815e-15\n",
      " 9.75143014e+00 6.37085998e+01 1.63966749e+02 6.16743724e+01\n",
      " 1.64088406e+02 5.76337612e+01 1.64256889e+02 5.19808750e+01\n",
      " 8.73706617e+01 4.87457184e+01 9.08062071e+01 5.26286842e+01\n",
      " 1.39759771e+01]\n",
      "18-th iteration, loss: 0.20538945704403153, 89 gd steps\n",
      "insert gradient: -0.0008120800987957596\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.61002453e+01 0.00000000e+00 4.88498131e-15\n",
      " 9.77761306e+00 6.37030998e+01 1.63974296e+02 6.16723151e+01\n",
      " 1.64095459e+02 5.76343311e+01 1.64260367e+02 5.19836309e+01\n",
      " 8.73743997e+01 4.87479509e+01 9.08068168e+01 5.26292286e+01\n",
      " 1.39759771e+01]\n",
      "19-th iteration, loss: 0.20538909860412485, 81 gd steps\n",
      "insert gradient: -0.0007958738023637226\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60895005e+01 0.00000000e+00 3.99680289e-15\n",
      " 9.80081723e+00 6.36981189e+01 1.63981164e+02 6.16704543e+01\n",
      " 1.64101778e+02 5.76348228e+01 1.64263509e+02 5.19860349e+01\n",
      " 8.73776954e+01 4.87499033e+01 9.08073654e+01 5.26297420e+01\n",
      " 1.39759771e+01]\n",
      "20-th iteration, loss: 0.20538877733729485, 75 gd steps\n",
      "insert gradient: -0.0007821033298365186\n",
      "20-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.0797022    9.82178528  63.69355443 163.98750691\n",
      "  61.6687391  164.10753097  57.63524666 164.26638692  51.98816961\n",
      "  87.38065577  48.75164982  90.80787076  52.63023445  13.97597715]\n",
      "21-th iteration, loss: 0.20538865786134256, 37 gd steps\n",
      "insert gradient: -0.0008226992187619137\n",
      "21-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60749333e+01 0.00000000e+00 9.76996262e-15\n",
      " 9.82725930e+00 6.36913862e+01 1.63990816e+02 6.16678817e+01\n",
      " 1.64110471e+02 5.76354149e+01 1.64267849e+02 5.19891446e+01\n",
      " 8.73820806e+01 4.87524259e+01 9.08080988e+01 5.26304809e+01\n",
      " 1.39759771e+01]\n",
      "22-th iteration, loss: 0.20538836624623596, 70 gd steps\n",
      "insert gradient: -0.0007859793601552376\n",
      "22-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60663906e+01 0.00000000e+00 3.99680289e-15\n",
      " 9.84722705e+00 6.36875153e+01 1.63996855e+02 6.16662366e+01\n",
      " 1.64115741e+02 5.76355878e+01 1.64270444e+02 5.19907784e+01\n",
      " 8.73846042e+01 4.87538027e+01 9.08085283e+01 5.26309712e+01\n",
      " 1.39759771e+01]\n",
      "23-th iteration, loss: 0.2053881038786944, 64 gd steps\n",
      "insert gradient: -0.0007658312798171803\n",
      "23-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.05835401   9.86502271  63.68379473 164.00235509\n",
      "  61.66470492 164.12052944  57.63581716 164.27285723  51.99241508\n",
      "  87.38699959  48.75522061  90.80898099  52.63145959  13.97597715]\n",
      "24-th iteration, loss: 0.20538800071931207, 34 gd steps\n",
      "insert gradient: -0.0008002739823555169\n",
      "24-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60541676e+01 0.00000000e+00 5.77315973e-15\n",
      " 9.86989438e+00 6.36818911e+01 1.64005367e+02 6.16639001e+01\n",
      " 1.64123117e+02 5.76359289e+01 1.64274164e+02 5.19932426e+01\n",
      " 8.73882409e+01 4.87559121e+01 9.08092034e+01 5.26317137e+01\n",
      " 1.39759771e+01]\n",
      "25-th iteration, loss: 0.2053877589852131, 61 gd steps\n",
      "insert gradient: -0.0007670227404764751\n",
      "25-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60469634e+01 0.00000000e+00 1.06581410e-14\n",
      " 9.88688174e+00 6.36786229e+01 1.64010622e+02 6.16624408e+01\n",
      " 1.64127569e+02 5.76360362e+01 1.64276395e+02 5.19945867e+01\n",
      " 8.73903501e+01 4.87570768e+01 9.08095919e+01 5.26321746e+01\n",
      " 1.39759771e+01]\n",
      "26-th iteration, loss: 0.20538753784065056, 57 gd steps\n",
      "insert gradient: -0.0007478182362636624\n",
      "26-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.04007661   9.90225945  63.67543192 164.0154742\n",
      "  61.66106978 164.13167833  57.63620735 164.27850287  51.99596723\n",
      "  87.39238758  48.75829141  90.80999867  52.63262727  13.97597715]\n",
      "27-th iteration, loss: 0.20538744613662732, 31 gd steps\n",
      "insert gradient: -0.0007788251256232533\n",
      "27-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.03628622   9.90667931  63.67370322 164.01826268\n",
      "  61.6603127  164.13401632  57.6363014  164.27970605  51.9967129\n",
      "  87.39350563  48.75892313  90.81021103  52.63287407  13.97597715]\n",
      "28-th iteration, loss: 0.2053873594777128, 31 gd steps\n",
      "insert gradient: -0.0007995713554175242\n",
      "28-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.03284217   9.91110727  63.67219033 164.02100914\n",
      "  61.65957832 164.13627211  57.63632139 164.28084324  51.99730765\n",
      "  87.39450283  48.75941768  90.81038413  52.63310366  13.97597715]\n",
      "29-th iteration, loss: 0.20538727600617082, 30 gd steps\n",
      "insert gradient: -0.0008138746410860824\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60296303e+01 0.00000000e+00 1.06581410e-14\n",
      " 9.91554132e+00 6.36708204e+01 1.64023720e+02 6.16588481e+01\n",
      " 1.64138462e+02 5.76362866e+01 1.64281933e+02 5.19978103e+01\n",
      " 8.73954296e+01 4.87598463e+01 9.08105428e+01 5.26333325e+01\n",
      " 1.39759771e+01]\n",
      "30-th iteration, loss: 0.20538706972985565, 54 gd steps\n",
      "insert gradient: -0.0007663386533356716\n",
      "30-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.02377155   9.93091914  63.66825426 164.0284464\n",
      "  61.65748073 164.14226619  57.63622079 164.28384383  51.99878445\n",
      "  87.39714433  48.76075916  90.81088551  52.63378331  13.97597715]\n",
      "31-th iteration, loss: 0.20538698966834368, 29 gd steps\n",
      "insert gradient: -0.0007870301341115096\n",
      "31-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60205492e+01 0.00000000e+00 7.54951657e-15\n",
      " 9.93507955e+00 6.36668326e+01 1.64031020e+02 6.16567468e+01\n",
      " 1.64144341e+02 5.76362299e+01 1.64284912e+02 5.19993793e+01\n",
      " 8.73981112e+01 4.87613053e+01 9.08110872e+01 5.26340345e+01\n",
      " 1.39759771e+01]\n",
      "32-th iteration, loss: 0.2053868030919399, 50 gd steps\n",
      "insert gradient: -0.00075038971153199\n",
      "32-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60150490e+01 0.00000000e+00 1.06581410e-14\n",
      " 9.94901390e+00 6.36643823e+01 1.64035351e+02 6.16554706e+01\n",
      " 1.64147812e+02 5.76362301e+01 1.64286702e+02 5.20003833e+01\n",
      " 8.73997454e+01 4.87622385e+01 9.08114332e+01 5.26344617e+01\n",
      " 1.39759771e+01]\n",
      "33-th iteration, loss: 0.20538663110840927, 48 gd steps\n",
      "insert gradient: -0.0007285434234737835\n",
      "33-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.00967384   9.96165525  63.66191661 164.03936546\n",
      "  61.65427708 164.15105236  57.63632651 164.28842012  52.00147557\n",
      "  87.40136109  48.76323537  90.81179098  52.63486762  13.97597715]\n",
      "34-th iteration, loss: 0.20538655608681508, 28 gd steps\n",
      "insert gradient: -0.0007533634824100399\n",
      "34-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.60065116e+01 0.00000000e+00 6.66133815e-15\n",
      " 9.96547398e+00 6.36604814e+01 1.64041796e+02 6.16535844e+01\n",
      " 1.64153006e+02 5.76363998e+01 1.64289464e+02 5.20021221e+01\n",
      " 8.74023110e+01 4.87637967e+01 9.08119905e+01 5.26350981e+01\n",
      " 1.39759771e+01]\n",
      "35-th iteration, loss: 0.205386393895828, 46 gd steps\n",
      "insert gradient: -0.0007267534230087598\n",
      "35-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          26.00150358   9.9776491   63.65821009 164.04567785\n",
      "  61.6524545  164.15609833  57.63647439 164.291105    52.00309517\n",
      "  87.40379002  48.76464798  90.81229683  52.63546132  13.97597715]\n",
      "36-th iteration, loss: 0.20538632299656545, 27 gd steps\n",
      "insert gradient: -0.000748840090150043\n",
      "36-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.99851402   9.98133416  63.6568613  164.04803266\n",
      "  61.65178658 164.15796883  57.63653608 164.29210783  52.00369346\n",
      "  87.40468244  48.76516129  90.81248097  52.63567883  13.97597715]\n",
      "37-th iteration, loss: 0.20538625481235434, 26 gd steps\n",
      "insert gradient: -0.0007644495420424139\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59957331e+01 0.00000000e+00 5.77315973e-15\n",
      " 9.98503773e+00 6.36556445e+01 1.64050368e+02 6.16511339e+01\n",
      " 1.64159796e+02 5.76365529e+01 1.64293072e+02 5.20041974e+01\n",
      " 8.74055004e+01 4.87655852e+01 9.08126382e+01 5.26358830e+01\n",
      " 1.39759771e+01]\n",
      "38-th iteration, loss: 0.20538610197530638, 44 gd steps\n",
      "insert gradient: -0.0007301187609362224\n",
      "38-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.99116767   9.99688289  63.65361868 164.05411665\n",
      "  61.65003515 164.16271363  57.63655914 164.29461315  52.00502455\n",
      "  87.4068431   48.76632232  90.81291492  52.63623145  13.97597715]\n",
      "39-th iteration, loss: 0.20538603619139817, 26 gd steps\n",
      "insert gradient: -0.0007482242865102031\n",
      "39-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.98841686  10.00042891  63.65239438 164.05637016\n",
      "  61.64938535 164.16446902  57.63659161 164.29555613  52.00556024\n",
      "  87.40766951  48.76679544  90.81309051  52.63644445  13.97597715]\n",
      "40-th iteration, loss: 0.20538597244594167, 25 gd steps\n",
      "insert gradient: -0.0007611862556137026\n",
      "40-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.98583069  10.00399231  63.65127463 164.05860882\n",
      "  61.6487479  164.16619106  57.63659    164.29646966  52.00602262\n",
      "  87.40843729  48.76719694  90.81324406  52.63664587  13.97597715]\n",
      "41-th iteration, loss: 0.20538591016211044, 25 gd steps\n",
      "insert gradient: -0.0007705949412571582\n",
      "41-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59833659e+01 0.00000000e+00 4.88498131e-15\n",
      " 1.00075674e+01 6.36502309e+01 1.64060832e+02 6.16481144e+01\n",
      " 1.64167883e+02 5.76365610e+01 1.64297360e+02 5.20064357e+01\n",
      " 8.74091672e+01 4.87675587e+01 9.08133872e+01 5.26368438e+01\n",
      " 1.39759771e+01]\n",
      "42-th iteration, loss: 0.20538576773510828, 42 gd steps\n",
      "insert gradient: -0.0007313332946214885\n",
      "42-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.97923837  10.01895504  63.64843603 164.06439397\n",
      "  61.64704281 164.17058841  57.63651557 164.2987928   52.0071584\n",
      "  87.41039699  48.76823288  90.81365169  52.6371858   13.97597715]\n",
      "43-th iteration, loss: 0.2053857071069604, 25 gd steps\n",
      "insert gradient: -0.0007461157125722528\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59767220e+01 0.00000000e+00 6.66133815e-15\n",
      " 1.00223500e+01 6.36473310e+01 1.64066531e+02 6.16464098e+01\n",
      " 1.64172219e+02 5.76365276e+01 1.64299676e+02 5.20076485e+01\n",
      " 8.74111663e+01 4.87686810e+01 9.08138229e+01 5.26373947e+01\n",
      " 1.39759771e+01]\n",
      "44-th iteration, loss: 0.2053855753044095, 40 gd steps\n",
      "insert gradient: -0.0007154057354277109\n",
      "44-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.97273564  10.03286833  63.64556057 164.0698589\n",
      "  61.64539708 164.17474913  57.63653767 164.30104649  52.00841468\n",
      "  87.41237068  48.76938691  90.8140917   52.6377198   13.97597715]\n",
      "45-th iteration, loss: 0.20538551658636484, 24 gd steps\n",
      "insert gradient: -0.0007317465927455572\n",
      "45-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59702437e+01 0.00000000e+00 5.77315973e-15\n",
      " 1.00361360e+01 6.36444514e+01 1.64071937e+02 6.16447807e+01\n",
      " 1.64176333e+02 5.76365742e+01 1.64301918e+02 5.20089254e+01\n",
      " 8.74131353e+01 4.87698430e+01 9.08142620e+01 5.26379200e+01\n",
      " 1.39759771e+01]\n",
      "46-th iteration, loss: 0.20538539258075827, 38 gd steps\n",
      "insert gradient: -0.0007047600575912106\n",
      "46-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          25.9664096   10.0460702   63.64273338 164.07510855\n",
      "  61.64381879 164.17873948  57.6366134  164.30323868  52.00968831\n",
      "  87.41429425  48.77052769  90.81451792  52.63822205  13.97597715]\n",
      "47-th iteration, loss: 0.2053853358217304, 24 gd steps\n",
      "insert gradient: -0.0007214052045837775\n",
      "47-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59639675e+01 0.00000000e+00 3.99680289e-15\n",
      " 1.00492339e+01 6.36416392e+01 1.64077137e+02 6.16432195e+01\n",
      " 1.64180281e+02 5.76366627e+01 1.64304095e+02 5.20101992e+01\n",
      " 8.74150425e+01 4.87709745e+01 9.08146825e+01 5.26384123e+01\n",
      " 1.39759771e+01]\n",
      "48-th iteration, loss: 0.20538521828269446, 37 gd steps\n",
      "insert gradient: -0.0006964414164362498\n",
      "48-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59602819e+01 0.00000000e+00 5.77315973e-15\n",
      " 1.00587094e+01 6.36399799e+01 1.64080184e+02 6.16422993e+01\n",
      " 1.64182584e+02 5.76367162e+01 1.64305369e+02 5.20109434e+01\n",
      " 8.74161548e+01 4.87716304e+01 9.08149249e+01 5.26386953e+01\n",
      " 1.39759771e+01]\n",
      "49-th iteration, loss: 0.2053851065993802, 36 gd steps\n",
      "insert gradient: -0.000679398000633213\n",
      "49-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 2.59566100e+01 0.00000000e+00 3.10862447e-15\n",
      " 1.00675967e+01 6.36382871e+01 1.64083090e+02 6.16414150e+01\n",
      " 1.64184794e+02 5.76368168e+01 1.64306614e+02 5.20117399e+01\n",
      " 8.74172735e+01 4.87723331e+01 9.08151782e+01 5.26389723e+01\n",
      " 1.39759771e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.534613819690818\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[ 45.50866784   0.         984.64208593]\n",
      "1-th iteration, loss: 0.7511086840957834, 11 gd steps\n",
      "insert gradient: -0.6337709553809843\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.69396356  62.22748438 120.07830316   0.         864.56378277]\n",
      "2-th iteration, loss: 0.5134778858735154, 34 gd steps\n",
      "insert gradient: -0.6978797637143401\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[5.74883790e-01 5.71962085e+01 9.91111071e+01 1.06639638e+02\n",
      " 2.29374065e+02 0.00000000e+00 6.35189718e+02]\n",
      "3-th iteration, loss: 0.435863419311209, 13 gd steps\n",
      "insert gradient: -0.5025873162858765\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  4.86282628  64.47267109  98.17893557  95.1823172  209.94660927\n",
      "  50.40874923 617.0414403    0.          18.14827766]\n",
      "4-th iteration, loss: 0.36664292814675303, 20 gd steps\n",
      "insert gradient: -0.43431302740223565\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[3.07816012e-01 6.21010797e+01 1.17359738e+02 8.15282026e+01\n",
      " 1.14212271e+02 0.00000000e+00 1.06054252e+02 4.67813130e+01\n",
      " 5.86291415e+02 4.32529875e+01 1.81482777e+01]\n",
      "5-th iteration, loss: 0.32665746145611035, 21 gd steps\n",
      "insert gradient: -0.1517572561076629\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.31306361  67.62899655 126.10533144  71.57957466  91.91195761\n",
      "  24.63226562  74.65682687  51.95061678 263.17118942   0.\n",
      " 315.8054273   43.6741307   18.14827766]\n",
      "6-th iteration, loss: 0.29183201045590373, 35 gd steps\n",
      "insert gradient: -0.09672325444724214\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.79904467  69.83196221 117.13939448  69.59710954 107.05829819\n",
      "  35.87956304  42.72508398  52.02980651 125.904106     0.\n",
      " 100.7232848   45.7850015  289.57572012  37.55298611  18.14827766]\n",
      "7-th iteration, loss: 0.27486849094117616, 21 gd steps\n",
      "insert gradient: -0.07655605547725414\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.43785816  66.29230537 118.09426255  72.81286889 112.94244646\n",
      "  41.71094625  27.20423329  59.46729843  96.7311325   20.09033077\n",
      "  68.70242369  58.02508548 209.95388105   0.          69.98462702\n",
      "  41.65886091  18.14827766]\n",
      "8-th iteration, loss: 0.22054378979519368, 105 gd steps\n",
      "insert gradient: -0.11635231099602134\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  3.09957185  70.54269721 125.87372713  82.05023613 117.70068398\n",
      "  62.96856652  60.44009416  35.40916188 100.53421304  57.56739374\n",
      "  49.26245209  22.09959193 124.64247625   0.          69.24582014\n",
      "  56.26336294  66.93024037  24.11226975  18.14827766]\n",
      "9-th iteration, loss: 0.1750954918445915, 44 gd steps\n",
      "insert gradient: -0.032904263974053355\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[1.82311139e+00 7.29091062e+01 1.20721514e+02 9.69177715e+01\n",
      " 1.13024970e+02 6.97732789e+01 1.04859980e+02 0.00000000e+00\n",
      " 2.30926389e-14 3.70990389e+01 6.87946576e+01 5.24563287e+01\n",
      " 1.90518886e+02 5.40013770e+01 4.19888337e+01 4.92184871e+01\n",
      " 1.18371517e+02 3.84830498e+01 1.81482777e+01]\n",
      "10-th iteration, loss: 0.16728438033960472, 999 gd steps\n",
      "insert gradient: -0.01847493616613919\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  1.25329838  67.81103101 123.84171749 100.31179395 113.54600289\n",
      "  70.10253317 102.47630726  43.05582258  66.16933606  47.10199722\n",
      " 118.81266267   0.          66.00703482  56.94108917  70.82884451\n",
      "  31.0945387  119.83198123  51.44824164  18.14827766]\n",
      "11-th iteration, loss: 0.1593717804830572, 74 gd steps\n",
      "insert gradient: -0.015038306220471039\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[3.08364533e+00 6.63510362e+01 1.36166246e+02 9.18700812e+01\n",
      " 1.20336209e+02 6.82925461e+01 1.10177279e+02 0.00000000e+00\n",
      " 2.48689958e-14 4.18350803e+01 7.56729614e+01 4.57290709e+01\n",
      " 9.56879838e+01 3.14140080e+01 2.66213546e+01 5.26719810e+01\n",
      " 8.83409816e+01 2.19827594e+01 9.32239434e+01 7.93894355e+01\n",
      " 1.81482777e+01]\n",
      "12-th iteration, loss: 0.1435838302055298, 36 gd steps\n",
      "insert gradient: -0.014800975476960401\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[2.11552275e+00 7.23667839e+01 1.28653133e+02 0.00000000e+00\n",
      " 2.48689958e-14 9.60576203e+01 1.33808774e+02 6.63074395e+01\n",
      " 1.14103419e+02 5.79029055e+01 7.08558397e+01 4.61507064e+01\n",
      " 9.40296410e+01 5.00951433e+01 1.94087073e+01 4.68534551e+01\n",
      " 8.78618639e+01 3.30797568e+01 6.99344250e+01 8.31427429e+01\n",
      " 1.81482777e+01]\n",
      "13-th iteration, loss: 0.14197067689399923, 18 gd steps\n",
      "insert gradient: -0.011126686665774875\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[  1.40019949  72.82438098 128.55808428  98.07924191 134.57217699\n",
      "  66.47356587 114.92275764  58.53551629  74.87408467  46.19198703\n",
      "  91.34883798  52.45125169  21.3421208   42.3425561   91.05256333\n",
      "  39.64053382  53.18231685  86.36157323  18.14827766]\n",
      "14-th iteration, loss: 0.14164625986384385, 23 gd steps\n",
      "insert gradient: -0.003313470759084974\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[  1.82581482  73.35801871 129.5265133   98.45546147 134.55544875\n",
      "  66.55324126 114.56167243  60.10282751  74.71913218  45.45289074\n",
      "  92.86598075  51.22987193  23.15238884  42.47390743  90.95803051\n",
      "  41.26041527  52.11670568  86.15781596  18.14827766]\n",
      "15-th iteration, loss: 0.14155023735226901, 25 gd steps\n",
      "insert gradient: -0.0021299171139267563\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[1.81151573e+00 7.35597392e+01 1.29978252e+02 9.88972527e+01\n",
      " 1.34429932e+02 6.68338021e+01 1.14471210e+02 6.08610581e+01\n",
      " 7.54704317e+01 0.00000000e+00 2.13162821e-14 4.52511205e+01\n",
      " 9.20533579e+01 5.14078603e+01 2.46813993e+01 4.14448274e+01\n",
      " 9.16092873e+01 4.22283399e+01 5.05740251e+01 8.63619182e+01\n",
      " 1.81482777e+01]\n",
      "16-th iteration, loss: 0.1415288201415665, 180 gd steps\n",
      "insert gradient: -0.00014310368390737025\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[  1.85482129  73.65044589 130.37965032  99.20623827 134.29775793\n",
      "  66.86555328 114.19120241  61.49431872  75.88678933  45.14021039\n",
      "  92.05389549  51.18599047  25.58761006  40.99829603  91.85967725\n",
      "  42.79799656  49.95591704  86.13661231  18.14827766]\n",
      "17-th iteration, loss: 0.1415280447582424, 23 gd steps\n",
      "insert gradient: -0.00027881257791616276\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87021548  73.64119855 130.51492241  99.18409228 134.27291836\n",
      "  66.88569825 114.2045235   61.53630787  75.91813701  45.12707549\n",
      "  92.06999768  51.16770415  25.87989851  40.77077208  91.92859154\n",
      "  42.90567834  49.74953088  86.12676401  18.14827766]\n",
      "18-th iteration, loss: 0.1415279653510671, 17 gd steps\n",
      "insert gradient: -3.595266206000262e-05\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87066689  73.64657475 130.52359234  99.19192959 134.26305915\n",
      "  66.88039306 114.19832721  61.55514752  75.93182688  45.13555551\n",
      "  92.06716674  51.1562599   25.8919111   40.76725205  91.93345898\n",
      "  42.91849907  49.74081199  86.11581138  18.14827766]\n",
      "19-th iteration, loss: 0.14152793336724082, 52 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.20401224344548e-06\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87287889  73.65064123 130.54754714  99.19863755 134.25370479\n",
      "  66.88363089 114.18852525  61.57853167  75.94393597  45.13025124\n",
      "  92.06705698  51.14916367  25.94183935  40.73574024  91.94672215\n",
      "  42.94569015  49.70378211  86.11132402  18.14827766]\n",
      "20-th iteration, loss: 0.1415279333643298, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.156350300560135e-06\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287859e+00 7.36506416e+01 1.30547549e+02 9.91986433e+01\n",
      " 1.34253706e+02 6.68836343e+01 1.14188526e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15785398e+01 7.59439395e+01 4.51302513e+01\n",
      " 9.20670559e+01 5.11491645e+01 2.59418434e+01 4.07357426e+01\n",
      " 9.19467241e+01 4.29456962e+01 4.97037845e+01 8.61113259e+01\n",
      " 1.81482777e+01]\n",
      "21-th iteration, loss: 0.1415279333605591, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.036039277366555e-06\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287829e+00 7.36506419e+01 1.30547551e+02 9.91986490e+01\n",
      " 1.34253708e+02 6.68836377e+01 1.14188527e+02 8.00897454e-06\n",
      " 8.04640610e-07 6.15785478e+01 7.59439429e+01 4.51302512e+01\n",
      " 9.20670547e+01 5.11491653e+01 2.59418474e+01 4.07357449e+01\n",
      " 9.19467259e+01 4.29457022e+01 4.97037870e+01 8.61113278e+01\n",
      " 1.81482777e+01]\n",
      "22-th iteration, loss: 0.14152793335687594, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.919159618939541e-06\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287799e+00 7.36506423e+01 1.30547553e+02 9.91986547e+01\n",
      " 1.34253710e+02 6.68836409e+01 1.14188528e+02 1.58981171e-05\n",
      " 1.51345316e-06 6.15785557e+01 7.59439462e+01 4.51302509e+01\n",
      " 9.20670535e+01 5.11491659e+01 2.59418514e+01 4.07357471e+01\n",
      " 9.19467278e+01 4.29457081e+01 4.97037893e+01 8.61113296e+01\n",
      " 1.81482777e+01]\n",
      "23-th iteration, loss: 0.14152793335327507, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.805615647208898e-06\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287769e+00 7.36506426e+01 1.30547554e+02 9.91986605e+01\n",
      " 1.34253711e+02 6.68836441e+01 1.14188528e+02 2.36710666e-05\n",
      " 2.12874724e-06 6.15785635e+01 7.59439495e+01 4.51302506e+01\n",
      " 9.20670522e+01 5.11491665e+01 2.59418554e+01 4.07357492e+01\n",
      " 9.19467296e+01 4.29457140e+01 4.97037917e+01 8.61113314e+01\n",
      " 1.81482777e+01]\n",
      "24-th iteration, loss: 0.14152793334975175, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.695312859183703e-06\n",
      "24-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287739e+00 7.36506429e+01 1.30547556e+02 9.91986662e+01\n",
      " 1.34253713e+02 6.68836473e+01 1.14188529e+02 3.13313628e-05\n",
      " 2.65279013e-06 0.00000000e+00 1.05879118e-22 6.15785711e+01\n",
      " 7.59439528e+01 4.51302500e+01 9.20670508e+01 5.11491670e+01\n",
      " 2.59418593e+01 4.07357513e+01 9.19467314e+01 4.29457199e+01\n",
      " 4.97037940e+01 8.61113332e+01 1.81482777e+01]\n",
      "25-th iteration, loss: 0.14152793334551128, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.521561361107582e-06\n",
      "25-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287709e+00 7.36506431e+01 1.30547558e+02 9.91986719e+01\n",
      " 1.34253714e+02 6.68836503e+01 1.14188529e+02 3.88388645e-05\n",
      " 3.07063217e-06 7.51617930e-06 4.17842035e-07 6.15785786e+01\n",
      " 7.59439559e+01 4.51302494e+01 9.20670494e+01 5.11491675e+01\n",
      " 2.59418632e+01 4.07357533e+01 9.19467331e+01 4.29457257e+01\n",
      " 4.97037964e+01 8.61113349e+01 1.81482777e+01]\n",
      "26-th iteration, loss: 0.14152793334139804, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.3551478746307066e-06\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[1.87287678e+00 7.36506434e+01 1.30547560e+02 9.91986777e+01\n",
      " 1.34253716e+02 6.68836533e+01 1.14188530e+02 4.61749543e-05\n",
      " 3.36991666e-06 1.48623135e-05 6.92540147e-07 0.00000000e+00\n",
      " 5.29395592e-23 6.15785860e+01 7.59439589e+01 4.51302485e+01\n",
      " 9.20670478e+01 5.11491678e+01 2.59418672e+01 4.07357553e+01\n",
      " 9.19467349e+01 4.29457315e+01 4.97037986e+01 8.61113366e+01\n",
      " 1.81482777e+01]\n",
      "27-th iteration, loss: 0.1415279333366954, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.133980065362102e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[1.87287648e+00 7.36506436e+01 1.30547562e+02 9.91986834e+01\n",
      " 1.34253717e+02 6.68836562e+01 1.14188530e+02 5.33070447e-05\n",
      " 3.53976796e-06 2.20054274e-05 8.13774927e-07 7.14537926e-06\n",
      " 1.21234779e-07 6.15785931e+01 7.59439619e+01 4.51302476e+01\n",
      " 9.20670463e+01 5.11491682e+01 2.59418712e+01 4.07357573e+01\n",
      " 9.19467366e+01 4.29457373e+01 4.97038009e+01 8.61113382e+01\n",
      " 1.81482777e+01]\n",
      "28-th iteration, loss: 0.14152793333217184, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.924755319817945e-06\n",
      "28-th iteration, new layer inserted. now 25 layers\n",
      "[1.87287617e+00 7.36506438e+01 1.30547564e+02 9.91986892e+01\n",
      " 1.34253719e+02 6.68836589e+01 1.14188530e+02 6.02242624e-05\n",
      " 3.57241983e-06 2.89342241e-05 7.74444214e-07 1.40768379e-05\n",
      " 5.85306189e-08 6.15786001e+01 7.59439647e+01 4.51302464e+01\n",
      " 9.20670446e+01 5.11491685e+01 2.59418752e+01 4.07357593e+01\n",
      " 9.19467383e+01 4.29457431e+01 4.97038031e+01 8.61113398e+01\n",
      " 1.81482777e+01]\n",
      "29-th iteration, loss: 0.14152793332781038, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.725996133853526e-06\n",
      "29-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287586e+00 7.36506440e+01 1.30547565e+02 9.91986950e+01\n",
      " 1.34253720e+02 6.68836616e+01 1.14188530e+02 6.69396964e-05\n",
      " 3.47576657e-06 3.56613440e-05 5.83143360e-07 6.15786276e+01\n",
      " 7.59439675e+01 4.51302451e+01 9.20670429e+01 5.11491688e+01\n",
      " 2.59418792e+01 4.07357612e+01 9.19467400e+01 4.29457489e+01\n",
      " 4.97038054e+01 8.61113413e+01 1.81482777e+01]\n",
      "30-th iteration, loss: 0.1415279333241902, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.592968152438851e-06\n",
      "30-th iteration, new layer inserted. now 25 layers\n",
      "[1.87287556e+00 7.36506443e+01 1.30547567e+02 9.91987008e+01\n",
      " 1.34253721e+02 6.68836642e+01 1.14188531e+02 7.35003338e-05\n",
      " 3.27020175e-06 4.22333511e-05 2.60925662e-07 0.00000000e+00\n",
      " 3.30872245e-23 6.15786342e+01 7.59439701e+01 4.51302436e+01\n",
      " 9.20670412e+01 5.11491691e+01 2.59418833e+01 4.07357632e+01\n",
      " 9.19467417e+01 4.29457546e+01 4.97038075e+01 8.61113428e+01\n",
      " 1.81482777e+01]\n",
      "31-th iteration, loss: 0.14152793332008157, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.4095687369254425e-06\n",
      "31-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287525e+00 7.36506445e+01 1.30547569e+02 9.91987067e+01\n",
      " 1.34253723e+02 6.68836668e+01 1.14188530e+02 7.98983835e-05\n",
      " 2.95999523e-06 0.00000000e+00 8.47032947e-22 6.15786956e+01\n",
      " 7.59439727e+01 4.51302421e+01 9.20670394e+01 5.11491694e+01\n",
      " 2.59418874e+01 4.07357652e+01 9.19467434e+01 4.29457604e+01\n",
      " 4.97038097e+01 8.61113443e+01 1.81482777e+01]\n",
      "32-th iteration, loss: 0.14152793331664362, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.287126608222617e-06\n",
      "32-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287495e+00 7.36506447e+01 1.30547571e+02 9.91987126e+01\n",
      " 1.34253724e+02 6.68836692e+01 1.14188530e+02 8.61546870e-05\n",
      " 2.54900880e-06 6.15787082e+01 7.59439752e+01 4.51302404e+01\n",
      " 9.20670376e+01 5.11491697e+01 2.59418915e+01 4.07357671e+01\n",
      " 9.19467451e+01 4.29457661e+01 4.97038119e+01 8.61113456e+01\n",
      " 1.81482777e+01]\n",
      "33-th iteration, loss: 0.14152793331380686, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.222261459683432e-06\n",
      "33-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287465e+00 7.36506449e+01 1.30547573e+02 9.91987185e+01\n",
      " 1.34253725e+02 6.68836717e+01 1.14188530e+02 9.23272214e-05\n",
      " 2.06587569e-06 6.15787143e+01 7.59439777e+01 4.51302386e+01\n",
      " 9.20670357e+01 5.11491699e+01 2.59418956e+01 4.07357690e+01\n",
      " 9.19467468e+01 4.29457718e+01 4.97038140e+01 8.61113470e+01\n",
      " 1.81482777e+01]\n",
      "34-th iteration, loss: 0.14152793331100783, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.1589805978459916e-06\n",
      "34-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287435e+00 7.36506451e+01 1.30547575e+02 9.91987244e+01\n",
      " 1.34253727e+02 6.68836740e+01 1.14188530e+02 9.84381924e-05\n",
      " 1.52514465e-06 6.15787205e+01 7.59439801e+01 4.51302368e+01\n",
      " 9.20670338e+01 5.11491701e+01 2.59418997e+01 4.07357709e+01\n",
      " 9.19467485e+01 4.29457775e+01 4.97038161e+01 8.61113483e+01\n",
      " 1.81482777e+01]\n",
      "35-th iteration, loss: 0.1415279333082449, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.097244910049502e-06\n",
      "35-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287405e+00 7.36506452e+01 1.30547577e+02 9.91987303e+01\n",
      " 1.34253728e+02 6.68836764e+01 1.14188530e+02 1.04489318e-04\n",
      " 9.27967138e-07 0.00000000e+00 7.94093388e-23 6.15787265e+01\n",
      " 7.59439825e+01 4.51302349e+01 9.20670319e+01 5.11491702e+01\n",
      " 2.59419037e+01 4.07357727e+01 9.19467502e+01 4.29457831e+01\n",
      " 4.97038182e+01 8.61113496e+01 1.81482777e+01]\n",
      "36-th iteration, loss: 0.14152793330501795, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.985512836958142e-06\n",
      "36-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287374e+00 7.36506454e+01 1.30547579e+02 9.91987361e+01\n",
      " 1.34253729e+02 6.68836787e+01 1.14188529e+02 1.10449897e-04\n",
      " 2.63221993e-07 0.00000000e+00 4.63221143e-23 6.15787384e+01\n",
      " 7.59439849e+01 4.51302329e+01 9.20670299e+01 5.11491703e+01\n",
      " 2.59419078e+01 4.07357745e+01 9.19467518e+01 4.29457887e+01\n",
      " 4.97038202e+01 8.61113509e+01 1.81482777e+01]\n",
      "37-th iteration, loss: 0.14152793330185665, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.9544220758116615e-06\n",
      "37-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87287344  73.65064554 130.54758044  99.19874199 134.25373069\n",
      "  66.88368093 114.18852887  61.57886646  75.94398721  45.1302309\n",
      "  92.0670279   51.14917032  25.94191184  40.73577629  91.94675342\n",
      "  42.94579434  49.70382225  86.11135216  18.14827766]\n",
      "38-th iteration, loss: 0.14152793329969354, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.923649104347448e-06\n",
      "38-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287313e+00 7.36506457e+01 1.30547582e+02 0.00000000e+00\n",
      " 2.48689958e-14 9.91987479e+01 1.34253732e+02 6.68836831e+01\n",
      " 1.14188528e+02 6.15788723e+01 7.59439895e+01 4.51302288e+01\n",
      " 9.20670259e+01 5.11491704e+01 2.59419159e+01 4.07357781e+01\n",
      " 9.19467550e+01 4.29457999e+01 4.97038243e+01 8.61113534e+01\n",
      " 1.81482777e+01]\n",
      "39-th iteration, loss: 0.14152793329703078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.877496146872537e-06\n",
      "39-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287282e+00 7.36506458e+01 1.30547584e+02 5.80105044e-06\n",
      " 1.85598697e-06 9.91987537e+01 1.34253733e+02 6.68836853e+01\n",
      " 1.14188528e+02 6.15788781e+01 7.59439918e+01 4.51302267e+01\n",
      " 9.20670238e+01 5.11491703e+01 2.59419199e+01 4.07357798e+01\n",
      " 9.19467566e+01 4.29458055e+01 4.97038263e+01 8.61113546e+01\n",
      " 1.81482777e+01]\n",
      "40-th iteration, loss: 0.14152793329441773, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.885464042376139e-06\n",
      "40-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287250e+00 7.36506459e+01 1.30547586e+02 1.14780302e-05\n",
      " 3.64930413e-06 9.91987594e+01 1.34253734e+02 6.68836874e+01\n",
      " 1.14188528e+02 6.15788839e+01 7.59439941e+01 4.51302246e+01\n",
      " 9.20670218e+01 5.11491703e+01 2.59419240e+01 4.07357815e+01\n",
      " 9.19467582e+01 4.29458111e+01 4.97038283e+01 8.61113558e+01\n",
      " 1.81482777e+01]\n",
      "41-th iteration, loss: 0.14152793329185173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.892654014866116e-06\n",
      "41-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287216e+00 7.36506459e+01 1.30547588e+02 1.70359982e-05\n",
      " 5.38264813e-06 9.91987649e+01 1.34253735e+02 6.68836895e+01\n",
      " 1.14188527e+02 6.15788897e+01 7.59439964e+01 4.51302225e+01\n",
      " 9.20670197e+01 5.11491702e+01 2.59419280e+01 4.07357831e+01\n",
      " 9.19467598e+01 4.29458166e+01 4.97038303e+01 8.61113570e+01\n",
      " 1.81482777e+01]\n",
      "42-th iteration, loss: 0.1415279332893303, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.899083617798953e-06\n",
      "42-th iteration, new layer inserted. now 21 layers\n",
      "[1.87287182e+00 7.36506459e+01 1.30547589e+02 2.24797518e-05\n",
      " 7.05856701e-06 9.91987704e+01 1.34253737e+02 6.68836914e+01\n",
      " 1.14188527e+02 6.15788955e+01 7.59439988e+01 4.51302204e+01\n",
      " 9.20670176e+01 5.11491700e+01 2.59419320e+01 4.07357848e+01\n",
      " 9.19467614e+01 4.29458222e+01 4.97038323e+01 8.61113581e+01\n",
      " 1.81482777e+01]\n",
      "43-th iteration, loss: 0.1415279332868512, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.904770797614858e-06\n",
      "43-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287146e+00 7.36506458e+01 1.30547591e+02 2.78138428e-05\n",
      " 8.67946990e-06 9.91987757e+01 1.34253738e+02 6.68836934e+01\n",
      " 1.14188526e+02 0.00000000e+00 1.42108547e-14 6.15789013e+01\n",
      " 7.59440011e+01 4.51302183e+01 9.20670155e+01 5.11491699e+01\n",
      " 2.59419360e+01 4.07357865e+01 9.19467630e+01 4.29458277e+01\n",
      " 4.97038343e+01 8.61113593e+01 1.81482777e+01]\n",
      "44-th iteration, loss: 0.14152793328393554, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.8592910843625684e-06\n",
      "44-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287110e+00 7.36506457e+01 1.30547593e+02 3.30322477e-05\n",
      " 1.02444121e-05 9.91987810e+01 1.34253738e+02 6.68836952e+01\n",
      " 1.14188526e+02 0.00000000e+00 2.84217094e-14 6.15789129e+01\n",
      " 7.59440035e+01 4.51302162e+01 9.20670134e+01 5.11491697e+01\n",
      " 2.59419399e+01 4.07357881e+01 9.19467646e+01 4.29458333e+01\n",
      " 4.97038362e+01 8.61113605e+01 1.81482777e+01]\n",
      "45-th iteration, loss: 0.1415279332810644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.814659142572142e-06\n",
      "45-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287072e+00 7.36506455e+01 1.30547594e+02 3.81645439e-05\n",
      " 1.17635151e-05 9.91987861e+01 1.34253739e+02 6.68836970e+01\n",
      " 1.14188525e+02 0.00000000e+00 2.84217094e-14 6.15789244e+01\n",
      " 7.59440058e+01 4.51302140e+01 9.20670112e+01 5.11491695e+01\n",
      " 2.59419439e+01 4.07357897e+01 9.19467662e+01 4.29458389e+01\n",
      " 4.97038382e+01 8.61113616e+01 1.81482777e+01]\n",
      "46-th iteration, loss: 0.141527933278236, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.770841795839161e-06\n",
      "46-th iteration, new layer inserted. now 23 layers\n",
      "[1.87287034e+00 7.36506453e+01 1.30547596e+02 4.32141964e-05\n",
      " 1.32387458e-05 9.91987912e+01 1.34253740e+02 6.68836988e+01\n",
      " 1.14188524e+02 0.00000000e+00 7.10542736e-15 6.15789359e+01\n",
      " 7.59440081e+01 4.51302119e+01 9.20670091e+01 5.11491692e+01\n",
      " 2.59419479e+01 4.07357913e+01 9.19467678e+01 4.29458444e+01\n",
      " 4.97038402e+01 8.61113628e+01 1.81482777e+01]\n",
      "47-th iteration, loss: 0.1415279332754487, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.72780796134089e-06\n",
      "47-th iteration, new layer inserted. now 23 layers\n",
      "[1.87286995e+00 7.36506450e+01 1.30547597e+02 4.81844751e-05\n",
      " 1.46719546e-05 9.91987962e+01 1.34253741e+02 6.68837005e+01\n",
      " 1.14188524e+02 0.00000000e+00 1.42108547e-14 6.15789472e+01\n",
      " 7.59440105e+01 4.51302097e+01 9.20670069e+01 5.11491690e+01\n",
      " 2.59419519e+01 4.07357930e+01 9.19467693e+01 4.29458500e+01\n",
      " 4.97038421e+01 8.61113639e+01 1.81482777e+01]\n",
      "48-th iteration, loss: 0.1415279332727009, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.685528485636098e-06\n",
      "48-th iteration, new layer inserted. now 21 layers\n",
      "[1.87286955e+00 7.36506448e+01 1.30547599e+02 5.30784677e-05\n",
      " 1.60648836e-05 9.91988011e+01 1.34253742e+02 6.68837021e+01\n",
      " 1.14188523e+02 6.15789585e+01 7.59440128e+01 4.51302074e+01\n",
      " 9.20670047e+01 5.11491687e+01 2.59419559e+01 4.07357946e+01\n",
      " 9.19467709e+01 4.29458555e+01 4.97038441e+01 8.61113650e+01\n",
      " 1.81482777e+01]\n",
      "49-th iteration, loss: 0.1415279332704341, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.6925862273900015e-06\n",
      "49-th iteration, new layer inserted. now 23 layers\n",
      "[1.87286915e+00 7.36506445e+01 1.30547600e+02 5.79087863e-05\n",
      " 1.74221232e-05 9.91988060e+01 1.34253742e+02 6.68837037e+01\n",
      " 1.14188522e+02 0.00000000e+00 1.06581410e-14 6.15789641e+01\n",
      " 7.59440151e+01 4.51302052e+01 9.20670025e+01 5.11491685e+01\n",
      " 2.59419600e+01 4.07357962e+01 9.19467725e+01 4.29458611e+01\n",
      " 4.97038460e+01 8.61113660e+01 1.81482777e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5314150056394755\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.38057759    0.         1012.89580432]\n",
      "1-th iteration, loss: 0.745144539899422, 11 gd steps\n",
      "insert gradient: -0.6284659419365621\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 41.43634346  62.36427952 234.69536929   0.         778.20043503]\n",
      "2-th iteration, loss: 0.6054138817966734, 13 gd steps\n",
      "insert gradient: -0.6534688212776626\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.84384875  77.65460158 220.25951325  41.21133146 238.22462297\n",
      "   0.         539.97581206]\n",
      "3-th iteration, loss: 0.4612300586637028, 26 gd steps\n",
      "insert gradient: -0.7219261542858607\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.14574322 225.72873405  46.58267508 158.52007166\n",
      "  55.06031415 354.84124793   0.         185.13456414]\n",
      "4-th iteration, loss: 0.3742493047739283, 14 gd steps\n",
      "insert gradient: -0.30132534922069776\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[8.35246989e-01 5.86395034e+01 2.17199487e+02 6.16031661e+01\n",
      " 1.41012795e+02 4.50833832e+01 3.36253118e+02 5.08487122e+01\n",
      " 1.85134564e+02 0.00000000e+00 8.70414851e-14]\n",
      "5-th iteration, loss: 0.30826897858513796, 19 gd steps\n",
      "insert gradient: -0.31620899080187664\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[2.80026919e+00 5.78207854e+01 1.07678687e+02 0.00000000e+00\n",
      " 1.07678687e+02 5.09023836e+01 1.23804813e+02 6.40655592e+01\n",
      " 3.18582122e+02 4.15159984e+01 1.75314397e+02 4.75192380e+01\n",
      " 7.59114324e-14]\n",
      "6-th iteration, loss: 0.2838209876737271, 51 gd steps\n",
      "insert gradient: -0.04134480299780947\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[1.77877256e+00 0.00000000e+00 5.82867088e-16 5.38080862e+01\n",
      " 8.64174282e+01 1.57640888e+01 8.10960029e+01 5.81021234e+01\n",
      " 1.28360906e+02 6.53387614e+01 3.00628807e+02 4.75725171e+01\n",
      " 1.40992528e+02 6.52114432e+01 5.61143819e-14]\n",
      "7-th iteration, loss: 0.28295466157321414, 27 gd steps\n",
      "insert gradient: -0.012456738752331107\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[1.94378942e+00 5.42596838e+01 8.64080038e+01 1.56976720e+01\n",
      " 8.08857847e+01 5.77843161e+01 1.27337877e+02 6.53843873e+01\n",
      " 1.47568910e+02 0.00000000e+00 1.47568910e+02 5.11220526e+01\n",
      " 1.37644863e+02 6.63600714e+01 5.67476672e-14]\n",
      "8-th iteration, loss: 0.2820914241281808, 27 gd steps\n",
      "insert gradient: -0.021824914642626578\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[2.09208296e+00 5.37135455e+01 8.62685683e+01 0.00000000e+00\n",
      " 5.32907052e-15 1.53031097e+01 8.02630743e+01 5.73846706e+01\n",
      " 1.27788131e+02 6.46274829e+01 1.42543939e+02 2.91866116e+00\n",
      " 1.41960582e+02 5.36611254e+01 1.35751969e+02 6.71034791e+01\n",
      " 4.97676433e-14]\n",
      "9-th iteration, loss: 0.28197166605393664, 15 gd steps\n",
      "insert gradient: -0.02624696372464284\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[2.27153679e+00 5.34532020e+01 8.60898070e+01 1.57176741e+01\n",
      " 8.00366110e+01 5.73759046e+01 1.27778153e+02 6.50006874e+01\n",
      " 1.41636515e+02 2.89249653e+00 6.13951453e+01 0.00000000e+00\n",
      " 7.89366154e+01 5.52561980e+01 1.34777159e+02 6.73185279e+01\n",
      " 5.80803009e-14]\n",
      "10-th iteration, loss: 0.255813654940317, 60 gd steps\n",
      "insert gradient: -0.08570703023765339\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[2.78012912e+00 5.26770295e+01 8.71567010e+01 0.00000000e+00\n",
      " 1.24344979e-14 1.97364835e+01 7.60986152e+01 5.69822915e+01\n",
      " 1.29226393e+02 6.61698450e+01 1.30636085e+02 3.96344856e+00\n",
      " 1.97034209e+00 6.57087773e+01 2.82531780e+01 5.73477277e+01\n",
      " 1.46775122e+02 7.79848599e+01 1.11360620e-13]\n",
      "11-th iteration, loss: 0.25426071778904286, 21 gd steps\n",
      "insert gradient: -0.06315790081435205\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[2.58829910e+00 5.27136323e+01 8.62971066e+01 2.06292717e+01\n",
      " 7.77932896e+01 5.81532057e+01 1.30210757e+02 6.58179321e+01\n",
      " 1.33348155e+02 7.13248816e+01 2.78913822e+01 5.54631429e+01\n",
      " 3.68894593e+01 0.00000000e+00 1.10668378e+02 8.11360967e+01\n",
      " 1.23807359e-13]\n",
      "12-th iteration, loss: 0.23464362449809664, 103 gd steps\n",
      "insert gradient: -0.0027819226420594673\n",
      "12-th iteration, new layer inserted. now 15 layers\n",
      "[3.13642548e+00 5.26566207e+01 8.54989334e+01 2.71373499e+01\n",
      " 8.00063319e+01 6.09981821e+01 1.36756085e+02 6.71162112e+01\n",
      " 1.38346333e+02 6.96046354e+01 1.58870430e+02 7.67582599e+00\n",
      " 1.06962432e+02 8.67645338e+01 3.67861414e-13]\n",
      "13-th iteration, loss: 0.23280400771467755, 34 gd steps\n",
      "insert gradient: -0.01745510582091114\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[3.24869345e+00 5.54567427e+01 8.66123162e+01 0.00000000e+00\n",
      " 2.30926389e-14 2.71076068e+01 7.98529452e+01 5.91142288e+01\n",
      " 1.39964977e+02 6.58945844e+01 1.41112349e+02 6.76377269e+01\n",
      " 1.67465940e+02 1.42600670e+01 8.43852678e+01 9.00787393e+01\n",
      " 4.46555530e-13]\n",
      "14-th iteration, loss: 0.2294582580630604, 77 gd steps\n",
      "insert gradient: -0.039752310649405063\n",
      "14-th iteration, new layer inserted. now 15 layers\n",
      "[1.85162078e+00 5.87606570e+01 8.92241391e+01 2.91710551e+01\n",
      " 7.96653631e+01 5.85880454e+01 1.44661056e+02 6.57791022e+01\n",
      " 1.45583360e+02 6.59117371e+01 1.68821731e+02 2.76032111e+01\n",
      " 5.91801615e+01 9.67118543e+01 5.20056353e-13]\n",
      "15-th iteration, loss: 0.22440691230939125, 22 gd steps\n",
      "insert gradient: -0.06285628669493719\n",
      "15-th iteration, new layer inserted. now 15 layers\n",
      "[2.48946245e+00 5.88945824e+01 9.03685875e+01 3.22465755e+01\n",
      " 7.99260645e+01 5.80740549e+01 1.50085287e+02 6.21382836e+01\n",
      " 1.49622543e+02 6.58542597e+01 1.58829140e+02 5.10737551e+01\n",
      " 2.79232343e+01 1.05814049e+02 6.02954942e-13]\n",
      "16-th iteration, loss: 0.22329547405960162, 16 gd steps\n",
      "insert gradient: -0.02930048569419768\n",
      "16-th iteration, new layer inserted. now 17 layers\n",
      "[1.44112063e+00 5.97893587e+01 9.11542237e+01 3.23466565e+01\n",
      " 8.03195404e+01 5.79575965e+01 1.52080573e+02 6.31207394e+01\n",
      " 1.52030222e+02 6.41868154e+01 1.59901477e+02 5.28719895e+01\n",
      " 2.59673627e+01 5.31987952e+01 0.00000000e+00 5.31987952e+01\n",
      " 6.07489317e-13]\n",
      "17-th iteration, loss: 0.1954859067041352, 31 gd steps\n",
      "insert gradient: -0.06424851060073859\n",
      "17-th iteration, new layer inserted. now 17 layers\n",
      "[1.94227295e+00 5.51460267e+01 8.89035751e+01 4.02091594e+01\n",
      " 8.74666413e+01 6.16254911e+01 1.53673294e+02 6.85661857e+01\n",
      " 1.54195823e+02 6.56833449e+01 1.60773670e+02 6.30757997e+01\n",
      " 2.62418070e+01 3.28368139e+01 5.36160744e+01 3.88169181e+01\n",
      " 6.51523352e-13]\n",
      "18-th iteration, loss: 0.17194776627361213, 24 gd steps\n",
      "insert gradient: -0.017320292748986273\n",
      "18-th iteration, new layer inserted. now 17 layers\n",
      "[3.18230581e+00 5.30543740e+01 8.97848175e+01 4.65970427e+01\n",
      " 9.04167385e+01 6.15765533e+01 1.58238124e+02 6.31984555e+01\n",
      " 1.67660742e+02 5.89187082e+01 1.72796504e+02 6.00787931e+01\n",
      " 3.73503364e+01 1.16299022e+01 1.17085807e+02 3.50559483e+01\n",
      " 6.25088894e-13]\n",
      "19-th iteration, loss: 0.14449468255164957, 67 gd steps\n",
      "insert gradient: -0.0064839104924876295\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[1.80465119e+00 5.28305306e+01 9.27497868e+01 5.12317139e+01\n",
      " 9.16682170e+01 5.32993007e+01 1.72615083e+02 5.54687268e+01\n",
      " 1.73054226e+02 5.70748793e+01 1.73255923e+02 5.85584728e+01\n",
      " 5.05224395e+01 1.98240145e+00 1.19075771e+02 9.87106755e+01\n",
      " 4.92452678e-13]\n",
      "20-th iteration, loss: 0.14366657682365436, 17 gd steps\n",
      "insert gradient: -0.010906766454386478\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[2.56597267e+00 5.15734033e+01 9.14651216e+01 5.13075269e+01\n",
      " 9.29656754e+01 5.41131878e+01 1.69327775e+02 5.84407809e+01\n",
      " 1.71105791e+02 5.67948927e+01 1.73818176e+02 6.04502706e+01\n",
      " 4.19012489e+01 7.71697237e+00 1.10175658e+02 1.00602865e+02\n",
      " 4.57364938e-13]\n",
      "21-th iteration, loss: 0.14334685877618675, 197 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.71022242997076e-06\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[2.11724369e+00 5.22649417e+01 9.19963858e+01 5.08514010e+01\n",
      " 9.21676784e+01 5.44540040e+01 1.69690736e+02 5.67904862e+01\n",
      " 1.71298098e+02 0.00000000e+00 7.10542736e-14 5.83094289e+01\n",
      " 1.72337929e+02 6.17958528e+01 3.48036729e+01 1.02096733e+01\n",
      " 1.10646927e+02 1.00362834e+02 2.95761536e-13]\n",
      "22-th iteration, loss: 0.14334685847331757, 12 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2864095291612622e-06\n",
      "22-th iteration, new layer inserted. now 19 layers\n",
      "[2.11683220e+00 5.22639402e+01 9.19961236e+01 0.00000000e+00\n",
      " 1.59872116e-14 5.08519208e+01 9.21676393e+01 5.44537328e+01\n",
      " 1.69690501e+02 5.67903827e+01 1.71298072e+02 5.83103226e+01\n",
      " 1.72338047e+02 6.17956472e+01 3.48025833e+01 1.02099693e+01\n",
      " 1.10646845e+02 1.00362727e+02 2.91741746e-13]\n",
      "23-th iteration, loss: 0.14334685847297327, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2757561224107254e-06\n",
      "23-th iteration, new layer inserted. now 17 layers\n",
      "[2.11683108e+00 5.22639365e+01 9.19961228e+01 5.08519234e+01\n",
      " 9.21676397e+01 5.44537333e+01 1.69690501e+02 5.67903829e+01\n",
      " 1.71298072e+02 5.83103235e+01 1.72338047e+02 6.17956475e+01\n",
      " 3.48025813e+01 1.02099699e+01 1.10646845e+02 1.00362727e+02\n",
      " 2.91742482e-13]\n",
      "24-th iteration, loss: 0.1433468584726595, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3030203746482107e-06\n",
      "24-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682997e+00 5.22639328e+01 9.19961219e+01 0.00000000e+00\n",
      " 1.24344979e-14 5.08519247e+01 9.21676401e+01 5.44537338e+01\n",
      " 1.69690501e+02 5.67903829e+01 1.71298071e+02 5.83103244e+01\n",
      " 1.72338047e+02 6.17956479e+01 3.48025794e+01 1.02099705e+01\n",
      " 1.10646845e+02 1.00362726e+02 2.91750696e-13]\n",
      "25-th iteration, loss: 0.14334685847232953, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2891810112175364e-06\n",
      "25-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682888e+00 5.22639293e+01 9.19961210e+01 0.00000000e+00\n",
      " 1.42108547e-14 5.08519273e+01 9.21676406e+01 5.44537343e+01\n",
      " 1.69690501e+02 5.67903830e+01 1.71298071e+02 5.83103253e+01\n",
      " 1.72338048e+02 6.17956482e+01 3.48025775e+01 1.02099711e+01\n",
      " 1.10646845e+02 1.00362725e+02 2.91749354e-13]\n",
      "26-th iteration, loss: 0.1433468584720066, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2756847074920567e-06\n",
      "26-th iteration, new layer inserted. now 17 layers\n",
      "[2.11682780e+00 5.22639258e+01 9.19961202e+01 5.08519299e+01\n",
      " 9.21676410e+01 5.44537348e+01 1.69690501e+02 5.67903831e+01\n",
      " 1.71298071e+02 5.83103262e+01 1.72338048e+02 6.17956486e+01\n",
      " 3.48025756e+01 1.02099717e+01 1.10646845e+02 1.00362725e+02\n",
      " 2.91744283e-13]\n",
      "27-th iteration, loss: 0.14334685847171336, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3003198210470663e-06\n",
      "27-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682674e+00 5.22639223e+01 9.19961194e+01 0.00000000e+00\n",
      " 2.30926389e-14 5.08519312e+01 9.21676414e+01 5.44537353e+01\n",
      " 1.69690501e+02 5.67903832e+01 1.71298071e+02 5.83103271e+01\n",
      " 1.72338048e+02 6.17956490e+01 3.48025737e+01 1.02099724e+01\n",
      " 1.10646845e+02 1.00362724e+02 2.91738693e-13]\n",
      "28-th iteration, loss: 0.14334685847140327, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2841470268527765e-06\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[2.11682569e+00 5.22639189e+01 9.19961186e+01 5.08519338e+01\n",
      " 9.21676418e+01 5.44537357e+01 1.69690501e+02 5.67903833e+01\n",
      " 1.71298071e+02 5.83103280e+01 1.72338048e+02 6.17956493e+01\n",
      " 3.48025718e+01 1.02099730e+01 1.10646845e+02 1.00362724e+02\n",
      " 2.91735509e-13]\n",
      "29-th iteration, loss: 0.14334685847112272, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3065647404182262e-06\n",
      "29-th iteration, new layer inserted. now 17 layers\n",
      "[2.11682466e+00 5.22639156e+01 9.19961178e+01 5.08519351e+01\n",
      " 9.21676422e+01 5.44537362e+01 1.69690501e+02 5.67903833e+01\n",
      " 1.71298070e+02 5.83103288e+01 1.72338049e+02 6.17956497e+01\n",
      " 3.48025699e+01 1.02099737e+01 1.10646845e+02 1.00362723e+02\n",
      " 2.91726037e-13]\n",
      "30-th iteration, loss: 0.1433468584708485, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3268251851869634e-06\n",
      "30-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682364e+00 5.22639123e+01 9.19961171e+01 0.00000000e+00\n",
      " 1.24344979e-14 5.08519364e+01 9.21676426e+01 5.44537366e+01\n",
      " 1.69690500e+02 5.67903834e+01 1.71298070e+02 5.83103297e+01\n",
      " 1.72338049e+02 6.17956501e+01 3.48025680e+01 1.02099744e+01\n",
      " 1.10646845e+02 1.00362723e+02 2.91715111e-13]\n",
      "31-th iteration, loss: 0.14334685847055573, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3058541956985663e-06\n",
      "31-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682263e+00 5.22639091e+01 9.19961164e+01 0.00000000e+00\n",
      " 2.66453526e-14 5.08519390e+01 9.21676430e+01 5.44537371e+01\n",
      " 1.69690500e+02 5.67903835e+01 1.71298070e+02 5.83103306e+01\n",
      " 1.72338049e+02 6.17956505e+01 3.48025662e+01 1.02099751e+01\n",
      " 1.10646845e+02 1.00362722e+02 2.91706480e-13]\n",
      "32-th iteration, loss: 0.1433468584702688, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2858912553996932e-06\n",
      "32-th iteration, new layer inserted. now 19 layers\n",
      "[2.11682164e+00 5.22639059e+01 9.19961156e+01 0.00000000e+00\n",
      " 1.59872116e-14 5.08519416e+01 9.21676434e+01 5.44537375e+01\n",
      " 1.69690500e+02 5.67903835e+01 1.71298070e+02 5.83103315e+01\n",
      " 1.72338049e+02 6.17956509e+01 3.48025643e+01 1.02099758e+01\n",
      " 1.10646845e+02 1.00362722e+02 2.91696127e-13]\n",
      "33-th iteration, loss: 0.1433468584699876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2668467781862676e-06\n",
      "33-th iteration, new layer inserted. now 17 layers\n",
      "[2.11682065e+00 5.22639028e+01 9.19961149e+01 5.08519442e+01\n",
      " 9.21676438e+01 5.44537379e+01 1.69690500e+02 5.67903836e+01\n",
      " 1.71298070e+02 5.83103324e+01 1.72338050e+02 6.17956513e+01\n",
      " 3.48025624e+01 1.02099765e+01 1.10646845e+02 1.00362722e+02\n",
      " 2.91682847e-13]\n",
      "34-th iteration, loss: 0.14334685846973433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2861010977238378e-06\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681968e+00 5.22638997e+01 9.19961142e+01 0.00000000e+00\n",
      " 3.90798505e-14 5.08519455e+01 9.21676442e+01 5.44537384e+01\n",
      " 1.69690500e+02 5.67903836e+01 1.71298069e+02 5.83103332e+01\n",
      " 1.72338050e+02 6.17956517e+01 3.48025606e+01 1.02099772e+01\n",
      " 1.10646845e+02 1.00362721e+02 2.91673534e-13]\n",
      "35-th iteration, loss: 0.1433468584694633, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2654126956241042e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681872e+00 5.22638966e+01 9.19961136e+01 0.00000000e+00\n",
      " 1.95399252e-14 5.08519480e+01 9.21676446e+01 5.44537388e+01\n",
      " 1.69690500e+02 5.67903837e+01 1.71298069e+02 5.83103341e+01\n",
      " 1.72338050e+02 6.17956521e+01 3.48025588e+01 1.02099780e+01\n",
      " 1.10646845e+02 1.00362721e+02 2.91670308e-13]\n",
      "36-th iteration, loss: 0.14334685846919745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2457185011100659e-06\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681777e+00 5.22638936e+01 9.19961129e+01 0.00000000e+00\n",
      " 1.59872116e-14 5.08519506e+01 9.21676449e+01 5.44537392e+01\n",
      " 1.69690499e+02 5.67903837e+01 1.71298069e+02 5.83103350e+01\n",
      " 1.72338050e+02 6.17956526e+01 3.48025569e+01 1.02099787e+01\n",
      " 1.10646845e+02 1.00362720e+02 2.91668812e-13]\n",
      "37-th iteration, loss: 0.14334685846893663, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2269310106951607e-06\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[2.11681683e+00 5.22638906e+01 9.19961122e+01 5.08519531e+01\n",
      " 9.21676453e+01 5.44537395e+01 1.69690499e+02 5.67903837e+01\n",
      " 1.71298069e+02 5.83103358e+01 1.72338051e+02 6.17956530e+01\n",
      " 3.48025551e+01 1.02099795e+01 1.10646846e+02 1.00362720e+02\n",
      " 2.91675458e-13]\n",
      "38-th iteration, loss: 0.14334685846870185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2452382225068003e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681590e+00 5.22638877e+01 9.19961116e+01 0.00000000e+00\n",
      " 1.59872116e-14 5.08519543e+01 9.21676457e+01 5.44537399e+01\n",
      " 1.69690499e+02 5.67903837e+01 1.71298069e+02 5.83103367e+01\n",
      " 1.72338051e+02 6.17956534e+01 3.48025533e+01 1.02099803e+01\n",
      " 1.10646846e+02 1.00362719e+02 2.91664527e-13]\n",
      "39-th iteration, loss: 0.14334685846845005, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2248921023517828e-06\n",
      "39-th iteration, new layer inserted. now 17 layers\n",
      "[2.11681499e+00 5.22638848e+01 9.19961110e+01 5.08519568e+01\n",
      " 9.21676460e+01 5.44537403e+01 1.69690499e+02 5.67903838e+01\n",
      " 1.71298068e+02 5.83103376e+01 1.72338051e+02 6.17956538e+01\n",
      " 3.48025514e+01 1.02099811e+01 1.10646846e+02 1.00362719e+02\n",
      " 2.91665350e-13]\n",
      "40-th iteration, loss: 0.14334685846822387, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2417061814240204e-06\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[2.11681408e+00 5.22638820e+01 9.19961104e+01 5.08519580e+01\n",
      " 9.21676464e+01 5.44537406e+01 1.69690498e+02 5.67903838e+01\n",
      " 1.71298068e+02 5.83103384e+01 1.72338051e+02 6.17956543e+01\n",
      " 3.48025496e+01 1.02099819e+01 1.10646846e+02 1.00362719e+02\n",
      " 2.91669600e-13]\n",
      "41-th iteration, loss: 0.14334685846800202, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.256744946764407e-06\n",
      "41-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681318e+00 5.22638792e+01 9.19961098e+01 0.00000000e+00\n",
      " 1.95399252e-14 5.08519593e+01 9.21676467e+01 5.44537410e+01\n",
      " 1.69690498e+02 5.67903838e+01 1.71298068e+02 5.83103393e+01\n",
      " 1.72338052e+02 6.17956547e+01 3.48025478e+01 1.02099827e+01\n",
      " 1.10646846e+02 1.00362718e+02 2.91661124e-13]\n",
      "42-th iteration, loss: 0.1433468584677624, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.233061957871669e-06\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681230e+00 5.22638764e+01 9.19961092e+01 0.00000000e+00\n",
      " 1.95399252e-14 5.08519618e+01 9.21676471e+01 5.44537414e+01\n",
      " 1.69690498e+02 5.67903838e+01 1.71298068e+02 5.83103401e+01\n",
      " 1.72338052e+02 6.17956551e+01 3.48025460e+01 1.02099835e+01\n",
      " 1.10646846e+02 1.00362718e+02 2.91653852e-13]\n",
      "43-th iteration, loss: 0.14334685846752718, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2106744429125064e-06\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[2.11681143e+00 5.22638737e+01 9.19961086e+01 5.08519642e+01\n",
      " 9.21676474e+01 5.44537417e+01 1.69690498e+02 5.67903838e+01\n",
      " 1.71298068e+02 5.83103410e+01 1.72338052e+02 6.17956556e+01\n",
      " 3.48025442e+01 1.02099843e+01 1.10646846e+02 1.00362717e+02\n",
      " 2.91652736e-13]\n",
      "44-th iteration, loss: 0.14334685846731657, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.225193660102686e-06\n",
      "44-th iteration, new layer inserted. now 19 layers\n",
      "[2.11681056e+00 5.22638710e+01 9.19961080e+01 0.00000000e+00\n",
      " 2.66453526e-14 5.08519655e+01 9.21676478e+01 5.44537420e+01\n",
      " 1.69690498e+02 5.67903838e+01 1.71298067e+02 5.83103418e+01\n",
      " 1.72338052e+02 6.17956560e+01 3.48025424e+01 1.02099851e+01\n",
      " 1.10646846e+02 1.00362717e+02 2.91643146e-13]\n",
      "45-th iteration, loss: 0.14334685846708892, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2019702025698372e-06\n",
      "45-th iteration, new layer inserted. now 19 layers\n",
      "[2.11680971e+00 5.22638684e+01 9.19961075e+01 0.00000000e+00\n",
      " 2.66453526e-14 5.08519679e+01 9.21676481e+01 5.44537424e+01\n",
      " 1.69690497e+02 5.67903838e+01 1.71298067e+02 5.83103427e+01\n",
      " 1.72338053e+02 6.17956565e+01 3.48025406e+01 1.02099860e+01\n",
      " 1.10646846e+02 1.00362717e+02 2.91632828e-13]\n",
      "46-th iteration, loss: 0.14334685846686523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1800133223987225e-06\n",
      "46-th iteration, new layer inserted. now 17 layers\n",
      "[2.11680886e+00 5.22638658e+01 9.19961069e+01 5.08519703e+01\n",
      " 9.21676485e+01 5.44537427e+01 1.69690497e+02 5.67903838e+01\n",
      " 1.71298067e+02 5.83103435e+01 1.72338053e+02 6.17956569e+01\n",
      " 3.48025388e+01 1.02099868e+01 1.10646846e+02 1.00362716e+02\n",
      " 2.91637836e-13]\n",
      "47-th iteration, loss: 0.1433468584666649, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1940258723984612e-06\n",
      "47-th iteration, new layer inserted. now 17 layers\n",
      "[2.11680802e+00 5.22638632e+01 9.19961064e+01 5.08519715e+01\n",
      " 9.21676488e+01 5.44537430e+01 1.69690497e+02 5.67903838e+01\n",
      " 1.71298067e+02 5.83103444e+01 1.72338053e+02 6.17956574e+01\n",
      " 3.48025371e+01 1.02099877e+01 1.10646847e+02 1.00362716e+02\n",
      " 2.91623076e-13]\n",
      "48-th iteration, loss: 0.14334685846646802, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2064647467888827e-06\n",
      "48-th iteration, new layer inserted. now 17 layers\n",
      "[2.11680719e+00 5.22638606e+01 9.19961059e+01 5.08519727e+01\n",
      " 9.21676491e+01 5.44537433e+01 1.69690496e+02 5.67903838e+01\n",
      " 1.71298066e+02 5.83103452e+01 1.72338054e+02 6.17956578e+01\n",
      " 3.48025353e+01 1.02099885e+01 1.10646847e+02 1.00362716e+02\n",
      " 2.91615015e-13]\n",
      "49-th iteration, loss: 0.14334685846627457, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2174359498136652e-06\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[2.11680638e+00 5.22638582e+01 9.19961054e+01 0.00000000e+00\n",
      " 2.66453526e-14 5.08519739e+01 9.21676495e+01 5.44537436e+01\n",
      " 1.69690496e+02 5.67903838e+01 1.71298066e+02 5.83103460e+01\n",
      " 1.72338054e+02 6.17956583e+01 3.48025335e+01 1.02099894e+01\n",
      " 1.10646847e+02 1.00362715e+02 2.91594190e-13]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5356751240451496\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.38963896    0.         1037.01237109]\n",
      "1-th iteration, loss: 0.747179520681565, 11 gd steps\n",
      "insert gradient: -0.620984082765369\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.16488755  62.36254011 240.28335428   0.         796.72901681]\n",
      "2-th iteration, loss: 0.6077154431727662, 13 gd steps\n",
      "insert gradient: -0.706305958117984\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.42419972  76.99074255 224.67306317  39.27090095 243.8966378\n",
      "   0.         552.83237901]\n",
      "3-th iteration, loss: 0.47180544783327516, 19 gd steps\n",
      "insert gradient: -0.5482736889660742\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          42.07636437 230.33905565  63.69237609 146.28375869\n",
      "  40.84433763 189.54252995   0.         363.28984907]\n",
      "4-th iteration, loss: 0.37838292473610274, 15 gd steps\n",
      "insert gradient: -0.6678044912573877\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.92342287  53.01860714 117.56820999   0.         109.1704807\n",
      "  63.73968766 139.5017768   47.42678728 168.97652328  41.53932429\n",
      " 363.28984907]\n",
      "5-th iteration, loss: 0.3103617914629566, 14 gd steps\n",
      "insert gradient: -0.31862311805843946\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  3.78496626  59.90273518  83.40665179  29.78959929  81.53213135\n",
      "  58.68513095 153.93354914  52.79788642 142.2868704   69.24301\n",
      " 363.28984907]\n",
      "6-th iteration, loss: 0.2899188537889716, 70 gd steps\n",
      "insert gradient: -0.36956075070659966\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          71.43731399  73.9509757   37.35131793  85.92843108\n",
      "  53.19010595 162.45904583  59.8090459  154.37333225  65.46255174\n",
      " 198.15809949   0.         165.13174958]\n",
      "7-th iteration, loss: 0.2113689139319492, 20 gd steps\n",
      "insert gradient: -0.10950860050725682\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 5.93893097e+01 8.78101174e+01 4.49318130e+01\n",
      " 8.64667652e+01 5.08007539e+01 1.64749521e+02 0.00000000e+00\n",
      " 9.23705556e-14 5.61045616e+01 1.62413446e+02 5.91797640e+01\n",
      " 1.78536400e+02 5.36175057e+01 1.65131750e+02]\n",
      "8-th iteration, loss: 0.2017753572078406, 79 gd steps\n",
      "insert gradient: -0.0874575902785212\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[0.00000000e+00 5.81414307e+01 9.98767913e+01 4.64536181e+01\n",
      " 8.45170641e+01 5.01113130e+01 1.76947001e+02 5.40607816e+01\n",
      " 1.72415780e+02 5.76420222e+01 1.68960749e+02 7.25911926e+01\n",
      " 1.65131750e+02 0.00000000e+00 1.42108547e-14]\n",
      "9-th iteration, loss: 0.14489898735108891, 80 gd steps\n",
      "insert gradient: -0.012485342340453254\n",
      "9-th iteration, new layer inserted. now 16 layers\n",
      "[  1.29637475  51.80764063  92.38366647  51.43222244  91.59975497\n",
      "  53.10025173 171.22928464  55.97466904 171.19964611  57.75811982\n",
      " 173.60500375  58.23481632  51.08352088   0.         122.60045012\n",
      "  99.35984514]\n",
      "10-th iteration, loss: 0.1433474016730479, 109 gd steps\n",
      "insert gradient: -0.00020771708522344346\n",
      "10-th iteration, new layer inserted. now 18 layers\n",
      "[2.11546237e+00 5.22686173e+01 9.19926410e+01 5.08600060e+01\n",
      " 9.21863276e+01 5.44688568e+01 1.69693593e+02 5.67914900e+01\n",
      " 1.71323147e+02 5.82966451e+01 1.72373242e+02 6.17402364e+01\n",
      " 3.52278513e+01 0.00000000e+00 1.06581410e-14 1.00831643e+01\n",
      " 1.10469151e+02 1.00391664e+02]\n",
      "11-th iteration, loss: 0.14334693108580315, 34 gd steps\n",
      "insert gradient: -2.3385710259077325e-05\n",
      "11-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11529729  52.26165759  91.99263551  50.85585415  92.17767736\n",
      "  54.4626748  169.68562298  56.79343912 171.30656564  58.30715573\n",
      " 172.35434932  61.77623     34.9531634   10.17496873 110.56197491\n",
      " 100.3743352 ]\n",
      "12-th iteration, loss: 0.14334685838979735, 90 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6140099136242254e-06\n",
      "12-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663418  52.26360878  91.99612538  50.85209101  92.16748399\n",
      "  54.45375263 169.69033814  56.79031459 171.29775103  58.31050838\n",
      " 172.33799516  61.79593749  34.79886163  10.21095945 110.64916115\n",
      " 100.36248145]\n",
      "13-th iteration, loss: 0.1433468583897171, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5227787010091246e-06\n",
      "13-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663435  52.26360947  91.99612599  50.8520926   92.16748455\n",
      "  54.45375269 169.69033785  56.79031453 171.29775075  58.31050831\n",
      " 172.3379948   61.79593713  34.7988606   10.21095917 110.6491605\n",
      " 100.36248109]\n",
      "14-th iteration, loss: 0.14334685838964523, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4375484511855818e-06\n",
      "14-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663451  52.26361011  91.99612655  50.8520941   92.16748506\n",
      "  54.45375273 169.69033758  56.79031448 171.2977505   58.31050825\n",
      " 172.33799446  61.79593681  34.79885959  10.21095894 110.64915986\n",
      " 100.36248074]\n",
      "15-th iteration, loss: 0.14334685838958058, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.357915775009074e-06\n",
      "15-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663466  52.2636107   91.99612707  50.85209552  92.16748555\n",
      "  54.45375274 169.69033732  56.79031445 171.29775026  58.31050823\n",
      " 172.33799414  61.79593651  34.79885861  10.21095875 110.64915924\n",
      " 100.36248041]\n",
      "16-th iteration, loss: 0.14334685838952232, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2835045544248362e-06\n",
      "16-th iteration, new layer inserted. now 16 layers\n",
      "[  2.1166348   52.26361124  91.99612755  50.85209685  92.167486\n",
      "  54.45375274 169.69033706  56.79031443 171.29775005  58.31050823\n",
      " 172.33799384  61.79593624  34.79885765  10.2109586  110.64915864\n",
      " 100.36248009]\n",
      "17-th iteration, loss: 0.1433468583894695, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2139640892081454e-06\n",
      "17-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663492  52.26361174  91.99612799  50.85209812  92.16748642\n",
      "  54.45375272 169.69033682  56.79031442 171.29774985  58.31050824\n",
      " 172.33799355  61.795936    34.79885671  10.21095849 110.64915805\n",
      " 100.36247978]\n",
      "18-th iteration, loss: 0.14334685838942152, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1489673777837258e-06\n",
      "18-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663504  52.2636122   91.9961284   50.85209931  92.16748682\n",
      "  54.45375268 169.69033659  56.79031443 171.29774966  58.31050828\n",
      " 172.33799329  61.79593577  34.79885578  10.21095842 110.64915747\n",
      " 100.36247948]\n",
      "19-th iteration, loss: 0.14334685838937772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0882095040392312e-06\n",
      "19-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663515e+00 5.22636126e+01 9.19961288e+01 0.00000000e+00\n",
      " 3.55271368e-14 5.08521004e+01 9.21674872e+01 5.44537526e+01\n",
      " 1.69690336e+02 5.67903144e+01 1.71297749e+02 5.83105083e+01\n",
      " 1.72337993e+02 6.17959356e+01 3.47988549e+01 1.02109584e+01\n",
      " 1.10649157e+02 1.00362479e+02]\n",
      "20-th iteration, loss: 0.14334685838932132, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.95485023847991e-07\n",
      "20-th iteration, new layer inserted. now 20 layers\n",
      "[2.11663525e+00 5.22636130e+01 9.19961291e+01 1.06192394e-06\n",
      " 3.43480716e-07 0.00000000e+00 9.26442286e-23 5.08521015e+01\n",
      " 9.21674875e+01 5.44537526e+01 1.69690336e+02 5.67903145e+01\n",
      " 1.71297749e+02 5.83105084e+01 1.72337993e+02 6.17959354e+01\n",
      " 3.47988540e+01 1.02109584e+01 1.10649156e+02 1.00362479e+02]\n",
      "21-th iteration, loss: 0.14334685838926, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.801352937658971e-07\n",
      "21-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663534e+00 5.22636133e+01 9.19961294e+01 2.02191240e-06\n",
      " 6.29165597e-07 9.62711362e-07 2.85684881e-07 0.00000000e+00\n",
      " 7.94093388e-23 5.08521025e+01 9.21674878e+01 5.44537525e+01\n",
      " 1.69690336e+02 5.67903145e+01 1.71297749e+02 5.83105085e+01\n",
      " 1.72337993e+02 6.17959352e+01 3.47988531e+01 1.02109584e+01\n",
      " 1.10649156e+02 1.00362479e+02]\n",
      "22-th iteration, loss: 0.14334685838920094, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.53243406522159e-07\n",
      "22-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663541e+00 5.22636136e+01 9.19961297e+01 2.85875459e-06\n",
      " 8.48551040e-07 1.80454118e-06 4.97438554e-07 8.44094553e-07\n",
      " 2.11753673e-07 5.08521033e+01 9.21674881e+01 5.44537523e+01\n",
      " 1.69690336e+02 5.67903146e+01 1.71297749e+02 5.83105086e+01\n",
      " 1.72337992e+02 6.17959351e+01 3.47988523e+01 1.02109585e+01\n",
      " 1.10649155e+02 1.00362478e+02]\n",
      "23-th iteration, loss: 0.14334685838915506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.479564646686449e-07\n",
      "23-th iteration, new layer inserted. now 24 layers\n",
      "[2.11663546e+00 5.22636138e+01 9.19961298e+01 3.56986439e-06\n",
      " 1.00237574e-06 2.52237777e-06 6.36957983e-07 1.56587452e-06\n",
      " 3.44581651e-07 0.00000000e+00 3.97046694e-23 5.08521040e+01\n",
      " 9.21674883e+01 5.44537522e+01 1.69690336e+02 5.67903146e+01\n",
      " 1.71297749e+02 5.83105088e+01 1.72337992e+02 6.17959350e+01\n",
      " 3.47988515e+01 1.02109586e+01 1.10649155e+02 1.00362478e+02]\n",
      "24-th iteration, loss: 0.1433468583891136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.420185118862123e-07\n",
      "24-th iteration, new layer inserted. now 26 layers\n",
      "[2.11663551e+00 5.22636139e+01 9.19961300e+01 4.17221341e-06\n",
      " 1.09948936e-06 3.13267298e-06 7.14075774e-07 2.18121914e-06\n",
      " 4.09286170e-07 6.18076248e-07 6.47045191e-08 0.00000000e+00\n",
      " 6.61744490e-24 5.08521047e+01 9.21674885e+01 5.44537520e+01\n",
      " 1.69690335e+02 5.67903147e+01 1.71297749e+02 5.83105089e+01\n",
      " 1.72337992e+02 6.17959349e+01 3.47988506e+01 1.02109588e+01\n",
      " 1.10649154e+02 1.00362478e+02]\n",
      "25-th iteration, loss: 0.14334685838907768, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4302415363957974e-07\n",
      "25-th iteration, new layer inserted. now 24 layers\n",
      "[2.11663554e+00 5.22636140e+01 9.19961300e+01 4.66834782e-06\n",
      " 1.14241110e-06 3.63752346e-06 7.32163639e-07 2.69173036e-06\n",
      " 4.10082695e-07 1.13183203e-06 6.06013230e-08 5.08521057e+01\n",
      " 9.21674886e+01 5.44537518e+01 1.69690335e+02 5.67903148e+01\n",
      " 1.71297749e+02 5.83105091e+01 1.72337992e+02 6.17959349e+01\n",
      " 3.47988499e+01 1.02109590e+01 1.10649154e+02 1.00362478e+02]\n",
      "26-th iteration, loss: 0.14334685838905112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.783247727448579e-07\n",
      "26-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663556e+00 5.22636140e+01 9.19961301e+01 5.07484509e-06\n",
      " 1.14038708e-06 4.05307705e-06 7.01303612e-07 3.11308808e-06\n",
      " 3.57884312e-07 5.08521077e+01 9.21674887e+01 5.44537516e+01\n",
      " 1.69690335e+02 5.67903149e+01 1.71297749e+02 5.83105093e+01\n",
      " 1.72337992e+02 6.17959348e+01 3.47988491e+01 1.02109592e+01\n",
      " 1.10649153e+02 1.00362477e+02]\n",
      "27-th iteration, loss: 0.14334685838902947, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.355224491724088e-07\n",
      "27-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663557e+00 5.22636140e+01 9.19961301e+01 5.42402593e-06\n",
      " 1.10909429e-06 4.41129817e-06 6.37880556e-07 3.47686870e-06\n",
      " 2.69782639e-07 5.08521080e+01 9.21674887e+01 5.44537514e+01\n",
      " 1.69690335e+02 5.67903150e+01 1.71297749e+02 5.83105095e+01\n",
      " 1.72337992e+02 6.17959348e+01 3.47988483e+01 1.02109595e+01\n",
      " 1.10649153e+02 1.00362477e+02]\n",
      "28-th iteration, loss: 0.14334685838900926, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9969925826936344e-07\n",
      "28-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663558e+00 5.22636140e+01 9.19961301e+01 5.73387075e-06\n",
      " 1.05722610e-06 4.72993519e-06 5.51042347e-07 3.80056244e-06\n",
      " 1.55381985e-07 5.08521084e+01 9.21674888e+01 5.44537511e+01\n",
      " 1.69690335e+02 5.67903151e+01 1.71297749e+02 5.83105097e+01\n",
      " 1.72337992e+02 6.17959348e+01 3.47988476e+01 1.02109598e+01\n",
      " 1.10649152e+02 1.00362477e+02]\n",
      "29-th iteration, loss: 0.14334685838899003, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.0456183074037714e-07\n",
      "29-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663558e+00 5.22636140e+01 9.19961300e+01 6.01156396e-06\n",
      " 9.88496889e-07 5.01600944e-06 4.44817164e-07 4.09100501e-06\n",
      " 1.90283137e-08 5.08521086e+01 9.21674888e+01 5.44537509e+01\n",
      " 1.69690335e+02 5.67903152e+01 1.71297749e+02 5.83105100e+01\n",
      " 1.72337992e+02 6.17959348e+01 3.47988469e+01 1.02109601e+01\n",
      " 1.10649152e+02 1.00362477e+02]\n",
      "30-th iteration, loss: 0.14334685838897185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1591082532783013e-07\n",
      "30-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663559e+00 5.22636139e+01 9.19961300e+01 6.26304080e-06\n",
      " 9.05967616e-07 5.27532246e-06 3.22524096e-07 5.08521133e+01\n",
      " 9.21674889e+01 5.44537506e+01 1.69690335e+02 5.67903153e+01\n",
      " 1.71297749e+02 5.83105102e+01 1.72337991e+02 6.17959348e+01\n",
      " 3.47988461e+01 0.00000000e+00 1.15463195e-14 1.02109604e+01\n",
      " 1.10649152e+02 1.00362477e+02]\n",
      "31-th iteration, loss: 0.1433468583889536, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.171341820003889e-07\n",
      "31-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663559e+00 5.22636139e+01 9.19961300e+01 6.49149315e-06\n",
      " 8.11429416e-07 5.51095676e-06 1.86166428e-07 5.08521135e+01\n",
      " 9.21674889e+01 5.44537504e+01 1.69690335e+02 5.67903155e+01\n",
      " 1.71297749e+02 5.83105104e+01 1.72337991e+02 6.17959348e+01\n",
      " 3.47988454e+01 0.00000000e+00 6.21724894e-15 1.02109610e+01\n",
      " 1.10649151e+02 1.00362477e+02]\n",
      "32-th iteration, loss: 0.14334685838893557, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1782005632404644e-07\n",
      "32-th iteration, new layer inserted. now 22 layers\n",
      "[2.11663559e+00 5.22636138e+01 9.19961299e+01 6.70891779e-06\n",
      " 7.10213164e-07 5.73481391e-06 4.12627453e-08 5.08521137e+01\n",
      " 9.21674889e+01 5.44537501e+01 1.69690334e+02 5.67903156e+01\n",
      " 1.71297749e+02 5.83105106e+01 1.72337991e+02 6.17959348e+01\n",
      " 3.47988447e+01 0.00000000e+00 6.21724894e-15 1.02109616e+01\n",
      " 1.10649151e+02 1.00362476e+02]\n",
      "33-th iteration, loss: 0.14334685838891792, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.184349144951922e-07\n",
      "33-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663558e+00 5.22636138e+01 9.19961299e+01 6.91692133e-06\n",
      " 6.03180302e-07 5.08521199e+01 9.21674889e+01 5.44537498e+01\n",
      " 1.69690334e+02 5.67903157e+01 1.71297749e+02 5.83105109e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988440e+01 1.02109623e+01\n",
      " 1.10649150e+02 1.00362476e+02]\n",
      "34-th iteration, loss: 0.14334685838890251, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.232485664037504e-07\n",
      "34-th iteration, new layer inserted. now 20 layers\n",
      "[2.11663558e+00 5.22636137e+01 9.19961298e+01 7.11561215e-06\n",
      " 4.90457479e-07 5.08521201e+01 9.21674890e+01 5.44537496e+01\n",
      " 1.69690334e+02 5.67903158e+01 1.71297749e+02 5.83105111e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988432e+01 0.00000000e+00\n",
      " 4.44089210e-15 1.02109626e+01 1.10649150e+02 1.00362476e+02]\n",
      "35-th iteration, loss: 0.14334685838888575, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2120449481799077e-07\n",
      "35-th iteration, new layer inserted. now 20 layers\n",
      "[2.11663558e+00 5.22636136e+01 9.19961297e+01 7.30697478e-06\n",
      " 3.73445539e-07 5.08521203e+01 9.21674890e+01 5.44537493e+01\n",
      " 1.69690334e+02 5.67903159e+01 1.71297749e+02 5.83105113e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988425e+01 0.00000000e+00\n",
      " 4.44089210e-15 1.02109633e+01 1.10649149e+02 1.00362476e+02]\n",
      "36-th iteration, loss: 0.14334685838886907, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1916231955274787e-07\n",
      "36-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663557e+00 5.22636136e+01 9.19961297e+01 7.49484336e-06\n",
      " 2.53599941e-07 5.08521205e+01 9.21674890e+01 5.44537491e+01\n",
      " 1.69690334e+02 5.67903161e+01 1.71297749e+02 5.83105116e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988418e+01 1.02109639e+01\n",
      " 1.10649149e+02 1.00362476e+02]\n",
      "37-th iteration, loss: 0.1433468583888539, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.235422697652663e-07\n",
      "37-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663557e+00 5.22636135e+01 9.19961296e+01 7.67958173e-06\n",
      " 1.31097017e-07 5.08521207e+01 9.21674890e+01 5.44537488e+01\n",
      " 1.69690334e+02 5.67903162e+01 1.71297749e+02 5.83105118e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988411e+01 1.02109642e+01\n",
      " 1.10649148e+02 1.00362475e+02]\n",
      "38-th iteration, loss: 0.14334685838883876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.27607571980854e-07\n",
      "38-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663557e+00 5.22636134e+01 9.19961295e+01 7.85837985e-06\n",
      " 5.05726240e-09 5.08521208e+01 9.21674890e+01 5.44537486e+01\n",
      " 1.69690334e+02 5.67903163e+01 1.71297749e+02 5.83105120e+01\n",
      " 1.72337991e+02 6.17959348e+01 3.47988404e+01 1.02109645e+01\n",
      " 1.10649148e+02 1.00362475e+02]\n",
      "39-th iteration, loss: 0.143346858388824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3180617284312414e-07\n",
      "39-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663556e+00 5.22636133e+01 9.19961295e+01 5.08521291e+01\n",
      " 9.21674890e+01 5.44537483e+01 1.69690334e+02 5.67903164e+01\n",
      " 1.71297749e+02 5.83105123e+01 1.72337991e+02 6.17959348e+01\n",
      " 3.47988397e+01 0.00000000e+00 3.55271368e-15 1.02109649e+01\n",
      " 1.10649148e+02 1.00362475e+02]\n",
      "40-th iteration, loss: 0.14334685838880817, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.274810746766088e-07\n",
      "40-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663556e+00 5.22636133e+01 9.19961294e+01 5.08521292e+01\n",
      " 9.21674890e+01 5.44537480e+01 1.69690334e+02 5.67903165e+01\n",
      " 1.71297749e+02 5.83105125e+01 1.72337991e+02 6.17959349e+01\n",
      " 3.47988389e+01 0.00000000e+00 3.55271368e-15 1.02109655e+01\n",
      " 1.10649147e+02 1.00362475e+02]\n",
      "41-th iteration, loss: 0.14334685838879238, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2341049512650443e-07\n",
      "41-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663555  52.2636132   91.99612934  50.85212939  92.16748902\n",
      "  54.45374779 169.69033344  56.79031667 171.29774895  58.31051271\n",
      " 172.33799054  61.79593488  34.79883823  10.2109662  110.64914668\n",
      " 100.36247478]\n",
      "42-th iteration, loss: 0.14334685838877814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2606278087873805e-07\n",
      "42-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663555  52.26361312  91.99612928  50.85212956  92.16748902\n",
      "  54.45374754 169.69033333  56.79031679 171.29774895  58.31051293\n",
      " 172.33799046  61.7959349   34.79883751  10.21096652 110.64914625\n",
      " 100.3624746 ]\n",
      "43-th iteration, loss: 0.14334685838876396, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2859990665633787e-07\n",
      "43-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663554e+00 5.22636130e+01 9.19961292e+01 5.08521297e+01\n",
      " 9.21674890e+01 5.44537473e+01 1.69690333e+02 5.67903169e+01\n",
      " 1.71297749e+02 5.83105132e+01 1.72337990e+02 6.17959349e+01\n",
      " 3.47988368e+01 0.00000000e+00 1.59872116e-14 1.02109668e+01\n",
      " 1.10649146e+02 1.00362474e+02]\n",
      "44-th iteration, loss: 0.1433468583887483, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.244424737239355e-07\n",
      "44-th iteration, new layer inserted. now 16 layers\n",
      "[  2.11663554  52.26361298  91.99612914  50.85212989  92.16748903\n",
      "  54.45374704 169.69033313  56.79031702 171.29774897  58.31051338\n",
      " 172.33799031  61.79593493  34.79883608  10.21096751 110.64914538\n",
      " 100.36247427]\n",
      "45-th iteration, loss: 0.1433468583887342, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2703236701764496e-07\n",
      "45-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663554e+00 5.22636129e+01 9.19961291e+01 5.08521301e+01\n",
      " 9.21674890e+01 5.44537468e+01 1.69690333e+02 5.67903171e+01\n",
      " 1.71297749e+02 5.83105136e+01 1.72337990e+02 6.17959349e+01\n",
      " 3.47988354e+01 0.00000000e+00 9.76996262e-15 1.02109678e+01\n",
      " 1.10649145e+02 1.00362474e+02]\n",
      "46-th iteration, loss: 0.14334685838871866, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2295466144279403e-07\n",
      "46-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663553e+00 5.22636128e+01 9.19961290e+01 5.08521302e+01\n",
      " 9.21674890e+01 5.44537465e+01 1.69690333e+02 5.67903173e+01\n",
      " 1.71297749e+02 5.83105138e+01 1.72337990e+02 6.17959350e+01\n",
      " 3.47988347e+01 0.00000000e+00 1.59872116e-14 1.02109685e+01\n",
      " 1.10649145e+02 1.00362474e+02]\n",
      "47-th iteration, loss: 0.14334685838870315, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1911305103623735e-07\n",
      "47-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663553e+00 5.22636128e+01 9.19961289e+01 5.08521304e+01\n",
      " 9.21674890e+01 5.44537463e+01 1.69690333e+02 5.67903174e+01\n",
      " 1.71297749e+02 5.83105140e+01 1.72337990e+02 6.17959350e+01\n",
      " 3.47988339e+01 0.00000000e+00 1.59872116e-14 1.02109691e+01\n",
      " 1.10649144e+02 1.00362474e+02]\n",
      "48-th iteration, loss: 0.14334685838868774, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1549131913932315e-07\n",
      "48-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663552e+00 5.22636127e+01 9.19961289e+01 5.08521305e+01\n",
      " 9.21674891e+01 5.44537461e+01 1.69690333e+02 5.67903175e+01\n",
      " 1.71297749e+02 5.83105143e+01 1.72337990e+02 6.17959350e+01\n",
      " 3.47988332e+01 0.00000000e+00 2.66453526e-15 1.02109698e+01\n",
      " 1.10649144e+02 1.00362474e+02]\n",
      "49-th iteration, loss: 0.1433468583886724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.12074426039827e-07\n",
      "49-th iteration, new layer inserted. now 18 layers\n",
      "[2.11663552e+00 5.22636126e+01 9.19961288e+01 5.08521307e+01\n",
      " 9.21674891e+01 5.44537458e+01 1.69690333e+02 5.67903176e+01\n",
      " 1.71297749e+02 5.83105145e+01 1.72337990e+02 6.17959350e+01\n",
      " 3.47988325e+01 0.00000000e+00 7.10542736e-15 1.02109704e+01\n",
      " 1.10649143e+02 1.00362473e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235572\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.39870033    0.         1061.12893786]\n",
      "1-th iteration, loss: 0.7491078933829634, 11 gd steps\n",
      "insert gradient: -0.6312101964433865\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.89280919  62.32063131 232.93074246   0.         828.1981954 ]\n",
      "2-th iteration, loss: 0.6037414218496021, 13 gd steps\n",
      "insert gradient: -0.6448764696018391\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.4022178   77.65582442 218.55906518  42.14426504 236.62805583\n",
      "   0.         591.57013957]\n",
      "3-th iteration, loss: 0.46926658064781984, 19 gd steps\n",
      "insert gradient: -0.7417006477635136\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          58.88620137 223.89947024  57.55102941 161.9791376\n",
      "  47.69263153 371.84408773   0.         219.72605184]\n",
      "4-th iteration, loss: 0.37571074747919997, 13 gd steps\n",
      "insert gradient: -0.3227278407579665\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[2.34465118e-01 6.42656055e+01 2.23887990e+02 5.62134905e+01\n",
      " 1.49009424e+02 5.02885678e+01 3.37502027e+02 4.88224369e+01\n",
      " 2.11588050e+02 0.00000000e+00 8.13800192e+00]\n",
      "5-th iteration, loss: 0.31483711985916263, 17 gd steps\n",
      "insert gradient: -0.3704346525082714\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.77065653  57.63300938 232.6427641   57.57780709 120.23681007\n",
      "  66.05141172 211.42191784   0.         120.81252448  39.08501987\n",
      " 184.86550459  47.51676344   8.13800192]\n",
      "6-th iteration, loss: 0.237385565401623, 34 gd steps\n",
      "insert gradient: -0.0876254629275029\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.24886575  66.53175642 226.46137627  60.82132996 109.62637503\n",
      "  64.472037   165.69387139  54.98577629  71.57992793  34.73104815\n",
      " 108.43061227   0.          86.74448982  49.07603204   8.13800192]\n",
      "7-th iteration, loss: 0.22872476851054066, 43 gd steps\n",
      "insert gradient: -0.06722258825454967\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.77233073  65.18103851 118.1547074    0.         118.1547074\n",
      "  61.30331772 110.2225945   68.47921143 159.0428035   63.25768765\n",
      "  83.52928401  36.30201206  75.05830686  22.01839206  56.91987351\n",
      "  54.65677891   8.13800192]\n",
      "8-th iteration, loss: 0.22353940712681492, 19 gd steps\n",
      "insert gradient: -0.038433886341938034\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  0.          66.5561083  111.15407141  10.1739153   98.86171468\n",
      "  66.0096024  112.98022435  72.65622733 152.06494477  67.68472861\n",
      "  85.93000006  35.37845549  65.74773928  28.2139516   57.45928702\n",
      "  53.42193532   8.13800192]\n",
      "9-th iteration, loss: 0.21570681277405937, 30 gd steps\n",
      "insert gradient: -0.08223503232120015\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  0.38504562  67.81898516 113.65323089  16.19419915  91.12168941\n",
      "  69.86063206 114.61536425  72.36711488 155.07394202  68.62603866\n",
      "  99.7193286   36.88109534  42.15290247  43.09362485  67.65741209\n",
      "  44.32976093   8.13800192]\n",
      "10-th iteration, loss: 0.20477218542069134, 22 gd steps\n",
      "insert gradient: -0.13331280666379308\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[6.28679144e-01 6.93832771e+01 1.17499503e+02 2.23938372e+01\n",
      " 9.20043322e+01 6.84656784e+01 1.12814260e+02 7.37492664e+01\n",
      " 1.55461865e+02 0.00000000e+00 4.61852778e-14 7.44015893e+01\n",
      " 1.01606744e+02 4.06047844e+01 3.08808939e+01 5.08698226e+01\n",
      " 8.28077180e+01 4.29580064e+01 8.13800192e+00]\n",
      "11-th iteration, loss: 0.18960134542197193, 22 gd steps\n",
      "insert gradient: -0.07198102074878314\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[1.51492903e+00 6.84670128e+01 1.23746101e+02 2.87396654e+01\n",
      " 8.61079653e+01 6.95962700e+01 1.18333244e+02 6.33574842e+01\n",
      " 1.56633044e+02 0.00000000e+00 1.77635684e-14 9.13294154e+01\n",
      " 1.00722969e+02 4.99399253e+01 2.93766757e+01 4.73305177e+01\n",
      " 9.35741455e+01 5.06563131e+01 8.13800192e+00]\n",
      "12-th iteration, loss: 0.18614609953929664, 14 gd steps\n",
      "insert gradient: -0.03977028003799568\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[1.74421626e+00 6.89175142e+01 1.25322427e+02 3.08351555e+01\n",
      " 8.57379917e+01 6.85948885e+01 1.17710967e+02 6.15568615e+01\n",
      " 1.56310525e+02 0.00000000e+00 3.55271368e-15 9.64310249e+01\n",
      " 1.00182250e+02 5.07172203e+01 3.10353442e+01 4.48479822e+01\n",
      " 9.49792137e+01 5.27922814e+01 8.13800192e+00]\n",
      "13-th iteration, loss: 0.18408004650317153, 99 gd steps\n",
      "insert gradient: -0.0037486262203891385\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[  1.96418598  66.88740311 129.44310684  33.1012382   83.46852924\n",
      "  71.59563543 115.49782817  60.19241501 155.7057065  102.1270057\n",
      "  97.65568153  50.88310296  34.52840137  41.48843049  99.60121247\n",
      "  55.66634607   8.13800192]\n",
      "14-th iteration, loss: 0.18387523084376023, 25 gd steps\n",
      "insert gradient: -0.008765373699008469\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[  2.69133218  64.112754   134.78627556  32.95403099  81.64162715\n",
      "  72.81943093 114.27283304  60.36694046 155.21588198 102.30945892\n",
      "  97.78418857  51.39014171  34.61737359  41.12760085 100.40037438\n",
      "  55.5066665    8.13800192]\n",
      "15-th iteration, loss: 0.18384220097964388, 14 gd steps\n",
      "insert gradient: -0.0031493787483932024\n",
      "15-th iteration, new layer inserted. now 19 layers\n",
      "[2.69227408e+00 6.37062370e+01 1.35588060e+02 3.27576815e+01\n",
      " 8.19231697e+01 7.26681293e+01 1.14347750e+02 6.04768851e+01\n",
      " 1.55185786e+02 1.02768005e+02 9.75492880e+01 5.15545102e+01\n",
      " 3.46142712e+01 0.00000000e+00 6.21724894e-15 4.07799589e+01\n",
      " 1.00337404e+02 5.55006684e+01 8.13800192e+00]\n",
      "16-th iteration, loss: 0.1838070978167619, 50 gd steps\n",
      "insert gradient: -0.001541122753530841\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[2.86627441e+00 6.27224776e+01 1.37487442e+02 3.23525259e+01\n",
      " 8.25214414e+01 7.25596382e+01 1.14858224e+02 0.00000000e+00\n",
      " 3.90798505e-14 6.02632832e+01 1.55390766e+02 1.02897092e+02\n",
      " 9.76010372e+01 5.15077730e+01 3.50981075e+01 4.07190719e+01\n",
      " 1.00529368e+02 5.55933816e+01 8.13800192e+00]\n",
      "17-th iteration, loss: 0.1838049554643929, 15 gd steps\n",
      "insert gradient: -0.0006213221904593371\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[2.93802825e+00 6.25043565e+01 1.37919113e+02 3.22042670e+01\n",
      " 8.26974444e+01 7.24814524e+01 1.14954532e+02 2.26092923e-02\n",
      " 2.88220923e-02 6.02782422e+01 1.55520004e+02 1.02857705e+02\n",
      " 9.75565898e+01 5.15317008e+01 3.51372458e+01 0.00000000e+00\n",
      " 1.15463195e-14 4.06447814e+01 1.00599385e+02 5.55101767e+01\n",
      " 8.13800192e+00]\n",
      "18-th iteration, loss: 0.18380352646753154, 62 gd steps\n",
      "insert gradient: -0.00017353081347475061\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[2.94301501e+00 6.24296893e+01 0.00000000e+00 2.84217094e-14\n",
      " 1.38069243e+02 3.21973358e+01 8.27612656e+01 7.24634574e+01\n",
      " 1.15068294e+02 6.02364106e+01 1.55499725e+02 1.02945822e+02\n",
      " 9.75925084e+01 5.15337343e+01 3.52242564e+01 4.06174134e+01\n",
      " 1.00572630e+02 5.55975209e+01 8.13800192e+00]\n",
      "19-th iteration, loss: 0.18380297718221963, 290 gd steps\n",
      "insert gradient: -2.926933664321583e-05\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[2.98662418e+00 6.22860668e+01 1.38358667e+02 3.21149199e+01\n",
      " 8.28760943e+01 7.24133501e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.15134955e+02 6.02294012e+01 1.55549046e+02 1.02949361e+02\n",
      " 9.76017418e+01 5.15459240e+01 3.52794768e+01 4.05795238e+01\n",
      " 1.00576114e+02 5.56000547e+01 8.13800192e+00]\n",
      "20-th iteration, loss: 0.18380293435971135, 19 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.331922043486538e-06\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[  2.9982439   62.2603259  138.404151    32.09604061  82.91708393\n",
      "  72.39320643 115.17608515  60.2200905  155.56357579 102.95335915\n",
      "  97.60149941  51.54263748  35.30838352  40.56463775 100.58061898\n",
      "  55.60490751   8.13800192]\n",
      "21-th iteration, loss: 0.1838029119555727, 35 gd steps\n",
      "insert gradient: -4.0780757624090735e-05\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[3.00502642e+00 6.22449472e+01 1.38434427e+02 3.20829086e+01\n",
      " 8.29385643e+01 7.23664172e+01 1.15207081e+02 0.00000000e+00\n",
      " 1.77635684e-14 6.02233242e+01 1.55575665e+02 1.02940999e+02\n",
      " 9.76073781e+01 5.15397885e+01 3.53299871e+01 4.05521654e+01\n",
      " 1.00570374e+02 5.56074767e+01 8.13800192e+00]\n",
      "22-th iteration, loss: 0.18380290877750102, 52 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.60135047935497e-06\n",
      "22-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708556e+00 6.22418802e+01 1.38439352e+02 3.20796057e+01\n",
      " 8.29463536e+01 7.23571309e+01 1.15219308e+02 6.02232244e+01\n",
      " 1.55580806e+02 1.02940110e+02 9.76058731e+01 5.15404768e+01\n",
      " 0.00000000e+00 1.15463195e-14 3.53348721e+01 4.05481766e+01\n",
      " 1.00568545e+02 5.56079230e+01 8.13800192e+00]\n",
      "23-th iteration, loss: 0.18380290877535713, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.546306958144723e-06\n",
      "23-th iteration, new layer inserted. now 17 layers\n",
      "[  3.0070857   62.241876   138.43935532  32.07960523  82.94635791\n",
      "  72.35712507 115.21930881  60.22322515 155.58080862 102.9401102\n",
      "  97.60587497  51.54047091  35.33488334  40.54817551 100.56854312\n",
      "  55.60792448   8.13800192]\n",
      "24-th iteration, loss: 0.18380290877356775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.5598087395615e-06\n",
      "24-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708585  62.24187181 138.43935836  32.07960481  82.94636224\n",
      "  72.35711931 115.21931007  60.22322598 155.58081091 102.94011019\n",
      "  97.60587684  51.54046502  35.3348889   40.54817434 100.56854082\n",
      "  55.60792597   8.13800192]\n",
      "25-th iteration, loss: 0.18380290877179378, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.572403012169296e-06\n",
      "25-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708601e+00 6.22418677e+01 1.38439361e+02 3.20796044e+01\n",
      " 8.29463666e+01 7.23571136e+01 1.15219311e+02 6.02232269e+01\n",
      " 1.55580813e+02 1.02940110e+02 9.76058787e+01 5.15404592e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53348945e+01 4.05481732e+01\n",
      " 1.00568539e+02 5.56079275e+01 8.13800192e+00]\n",
      "26-th iteration, loss: 0.18380290876969801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.517937351266898e-06\n",
      "26-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708619  62.24186358 138.43936447  32.07960402  82.94637097\n",
      "  72.35710806 115.21931279  60.22322793 155.58081552 102.94011023\n",
      "  97.6058807   51.54045349  35.33490564  40.54817206 100.56853625\n",
      "  55.60792896   8.13800192]\n",
      "27-th iteration, loss: 0.18380290876794986, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.531546955681205e-06\n",
      "27-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708637e+00 6.22418595e+01 1.38439368e+02 3.20796037e+01\n",
      " 8.29463754e+01 7.23571025e+01 1.15219314e+02 6.02232290e+01\n",
      " 1.55580818e+02 1.02940110e+02 9.76058827e+01 5.15404478e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53349112e+01 4.05481709e+01\n",
      " 1.00568534e+02 5.56079305e+01 8.13800192e+00]\n",
      "28-th iteration, loss: 0.18380290876588293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.478552241986836e-06\n",
      "28-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708656e+00 6.22418555e+01 1.38439371e+02 3.20796034e+01\n",
      " 8.29463798e+01 7.23570971e+01 1.15219316e+02 6.02232302e+01\n",
      " 1.55580820e+02 1.02940110e+02 9.76058846e+01 5.15404421e+01\n",
      " 0.00000000e+00 1.15463195e-14 3.53349223e+01 4.05481697e+01\n",
      " 1.00568532e+02 5.56079320e+01 8.13800192e+00]\n",
      "29-th iteration, loss: 0.18380290876383212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.427965152494829e-06\n",
      "29-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708676e+00 6.22418516e+01 1.38439374e+02 3.20796031e+01\n",
      " 8.29463843e+01 7.23570918e+01 1.15219317e+02 6.02232314e+01\n",
      " 1.55580822e+02 1.02940110e+02 9.76058866e+01 5.15404365e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53349332e+01 4.05481685e+01\n",
      " 1.00568529e+02 5.56079335e+01 8.13800192e+00]\n",
      "30-th iteration, loss: 0.18380290876179664, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.37963681826917e-06\n",
      "30-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708697  62.24184762 138.43937686  32.07960281  82.94638877\n",
      "  72.35708649 115.21931885  60.22323263 155.58082476 102.94011025\n",
      "  97.60588865  51.54043081  35.33494412  40.5481672  100.5685271\n",
      "  55.60793502   8.13800192]\n",
      "31-th iteration, loss: 0.18380290876008923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.397372667542212e-06\n",
      "31-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708719e+00 6.22418437e+01 1.38439380e+02 3.20796026e+01\n",
      " 8.29463933e+01 7.23570813e+01 1.15219320e+02 6.02232339e+01\n",
      " 1.55580827e+02 1.02940110e+02 9.76058906e+01 5.15404251e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53349495e+01 4.05481659e+01\n",
      " 1.00568525e+02 5.56079366e+01 8.13800192e+00]\n",
      "32-th iteration, loss: 0.18380290875807667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.349857559560077e-06\n",
      "32-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708741e+00 6.22418398e+01 1.38439383e+02 3.20796023e+01\n",
      " 8.29463978e+01 7.23570761e+01 1.15219322e+02 6.02232352e+01\n",
      " 1.55580829e+02 1.02940110e+02 9.76058927e+01 5.15404196e+01\n",
      " 0.00000000e+00 1.95399252e-14 3.53349603e+01 4.05481646e+01\n",
      " 1.00568523e+02 5.56079381e+01 8.13800192e+00]\n",
      "33-th iteration, loss: 0.18380290875607747, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.304410710667176e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708764e+00 6.22418360e+01 1.38439386e+02 3.20796021e+01\n",
      " 8.29464023e+01 7.23570710e+01 1.15219324e+02 6.02232366e+01\n",
      " 1.55580832e+02 1.02940110e+02 9.76058947e+01 5.15404140e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53349711e+01 4.05481633e+01\n",
      " 1.00568520e+02 5.56079397e+01 8.13800192e+00]\n",
      "34-th iteration, loss: 0.1838029087540911, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.260905692767963e-06\n",
      "34-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708788  62.24183219 138.43938944  32.07960195  82.94640691\n",
      "  72.35706592 115.2193256   60.22323803 155.58083396 102.94011003\n",
      "  97.60589675  51.54040843  35.33498169  40.54816188 100.56851798\n",
      "  55.60794124   8.13800192]\n",
      "35-th iteration, loss: 0.18380290875241728, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.281771255232537e-06\n",
      "35-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708813  62.2418284  138.43939262  32.07960177  82.94641148\n",
      "  72.35706089 115.21932738  60.22323943 155.58083624 102.9401099\n",
      "  97.60589878  51.54040285  35.33498696  40.54816046 100.56851571\n",
      "  55.60794283   8.13800192]\n",
      "36-th iteration, loss: 0.18380290875075295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.301325313776052e-06\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708837e+00 6.22418246e+01 1.38439396e+02 3.20796016e+01\n",
      " 8.29464161e+01 7.23570559e+01 1.15219329e+02 6.02232409e+01\n",
      " 1.55580839e+02 1.02940110e+02 9.76059008e+01 5.15403973e+01\n",
      " 0.00000000e+00 1.95399252e-14 3.53349922e+01 4.05481591e+01\n",
      " 1.00568513e+02 5.56079444e+01 8.13800192e+00]\n",
      "37-th iteration, loss: 0.18380290874879293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.256704580088019e-06\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708863  62.24182089 138.43939897  32.07960143  82.94642066\n",
      "  72.35705095 115.21933102  60.22324234 155.58084079 102.94010965\n",
      "  97.60590291  51.5403919   35.33500287  40.54815773 100.5685112\n",
      "  55.60794604   8.13800192]\n",
      "38-th iteration, loss: 0.18380290874714428, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.2764028454525714e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708889e+00 6.22418172e+01 1.38439402e+02 3.20796013e+01\n",
      " 8.29464253e+01 7.23570460e+01 1.15219333e+02 6.02232438e+01\n",
      " 1.55580843e+02 1.02940109e+02 9.76059050e+01 5.15403864e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53350081e+01 4.05481563e+01\n",
      " 1.00568509e+02 5.56079477e+01 8.13800192e+00]\n",
      "39-th iteration, loss: 0.18380290874520233, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.232235903732118e-06\n",
      "39-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708916  62.24181348 138.43940536  32.07960111  82.94642987\n",
      "  72.35704116 115.21933477  60.22324533 155.58084531 102.94010933\n",
      "  97.60590707  51.54038107  35.3350187   40.54815497 100.56850673\n",
      "  55.60794929   8.13800192]\n",
      "40-th iteration, loss: 0.18380290874356844, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.252050453850641e-06\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00708943  62.24180981 138.43940856  32.07960096  82.94643449\n",
      "  72.35703631 115.21933668  60.22324683 155.58084755 102.94010913\n",
      "  97.60590915  51.54037567  35.33502394  40.54815357 100.5685045\n",
      "  55.60795093   8.13800192]\n",
      "41-th iteration, loss: 0.18380290874194286, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.27060948988278e-06\n",
      "41-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708970e+00 6.22418062e+01 1.38439412e+02 3.20796008e+01\n",
      " 8.29464391e+01 7.23570315e+01 1.15219339e+02 6.02232484e+01\n",
      " 1.55580850e+02 1.02940109e+02 9.76059112e+01 5.15403703e+01\n",
      " 0.00000000e+00 1.95399252e-14 3.53350292e+01 4.05481522e+01\n",
      " 1.00568502e+02 5.56079526e+01 8.13800192e+00]\n",
      "42-th iteration, loss: 0.18380290874002403, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.225493141017938e-06\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[3.00708998e+00 6.22418025e+01 1.38439415e+02 3.20796007e+01\n",
      " 8.29464437e+01 7.23570267e+01 1.15219341e+02 6.02232499e+01\n",
      " 1.55580852e+02 1.02940109e+02 9.76059134e+01 5.15403651e+01\n",
      " 0.00000000e+00 1.95399252e-14 3.53350398e+01 4.05481509e+01\n",
      " 1.00568500e+02 5.56079542e+01 8.13800192e+00]\n",
      "43-th iteration, loss: 0.18380290873811583, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.182303224695641e-06\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00709027  62.24179891 138.43941818  32.07960051  82.94644837\n",
      "  72.35702196 115.21934258  60.22325147 155.58085426 102.94010847\n",
      "  97.60591548  51.54035981  35.33505022  40.5481495  100.56849789\n",
      "  55.60795592   8.13800192]\n",
      "44-th iteration, loss: 0.18380290873650937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.202413091904681e-06\n",
      "44-th iteration, new layer inserted. now 19 layers\n",
      "[3.00709056e+00 6.22417953e+01 1.38439421e+02 3.20796004e+01\n",
      " 8.29464530e+01 7.23570172e+01 1.15219345e+02 6.02232530e+01\n",
      " 1.55580856e+02 1.02940108e+02 9.76059176e+01 5.15403545e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53350554e+01 4.05481481e+01\n",
      " 1.00568496e+02 5.56079576e+01 8.13800192e+00]\n",
      "45-th iteration, loss: 0.183802908734617, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.159565655594801e-06\n",
      "45-th iteration, new layer inserted. now 19 layers\n",
      "[3.00709085e+00 6.22417917e+01 1.38439425e+02 3.20796002e+01\n",
      " 8.29464577e+01 7.23570125e+01 1.15219347e+02 6.02232546e+01\n",
      " 1.55580859e+02 1.02940108e+02 9.76059197e+01 5.15403493e+01\n",
      " 0.00000000e+00 1.15463195e-14 3.53350658e+01 4.05481467e+01\n",
      " 1.00568494e+02 5.56079593e+01 8.13800192e+00]\n",
      "46-th iteration, loss: 0.18380290873273455, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.118510116541803e-06\n",
      "46-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00709115  62.24178818 138.43942786  32.07960009  82.94646232\n",
      "  72.35700787 115.2193487   60.22325618 155.58086087 102.94010761\n",
      "  97.60592184  51.54034411  35.33507616  40.54814529 100.56849134\n",
      "  55.60796103   8.13800192]\n",
      "47-th iteration, loss: 0.18380290873114613, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.139878857388755e-06\n",
      "47-th iteration, new layer inserted. now 19 layers\n",
      "[3.00709145e+00 6.22417846e+01 1.38439431e+02 3.20796000e+01\n",
      " 8.29464670e+01 7.23570032e+01 1.15219351e+02 6.02232577e+01\n",
      " 1.55580863e+02 1.02940107e+02 9.76059239e+01 5.15403389e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53350813e+01 4.05481438e+01\n",
      " 1.00568489e+02 5.56079628e+01 8.13800192e+00]\n",
      "48-th iteration, loss: 0.1838029087292785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.0989614134763784e-06\n",
      "48-th iteration, new layer inserted. now 19 layers\n",
      "[3.00709176e+00 6.22417811e+01 1.38439434e+02 3.20795998e+01\n",
      " 8.29464716e+01 7.23569986e+01 1.15219353e+02 6.02232593e+01\n",
      " 1.55580865e+02 1.02940107e+02 9.76059261e+01 5.15403337e+01\n",
      " 0.00000000e+00 6.21724894e-15 3.53350916e+01 4.05481424e+01\n",
      " 1.00568487e+02 5.56079645e+01 8.13800192e+00]\n",
      "49-th iteration, loss: 0.18380290872742017, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.0597239230362e-06\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[3.00709207e+00 6.22417776e+01 1.38439438e+02 3.20795997e+01\n",
      " 8.29464763e+01 7.23569940e+01 1.15219355e+02 6.02232609e+01\n",
      " 1.55580867e+02 1.02940107e+02 9.76059282e+01 5.15403286e+01\n",
      " 0.00000000e+00 1.33226763e-14 3.53351018e+01 4.05481410e+01\n",
      " 1.00568485e+02 5.56079663e+01 8.13800192e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010542\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.4077617     0.         1085.24550463]\n",
      "1-th iteration, loss: 0.7509319290812663, 11 gd steps\n",
      "insert gradient: -0.6359837605807965\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 238.22462297   0.         847.02088166]\n",
      "2-th iteration, loss: 0.6048993495568986, 13 gd steps\n",
      "insert gradient: -0.7048760824169851\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  2.83737358  76.77649234 222.66345461  40.66688903 242.00596619\n",
      "   0.         605.01491547]\n",
      "3-th iteration, loss: 0.4632257787394185, 26 gd steps\n",
      "insert gradient: -0.7402955549945717\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.63227794 228.81540659  46.21690719 158.54725272\n",
      "  53.80694641 363.00894928   0.         242.00596619]\n",
      "4-th iteration, loss: 0.38221674101203595, 13 gd steps\n",
      "insert gradient: -0.38894006413275084\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.58916106  61.08569753 219.45914128  63.26181164 144.49737965\n",
      "  43.92495199 340.51193777  50.05630155 206.15323046   0.\n",
      "  35.85273573]\n",
      "5-th iteration, loss: 0.3116027425376996, 15 gd steps\n",
      "insert gradient: -0.44707162051825305\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  3.33547702  58.77358175 229.54981606  60.8164209  107.88272381\n",
      "  64.04409738 224.03068395   0.         104.54765251  43.77900518\n",
      " 180.52689466  49.62928219  35.85273573]\n",
      "6-th iteration, loss: 0.2613242974937568, 14 gd steps\n",
      "insert gradient: -0.30806623790949456\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          63.97479744 117.86985557   0.          94.29588446\n",
      "  69.95647986 112.86358227  64.06152425 164.87274763  48.51218557\n",
      "  72.55527405  33.21300995 181.91681135  68.90521004  35.85273573]\n",
      "7-th iteration, loss: 0.23376126296975552, 40 gd steps\n",
      "insert gradient: -0.07135907692648347\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.32688199  68.7164174  114.92560501  12.47217289  74.26145486\n",
      "  65.06165436 110.33754984  65.87248516 162.19415207  55.34717243\n",
      "  72.28005341  33.6019574  109.79977913   0.          85.39982821\n",
      "  50.48115187  35.85273573]\n",
      "8-th iteration, loss: 0.22733724155690732, 32 gd steps\n",
      "insert gradient: -0.00407672855636827\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  0.62972448  67.38327475 119.38486133  12.20221006  77.74544303\n",
      "  63.37044171 113.87457837  68.30016951 158.20464804  58.8145633\n",
      "  81.95293413  34.95997939  74.73819458  19.21971187  61.41859439\n",
      "  56.78402659  35.85273573]\n",
      "9-th iteration, loss: 0.22608800145331512, 41 gd steps\n",
      "insert gradient: -0.043596829775909805\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[8.61212838e-01 6.68659351e+01 1.17377907e+02 9.53282202e+00\n",
      " 9.10322787e+01 6.42208418e+01 1.11873934e+02 0.00000000e+00\n",
      " 1.24344979e-14 7.05323854e+01 1.54498335e+02 6.36319494e+01\n",
      " 8.53861002e+01 3.43207430e+01 6.57639767e+01 2.47445676e+01\n",
      " 6.27158176e+01 5.47396768e+01 3.58527357e+01]\n",
      "10-th iteration, loss: 0.22123879540417082, 17 gd steps\n",
      "insert gradient: -0.05430825707644414\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[7.98367023e-01 6.78885307e+01 1.12905353e+02 1.26784670e+01\n",
      " 9.22099251e+01 6.78103913e+01 1.15294778e+02 7.17440887e+01\n",
      " 1.54654811e+02 6.87532026e+01 9.09074444e+01 3.59732095e+01\n",
      " 6.06187233e+01 0.00000000e+00 2.13162821e-14 3.05947565e+01\n",
      " 5.85175173e+01 5.31876117e+01 3.58527357e+01]\n",
      "11-th iteration, loss: 0.21538125399445354, 17 gd steps\n",
      "insert gradient: -0.12923143110990867\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[0.00000000e+00 6.87312929e+01 1.13874967e+02 0.00000000e+00\n",
      " 2.48689958e-14 1.68486122e+01 9.11730396e+01 6.52427476e+01\n",
      " 1.18419334e+02 7.65299871e+01 1.55205704e+02 6.89161128e+01\n",
      " 1.00414594e+02 3.73598330e+01 4.53383271e+01 4.31363495e+01\n",
      " 6.46766107e+01 4.98651728e+01 3.58527357e+01]\n",
      "12-th iteration, loss: 0.20476621986943278, 18 gd steps\n",
      "insert gradient: -0.09396582476488585\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[6.98598149e-01 6.99936326e+01 1.16477049e+02 2.05391549e+00\n",
      " 6.00228882e-01 0.00000000e+00 1.38777878e-17 1.93721776e+01\n",
      " 9.06596031e+01 6.60170698e+01 1.16578401e+02 7.58071163e+01\n",
      " 1.57447518e+02 7.51003861e+01 1.01221661e+02 4.05656182e+01\n",
      " 2.79656716e+01 4.92009532e+01 8.49143509e+01 4.56322586e+01\n",
      " 3.58527357e+01]\n",
      "13-th iteration, loss: 0.19309929285659985, 18 gd steps\n",
      "insert gradient: -0.050349248980551244\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[1.57512323e+00 6.96211515e+01 1.21738499e+02 2.82398793e+00\n",
      " 2.20201717e+00 1.83967835e+00 6.32509667e-01 2.13954290e+01\n",
      " 8.90829177e+01 6.59320812e+01 1.11996305e+02 6.91629124e+01\n",
      " 1.57591773e+02 0.00000000e+00 1.77635684e-14 8.65249376e+01\n",
      " 1.03619787e+02 4.75196811e+01 2.76426560e+01 4.65411425e+01\n",
      " 9.10894137e+01 4.44139229e+01 3.58527357e+01]\n",
      "14-th iteration, loss: 0.18451239842591993, 97 gd steps\n",
      "insert gradient: -0.005787185032213287\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[  2.03206338  67.06057771 127.17280746   2.20367604   0.93713344\n",
      "  31.4077461   83.09349862  69.33128333 118.30551364  60.728321\n",
      " 156.52665165 100.28430884  98.94339136  50.27871592  33.41697271\n",
      "  42.79890004  97.52215952  54.66217874  35.85273573]\n",
      "15-th iteration, loss: 0.18400515495858213, 22 gd steps\n",
      "insert gradient: -0.004569589084853953\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[2.56455195e+00 6.56508561e+01 1.31886457e+02 3.47232305e-01\n",
      " 2.51088376e-01 3.22994045e+01 8.27893544e+01 7.18256129e+01\n",
      " 1.14356397e+02 6.10018096e+01 1.55115443e+02 1.02092428e+02\n",
      " 9.76311030e+01 5.09209127e+01 3.53971861e+01 0.00000000e+00\n",
      " 6.21724894e-15 4.03904269e+01 1.00486064e+02 5.52511147e+01\n",
      " 3.58527357e+01]\n",
      "16-th iteration, loss: 0.18384698096444868, 24 gd steps\n",
      "insert gradient: -0.0038016436213373986\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[2.51742513e+00 6.38722679e+01 1.35415086e+02 3.27471906e+01\n",
      " 8.23544645e+01 0.00000000e+00 1.24344979e-14 7.22857110e+01\n",
      " 1.14826671e+02 6.02797324e+01 1.55521059e+02 1.02567002e+02\n",
      " 9.78180982e+01 5.12820632e+01 3.47084096e+01 4.11523117e+01\n",
      " 9.98449180e+01 5.56761101e+01 3.58527357e+01]\n",
      "17-th iteration, loss: 0.18381457532292017, 28 gd steps\n",
      "insert gradient: -0.0009434717180111637\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[2.85039800e+00 6.30892211e+01 1.36808545e+02 3.24084316e+01\n",
      " 8.24994850e+01 7.25596391e+01 1.14719150e+02 6.03428171e+01\n",
      " 1.55336978e+02 0.00000000e+00 4.61852778e-14 1.02786448e+02\n",
      " 9.76572234e+01 5.14579644e+01 3.50582864e+01 4.06553430e+01\n",
      " 1.00712967e+02 5.54706889e+01 3.58527357e+01]\n",
      "18-th iteration, loss: 0.1838062009205325, 28 gd steps\n",
      "insert gradient: -0.0011882694701718774\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[2.89281284e+00 6.26443090e+01 1.37647410e+02 3.22717862e+01\n",
      " 8.26297657e+01 7.25024966e+01 1.14959050e+02 6.02600962e+01\n",
      " 1.55490720e+02 0.00000000e+00 4.61852778e-14 1.02827202e+02\n",
      " 9.76528135e+01 5.14910238e+01 3.51473053e+01 4.06858052e+01\n",
      " 1.00605113e+02 5.55267798e+01 3.58527357e+01]\n",
      "19-th iteration, loss: 0.18380322535251403, 454 gd steps\n",
      "insert gradient: -0.0001185734801152074\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[2.94891463e+00 6.23714126e+01 0.00000000e+00 2.13162821e-14\n",
      " 1.38186204e+02 3.21661406e+01 8.28126079e+01 7.24413272e+01\n",
      " 1.15094948e+02 6.02324948e+01 1.55522999e+02 1.02950048e+02\n",
      " 9.75935887e+01 5.15388539e+01 3.52480885e+01 4.06014122e+01\n",
      " 1.00573836e+02 5.56017852e+01 3.58527357e+01]\n",
      "20-th iteration, loss: 0.18380293970973613, 206 gd steps\n",
      "insert gradient: -2.28691540933237e-05\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[  2.99861517  62.26805019 138.39097654  32.10033109  82.90427783\n",
      "  72.39400314 115.16253407  60.22780987 155.55811455 102.94777626\n",
      "  97.60270248  51.54360119  35.30082522  40.56750041 100.5742031\n",
      "  55.60376072  35.85273573]\n",
      "21-th iteration, loss: 0.1838029130798351, 22 gd steps\n",
      "insert gradient: -5.3120955034783623e-05\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[3.00900432e+00 6.22499705e+01 1.38423122e+02 3.20796645e+01\n",
      " 8.29474033e+01 7.23576559e+01 1.15212123e+02 6.02267426e+01\n",
      " 1.55578433e+02 1.02938804e+02 9.76058972e+01 5.15385341e+01\n",
      " 3.53304653e+01 0.00000000e+00 9.76996262e-15 4.05500046e+01\n",
      " 1.00565122e+02 5.56069111e+01 3.58527357e+01]\n",
      "22-th iteration, loss: 0.1838029077109719, 82 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.046657593162738e-06\n",
      "22-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819576  62.24220294 138.43782811  32.07798978  82.95252616\n",
      "  72.35140846 115.22593741  60.22340222 155.58391496 102.93775764\n",
      "  97.60789876  51.53717704  35.34178852  40.54578761 100.5660861\n",
      "  55.60991875  35.85273573]\n",
      "23-th iteration, loss: 0.18380290771033594, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.042341898312738e-06\n",
      "23-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819597e+00 6.22422003e+01 0.00000000e+00 2.84217094e-14\n",
      " 1.38437833e+02 3.20779882e+01 8.29525288e+01 7.23514065e+01\n",
      " 1.15225941e+02 6.02234018e+01 1.55583916e+02 1.02937758e+02\n",
      " 9.76078989e+01 5.15371771e+01 3.53417890e+01 4.05457864e+01\n",
      " 1.00566086e+02 5.56099188e+01 3.58527357e+01]\n",
      "24-th iteration, loss: 0.18380290770942662, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.015712140773422e-06\n",
      "24-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819617  62.24219773 138.43784319  32.07798663  82.95253132\n",
      "  72.35140459 115.22594393  60.22340133 155.58391651 102.93775758\n",
      "  97.60789897  51.53717723  35.3417895   40.54578513 100.56608539\n",
      "  55.60991881  35.85273573]\n",
      "25-th iteration, loss: 0.18380290770879318, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.0125102372132774e-06\n",
      "25-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819637  62.24219511 138.4378482   32.07798502  82.95253389\n",
      "  72.35140264 115.22594718  60.22340091 155.5839173  102.93775757\n",
      "  97.60789909  51.53717736  35.34179002  40.54578394 100.56608504\n",
      "  55.60991885  35.85273573]\n",
      "26-th iteration, loss: 0.18380290770816074, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.009445354379708e-06\n",
      "26-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819657  62.24219249 138.43785321  32.07798341  82.95253645\n",
      "  72.3514007  115.22595043  60.22340048 155.58391809 102.93775757\n",
      "  97.60789922  51.53717751  35.34179056  40.54578276 100.5660847\n",
      "  55.60991889  35.85273573]\n",
      "27-th iteration, loss: 0.18380290770752916, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.006507551528416e-06\n",
      "27-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819677e+00 6.22421899e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.38437858e+02 3.20779818e+01 8.29525390e+01 7.23513987e+01\n",
      " 1.15225954e+02 6.02234001e+01 1.55583919e+02 1.02937758e+02\n",
      " 9.76078993e+01 5.15371777e+01 3.53417911e+01 4.05457816e+01\n",
      " 1.00566084e+02 5.56099189e+01 3.58527357e+01]\n",
      "28-th iteration, loss: 0.1838029077066278, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.981323911102092e-06\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819698  62.24218725 138.43786819  32.07798017  82.95254156\n",
      "  72.35139678 115.22595693  60.22339966 155.58391967 102.93775759\n",
      "  97.60789949  51.53717785  35.34179168  40.54578046 100.56608403\n",
      "  55.60991897  35.85273573]\n",
      "29-th iteration, loss: 0.18380290770599825, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.979335603587994e-06\n",
      "29-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819717e+00 6.22421846e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.38437873e+02 3.20779785e+01 8.29525441e+01 7.23513948e+01\n",
      " 1.15225960e+02 6.02233993e+01 1.55583920e+02 1.02937758e+02\n",
      " 9.76078996e+01 5.15371780e+01 3.53417923e+01 4.05457793e+01\n",
      " 1.00566084e+02 5.56099190e+01 3.58527357e+01]\n",
      "30-th iteration, loss: 0.18380290770510163, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.955159324228382e-06\n",
      "30-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819737e+00 6.22421820e+01 0.00000000e+00 1.95399252e-14\n",
      " 1.38437883e+02 3.20779768e+01 8.29525466e+01 7.23513928e+01\n",
      " 1.15225963e+02 6.02233989e+01 1.55583921e+02 1.02937758e+02\n",
      " 9.76078998e+01 5.15371783e+01 3.53417929e+01 4.05457782e+01\n",
      " 1.00566083e+02 5.56099191e+01 3.58527357e+01]\n",
      "31-th iteration, loss: 0.1838029077042084, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.9318583325565195e-06\n",
      "31-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819756  62.24217931 138.43789296  32.07797514  82.95254914\n",
      "  72.35139084 115.22596662  60.22339847 155.58392209 102.93775769\n",
      "  97.60789996  51.53717849  35.34179348  40.54577717 100.56608305\n",
      "  55.6099191   35.85273573]\n",
      "32-th iteration, loss: 0.1838029077035814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.931446788428892e-06\n",
      "32-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819775  62.24217664 138.43789789  32.07797343  82.95255164\n",
      "  72.35138884 115.22596985  60.22339808 155.58392291 102.93775775\n",
      "  97.60790013  51.53717874  35.34179411  40.54577611 100.56608274\n",
      "  55.60991914  35.85273573]\n",
      "33-th iteration, loss: 0.18380290770295515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.931014131803805e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819794e+00 6.22421740e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.38437903e+02 3.20779717e+01 8.29525541e+01 7.23513869e+01\n",
      " 1.15225973e+02 6.02233977e+01 1.55583924e+02 1.02937758e+02\n",
      " 9.76079003e+01 5.15371790e+01 3.53417947e+01 4.05457751e+01\n",
      " 1.00566082e+02 5.56099192e+01 3.58527357e+01]\n",
      "34-th iteration, loss: 0.1838029077020667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.908505648760131e-06\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819813e+00 6.22421713e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.38437913e+02 3.20779700e+01 8.29525566e+01 7.23513848e+01\n",
      " 1.15225976e+02 6.02233973e+01 1.55583925e+02 1.02937758e+02\n",
      " 9.76079005e+01 5.15371793e+01 3.53417954e+01 4.05457740e+01\n",
      " 1.00566082e+02 5.56099192e+01 3.58527357e+01]\n",
      "35-th iteration, loss: 0.1838029077011813, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.886778622014503e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819831e+00 6.22421686e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.38437922e+02 3.20779682e+01 8.29525591e+01 7.23513828e+01\n",
      " 1.15225979e+02 6.02233969e+01 1.55583925e+02 1.02937758e+02\n",
      " 9.76079007e+01 5.15371796e+01 3.53417961e+01 4.05457730e+01\n",
      " 1.00566082e+02 5.56099193e+01 3.58527357e+01]\n",
      "36-th iteration, loss: 0.1838029077002987, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.865791054371738e-06\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819849e+00 6.22421659e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.38437932e+02 3.20779664e+01 8.29525616e+01 7.23513808e+01\n",
      " 1.15225983e+02 6.02233966e+01 1.55583926e+02 1.02937758e+02\n",
      " 9.76079009e+01 5.15371799e+01 3.53417967e+01 4.05457720e+01\n",
      " 1.00566081e+02 5.56099193e+01 3.58527357e+01]\n",
      "37-th iteration, loss: 0.18380290769941884, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.845503486774217e-06\n",
      "37-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819867e+00 6.22421632e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.38437942e+02 3.20779646e+01 8.29525641e+01 7.23513788e+01\n",
      " 1.15225986e+02 6.02233962e+01 1.55583927e+02 1.02937758e+02\n",
      " 9.76079011e+01 5.15371802e+01 3.53417974e+01 4.05457710e+01\n",
      " 1.00566081e+02 5.56099194e+01 3.58527357e+01]\n",
      "38-th iteration, loss: 0.18380290769854157, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.82587884120631e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819884e+00 6.22421604e+01 0.00000000e+00 1.42108547e-14\n",
      " 1.38437952e+02 3.20779628e+01 8.29525665e+01 7.23513767e+01\n",
      " 1.15225989e+02 6.02233958e+01 1.55583928e+02 1.02937758e+02\n",
      " 9.76079013e+01 5.15371805e+01 3.53417981e+01 4.05457700e+01\n",
      " 1.00566081e+02 5.56099194e+01 3.58527357e+01]\n",
      "39-th iteration, loss: 0.1838029076976668, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.806882265905952e-06\n",
      "39-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819901  62.24215764 138.43796118  32.07796093  82.95256893\n",
      "  72.35137467 115.22599227  60.22339548 155.58392876 102.9377584\n",
      "  97.60790148  51.53718083  35.34179885  40.54576907 100.56608061\n",
      "  55.60991946  35.85273573]\n",
      "40-th iteration, loss: 0.18380290769704472, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.81003568808831e-06\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819918  62.24215487 138.43796599  32.07795906  82.95257137\n",
      "  72.35137262 115.22599545  60.22339512 155.58392962 102.93775853\n",
      "  97.6079017   51.53718118  35.34179958  40.54576812 100.56608031\n",
      "  55.6099195   35.85273573]\n",
      "41-th iteration, loss: 0.18380290769642332, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.812981344740077e-06\n",
      "41-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819934  62.2421521  138.4379708   32.0779572   82.95257381\n",
      "  72.35137058 115.22599864  60.22339476 155.58393048 102.93775867\n",
      "  97.60790191  51.53718154  35.34180031  40.54576717 100.56608002\n",
      "  55.60991955  35.85273573]\n",
      "42-th iteration, loss: 0.1838029076958027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.81572970190753e-06\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[3.00819951e+00 6.22421493e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.38437976e+02 3.20779554e+01 8.29525763e+01 7.23513685e+01\n",
      " 1.15226002e+02 6.02233944e+01 1.55583931e+02 1.02937759e+02\n",
      " 9.76079021e+01 5.15371819e+01 3.53418010e+01 4.05457662e+01\n",
      " 1.00566080e+02 5.56099196e+01 3.58527357e+01]\n",
      "43-th iteration, loss: 0.18380290769493168, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.7967074339360505e-06\n",
      "43-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819969  62.24214658 138.43798521  32.07795351  82.95257869\n",
      "  72.35136651 115.22600501  60.22339406 155.58393219 102.93775892\n",
      "  97.60790234  51.53718224  35.34180179  40.54576529 100.56607943\n",
      "  55.60991963  35.85273573]\n",
      "44-th iteration, loss: 0.1838029076943122, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.79978871291343e-06\n",
      "44-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00819985  62.24214381 138.43799001  32.07795165  82.95258113\n",
      "  72.35136447 115.22600819  60.22339372 155.58393306 102.93775906\n",
      "  97.60790256  51.53718261  35.34180254  40.54576436 100.56607913\n",
      "  55.60991967  35.85273573]\n",
      "45-th iteration, loss: 0.1838029076936935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.80266532395364e-06\n",
      "45-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00820002  62.24214105 138.43799481  32.07794981  82.95258357\n",
      "  72.35136244 115.22601137  60.22339337 155.58393392 102.93775919\n",
      "  97.60790278  51.53718297  35.34180329  40.54576344 100.56607884\n",
      "  55.6099197   35.85273573]\n",
      "46-th iteration, loss: 0.18380290769307545, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.805347456653588e-06\n",
      "46-th iteration, new layer inserted. now 19 layers\n",
      "[3.00820020e+00 6.22421383e+01 0.00000000e+00 8.88178420e-15\n",
      " 1.38438000e+02 3.20779480e+01 8.29525860e+01 7.23513604e+01\n",
      " 1.15226015e+02 6.02233930e+01 1.55583935e+02 1.02937759e+02\n",
      " 9.76079030e+01 5.15371833e+01 3.53418041e+01 4.05457625e+01\n",
      " 1.00566079e+02 5.56099197e+01 3.58527357e+01]\n",
      "47-th iteration, loss: 0.18380290769220814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.786309937696896e-06\n",
      "47-th iteration, new layer inserted. now 17 layers\n",
      "[  3.00820037  62.24213556 138.43800919  32.07794613  82.95258846\n",
      "  72.3513584  115.22601774  60.22339269 155.58393564 102.93775945\n",
      "  97.60790321  51.5371837   35.34180482  40.54576159 100.56607824\n",
      "  55.60991976  35.85273573]\n",
      "48-th iteration, loss: 0.18380290769159124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.789328359243339e-06\n",
      "48-th iteration, new layer inserted. now 19 layers\n",
      "[3.00820054e+00 6.22421328e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.38438014e+02 3.20779443e+01 8.29525909e+01 7.23513564e+01\n",
      " 1.15226021e+02 6.02233924e+01 1.55583936e+02 1.02937760e+02\n",
      " 9.76079034e+01 5.15371841e+01 3.53418056e+01 4.05457607e+01\n",
      " 1.00566078e+02 5.56099198e+01 3.58527357e+01]\n",
      "49-th iteration, loss: 0.18380290769072669, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.7706760998670735e-06\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[3.00820071e+00 6.22421300e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.38438024e+02 3.20779424e+01 8.29525933e+01 7.23513543e+01\n",
      " 1.15226024e+02 6.02233920e+01 1.55583937e+02 1.02937760e+02\n",
      " 9.76079036e+01 5.15371845e+01 3.53418064e+01 4.05457598e+01\n",
      " 1.00566078e+02 5.56099198e+01 3.58527357e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.530046482060192\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.41682307    0.         1109.3620714 ]\n",
      "1-th iteration, loss: 0.7526539900984642, 11 gd steps\n",
      "insert gradient: -0.6550744619673012\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 44.35070037  62.11407022 121.75925174   0.         987.60281966]\n",
      "2-th iteration, loss: 0.5102011032548628, 36 gd steps\n",
      "insert gradient: -0.6604019490515255\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.7576399   56.28121607  96.53715174 100.34849065 221.70675543\n",
      "   0.         765.89606423]\n",
      "3-th iteration, loss: 0.4361613738835203, 13 gd steps\n",
      "insert gradient: -0.669297572980841\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.73438348  65.68091703  96.72868772  90.14420253 110.3418193\n",
      "   0.          92.91942678  49.59822957 765.89606423]\n",
      "4-th iteration, loss: 0.3429470176849258, 32 gd steps\n",
      "insert gradient: -0.34347864020236146\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.62225259  79.36385837 127.02672941  76.00865531  94.44401319\n",
      "  44.2962045   65.96085263  48.6626052  510.59737615   0.\n",
      " 255.29868808]\n",
      "5-th iteration, loss: 0.289787631242014, 54 gd steps\n",
      "insert gradient: -0.16822896427863757\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.68548576  68.38964831 135.64816259  79.2510114  104.29159939\n",
      "  46.39011368  61.57669213  43.63493409 128.7838161    0.\n",
      " 343.42350961  53.47602569 255.29868808]\n",
      "6-th iteration, loss: 0.23655755356504865, 28 gd steps\n",
      "insert gradient: -0.19383113027283547\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  5.43161368  67.03372348 123.66711949  84.05648457 123.11788927\n",
      "  56.48665283  68.27828748  34.13577137  91.14426508  49.13913132\n",
      " 221.86845005   0.          85.33401925  46.8041036  255.29868808]\n",
      "7-th iteration, loss: 0.18869222573809502, 323 gd steps\n",
      "insert gradient: -0.09480872019724824\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.53889928  69.88320474 133.35310145  96.9497081  115.36976244\n",
      "  66.33347855 100.00699779  40.75069162  77.06088042  49.74898353\n",
      " 187.75150047  50.11141721  59.93914668  40.99094965 175.51784805\n",
      "   0.          79.78084002]\n",
      "8-th iteration, loss: 0.16598134261768058, 95 gd steps\n",
      "insert gradient: -0.018080742622983398\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  1.27048383  68.5358517  123.79862878 101.72602191 112.25310241\n",
      "  70.63207963 102.24633208  44.07530962  66.55576128  46.03318854\n",
      " 120.38318983   0.          66.87954991  52.33526656  95.25362375\n",
      "  21.18811469 123.87143534  50.89792109  79.78084002]\n",
      "9-th iteration, loss: 0.1633859215468133, 18 gd steps\n",
      "insert gradient: -0.026924159192300127\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[2.24240511e+00 6.70286131e+01 1.28213820e+02 9.86302543e+01\n",
      " 1.13269794e+02 7.20204092e+01 1.03145984e+02 4.22413602e+01\n",
      " 7.23851169e+01 4.64583375e+01 1.02250710e+02 0.00000000e+00\n",
      " 4.26325641e-14 1.09491459e+01 5.12920227e+01 5.45059845e+01\n",
      " 1.01872819e+02 1.71987904e+01 1.21671366e+02 5.65898900e+01\n",
      " 7.97808400e+01]\n",
      "10-th iteration, loss: 0.1617843903395746, 38 gd steps\n",
      "insert gradient: -0.010215985717619763\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  1.91549139  66.13799128 132.03154253  96.47724279 113.56581726\n",
      "  72.79695911 104.68329322  42.50926049  75.02016944  44.72031897\n",
      "  99.43791865  21.26859324  35.27220781  54.84058161 101.77768367\n",
      "  13.38079563 123.85265992  60.86555062  79.78084002]\n",
      "11-th iteration, loss: 0.1574247850595268, 82 gd steps\n",
      "insert gradient: -0.05214363269752706\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[3.42784572e+00 6.76774278e+01 1.36643148e+02 9.20877589e+01\n",
      " 1.22247200e+02 6.76181627e+01 1.11583661e+02 0.00000000e+00\n",
      " 1.42108547e-14 4.43731824e+01 7.46339065e+01 4.52845305e+01\n",
      " 9.63622725e+01 3.31243448e+01 2.57685496e+01 5.23393372e+01\n",
      " 8.73149901e+01 2.30627330e+01 9.07066042e+01 8.02152956e+01\n",
      " 7.97808400e+01]\n",
      "12-th iteration, loss: 0.14265969673742707, 35 gd steps\n",
      "insert gradient: -0.008946249118489413\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[2.01085733e+00 7.33419849e+01 1.27902326e+02 9.70774347e+01\n",
      " 1.35023225e+02 6.57708164e+01 1.14084446e+02 5.90079168e+01\n",
      " 7.18513911e+01 4.63986248e+01 9.28199134e+01 5.16019691e+01\n",
      " 0.00000000e+00 1.42108547e-14 1.83569534e+01 4.63391097e+01\n",
      " 8.86138298e+01 3.53254216e+01 6.33959825e+01 8.41948752e+01\n",
      " 7.97808400e+01]\n",
      "13-th iteration, loss: 0.14154001449089706, 91 gd steps\n",
      "insert gradient: -0.0006007902570184534\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[  1.90393775  73.65501644 130.13528965  99.0708145  134.43246228\n",
      "  66.72482429 114.60692684  60.96903779  75.74811143  45.26541331\n",
      "  92.12463959  51.17180198  25.00736509  41.45007634  91.69722378\n",
      "  42.32811249  50.59451136  86.30282292  79.78084002]\n",
      "14-th iteration, loss: 0.14152952655939235, 26 gd steps\n",
      "insert gradient: -0.0008451879350848784\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[  1.86057295  73.65656897 130.42212411  99.12963745 134.28043016\n",
      "  66.83793315 114.2387448   61.40724889  75.78981282  45.16785808\n",
      "  92.07888512  51.1903231   25.63230074  40.92565492  91.83021119\n",
      "  42.77378726  49.93084415  86.17863572  79.78084002]\n",
      "15-th iteration, loss: 0.14152810086261314, 31 gd steps\n",
      "insert gradient: -0.00014963910569621187\n",
      "15-th iteration, new layer inserted. now 19 layers\n",
      "[  1.86678927  73.65262842 130.48549732  99.18726598 134.27048116\n",
      "  66.87337172 114.20277257  61.52724547  75.90695764  45.13636657\n",
      "  92.06457998  51.16766835  25.80944551  40.82609988  91.90934977\n",
      "  42.8850924   49.79421935  86.12475827  79.78084002]\n",
      "16-th iteration, loss: 0.14152793713865752, 51 gd steps\n",
      "insert gradient: -2.8934245381786387e-05\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87241172  73.6519368  130.54261195  99.1971824  134.25458629\n",
      "  66.88031403 114.19014253  61.57288646  75.94028134  45.1320462\n",
      "  92.06709978  51.15088177  25.92956823  40.74341181  91.94251863\n",
      "  42.93958039  49.71238037  86.11260982  79.78084002]\n",
      "17-th iteration, loss: 0.14152793330546326, 30 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.391717618172409e-06\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[1.87228863e+00 7.36496858e+01 1.30547231e+02 9.91996995e+01\n",
      " 1.34253455e+02 6.68846108e+01 1.14187718e+02 0.00000000e+00\n",
      " 3.55271368e-14 6.15792648e+01 7.59439823e+01 4.51299457e+01\n",
      " 9.20667174e+01 5.11496562e+01 2.59406142e+01 4.07366661e+01\n",
      " 9.19464295e+01 4.29460468e+01 4.97045888e+01 8.61105129e+01\n",
      " 7.97808400e+01]\n",
      "18-th iteration, loss: 0.1415279333018031, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.297998909878125e-06\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87228941  73.64968837 130.54723142  99.19969861 134.25345395\n",
      "  66.88460672 114.18771727  61.57927948  75.94398493  45.12994218\n",
      "  92.0667136   51.14965287  25.94061902  40.73666701  91.94643069\n",
      "  42.94605263  49.70459163  86.11051652  79.78084002]\n",
      "19-th iteration, loss: 0.14152793329896002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.269170274532613e-06\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[  1.8722902   73.64969098 130.54723198  99.19969788 134.25345277\n",
      "  66.88460277 114.18771693  61.57928675  75.94398756  45.12993871\n",
      "  92.06670983  51.14964963  25.94062386  40.7366679   91.94643189\n",
      "  42.94605838  49.70459444  86.11052012  79.78084002]\n",
      "20-th iteration, loss: 0.14152793329617058, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.24019150179811e-06\n",
      "20-th iteration, new layer inserted. now 19 layers\n",
      "[  1.872291    73.64969364 130.5472326   99.19969724 134.25345164\n",
      "  66.88459892 114.1877166   61.57929398  75.94399019  45.12993529\n",
      "  92.06670611  51.14964642  25.94062865  40.73666878  91.94643308\n",
      "  42.94606407  49.70459719  86.11052367  79.78084002]\n",
      "21-th iteration, loss: 0.14152793329343247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.211089733896047e-06\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229180e+00 7.36496963e+01 1.30547233e+02 9.91996967e+01\n",
      " 1.34253451e+02 6.68845951e+01 1.14187716e+02 0.00000000e+00\n",
      " 4.26325641e-14 6.15793012e+01 7.59439928e+01 4.51299319e+01\n",
      " 9.20667024e+01 5.11496432e+01 2.59406334e+01 4.07366696e+01\n",
      " 9.19464343e+01 4.29460697e+01 4.97045999e+01 8.61105271e+01\n",
      " 7.97808400e+01]\n",
      "22-th iteration, loss: 0.14152793329002564, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.119872350619002e-06\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229261e+00 7.36496991e+01 1.30547234e+02 9.91996962e+01\n",
      " 1.34253450e+02 6.68845914e+01 1.14187716e+02 0.00000000e+00\n",
      " 4.26325641e-14 6.15793155e+01 7.59439954e+01 4.51299286e+01\n",
      " 9.20666988e+01 5.11496401e+01 2.59406381e+01 4.07366705e+01\n",
      " 9.19464354e+01 4.29460752e+01 4.97046025e+01 8.61105306e+01\n",
      " 7.97808400e+01]\n",
      "23-th iteration, loss: 0.1415279332866898, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.030759042876016e-06\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229343e+00 7.36497018e+01 1.30547235e+02 9.91996959e+01\n",
      " 1.34253449e+02 6.68845878e+01 1.14187716e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.15793296e+01 7.59439980e+01 4.51299252e+01\n",
      " 9.20666952e+01 5.11496369e+01 2.59406429e+01 4.07366713e+01\n",
      " 9.19464366e+01 4.29460807e+01 4.97046051e+01 8.61105339e+01\n",
      " 7.97808400e+01]\n",
      "24-th iteration, loss: 0.14152793328342214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.9436921394372575e-06\n",
      "24-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87229426  73.6497046  130.54723546  99.19969565 134.25344756\n",
      "  66.88458419 114.1877153   61.57934347  75.94400051  45.12992194\n",
      "  92.06669157  51.14963387  25.94064759  40.73667217  91.94643775\n",
      "  42.94608619  49.70460769  86.11053721  79.78084002]\n",
      "25-th iteration, loss: 0.14152793328088545, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.918326686221444e-06\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229510e+00 7.36497074e+01 1.30547236e+02 9.91996955e+01\n",
      " 1.34253447e+02 6.68845807e+01 1.14187715e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.15793504e+01 7.59440030e+01 4.51299187e+01\n",
      " 9.20666880e+01 5.11496308e+01 2.59406523e+01 4.07366730e+01\n",
      " 9.19464389e+01 4.29460916e+01 4.97046102e+01 8.61105404e+01\n",
      " 7.97808400e+01]\n",
      "26-th iteration, loss: 0.1415279332777299, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.8333361704108576e-06\n",
      "26-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87229594  73.6497103  130.54723715  99.19969544 134.2534458\n",
      "  66.88457721 114.18771459  61.57936407  75.94400552  45.12991542\n",
      "  92.06668452  51.14962785  25.940657    40.73667388  91.94644008\n",
      "  42.94609694  49.70461266  86.11054361  79.78084002]\n",
      "27-th iteration, loss: 0.14152793327527993, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.809051107523873e-06\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229679e+00 7.36497132e+01 1.30547238e+02 9.91996955e+01\n",
      " 1.34253445e+02 6.68845738e+01 1.14187714e+02 0.00000000e+00\n",
      " 4.26325641e-14 6.15793709e+01 7.59440080e+01 4.51299122e+01\n",
      " 9.20666811e+01 5.11496249e+01 2.59406617e+01 4.07366747e+01\n",
      " 9.19464412e+01 4.29461022e+01 4.97046151e+01 8.61105467e+01\n",
      " 7.97808400e+01]\n",
      "28-th iteration, loss: 0.14152793327222868, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.72608892171207e-06\n",
      "28-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87229764  73.64971611 130.54723897  99.19969558 134.2534442\n",
      "  66.88457048 114.18771387  61.57938435  75.94401046  45.12990904\n",
      "  92.06667761  51.14962197  25.94066635  40.73667559  91.94644239\n",
      "  42.94610749  49.70461744  86.11054978  79.78084002]\n",
      "29-th iteration, loss: 0.14152793326985896, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.702862531182008e-06\n",
      "29-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87229851  73.64971906 130.54723994  99.19969578 134.25344346\n",
      "  66.8845672  114.1877135   61.57939104  75.94401291  45.1299059\n",
      "  92.06667421  51.1496191   25.94067101  40.73667645  91.94644354\n",
      "  42.94611269  49.70461976  86.11055278  79.78084002]\n",
      "30-th iteration, loss: 0.141527933267525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.679496850297475e-06\n",
      "30-th iteration, new layer inserted. now 21 layers\n",
      "[1.87229937e+00 7.36497220e+01 1.30547241e+02 9.91996961e+01\n",
      " 1.34253443e+02 6.68845640e+01 1.14187713e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15793977e+01 7.59440154e+01 4.51299028e+01\n",
      " 9.20666708e+01 5.11496162e+01 2.59406756e+01 4.07366773e+01\n",
      " 9.19464447e+01 4.29461178e+01 4.97046220e+01 8.61105557e+01\n",
      " 7.97808400e+01]\n",
      "31-th iteration, loss: 0.14152793326460994, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.598598311656274e-06\n",
      "31-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230024  73.64972502 130.54724196  99.19969638 134.25344207\n",
      "  66.88456083 114.18771278  61.57941092  75.94401779  45.12989973\n",
      "  92.06666751  51.1496134   25.94068025  40.73667812  91.94644583\n",
      "  42.94612294  49.70462428  86.11055864  79.78084002]\n",
      "32-th iteration, loss: 0.14152793326234792, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.57629907985063e-06\n",
      "32-th iteration, new layer inserted. now 21 layers\n",
      "[1.87230111e+00 7.36497280e+01 1.30547243e+02 9.91996968e+01\n",
      " 1.34253441e+02 6.68845577e+01 1.14187712e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15794175e+01 7.59440202e+01 4.51298967e+01\n",
      " 9.20666642e+01 5.11496106e+01 2.59406849e+01 4.07366790e+01\n",
      " 9.19464470e+01 4.29461280e+01 4.97046265e+01 8.61105615e+01\n",
      " 7.97808400e+01]\n",
      "33-th iteration, loss: 0.14152793325952137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.497355100468169e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230199  73.64973107 130.5472441   99.19969726 134.25344081\n",
      "  66.88455466 114.18771204  61.57943049  75.9440226   45.12989369\n",
      "  92.06666093  51.14960785  25.94068944  40.73667979  91.9464481\n",
      "  42.94613301  49.70462864  86.11056429  79.78084002]\n",
      "34-th iteration, loss: 0.14152793325732635, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.476090601640462e-06\n",
      "34-th iteration, new layer inserted. now 21 layers\n",
      "[1.87230287e+00 7.36497341e+01 1.30547245e+02 9.91996978e+01\n",
      " 1.34253440e+02 6.68845517e+01 1.14187712e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15794370e+01 7.59440250e+01 4.51298907e+01\n",
      " 9.20666577e+01 5.11496051e+01 2.59406940e+01 4.07366806e+01\n",
      " 9.19464492e+01 4.29461380e+01 4.97046308e+01 8.61105670e+01\n",
      " 7.97808400e+01]\n",
      "35-th iteration, loss: 0.14152793325458282, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.3990423570214844e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230375  73.64973718 130.54724635  99.19969842 134.25343968\n",
      "  66.88454868 114.18771128  61.57944976  75.94402736  45.12988777\n",
      "  92.06665448  51.14960242  25.94069858  40.73668145  91.94645036\n",
      "  42.94614291  49.70463284  86.11056974  79.78084002]\n",
      "36-th iteration, loss: 0.1415279332524504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.378779890672291e-06\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230464  73.64974027 130.54724752  99.19969911 134.25343916\n",
      "  66.88454577 114.18771089  61.57945612  75.94402972  45.12988485\n",
      "  92.0666513   51.14959976  25.94070315  40.73668229  91.94645149\n",
      "  42.9461478   49.70463488  86.11057239  79.78084002]\n",
      "37-th iteration, loss: 0.14152793325034596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.35837871146746e-06\n",
      "37-th iteration, new layer inserted. now 21 layers\n",
      "[1.87230553e+00 7.36497434e+01 1.30547249e+02 9.91996998e+01\n",
      " 1.34253439e+02 6.68845429e+01 1.14187711e+02 0.00000000e+00\n",
      " 4.26325641e-14 6.15794625e+01 7.59440321e+01 4.51298820e+01\n",
      " 9.20666481e+01 5.11495971e+01 2.59407077e+01 4.07366831e+01\n",
      " 9.19464526e+01 4.29461526e+01 4.97046369e+01 8.61105750e+01\n",
      " 7.97808400e+01]\n",
      "38-th iteration, loss: 0.14152793324771112, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.2832242759960696e-06\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230642  73.64974647 130.54724991  99.19970062 134.25343819\n",
      "  66.88454008 114.18771011  61.57947504  75.94403442  45.12987913\n",
      "  92.06664502  51.1495945   25.9407122   40.73668393  91.94645373\n",
      "  42.94615745  49.70463885  86.11057756  79.78084002]\n",
      "39-th iteration, loss: 0.14152793324566346, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.263818620237913e-06\n",
      "39-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230732  73.6497496  130.54725115  99.19970148 134.25343775\n",
      "  66.8845373  114.18770972  61.57948129  75.94403676  45.12987631\n",
      "  92.06664193  51.14959192  25.94071673  40.73668476  91.94645485\n",
      "  42.94616222  49.70464078  86.11058008  79.78084002]\n",
      "40-th iteration, loss: 0.1415279332436412, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.244280916916889e-06\n",
      "40-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230821  73.64975274 130.5472524   99.19970239 134.25343734\n",
      "  66.88453457 114.18770932  61.57948751  75.94403909  45.12987353\n",
      "  92.06663887  51.14958936  25.94072123  40.73668558  91.94645596\n",
      "  42.94616694  49.70464268  86.11058255  79.78084002]\n",
      "41-th iteration, loss: 0.14152793324164356, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.224625723928141e-06\n",
      "41-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87230911  73.64975589 130.54725367  99.19970334 134.25343694\n",
      "  66.88453189 114.18770894  61.57949372  75.94404142  45.12987079\n",
      "  92.06663584  51.14958682  25.94072571  40.73668639  91.94645707\n",
      "  42.94617163  49.70464454  86.11058499  79.78084002]\n",
      "42-th iteration, loss: 0.14152793323966978, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.2048666638030495e-06\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87231001  73.64975904 130.54725497  99.19970433 134.25343657\n",
      "  66.88452926 114.18770856  61.57949991  75.94404375  45.12986809\n",
      "  92.06663284  51.1495843   25.94073017  40.73668719  91.94645817\n",
      "  42.94617627  49.70464637  86.11058739  79.78084002]\n",
      "43-th iteration, loss: 0.14152793323771917, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.185016489990313e-06\n",
      "43-th iteration, new layer inserted. now 21 layers\n",
      "[1.87231091e+00 7.36497622e+01 1.30547256e+02 9.91997054e+01\n",
      " 1.34253436e+02 6.68845267e+01 1.14187708e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15795061e+01 7.59440461e+01 4.51298654e+01\n",
      " 9.20666299e+01 5.11495818e+01 2.59407346e+01 4.07366880e+01\n",
      " 9.19464593e+01 4.29461809e+01 4.97046482e+01 8.61105898e+01\n",
      " 7.97808400e+01]\n",
      "44-th iteration, loss: 0.14152793323526397, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.111965837354382e-06\n",
      "44-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87231181  73.64976536 130.54725759  99.19970644 134.25343588\n",
      "  66.8845241  114.1877078   61.5795183   75.9440484   45.12986279\n",
      "  92.0666269   51.1495793   25.94073903  40.73668875  91.94646036\n",
      "  42.94618544  49.70464992  86.11059207  79.78084002]\n",
      "45-th iteration, loss: 0.14152793323336105, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.093160848664043e-06\n",
      "45-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87231271  73.64976854 130.54725894  99.19970757 134.25343557\n",
      "  66.88452157 114.1877074   61.57952437  75.94405071  45.12986018\n",
      "  92.06662398  51.14957685  25.94074345  40.73668954  91.94646146\n",
      "  42.94618997  49.70465166  86.11059435  79.78084002]\n",
      "46-th iteration, loss: 0.1415279332314794, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.0742613543703395e-06\n",
      "46-th iteration, new layer inserted. now 21 layers\n",
      "[1.87231361e+00 7.36497717e+01 1.30547260e+02 9.91997087e+01\n",
      " 1.34253435e+02 6.68845191e+01 1.14187707e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.15795304e+01 7.59440530e+01 4.51298576e+01\n",
      " 9.20666211e+01 5.11495744e+01 2.59407479e+01 4.07366903e+01\n",
      " 9.19464626e+01 4.29461945e+01 4.97046534e+01 8.61105966e+01\n",
      " 7.97808400e+01]\n",
      "47-th iteration, loss: 0.14152793322911023, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.00311540322571e-06\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[1.87231451e+00 7.36497749e+01 1.30547262e+02 9.91997099e+01\n",
      " 1.34253435e+02 6.68845166e+01 1.14187707e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.15795424e+01 7.59440553e+01 4.51298550e+01\n",
      " 9.20666182e+01 5.11495720e+01 2.59407523e+01 4.07366911e+01\n",
      " 9.19464636e+01 4.29461989e+01 4.97046550e+01 8.61105988e+01\n",
      " 7.97808400e+01]\n",
      "48-th iteration, loss: 0.1415279332267761, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.933678849150972e-06\n",
      "48-th iteration, new layer inserted. now 19 layers\n",
      "[  1.87231541  73.64977807 130.54726306  99.1997112  134.25343472\n",
      "  66.88451418 114.18770619  61.5795543   75.94405758  45.12985249\n",
      "  92.06661534  51.14956963  25.94075665  40.73669188  91.94646473\n",
      "  42.94620336  49.70465665  86.11060095  79.78084002]\n",
      "49-th iteration, loss: 0.14152793322496102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.916852731937765e-06\n",
      "49-th iteration, new layer inserted. now 21 layers\n",
      "[1.87231631e+00 7.36497813e+01 1.30547264e+02 9.91997125e+01\n",
      " 1.34253434e+02 6.68845118e+01 1.14187706e+02 0.00000000e+00\n",
      " 4.26325641e-14 6.15795602e+01 7.59440598e+01 4.51298500e+01\n",
      " 9.20666125e+01 5.11495673e+01 2.59407610e+01 4.07366927e+01\n",
      " 9.19464658e+01 4.29462078e+01 4.97046583e+01 8.61106031e+01\n",
      " 7.97808400e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.533017523042529\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.683296      0.         1138.22122662]\n",
      "1-th iteration, loss: 0.7457663600615948, 11 gd steps\n",
      "insert gradient: -0.6282758508205019\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 41.65502839  62.36794553 235.97269332   0.         902.24853329]\n",
      "2-th iteration, loss: 0.6058144988221859, 13 gd steps\n",
      "insert gradient: -0.6674500444672876\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.73182979  77.50767926 221.26029481  40.8629459  239.37205985\n",
      "   0.         662.87647344]\n",
      "3-th iteration, loss: 0.46244924225955863, 25 gd steps\n",
      "insert gradient: -0.7426225261754482\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.15876457 226.60953846  45.48291569 159.09473236\n",
      "  55.94268804 359.84722844   0.         303.029245  ]\n",
      "4-th iteration, loss: 0.3794928962045342, 14 gd steps\n",
      "insert gradient: -0.38916927868243295\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.5791041   59.37869224 217.16928896  64.03285714 142.6949134\n",
      "  44.26730365 339.21798512  49.78272031 202.01949667   0.\n",
      " 101.00974833]\n",
      "5-th iteration, loss: 0.310561686019568, 15 gd steps\n",
      "insert gradient: -0.48738915845162734\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  3.65612042  58.83104763 226.61637543  60.98867471 106.77680388\n",
      "  63.3881031  327.15508619  43.41128021  96.69570152   0.\n",
      "  80.57975127  49.35464986 101.00974833]\n",
      "6-th iteration, loss: 0.28148606597318937, 57 gd steps\n",
      "insert gradient: -0.18369531569534628\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          70.04844864 203.2229723   62.16916737 108.33993958\n",
      "  60.21135189 224.32973262   0.         112.16486631  44.27926031\n",
      "  85.63274744  42.78142866  14.44410503  46.90504031 101.00974833]\n",
      "7-th iteration, loss: 0.24355952835592218, 19 gd steps\n",
      "insert gradient: -0.0903914269698088\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[0.00000000e+00 6.71165571e+01 2.14598744e+02 5.96172153e+01\n",
      " 1.08913542e+02 6.85144541e+01 1.64151420e+02 0.00000000e+00\n",
      " 2.48689958e-14 4.58247691e+01 7.32113336e+01 4.42362232e+01\n",
      " 9.24763240e+01 2.34378818e+01 7.13506157e+00 7.00824140e+01\n",
      " 1.01009748e+02]\n",
      "8-th iteration, loss: 0.23703474311666753, 19 gd steps\n",
      "insert gradient: -0.07189184501193904\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  0.          68.81785238 122.80847331   0.          95.51770146\n",
      "  60.10454485 111.57787978  62.35080967 165.62168803  55.88640859\n",
      "  79.3298789   40.20034276  87.36542122  31.2113241    9.14796963\n",
      "  63.1591725  101.00974833]\n",
      "9-th iteration, loss: 0.2269881211042293, 56 gd steps\n",
      "insert gradient: -0.02491416794504693\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[1.66145620e+00 6.55381856e+01 1.19770127e+02 0.00000000e+00\n",
      " 1.42108547e-14 8.75352923e+00 8.85942317e+01 6.40955456e+01\n",
      " 1.11126958e+02 7.21500988e+01 1.55622035e+02 5.94929069e+01\n",
      " 8.28345063e+01 3.57222351e+01 7.16579543e+01 2.13877287e+01\n",
      " 6.19918911e+01 5.48618143e+01 1.01009748e+02]\n",
      "10-th iteration, loss: 0.22531434222371322, 20 gd steps\n",
      "insert gradient: -0.04611430221310774\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[1.11684934e+00 6.76251834e+01 1.14552220e+02 1.08299621e+01\n",
      " 9.09621521e+01 0.00000000e+00 1.59872116e-14 6.51960842e+01\n",
      " 1.10956799e+02 7.11922650e+01 1.53987104e+02 6.72269039e+01\n",
      " 8.62139519e+01 3.47187488e+01 6.52808677e+01 2.75549398e+01\n",
      " 6.03514603e+01 4.80422318e+01 1.01009748e+02]\n",
      "11-th iteration, loss: 0.22084245198715027, 18 gd steps\n",
      "insert gradient: -0.052705496333288124\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[4.27584525e-01 6.83801534e+01 1.13927251e+02 1.37027624e+01\n",
      " 9.13175448e+01 0.00000000e+00 1.59872116e-14 6.64984919e+01\n",
      " 1.10980646e+02 7.28377408e+01 1.53766090e+02 6.94948081e+01\n",
      " 9.28778247e+01 3.53635439e+01 5.85684309e+01 3.17611341e+01\n",
      " 6.17876649e+01 4.97893284e+01 1.01009748e+02]\n",
      "12-th iteration, loss: 0.2119049477101269, 20 gd steps\n",
      "insert gradient: -0.05493653689999142\n",
      "12-th iteration, new layer inserted. now 17 layers\n",
      "[  0.72883877  70.03661686 114.67140059  17.61622779  92.60483108\n",
      "  67.8403263  110.12061231  77.73957872 156.26313359  69.93732034\n",
      "  98.69824291  38.27429234  34.22757939  46.71518918  69.43973431\n",
      "  45.72191877 101.00974833]\n",
      "13-th iteration, loss: 0.203588463077225, 17 gd steps\n",
      "insert gradient: -0.13172214228238896\n",
      "13-th iteration, new layer inserted. now 18 layers\n",
      "[  1.33801029  69.32491302 119.34242006  22.02864122  89.24687237\n",
      "  68.24976635 109.12758435  75.64416687 157.60516843  76.88210049\n",
      "  99.20332155  42.23617692  28.8617566   46.39514561  92.77496024\n",
      "  41.95429757 101.00974833   0.        ]\n",
      "14-th iteration, loss: 0.16255318512355593, 74 gd steps\n",
      "insert gradient: -0.011432709208389414\n",
      "14-th iteration, new layer inserted. now 20 layers\n",
      "[1.50765909e+00 6.17830201e+01 1.23086929e+02 3.58813848e+01\n",
      " 9.94948676e+01 6.21372005e+01 1.13561329e+02 6.54945334e+01\n",
      " 1.72611837e+02 7.77679644e+01 1.04621412e+02 4.26987584e+01\n",
      " 1.58521260e+01 5.64361918e+01 9.41376637e+01 0.00000000e+00\n",
      " 1.24344979e-14 4.24063774e+01 2.70323897e+01 5.07346546e+01]\n",
      "15-th iteration, loss: 0.16221213528792258, 204 gd steps\n",
      "insert gradient: -0.00021803719163987874\n",
      "15-th iteration, new layer inserted. now 18 layers\n",
      "[  2.76523477  61.4445648  121.88306853  37.47633252  98.58215469\n",
      "  63.00982831 114.78412079  64.76022196 173.96809187  77.86080426\n",
      " 104.82290701  40.22275116  20.25078466  55.44036157  95.23654533\n",
      "  42.2599122   25.94444425  52.43678715]\n",
      "16-th iteration, loss: 0.16221004312897097, 68 gd steps\n",
      "insert gradient: -4.968651042910461e-05\n",
      "16-th iteration, new layer inserted. now 18 layers\n",
      "[  3.49843089  61.23043924 121.92412635  37.53230665  98.494509\n",
      "  63.05457825 114.80196121  64.78005521 173.95379412  77.88852997\n",
      " 104.82314396  40.02967629  20.60667385  55.37058457  95.26919595\n",
      "  42.32227153  25.70418722  52.56775547]\n",
      "17-th iteration, loss: 0.1622099802039568, 14 gd steps\n",
      "insert gradient: -0.0003612313620755456\n",
      "17-th iteration, new layer inserted. now 20 layers\n",
      "[3.68000729e+00 6.11635568e+01 1.21972966e+02 3.75065960e+01\n",
      " 9.85389770e+01 6.30345117e+01 1.14828865e+02 6.47752932e+01\n",
      " 1.73974367e+02 7.78790088e+01 1.04819411e+02 4.00736972e+01\n",
      " 2.05276567e+01 0.00000000e+00 1.77635684e-15 5.53751193e+01\n",
      " 9.52682731e+01 4.23154943e+01 2.57659447e+01 5.25320443e+01]\n",
      "18-th iteration, loss: 0.16220995067547664, 7 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.528287742393144e-06\n",
      "18-th iteration, new layer inserted. now 22 layers\n",
      "[3.68001481e+00 6.11630874e+01 1.21972553e+02 3.75059765e+01\n",
      " 9.85385709e+01 6.30338219e+01 1.14828338e+02 6.47758399e+01\n",
      " 1.73974794e+02 7.78796118e+01 1.04820636e+02 4.00764375e+01\n",
      " 0.00000000e+00 6.21724894e-15 2.05300164e+01 3.46596357e-03\n",
      " 2.35185916e-03 5.53785905e+01 9.52693296e+01 4.23160519e+01\n",
      " 2.57659933e+01 5.25328566e+01]\n",
      "19-th iteration, loss: 0.16220994208670356, 12 gd steps\n",
      "insert gradient: -6.422184091803991e-05\n",
      "19-th iteration, new layer inserted. now 22 layers\n",
      "[3.68178412e+00 6.11588019e+01 1.21973556e+02 3.75078436e+01\n",
      " 9.85390544e+01 6.30362726e+01 1.14825987e+02 6.47740016e+01\n",
      " 1.73974904e+02 7.78799396e+01 1.04822804e+02 4.00729977e+01\n",
      " 2.05352484e+01 1.38701331e-03 2.77707506e-03 0.00000000e+00\n",
      " 7.04731412e-19 5.53784609e+01 9.52689336e+01 4.23143454e+01\n",
      " 2.57584057e+01 5.25396428e+01]\n",
      "20-th iteration, loss: 0.16220993911707066, 9 gd steps\n",
      "insert gradient: -3.6837777613840466e-05\n",
      "20-th iteration, new layer inserted. now 22 layers\n",
      "[3.68226248e+00 6.11577475e+01 1.21973957e+02 3.75084671e+01\n",
      " 9.85389647e+01 6.30360104e+01 1.14825476e+02 6.47736274e+01\n",
      " 1.73975055e+02 7.78804292e+01 1.04823312e+02 4.00730647e+01\n",
      " 2.05368308e+01 1.45600934e-03 4.07380809e-03 5.89562880e-04\n",
      " 1.23480443e-03 5.53791071e+01 9.52682084e+01 4.23119823e+01\n",
      " 2.57556425e+01 5.25389214e+01]\n",
      "21-th iteration, loss: 0.1622099324931829, 65 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.514729552219189e-06\n",
      "21-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894425   61.15291878 121.97788696  37.51053005  98.53737429\n",
      "  63.0383155  114.8232548   64.77465666 173.97594305  77.88105292\n",
      " 104.82400116  40.06373302  20.5575237   55.37944941  95.26878248\n",
      "  42.31411208  25.74483184  52.54758571]\n",
      "22-th iteration, loss: 0.162209932491126, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.532882055815919e-06\n",
      "22-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894487   61.15291382 121.97789068  37.51053173  98.53737309\n",
      "  63.03831619 114.82325483  64.7746569  173.97594281  77.88105386\n",
      " 104.82400105  40.06372747  20.55752587  55.37944881  95.26878313\n",
      "  42.31411322  25.74482672  52.54759023]\n",
      "23-th iteration, loss: 0.16220993248908225, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.547902564003292e-06\n",
      "23-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894549   61.15290886 121.97789438  37.51053338  98.53737188\n",
      "  63.03831686 114.82325484  64.77465716 173.97594259  77.88105482\n",
      " 104.82400097  40.063722    20.55752812  55.3794483   95.2687838\n",
      "  42.31411439  25.74482163  52.54759477]\n",
      "24-th iteration, loss: 0.16220993248705046, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.560072126752401e-06\n",
      "24-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894611   61.1529039  121.97789807  37.510535    98.53737065\n",
      "  63.03831751 114.82325484  64.77465743 173.97594238  77.88105579\n",
      " 104.82400091  40.06371662  20.55753043  55.37944787  95.2687845\n",
      "  42.31411559  25.74481659  52.54759932]\n",
      "25-th iteration, loss: 0.16220993248502963, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.569647972603351e-06\n",
      "25-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894673   61.15289895 121.97790175  37.51053661  98.53736941\n",
      "  63.03831814 114.82325483  64.77465771 173.97594218  77.88105676\n",
      " 104.82400088  40.06371131  20.55753281  55.37944752  95.26878521\n",
      "  42.31411682  25.74481157  52.54760389]\n",
      "26-th iteration, loss: 0.16220993248301876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.576865514395607e-06\n",
      "26-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6894735   61.15289401 121.97790542  37.51053819  98.53736816\n",
      "  63.03831875 114.82325481  64.774658   173.97594199  77.88105774\n",
      " 104.82400086  40.06370608  20.55753525  55.37944722  95.26878594\n",
      "  42.31411807  25.74480658  52.54760846]\n",
      "27-th iteration, loss: 0.16220993248101723, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.581940185590877e-06\n",
      "27-th iteration, new layer inserted. now 20 layers\n",
      "[3.68947969e+00 6.11528891e+01 1.21977909e+02 3.75105398e+01\n",
      " 9.85373669e+01 6.30383193e+01 1.14823255e+02 6.47746583e+01\n",
      " 1.73975942e+02 7.78810587e+01 1.04824001e+02 4.00637009e+01\n",
      " 2.05575377e+01 5.53794470e+01 9.52687867e+01 4.23141193e+01\n",
      " 2.57448016e+01 0.00000000e+00 5.32907052e-15 5.25476130e+01]\n",
      "28-th iteration, loss: 0.16220993247876667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.528529500745509e-06\n",
      "28-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68948589  61.15288413 121.97791274  37.5105413   98.53736563\n",
      "  63.03831991 114.82325472  64.77465859 173.97594164  77.88105973\n",
      " 104.82400088  40.06369583  20.5575403   55.37944682  95.26878744\n",
      "  42.31412062  25.74479669  52.54762222]\n",
      "29-th iteration, loss: 0.16220993247678162, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.532693149195422e-06\n",
      "29-th iteration, new layer inserted. now 20 layers\n",
      "[3.68949209e+00 6.11528792e+01 1.21977916e+02 3.75105428e+01\n",
      " 9.85373644e+01 6.30383205e+01 1.14823255e+02 6.47746589e+01\n",
      " 1.73975941e+02 7.78810607e+01 1.04824001e+02 4.00636908e+01\n",
      " 2.05575429e+01 5.53794467e+01 9.52687882e+01 4.23141219e+01\n",
      " 2.57447917e+01 0.00000000e+00 5.32907052e-15 5.25476268e+01]\n",
      "30-th iteration, loss: 0.16220993247455243, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.479118163476913e-06\n",
      "30-th iteration, new layer inserted. now 20 layers\n",
      "[3.68949828e+00 6.11528743e+01 1.21977920e+02 3.75105444e+01\n",
      " 9.85373631e+01 6.30383210e+01 1.14823255e+02 6.47746592e+01\n",
      " 1.73975941e+02 7.78810617e+01 1.04824001e+02 4.00636858e+01\n",
      " 2.05575455e+01 5.53794465e+01 9.52687889e+01 4.23141231e+01\n",
      " 2.57447868e+01 0.00000000e+00 5.32907052e-15 5.25476358e+01]\n",
      "31-th iteration, loss: 0.16220993247233642, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.427326239654735e-06\n",
      "31-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68950447  61.15286932 121.97792367  37.51054588  98.53736178\n",
      "  63.03832158 114.82325455  64.7746595  173.97594114  77.88106276\n",
      " 104.82400098  40.06368078  20.55754814  55.37944644  95.26878967\n",
      "  42.31412434  25.74478185  52.5476448 ]\n",
      "32-th iteration, loss: 0.16220993247037366, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.431880537359336e-06\n",
      "32-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68951066  61.15286438 121.9779273   37.5105474   98.53736049\n",
      "  63.03832212 114.82325449  64.77465981 173.97594098  77.88106379\n",
      " 104.82400103  40.06367583  20.55755081  55.37944635  95.26879039\n",
      "  42.31412551  25.74477687  52.54764924]\n",
      "33-th iteration, loss: 0.16220993246841822, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.434711731251137e-06\n",
      "33-th iteration, new layer inserted. now 20 layers\n",
      "[3.68951685e+00 6.11528594e+01 1.21977931e+02 3.75105489e+01\n",
      " 9.85373592e+01 6.30383227e+01 1.14823254e+02 6.47746601e+01\n",
      " 1.73975941e+02 7.78810648e+01 1.04824001e+02 4.00636709e+01\n",
      " 2.05575535e+01 5.53794463e+01 9.52687911e+01 4.23141267e+01\n",
      " 2.57447719e+01 0.00000000e+00 2.22044605e-15 5.25476537e+01]\n",
      "34-th iteration, loss: 0.16220993246622842, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.381238820875587e-06\n",
      "34-th iteration, new layer inserted. now 20 layers\n",
      "[3.68952304e+00 6.11528545e+01 1.21977935e+02 3.75105504e+01\n",
      " 9.85373579e+01 6.30383232e+01 1.14823254e+02 6.47746604e+01\n",
      " 1.73975941e+02 7.78810659e+01 1.04824001e+02 4.00636661e+01\n",
      " 2.05575563e+01 5.53794463e+01 9.52687919e+01 4.23141279e+01\n",
      " 2.57447670e+01 0.00000000e+00 6.21724894e-15 5.25476626e+01]\n",
      "35-th iteration, loss: 0.16220993246405072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.329702235439049e-06\n",
      "35-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68952923  61.15284958 121.97793816  37.51055186  98.53735658\n",
      "  63.03832368 114.82325424  64.77466072 173.97594053  77.8810669\n",
      " 104.82400124  40.06366127  20.55755905  55.3794463   95.26879258\n",
      "  42.31412905  25.74476205  52.54767133]\n",
      "36-th iteration, loss: 0.16220993246211474, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.333454235894893e-06\n",
      "36-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68953542  61.15284465 121.97794178  37.51055333  98.53735527\n",
      "  63.0383242  114.82325416  64.77466103 173.97594039  77.88106795\n",
      " 104.82400133  40.06365647  20.55756184  55.3794463   95.26879329\n",
      "  42.31413016  25.74475708  52.54767566]\n",
      "37-th iteration, loss: 0.16220993246018522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.335666153696035e-06\n",
      "37-th iteration, new layer inserted. now 20 layers\n",
      "[3.68954160e+00 6.11528397e+01 1.21977945e+02 3.75105548e+01\n",
      " 9.85373540e+01 6.30383247e+01 1.14823254e+02 6.47746613e+01\n",
      " 1.73975940e+02 7.78810690e+01 1.04824001e+02 4.00636517e+01\n",
      " 2.05575647e+01 5.53794463e+01 9.52687940e+01 4.23141313e+01\n",
      " 2.57447521e+01 0.00000000e+00 6.21724894e-15 5.25476800e+01]\n",
      "38-th iteration, loss: 0.16220993245803123, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.282948487151021e-06\n",
      "38-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68954779  61.1528348  121.97794898  37.51055625  98.53735263\n",
      "  63.03832519 114.82325396  64.77466164 173.97594011  77.88107006\n",
      " 104.82400152  40.06364703  20.55756755  55.37944642  95.26879472\n",
      "  42.31413243  25.74474721  52.54768868]\n",
      "39-th iteration, loss: 0.16220993245611345, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.285140366009358e-06\n",
      "39-th iteration, new layer inserted. now 20 layers\n",
      "[3.68955397e+00 6.11528299e+01 1.21977953e+02 3.75105577e+01\n",
      " 9.85373513e+01 6.30383257e+01 1.14823254e+02 6.47746619e+01\n",
      " 1.73975940e+02 7.78810711e+01 1.04824002e+02 4.00636423e+01\n",
      " 2.05575704e+01 5.53794465e+01 9.52687954e+01 4.23141335e+01\n",
      " 2.57447423e+01 0.00000000e+00 2.22044605e-15 5.25476930e+01]\n",
      "40-th iteration, loss: 0.1622099324539763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.23307365894687e-06\n",
      "40-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68956016  61.15282496 121.97795618  37.51055913  98.53734998\n",
      "  63.03832615 114.82325374  64.77466224 173.97593983  77.88107217\n",
      " 104.82400173  40.06363771  20.55757336  55.37944662  95.26879613\n",
      "  42.31413463  25.74473734  52.54770155]\n",
      "41-th iteration, loss: 0.1622099324520698, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.235319519929995e-06\n",
      "41-th iteration, new layer inserted. now 20 layers\n",
      "[3.68956634e+00 6.11528200e+01 1.21977960e+02 3.75105606e+01\n",
      " 9.85373487e+01 6.30383266e+01 1.14823254e+02 6.47746625e+01\n",
      " 1.73975940e+02 7.78810732e+01 1.04824002e+02 4.00636331e+01\n",
      " 2.05575763e+01 5.53794467e+01 9.52687968e+01 4.23141357e+01\n",
      " 2.57447324e+01 0.00000000e+00 2.22044605e-15 5.25477058e+01]\n",
      "42-th iteration, loss: 0.1622099324499488, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.183960414667197e-06\n",
      "42-th iteration, new layer inserted. now 20 layers\n",
      "[3.68957252e+00 6.11528151e+01 1.21977963e+02 3.75105620e+01\n",
      " 9.85373473e+01 6.30383271e+01 1.14823254e+02 6.47746628e+01\n",
      " 1.73975940e+02 7.78810743e+01 1.04824002e+02 4.00636285e+01\n",
      " 2.05575793e+01 5.53794469e+01 9.52687975e+01 4.23141368e+01\n",
      " 2.57447275e+01 0.00000000e+00 6.21724894e-15 5.25477143e+01]\n",
      "43-th iteration, loss: 0.16220993244783824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.134631823921472e-06\n",
      "43-th iteration, new layer inserted. now 18 layers\n",
      "[  3.6895787   61.15281021 121.97796694  37.51056342  98.53734598\n",
      "  63.03832755 114.82325339  64.77466312 173.97593943  77.88107536\n",
      " 104.82400208  40.06362394  20.55758225  55.379447    95.26879818\n",
      "  42.3141378   25.74472252  52.54772265]\n",
      "44-th iteration, loss: 0.1622099324459475, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.138291029513609e-06\n",
      "44-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68958488  61.1528053  121.97797052  37.51056485  98.53734465\n",
      "  63.03832802 114.82325326  64.77466341 173.9759393   77.88107643\n",
      " 104.8240022   40.06361938  20.55758524  55.37944714  95.26879884\n",
      "  42.31413879  25.74471755  52.54772679]\n",
      "45-th iteration, loss: 0.16220993244406204, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.140620114591489e-06\n",
      "45-th iteration, new layer inserted. now 20 layers\n",
      "[3.68959106e+00 6.11528004e+01 1.21977974e+02 3.75105663e+01\n",
      " 9.85373433e+01 6.30383285e+01 1.14823253e+02 6.47746637e+01\n",
      " 1.73975939e+02 7.78810775e+01 1.04824002e+02 4.00636149e+01\n",
      " 2.05575883e+01 5.53794473e+01 9.52687995e+01 4.23141398e+01\n",
      " 2.57447126e+01 0.00000000e+00 5.32907052e-15 5.25477309e+01]\n",
      "46-th iteration, loss: 0.16220993244197138, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.090586027277056e-06\n",
      "46-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68959723  61.15279549 121.97797768  37.51056767  98.53734197\n",
      "  63.03832892 114.823253    64.77466398 173.97593904  77.88107857\n",
      " 104.82400246  40.0636104   20.55759133  55.3794475   95.26880017\n",
      "  42.31414082  25.74470768  52.54773923]\n",
      "47-th iteration, loss: 0.16220993244009577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.09313506177197e-06\n",
      "47-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68960341  61.15279059 121.97798125  37.51056908  98.53734062\n",
      "  63.03832937 114.82325286  64.77466427 173.97593891  77.88107964\n",
      " 104.8240026   40.06360594  20.5575944   55.37944768  95.26880082\n",
      "  42.31414179  25.74470273  52.54774332]\n",
      "48-th iteration, loss: 0.16220993243822499, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.094498451572692e-06\n",
      "48-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68960959  61.15278569 121.97798482  37.51057048  98.53733928\n",
      "  63.03832981 114.82325272  64.77466455 173.97593878  77.88108071\n",
      " 104.82400274  40.06360152  20.5575975   55.3794479   95.26880147\n",
      "  42.31414278  25.7446978   52.54774742]\n",
      "49-th iteration, loss: 0.16220993243635887, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.094781853715536e-06\n",
      "49-th iteration, new layer inserted. now 18 layers\n",
      "[  3.68961576  61.1527808  121.97798839  37.51057187  98.53733793\n",
      "  63.03833024 114.82325256  64.77466483 173.97593866  77.88108178\n",
      " 104.82400288  40.06359714  20.55760063  55.37944814  95.26880213\n",
      "  42.31414378  25.7446929   52.54775152]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224615\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.59145123    0.         1162.43869952]\n",
      "1-th iteration, loss: 0.7475736523080361, 11 gd steps\n",
      "insert gradient: -0.6191983139760763\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.31049111  62.35738846 240.99338893   0.         921.4453106 ]\n",
      "2-th iteration, loss: 0.6080524105521316, 13 gd steps\n",
      "insert gradient: -0.7115673117266892\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.34729714  76.89556984 225.21813808  38.97551214 244.4650824\n",
      "   0.         676.98022819]\n",
      "3-th iteration, loss: 0.4298550710360744, 53 gd steps\n",
      "insert gradient: -0.6809629591259072\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.4266346   48.2035365  208.30701739  58.31057484 127.7430007\n",
      "  58.979767   348.16126021   0.         328.81896798]\n",
      "4-th iteration, loss: 0.33326158695750574, 38 gd steps\n",
      "insert gradient: -0.17544261885998408\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.24897018  51.57368698 204.34222331  56.46573489 123.24976103\n",
      "  59.54155589 295.78335171  54.24989749 182.67720443   0.\n",
      " 146.14176355]\n",
      "5-th iteration, loss: 0.31016895098975417, 17 gd steps\n",
      "insert gradient: -0.3133066532053943\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          45.24060387 106.701348     0.         106.701348\n",
      "  56.10576113 122.22868061  63.55195473 296.1252529   50.52685573\n",
      " 170.27239534  32.51175504 146.14176355]\n",
      "6-th iteration, loss: 0.2823783590524015, 73 gd steps\n",
      "insert gradient: -0.015571193867669654\n",
      "6-th iteration, new layer inserted. now 14 layers\n",
      "[  2.41636108  53.95683325  86.21251276  15.5964871   80.44430792\n",
      "  56.83529917 127.4518925   64.60099982 287.19865914  57.62927412\n",
      " 131.83167272  67.80073642 146.14176355   0.        ]\n",
      "7-th iteration, loss: 0.21852995745459686, 51 gd steps\n",
      "insert gradient: -0.15046436073880898\n",
      "7-th iteration, new layer inserted. now 16 layers\n",
      "[  1.97688971  43.66310021  88.33737652  35.28284469  74.85812435\n",
      "  56.57767347 130.08128922  60.95918258  80.2810021    0.\n",
      " 192.67440503  76.0843273  130.70562948  76.06225264 120.2187608\n",
      "  83.96566893]\n",
      "8-th iteration, loss: 0.14848873174864347, 160 gd steps\n",
      "insert gradient: -0.0065497377849161745\n",
      "8-th iteration, new layer inserted. now 18 layers\n",
      "[  1.63250286  43.29124267  77.14131339  43.90245703  89.05061961\n",
      "  53.01855136  66.97607113   0.         100.46410669  16.64474216\n",
      "  79.84263167  78.74659484 122.38920361  77.40325369 128.05811527\n",
      "  79.37914196 131.43943668  80.57486606]\n",
      "9-th iteration, loss: 0.14835285012437996, 231 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9972688523034247e-06\n",
      "9-th iteration, new layer inserted. now 18 layers\n",
      "[  1.59688944  43.72836192  76.92991825  43.96792712  89.57010256\n",
      "  53.75435015  65.37381461   2.9379676   90.7102584   17.47012089\n",
      "  82.91287016  79.41444671 121.48925539  77.83405414 127.56852375\n",
      "  79.63211032 131.40405642  80.70653609]\n",
      "10-th iteration, loss: 0.14835284761821382, 24 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.406785347730284e-06\n",
      "10-th iteration, new layer inserted. now 18 layers\n",
      "[  1.59790097  43.728684    76.92844544  43.96752637  89.57183861\n",
      "  53.7523849   65.40505535   2.9403762   90.66875317  17.47534774\n",
      "  82.90726598  79.4159663  121.48848017  77.83509787 127.56750709\n",
      "  79.63287847 131.40486835  80.70650642]\n",
      "11-th iteration, loss: 0.14835284761531278, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.764158202535915e-06\n",
      "11-th iteration, new layer inserted. now 18 layers\n",
      "[  1.59790083  43.72868713  76.92845203  43.96753562  89.5718395\n",
      "  53.75238747  65.40505681   2.94037231  90.66875118  17.47534622\n",
      "  82.90726653  79.41596405 121.48847895  77.83509298 127.56750542\n",
      "  79.63287482 131.40486723  80.70650614]\n",
      "12-th iteration, loss: 0.14835284761272063, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.168867594729915e-06\n",
      "12-th iteration, new layer inserted. now 18 layers\n",
      "[  1.59790061  43.72868992  76.92845833  43.96754423  89.57184014\n",
      "  53.75238984  65.40505826   2.94036855  90.66874918  17.47534464\n",
      "  82.90726704  79.41596187 121.4884778   77.83508827 127.56750384\n",
      "  79.63287133 131.40486615  80.70650587]\n",
      "13-th iteration, loss: 0.14835284761039674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.617326171376776e-06\n",
      "13-th iteration, new layer inserted. now 18 layers\n",
      "[  1.59790032  43.72869239  76.92846434  43.96755227  89.57184054\n",
      "  53.75239202  65.4050597    2.94036494  90.66874718  17.475343\n",
      "  82.90726753  79.41595974 121.4884767   77.83508376 127.56750234\n",
      "  79.63286798 131.40486511  80.70650561]\n",
      "14-th iteration, loss: 0.14835284760830628, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.106222432331267e-06\n",
      "14-th iteration, new layer inserted. now 20 layers\n",
      "[1.59789996e+00 4.37286946e+01 7.69284701e+01 0.00000000e+00\n",
      " 8.88178420e-15 4.39675598e+01 8.95718407e+01 5.37523940e+01\n",
      " 6.54050611e+01 2.94036146e+00 9.06687452e+01 1.74753413e+01\n",
      " 8.29072680e+01 7.94159577e+01 1.21488476e+02 7.78350794e+01\n",
      " 1.27567501e+02 7.96328648e+01 1.31404864e+02 8.07065054e+01]\n",
      "15-th iteration, loss: 0.14835284760544815, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.2663661174366544e-06\n",
      "15-th iteration, new layer inserted. now 22 layers\n",
      "[1.59789954e+00 4.37286964e+01 7.69284756e+01 6.89016260e-06\n",
      " 5.47014840e-06 0.00000000e+00 1.37642854e-21 4.39675666e+01\n",
      " 8.95718407e+01 5.37523958e+01 6.54050625e+01 2.94035811e+00\n",
      " 9.06687432e+01 1.74753396e+01 8.29072684e+01 7.94159557e+01\n",
      " 1.21488475e+02 7.78350752e+01 1.27567500e+02 7.96328617e+01\n",
      " 1.31404863e+02 8.07065051e+01]\n",
      "16-th iteration, loss: 0.14835284760233453, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.2103581732790295e-06\n",
      "16-th iteration, new layer inserted. now 22 layers\n",
      "[1.59789901e+00 4.37286978e+01 7.69284806e+01 1.28435805e-05\n",
      " 1.04717842e-05 5.99106673e-06 5.00163581e-06 4.39675726e+01\n",
      " 8.95718402e+01 5.37523973e+01 6.54050639e+01 2.94035490e+00\n",
      " 9.06687412e+01 1.74753378e+01 8.29072688e+01 7.94159537e+01\n",
      " 1.21488474e+02 7.78350713e+01 1.27567498e+02 7.96328588e+01\n",
      " 1.31404862e+02 8.07065049e+01]\n",
      "17-th iteration, loss: 0.1483528475999217, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.33290497375541e-06\n",
      "17-th iteration, new layer inserted. now 24 layers\n",
      "[1.59789837e+00 4.37286987e+01 7.69284852e+01 1.77208401e-05\n",
      " 1.49683329e-05 1.09403995e-05 9.45695032e-06 0.00000000e+00\n",
      " 2.11758237e-22 4.39675776e+01 8.95718394e+01 5.37523985e+01\n",
      " 6.54050653e+01 2.94035185e+00 9.06687392e+01 1.74753359e+01\n",
      " 8.29072692e+01 7.94159518e+01 1.21488473e+02 7.78350675e+01\n",
      " 1.27567497e+02 7.96328561e+01 1.31404861e+02 8.07065046e+01]\n",
      "18-th iteration, loss: 0.1483528475976308, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8451952933762925e-06\n",
      "18-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789763e+00 4.37286991e+01 0.00000000e+00 1.24344979e-14\n",
      " 7.69284894e+01 2.16380162e-05 1.90106459e-05 1.49605969e-05\n",
      " 1.34239648e-05 4.08528593e-06 3.96701448e-06 4.39675817e+01\n",
      " 8.95718382e+01 5.37523993e+01 6.54050666e+01 2.94034895e+00\n",
      " 9.06687371e+01 1.74753340e+01 8.29072695e+01 7.94159500e+01\n",
      " 1.21488472e+02 7.78350640e+01 1.27567496e+02 7.96328535e+01\n",
      " 1.31404860e+02 8.07065043e+01]\n",
      "19-th iteration, loss: 0.148352847595725, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4563811106043817e-06\n",
      "19-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789679e+00 4.37286990e+01 0.00000000e+00 1.24344979e-14\n",
      " 7.69284968e+01 2.45588794e-05 2.25986897e-05 1.80123031e-05\n",
      " 1.69090405e-05 7.22938422e-06 7.42397269e-06 4.39675849e+01\n",
      " 8.95718366e+01 5.37523999e+01 6.54050678e+01 2.94034621e+00\n",
      " 9.06687351e+01 1.74753320e+01 8.29072698e+01 7.94159482e+01\n",
      " 1.21488471e+02 7.78350606e+01 1.27567495e+02 7.96328511e+01\n",
      " 1.31404860e+02 8.07065040e+01]\n",
      "20-th iteration, loss: 0.1483528475942288, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1479556351401635e-06\n",
      "20-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789587e+00 4.37286985e+01 0.00000000e+00 3.55271368e-15\n",
      " 7.69285036e+01 2.66547724e-05 2.58039701e-05 2.02637344e-05\n",
      " 1.99903489e-05 9.59719424e-06 1.04555239e-05 4.39675873e+01\n",
      " 8.95718347e+01 5.37524002e+01 6.54050691e+01 2.94034364e+00\n",
      " 9.06687331e+01 1.74753300e+01 8.29072701e+01 7.94159465e+01\n",
      " 1.21488470e+02 7.78350574e+01 1.27567494e+02 7.96328488e+01\n",
      " 1.31404859e+02 8.07065037e+01]\n",
      "21-th iteration, loss: 0.14835284759299422, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.9024294410305194e-06\n",
      "21-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789488e+00 4.37286978e+01 0.00000000e+00 1.24344979e-14\n",
      " 7.69285098e+01 2.81025619e-05 2.87067714e-05 2.18891232e-05\n",
      " 2.27536824e-05 1.13601692e-05 1.31528034e-05 4.39675891e+01\n",
      " 8.95718326e+01 5.37524004e+01 6.54050704e+01 2.94034121e+00\n",
      " 9.06687311e+01 1.74753280e+01 8.29072703e+01 7.94159449e+01\n",
      " 1.21488470e+02 7.78350544e+01 1.27567493e+02 7.96328466e+01\n",
      " 1.31404858e+02 8.07065034e+01]\n",
      "22-th iteration, loss: 0.14835284759193007, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.7061422793913093e-06\n",
      "22-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789384e+00 4.37286968e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285155e+01 2.90403594e-05 3.13699418e-05 2.30244988e-05\n",
      " 2.52661979e-05 1.26521502e-05 1.55871310e-05 4.39675905e+01\n",
      " 8.95718303e+01 5.37524005e+01 6.54050717e+01 2.94033892e+00\n",
      " 9.06687291e+01 1.74753260e+01 8.29072706e+01 7.94159434e+01\n",
      " 1.21488469e+02 7.78350516e+01 1.27567492e+02 7.96328446e+01\n",
      " 1.31404857e+02 8.07065031e+01]\n",
      "23-th iteration, loss: 0.14835284759097994, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.5484235950862307e-06\n",
      "23-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789276e+00 4.37286956e+01 0.00000000e+00 1.42108547e-14\n",
      " 7.69285208e+01 2.95760179e-05 3.38427097e-05 2.37760653e-05\n",
      " 2.75804965e-05 1.35776148e-05 1.78143494e-05 4.39675915e+01\n",
      " 8.95718279e+01 5.37524005e+01 6.54050730e+01 2.94033676e+00\n",
      " 9.06687271e+01 1.74753240e+01 8.29072708e+01 7.94159418e+01\n",
      " 1.21488469e+02 7.78350488e+01 1.27567492e+02 7.96328426e+01\n",
      " 1.31404857e+02 8.07065028e+01]\n",
      "24-th iteration, loss: 0.14835284759010864, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4209371271447885e-06\n",
      "24-th iteration, new layer inserted. now 24 layers\n",
      "[1.59789165e+00 4.37286942e+01 7.69285259e+01 2.97937662e-05\n",
      " 3.61636647e-05 2.42267407e-05 2.97378094e-05 1.42181168e-05\n",
      " 1.98782125e-05 4.39675923e+01 8.95718254e+01 5.37524005e+01\n",
      " 6.54050743e+01 2.94033473e+00 9.06687252e+01 1.74753221e+01\n",
      " 8.29072711e+01 7.94159404e+01 1.21488468e+02 7.78350461e+01\n",
      " 1.27567491e+02 7.96328407e+01 1.31404856e+02 8.07065025e+01]\n",
      "25-th iteration, loss: 0.14835284758936937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3400112090753338e-06\n",
      "25-th iteration, new layer inserted. now 26 layers\n",
      "[1.59789051e+00 4.37286927e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285283e+01 2.97631768e-05 3.83649504e-05 2.44450526e-05\n",
      " 3.17723513e-05 1.46411034e-05 2.18148962e-05 4.39675929e+01\n",
      " 8.95718229e+01 5.37524004e+01 6.54050756e+01 2.94033280e+00\n",
      " 9.06687233e+01 1.74753201e+01 8.29072714e+01 7.94159390e+01\n",
      " 1.21488467e+02 7.78350436e+01 1.27567490e+02 7.96328389e+01\n",
      " 1.31404855e+02 8.07065023e+01]\n",
      "26-th iteration, loss: 0.14835284758859213, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2508933204713622e-06\n",
      "26-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788936e+00 4.37286912e+01 0.00000000e+00 1.24344979e-14\n",
      " 7.69285329e+01 2.95647062e-05 4.04881145e-05 2.45106341e-05\n",
      " 3.37272689e-05 1.49253625e-05 2.36690443e-05 4.39675933e+01\n",
      " 8.95718203e+01 5.37524004e+01 6.54050770e+01 2.94033098e+00\n",
      " 9.06687214e+01 1.74753182e+01 8.29072717e+01 7.94159376e+01\n",
      " 1.21488467e+02 7.78350411e+01 1.27567490e+02 7.96328371e+01\n",
      " 1.31404854e+02 8.07065020e+01]\n",
      "27-th iteration, loss: 0.14835284758784958, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1772838144179344e-06\n",
      "27-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788819e+00 4.37286896e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285374e+01 2.91969220e-05 4.25282874e-05 2.44215150e-05\n",
      " 3.55987440e-05 1.50683760e-05 2.54377935e-05 4.39675936e+01\n",
      " 8.95718177e+01 5.37524004e+01 6.54050784e+01 2.94032926e+00\n",
      " 9.06687196e+01 1.74753163e+01 8.29072719e+01 7.94159362e+01\n",
      " 1.21488467e+02 7.78350387e+01 1.27567489e+02 7.96328354e+01\n",
      " 1.31404854e+02 8.07065018e+01]\n",
      "28-th iteration, loss: 0.14835284758713502, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1158829305228815e-06\n",
      "28-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788701e+00 4.37286879e+01 0.00000000e+00 1.77635684e-15\n",
      " 7.69285418e+01 2.86930823e-05 4.45010645e-05 2.42103819e-05\n",
      " 3.74034366e-05 1.51022563e-05 2.71387759e-05 4.39675938e+01\n",
      " 8.95718151e+01 5.37524004e+01 6.54050799e+01 2.94032763e+00\n",
      " 9.06687177e+01 1.74753145e+01 8.29072722e+01 7.94159349e+01\n",
      " 1.21488466e+02 7.78350363e+01 1.27567488e+02 7.96328338e+01\n",
      " 1.31404853e+02 8.07065015e+01]\n",
      "29-th iteration, loss: 0.1483528475864437, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.064109856757917e-06\n",
      "29-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788581e+00 4.37286862e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285460e+01 2.80791692e-05 4.64187503e-05 2.39027535e-05\n",
      " 3.91544912e-05 1.50520623e-05 2.87858871e-05 4.39675940e+01\n",
      " 8.95718125e+01 5.37524004e+01 6.54050813e+01 2.94032610e+00\n",
      " 9.06687160e+01 1.74753127e+01 8.29072726e+01 7.94159336e+01\n",
      " 1.21488466e+02 7.78350341e+01 1.27567488e+02 7.96328322e+01\n",
      " 1.31404853e+02 8.07065013e+01]\n",
      "30-th iteration, loss: 0.14835284758577233, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.019945264125348e-06\n",
      "30-th iteration, new layer inserted. now 24 layers\n",
      "[1.59788461e+00 4.37286844e+01 7.69285501e+01 2.73754835e-05\n",
      " 4.82910755e-05 2.35185511e-05 4.08623024e-05 1.49373460e-05\n",
      " 3.03901004e-05 4.39675940e+01 8.95718099e+01 5.37524005e+01\n",
      " 6.54050828e+01 2.94032465e+00 9.06687142e+01 1.74753109e+01\n",
      " 8.29072729e+01 7.94159324e+01 1.21488465e+02 7.78350319e+01\n",
      " 1.27567487e+02 7.96328306e+01 1.31404852e+02 8.07065011e+01]\n",
      "31-th iteration, loss: 0.14835284758517275, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0013478551008904e-06\n",
      "31-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788341e+00 4.37286826e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285522e+01 2.65980026e-05 5.01255319e-05 2.30734399e-05\n",
      " 4.25348891e-05 1.47734752e-05 3.19598788e-05 4.39675941e+01\n",
      " 8.95718072e+01 5.37524006e+01 6.54050843e+01 2.94032328e+00\n",
      " 9.06687125e+01 1.74753091e+01 8.29072732e+01 7.94159312e+01\n",
      " 1.21488465e+02 7.78350297e+01 1.27567487e+02 7.96328291e+01\n",
      " 1.31404851e+02 8.07065009e+01]\n",
      "32-th iteration, loss: 0.14835284758453268, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.96458297352583e-06\n",
      "32-th iteration, new layer inserted. now 24 layers\n",
      "[1.59788220e+00 4.37286808e+01 7.69285562e+01 2.57904497e-05\n",
      " 5.19471131e-05 2.26108826e-05 4.41976643e-05 1.46036700e-05\n",
      " 3.35209736e-05 4.39675941e+01 8.95718046e+01 5.37524008e+01\n",
      " 6.54050859e+01 2.94032198e+00 9.06687108e+01 1.74753074e+01\n",
      " 8.29072736e+01 7.94159300e+01 1.21488465e+02 7.78350276e+01\n",
      " 1.27567487e+02 7.96328276e+01 1.31404851e+02 8.07065007e+01]\n",
      "33-th iteration, loss: 0.14835284758395856, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9514364328503157e-06\n",
      "33-th iteration, new layer inserted. now 26 layers\n",
      "[1.59788099e+00 4.37286790e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285581e+01 2.49232961e-05 5.37382454e-05 2.21012618e-05\n",
      " 4.58331743e-05 1.43982455e-05 3.50559718e-05 4.39675942e+01\n",
      " 8.95718021e+01 5.37524010e+01 6.54050875e+01 2.94032075e+00\n",
      " 9.06687091e+01 1.74753057e+01 8.29072739e+01 7.94159288e+01\n",
      " 1.21488465e+02 7.78350256e+01 1.27567486e+02 7.96328262e+01\n",
      " 1.31404850e+02 8.07065005e+01]\n",
      "34-th iteration, loss: 0.1483528475833444, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9195278330940622e-06\n",
      "34-th iteration, new layer inserted. now 26 layers\n",
      "[1.59787978e+00 4.37286772e+01 0.00000000e+00 1.42108547e-14\n",
      " 7.69285620e+01 2.40371590e-05 5.55224736e-05 2.15849852e-05\n",
      " 4.74652877e-05 1.41974217e-05 3.65889873e-05 4.39675942e+01\n",
      " 8.95717995e+01 5.37524013e+01 6.54050891e+01 2.94031958e+00\n",
      " 9.06687074e+01 1.74753040e+01 8.29072743e+01 7.94159277e+01\n",
      " 1.21488464e+02 7.78350236e+01 1.27567486e+02 7.96328248e+01\n",
      " 1.31404850e+02 8.07065004e+01]\n",
      "35-th iteration, loss: 0.14835284758274234, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8912502427759067e-06\n",
      "35-th iteration, new layer inserted. now 26 layers\n",
      "[1.59787856e+00 4.37286754e+01 0.00000000e+00 3.55271368e-15\n",
      " 7.69285659e+01 2.31008299e-05 5.72818252e-05 2.10307966e-05\n",
      " 4.90760778e-05 1.39699192e-05 3.81020618e-05 4.39675942e+01\n",
      " 8.95717970e+01 5.37524015e+01 6.54050907e+01 2.94031848e+00\n",
      " 9.06687058e+01 1.74753023e+01 8.29072747e+01 7.94159265e+01\n",
      " 1.21488464e+02 7.78350217e+01 1.27567486e+02 7.96328235e+01\n",
      " 1.31404849e+02 8.07065002e+01]\n",
      "36-th iteration, loss: 0.14835284758215125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8658767940504255e-06\n",
      "36-th iteration, new layer inserted. now 24 layers\n",
      "[1.59787735e+00 4.37286735e+01 7.69285697e+01 2.21215326e-05\n",
      " 5.90200469e-05 2.04457488e-05 5.06695523e-05 1.37226439e-05\n",
      " 3.95993866e-05 4.39675942e+01 8.95717944e+01 5.37524019e+01\n",
      " 6.54050923e+01 2.94031744e+00 9.06687042e+01 1.74753006e+01\n",
      " 8.29072750e+01 7.94159255e+01 1.21488464e+02 7.78350198e+01\n",
      " 1.27567485e+02 7.96328222e+01 1.31404849e+02 8.07065001e+01]\n",
      "37-th iteration, loss: 0.14835284758161693, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8610357810600855e-06\n",
      "37-th iteration, new layer inserted. now 24 layers\n",
      "[1.59787613e+00 4.37286717e+01 7.69285716e+01 2.11039117e-05\n",
      " 6.07393067e-05 1.98343408e-05 5.22480917e-05 1.34599756e-05\n",
      " 4.10834780e-05 4.39675943e+01 8.95717919e+01 5.37524023e+01\n",
      " 6.54050940e+01 2.94031646e+00 9.06687026e+01 1.74752990e+01\n",
      " 8.29072754e+01 7.94159244e+01 1.21488464e+02 7.78350179e+01\n",
      " 1.27567485e+02 7.96328209e+01 1.31404848e+02 8.07065000e+01]\n",
      "38-th iteration, loss: 0.1483528475810904, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8548349612438547e-06\n",
      "38-th iteration, new layer inserted. now 26 layers\n",
      "[1.59787491e+00 4.37286698e+01 0.00000000e+00 3.55271368e-15\n",
      " 7.69285734e+01 2.00828682e-05 6.24600765e-05 1.92313431e-05\n",
      " 5.38323491e-05 1.32165820e-05 4.25750955e-05 4.39675943e+01\n",
      " 8.95717894e+01 5.37524027e+01 6.54050957e+01 2.94031553e+00\n",
      " 9.06687011e+01 1.74752974e+01 8.29072758e+01 7.94159233e+01\n",
      " 1.21488464e+02 7.78350161e+01 1.27567485e+02 7.96328196e+01\n",
      " 1.31404848e+02 8.07064999e+01]\n",
      "39-th iteration, loss: 0.14835284758052503, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8295339947915261e-06\n",
      "39-th iteration, new layer inserted. now 26 layers\n",
      "[1.59787370e+00 4.37286680e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285771e+01 1.90557177e-05 6.41819618e-05 1.86340817e-05\n",
      " 5.54218723e-05 1.29898285e-05 4.40736539e-05 4.39675944e+01\n",
      " 8.95717869e+01 5.37524031e+01 6.54050974e+01 2.94031465e+00\n",
      " 9.06686995e+01 1.74752958e+01 8.29072763e+01 7.94159223e+01\n",
      " 1.21488464e+02 7.78350143e+01 1.27567485e+02 7.96328184e+01\n",
      " 1.31404847e+02 8.07064998e+01]\n",
      "40-th iteration, loss: 0.1483528475799677, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8067112082872515e-06\n",
      "40-th iteration, new layer inserted. now 26 layers\n",
      "[1.59787249e+00 4.37286661e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.69285808e+01 1.79893836e-05 6.58862181e-05 1.80094878e-05\n",
      " 5.69978772e-05 1.27466825e-05 4.55602547e-05 4.39675944e+01\n",
      " 8.95717845e+01 5.37524036e+01 6.54050991e+01 2.94031381e+00\n",
      " 9.06686980e+01 1.74752943e+01 8.29072767e+01 7.94159213e+01\n",
      " 1.21488464e+02 7.78350126e+01 1.27567485e+02 7.96328172e+01\n",
      " 1.31404847e+02 8.07064997e+01]\n",
      "41-th iteration, loss: 0.14835284757941772, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.78587801984608e-06\n",
      "41-th iteration, new layer inserted. now 24 layers\n",
      "[1.59787127e+00 4.37286643e+01 7.69285844e+01 1.68886808e-05\n",
      " 6.75754401e-05 1.73622548e-05 5.85631467e-05 1.24917444e-05\n",
      " 4.70377936e-05 4.39675945e+01 8.95717821e+01 5.37524042e+01\n",
      " 6.54051009e+01 2.94031302e+00 9.06686965e+01 1.74752927e+01\n",
      " 8.29072771e+01 7.94159203e+01 1.21488464e+02 7.78350109e+01\n",
      " 1.27567485e+02 7.96328160e+01 1.31404847e+02 8.07064996e+01]\n",
      "42-th iteration, loss: 0.14835284757891742, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7841225698923414e-06\n",
      "42-th iteration, new layer inserted. now 24 layers\n",
      "[1.59787005e+00 4.37286624e+01 7.69285862e+01 1.57559379e-05\n",
      " 6.92506429e-05 1.66946079e-05 6.01188518e-05 1.22271655e-05\n",
      " 4.85075227e-05 4.39675945e+01 8.95717796e+01 5.37524047e+01\n",
      " 6.54051027e+01 2.94031228e+00 9.06686950e+01 1.74752912e+01\n",
      " 8.29072775e+01 7.94159193e+01 1.21488464e+02 7.78350093e+01\n",
      " 1.27567485e+02 7.96328149e+01 1.31404846e+02 8.07064995e+01]\n",
      "43-th iteration, loss: 0.14835284757842232, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.796065861481366e-06\n",
      "43-th iteration, new layer inserted. now 26 layers\n",
      "[1.59786884e+00 4.37286605e+01 7.69285880e+01 1.46238899e-05\n",
      " 7.09311451e-05 1.60391857e-05 6.16844513e-05 1.19855185e-05\n",
      " 4.99889673e-05 4.39675946e+01 8.95717772e+01 5.37524053e+01\n",
      " 0.00000000e+00 1.24344979e-14 6.54051044e+01 2.94031157e+00\n",
      " 9.06686935e+01 1.74752897e+01 8.29072780e+01 7.94159183e+01\n",
      " 1.21488464e+02 7.78350077e+01 1.27567484e+02 7.96328138e+01\n",
      " 1.31404846e+02 8.07064995e+01]\n",
      "44-th iteration, loss: 0.14835284757788214, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8005272861793658e-06\n",
      "44-th iteration, new layer inserted. now 26 layers\n",
      "[1.59786763e+00 4.37286587e+01 7.69285898e+01 1.34818772e-05\n",
      " 7.26123423e-05 1.53853648e-05 6.32552569e-05 1.17562482e-05\n",
      " 5.14772811e-05 4.39675947e+01 8.95717749e+01 5.37524060e+01\n",
      " 1.80012404e-06 6.36745692e-07 6.54051062e+01 2.94031089e+00\n",
      " 9.06686921e+01 1.74752882e+01 8.29072784e+01 7.94159174e+01\n",
      " 1.21488464e+02 7.78350061e+01 1.27567484e+02 7.96328127e+01\n",
      " 1.31404845e+02 8.07064994e+01]\n",
      "45-th iteration, loss: 0.14835284757734457, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.804797870401796e-06\n",
      "45-th iteration, new layer inserted. now 26 layers\n",
      "[1.59786642e+00 4.37286568e+01 7.69285916e+01 1.23186237e-05\n",
      " 7.42916137e-05 1.47218742e-05 6.48286367e-05 1.15281193e-05\n",
      " 5.29697471e-05 4.39675949e+01 8.95717725e+01 5.37524066e+01\n",
      " 3.60330243e-06 1.28306627e-06 6.54051081e+01 2.94031022e+00\n",
      " 9.06686906e+01 1.74752867e+01 8.29072789e+01 7.94159165e+01\n",
      " 1.21488464e+02 7.78350045e+01 1.27567484e+02 7.96328117e+01\n",
      " 1.31404845e+02 8.07064994e+01]\n",
      "46-th iteration, loss: 0.14835284757680936, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8088822507688179e-06\n",
      "46-th iteration, new layer inserted. now 26 layers\n",
      "[1.59786522e+00 4.37286550e+01 7.69285933e+01 1.11344485e-05\n",
      " 7.59693546e-05 1.40490196e-05 6.64050527e-05 1.13014554e-05\n",
      " 5.44668195e-05 4.39675950e+01 8.95717702e+01 5.37524073e+01\n",
      " 5.40932823e-06 1.93849257e-06 6.54051099e+01 2.94030956e+00\n",
      " 9.06686892e+01 1.74752851e+01 8.29072793e+01 7.94159155e+01\n",
      " 1.21488464e+02 7.78350030e+01 1.27567484e+02 7.96328106e+01\n",
      " 1.31404845e+02 8.07064994e+01]\n",
      "47-th iteration, loss: 0.14835284757627634, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8127861247276308e-06\n",
      "47-th iteration, new layer inserted. now 28 layers\n",
      "[1.59786401e+00 4.37286532e+01 7.69285951e+01 9.92959244e-06\n",
      " 7.76459210e-05 1.33670312e-05 6.79849250e-05 1.10765075e-05\n",
      " 5.59689082e-05 4.39675952e+01 8.95717679e+01 5.37524080e+01\n",
      " 7.21800058e-06 2.60255325e-06 0.00000000e+00 7.41153829e-22\n",
      " 6.54051117e+01 2.94030891e+00 9.06686877e+01 1.74752836e+01\n",
      " 8.29072798e+01 7.94159146e+01 1.21488464e+02 7.78350015e+01\n",
      " 1.27567484e+02 7.96328096e+01 1.31404844e+02 8.07064994e+01]\n",
      "48-th iteration, loss: 0.14835284757569506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8040909423191355e-06\n",
      "48-th iteration, new layer inserted. now 30 layers\n",
      "[1.59786281e+00 4.37286513e+01 7.69285968e+01 8.69795145e-06\n",
      " 7.93186369e-05 1.26697970e-05 6.95656408e-05 1.08471873e-05\n",
      " 5.74733887e-05 4.39675953e+01 8.95717655e+01 5.37524086e+01\n",
      " 9.02380815e-06 3.26729074e-06 1.81101722e-06 6.64737493e-07\n",
      " 0.00000000e+00 1.32348898e-22 6.54051135e+01 2.94030825e+00\n",
      " 9.06686863e+01 1.74752821e+01 8.29072802e+01 7.94159137e+01\n",
      " 1.21488464e+02 7.78350001e+01 1.27567484e+02 7.96328087e+01\n",
      " 1.31404844e+02 8.07064994e+01]\n",
      "49-th iteration, loss: 0.14835284757506728, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.784404145307599e-06\n",
      "49-th iteration, new layer inserted. now 32 layers\n",
      "[1.59786161e+00 4.37286495e+01 7.69285986e+01 7.42594781e-06\n",
      " 8.09835928e-05 1.19437128e-05 7.11433954e-05 1.05998965e-05\n",
      " 5.89764863e-05 4.39675955e+01 8.95717632e+01 5.37524093e+01\n",
      " 1.08135419e-05 3.90936970e-06 3.60729123e-06 1.30319126e-06\n",
      " 1.79760464e-06 6.38453763e-07 0.00000000e+00 2.64697796e-23\n",
      " 6.54051153e+01 2.94030757e+00 9.06686848e+01 1.74752805e+01\n",
      " 8.29072806e+01 7.94159128e+01 1.21488464e+02 7.78349986e+01\n",
      " 1.27567485e+02 7.96328077e+01 1.31404844e+02 8.07064994e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536802874516853\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.49960647    0.         1186.65617243]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6283943711284979\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 42.96560419  62.31421067 231.54266779   0.         955.11350464]\n",
      "2-th iteration, loss: 0.6033146626312784, 13 gd steps\n",
      "insert gradient: -0.6169379414974001\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.44824945  77.80824476 217.42085927  42.48470941 233.90534807\n",
      "   0.         721.20815656]\n",
      "3-th iteration, loss: 0.46910151573583664, 19 gd steps\n",
      "insert gradient: -0.7496098430364014\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          58.27331189 222.55420119  57.57923385 162.51373327\n",
      "  46.72339594 618.17841991   0.         103.02973665]\n",
      "4-th iteration, loss: 0.3722518413401276, 13 gd steps\n",
      "insert gradient: -0.46208807736644114\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          52.35993497 235.44357053  62.27547329 143.43721499\n",
      "  54.29651501 212.72522006   0.         361.6328741   51.89595675\n",
      " 103.02973665]\n",
      "5-th iteration, loss: 0.30108939233994164, 19 gd steps\n",
      "insert gradient: -0.6146371735490441\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  4.12707952  59.92533581 235.32178398  55.89090179 129.85259747\n",
      "  50.81567289 161.93174338  61.45205544  90.11990983   0.\n",
      " 240.31975954  41.22801214 103.02973665]\n",
      "6-th iteration, loss: 0.2373455602168038, 48 gd steps\n",
      "insert gradient: -0.07704526809674091\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[1.28718325e-02 6.60883788e+01 2.26974726e+02 6.07254920e+01\n",
      " 1.09809022e+02 6.51440828e+01 1.65307421e+02 5.47705296e+01\n",
      " 7.13757423e+01 3.48581634e+01 1.09233161e+02 0.00000000e+00\n",
      " 8.73865288e+01 4.79826416e+01 1.03029737e+02]\n",
      "7-th iteration, loss: 0.23030478040580968, 21 gd steps\n",
      "insert gradient: -0.1019291480628043\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[1.33468861e-01 6.33600301e+01 1.29918559e+02 0.00000000e+00\n",
      " 1.01047768e+02 6.26402765e+01 1.10025458e+02 6.58814318e+01\n",
      " 1.59911744e+02 6.10467659e+01 8.03884874e+01 3.82716434e+01\n",
      " 7.36442976e+01 2.03279468e+01 5.56264119e+01 5.52415952e+01\n",
      " 1.03029737e+02]\n",
      "8-th iteration, loss: 0.22554525616791307, 21 gd steps\n",
      "insert gradient: -0.03072610436439112\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47388181  67.64355383 116.3088213    9.54385196  92.3042429\n",
      "  65.32764558 112.22166264  72.82187058 151.85658193  64.53636346\n",
      "  85.32823707  35.04865254  69.17776807  24.47860541  60.06774529\n",
      "  56.00302289 103.02973665]\n",
      "9-th iteration, loss: 0.2223837524916278, 19 gd steps\n",
      "insert gradient: -0.041680069149012455\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[2.41987939e-02 6.67084833e+01 1.13811701e+02 1.28805851e+01\n",
      " 9.17083705e+01 6.75147117e+01 1.11195716e+02 0.00000000e+00\n",
      " 3.90798505e-14 7.23768673e+01 1.53541776e+02 6.74898036e+01\n",
      " 9.02313956e+01 3.46856032e+01 6.18161123e+01 3.06140799e+01\n",
      " 6.08287471e+01 4.96043534e+01 1.03029737e+02]\n",
      "10-th iteration, loss: 0.2173441543192507, 17 gd steps\n",
      "insert gradient: -0.060127615058997386\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[0.00000000e+00 6.88859682e+01 1.14019536e+02 0.00000000e+00\n",
      " 1.42108547e-14 1.41958631e+01 9.20725842e+01 6.86465113e+01\n",
      " 1.12070957e+02 7.35820361e+01 1.55826089e+02 6.82566512e+01\n",
      " 9.79300312e+01 3.70526466e+01 4.70676475e+01 3.79380778e+01\n",
      " 6.49500872e+01 4.69938094e+01 1.03029737e+02]\n",
      "11-th iteration, loss: 0.20450520648360435, 24 gd steps\n",
      "insert gradient: -0.0931477067040553\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[1.57779671e+00 7.00832852e+01 1.18772557e+02 2.24593771e+01\n",
      " 8.97140311e+01 6.47868166e+01 1.12197924e+02 7.67470739e+01\n",
      " 1.57002528e+02 7.73420424e+01 1.02057022e+02 3.88539136e+01\n",
      " 3.06970285e+01 4.83844668e+01 8.27225296e+01 4.11873312e+01\n",
      " 1.03029737e+02 0.00000000e+00 1.24344979e-14]\n",
      "12-th iteration, loss: 0.16221562126277644, 127 gd steps\n",
      "insert gradient: -0.002156649587091054\n",
      "12-th iteration, new layer inserted. now 18 layers\n",
      "[  2.61788112  61.15362432 122.70127649  37.19534009  98.84411299\n",
      "  63.04738132 114.91167026  64.6850927  174.04476117  77.95415494\n",
      " 104.76254766  40.11683224  20.59788416  55.32013498  95.26265087\n",
      "  42.3731754   25.6283234   52.53172988]\n",
      "13-th iteration, loss: 0.16220997031065182, 88 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 1.4361704510072443e-18\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[  3.6092535   61.15484181 122.03006481  37.48920389  98.55379694\n",
      "  63.04530136 114.82786126  64.77108552 173.98355025  77.8838988\n",
      " 104.82111892  40.06202757  20.58490719  55.37343225  95.26473533\n",
      "  42.32422386  25.73049939  52.55052977   0.        ]\n",
      "14-th iteration, loss: 0.1622099585265021, 14 gd steps\n",
      "insert gradient: -9.395127970727248e-05\n",
      "14-th iteration, new layer inserted. now 20 layers\n",
      "[3.61326581e+00 6.11529524e+01 1.22026933e+02 3.74911323e+01\n",
      " 9.85529331e+01 6.30426866e+01 1.14827449e+02 6.47714777e+01\n",
      " 1.73982689e+02 7.78849668e+01 1.04821068e+02 4.00571796e+01\n",
      " 2.05783528e+01 0.00000000e+00 4.44089210e-16 5.53718508e+01\n",
      " 9.52672846e+01 4.23235956e+01 2.57286012e+01 5.25516259e+01]\n",
      "15-th iteration, loss: 0.16220994353047136, 30 gd steps\n",
      "insert gradient: -5.936593970783906e-05\n",
      "15-th iteration, new layer inserted. now 20 layers\n",
      "[3.64010509e+00 6.11511595e+01 1.22010994e+02 3.74993222e+01\n",
      " 9.85474542e+01 6.30402790e+01 1.14824982e+02 6.47732309e+01\n",
      " 1.73978337e+02 7.78837896e+01 1.04821974e+02 4.00592948e+01\n",
      " 2.05719379e+01 0.00000000e+00 2.22044605e-15 5.53745339e+01\n",
      " 9.52687992e+01 4.23194629e+01 2.57328497e+01 5.25519149e+01]\n",
      "16-th iteration, loss: 0.16220994110486775, 15 gd steps\n",
      "insert gradient: -2.148226094513783e-05\n",
      "16-th iteration, new layer inserted. now 18 layers\n",
      "[  3.64412666  61.15086553 122.00853929  37.50016128  98.54642965\n",
      "  63.03966594 114.82454967  64.77391609 173.97800957  77.88379075\n",
      " 104.82229083  40.05891203  20.57059633  55.37583904  95.26937145\n",
      "  42.319604    25.73343166  52.55221939]\n",
      "17-th iteration, loss: 0.1622099362742523, 12 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 4.554591494127719e-17\n",
      "17-th iteration, new layer inserted. now 20 layers\n",
      "[3.66267261e+00 6.11504022e+01 1.22000242e+02 3.75032482e+01\n",
      " 9.85424916e+01 6.30406120e+01 1.14824065e+02 6.47746929e+01\n",
      " 1.73976826e+02 7.78829021e+01 1.04822752e+02 4.00599057e+01\n",
      " 2.05690644e+01 5.53768388e+01 9.52687850e+01 4.23181214e+01\n",
      " 2.57364905e+01 5.25514350e+01 0.00000000e+00 5.32907052e-15]\n",
      "18-th iteration, loss: 0.16220993045982254, 27 gd steps\n",
      "insert gradient: -1.5092481294708497e-05\n",
      "18-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71230089  61.14169787 121.98948986  37.5095972   98.53552665\n",
      "  63.04054043 114.82268372  64.77571758 173.97723743  77.88106808\n",
      " 104.8239754   40.06011253  20.56636591  55.37716917  95.26840228\n",
      "  42.31635708  25.73802454  52.55114474]\n",
      "19-th iteration, loss: 0.16220993029110928, 12 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.530231197967336e-07\n",
      "19-th iteration, new layer inserted. now 20 layers\n",
      "[3.71272993e+00 6.11415710e+01 1.21989161e+02 0.00000000e+00\n",
      " 2.84217094e-14 3.75096443e+01 9.85356378e+01 6.30402768e+01\n",
      " 1.14822680e+02 6.47754397e+01 1.73976860e+02 7.78816506e+01\n",
      " 1.04824078e+02 4.00596892e+01 2.05659022e+01 5.53779267e+01\n",
      " 9.52691512e+01 4.23166698e+01 2.57379632e+01 5.25513977e+01]\n",
      "20-th iteration, loss: 0.16220993028917202, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4709196185921157e-07\n",
      "20-th iteration, new layer inserted. now 20 layers\n",
      "[3.71273168e+00 6.11415696e+01 1.21989159e+02 0.00000000e+00\n",
      " 2.48689958e-14 3.75096455e+01 9.85356379e+01 6.30402746e+01\n",
      " 1.14822679e+02 6.47754363e+01 1.73976857e+02 7.78816504e+01\n",
      " 1.04824076e+02 4.00596838e+01 2.05658975e+01 5.53779219e+01\n",
      " 9.52691490e+01 4.23166649e+01 2.57379602e+01 5.25513945e+01]\n",
      "21-th iteration, loss: 0.16220993028751976, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.416967277421657e-07\n",
      "21-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71273345  61.14156833 121.98915804  37.50964658  98.53563801\n",
      "  63.04027244 114.82267879  64.77543314 173.9768543   77.88165031\n",
      " 104.82407417  40.05967878  20.56589315  55.37791767  95.2691471\n",
      "  42.3166604   25.73795736  52.55139152]\n",
      "22-th iteration, loss: 0.16220993028610786, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.455646034628379e-07\n",
      "22-th iteration, new layer inserted. now 20 layers\n",
      "[3.71273523e+00 6.11415672e+01 1.21989157e+02 0.00000000e+00\n",
      " 1.06581410e-14 3.75096471e+01 9.85356381e+01 6.30402704e+01\n",
      " 1.14822678e+02 6.47754303e+01 1.73976852e+02 7.78816503e+01\n",
      " 1.04824073e+02 4.00596741e+01 2.05658891e+01 5.53779140e+01\n",
      " 9.52691454e+01 4.23166562e+01 2.57379548e+01 5.25513888e+01]\n",
      "23-th iteration, loss: 0.1622099302848915, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4065459039026164e-07\n",
      "23-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71273704  61.14156609 121.98915514  37.50964819  98.53563829\n",
      "  63.04026839 114.82267753  64.77542775 173.97684907  77.88165046\n",
      " 104.82407113  40.05966986  20.5658852   55.37791069  95.26914397\n",
      "  42.31665232  25.73795234  52.55138626]\n",
      "24-th iteration, loss: 0.16220993028384317, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.447710833861263e-07\n",
      "24-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71273886  61.14156509 121.98915368  37.50964872  98.53563845\n",
      "  63.04026647 114.82267693  64.77542545 173.97684661  77.88165066\n",
      " 104.82406983  40.0596659   20.56588157  55.37790784  95.26914271\n",
      "  42.31664871  25.73795009  52.55138393]\n",
      "25-th iteration, loss: 0.16220993028293298, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4882330691441775e-07\n",
      "25-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71274069  61.14156417 121.9891522   37.50964924  98.53563864\n",
      "  63.04026463 114.82267636  64.77542338 173.97684423  77.88165093\n",
      " 104.82406865  40.05966224  20.56587815  55.37790537  95.26914164\n",
      "  42.31664534  25.73794799  52.55138177]\n",
      "26-th iteration, loss: 0.16220993028213812, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.5281901339277513e-07\n",
      "26-th iteration, new layer inserted. now 20 layers\n",
      "[3.71274254e+00 6.11415633e+01 1.21989151e+02 0.00000000e+00\n",
      " 1.06581410e-14 3.75096498e+01 9.85356388e+01 6.30402629e+01\n",
      " 1.14822676e+02 6.47754215e+01 1.73976842e+02 7.78816513e+01\n",
      " 1.04824068e+02 4.00596588e+01 2.05658749e+01 5.53779032e+01\n",
      " 9.52691407e+01 4.23166422e+01 2.57379460e+01 5.25513798e+01]\n",
      "27-th iteration, loss: 0.16220993028143685, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.482676361882647e-07\n",
      "27-th iteration, new layer inserted. now 18 layers\n",
      "[  3.7127444   61.14156256 121.98914924  37.5096508   98.53563906\n",
      "  63.04026113 114.82267526  64.77541985 173.97683971  77.88165166\n",
      " 104.82406662  40.05965568  20.56587185  55.37790144  95.26913996\n",
      "  42.31663928  25.7379442   52.55137792]\n",
      "28-th iteration, loss: 0.1622099302808188, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.525237972594226e-07\n",
      "28-th iteration, new layer inserted. now 20 layers\n",
      "[3.71274627e+00 6.11415618e+01 1.21989148e+02 0.00000000e+00\n",
      " 2.84217094e-14 3.75096513e+01 9.85356393e+01 6.30402595e+01\n",
      " 1.14822675e+02 6.47754183e+01 1.73976838e+02 7.78816521e+01\n",
      " 1.04824066e+02 4.00596527e+01 2.05658689e+01 5.53778999e+01\n",
      " 9.52691393e+01 4.23166366e+01 2.57379425e+01 5.25513762e+01]\n",
      "29-th iteration, loss: 0.16220993028026534, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.826180107038306e-07\n",
      "29-th iteration, new layer inserted. now 20 layers\n",
      "[3.71274815e+00 6.11415612e+01 1.21989146e+02 3.75096523e+01\n",
      " 9.85356395e+01 6.30402578e+01 1.14822674e+02 6.47754170e+01\n",
      " 1.73976835e+02 0.00000000e+00 4.97379915e-14 7.78816526e+01\n",
      " 1.04824065e+02 4.00596500e+01 2.05658662e+01 5.53778986e+01\n",
      " 9.52691388e+01 4.23166340e+01 2.57379409e+01 5.25513746e+01]\n",
      "30-th iteration, loss: 0.1622099302797681, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.10701800857378e-07\n",
      "30-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71275005  61.14156056 121.98914475  37.50965285  98.5356398\n",
      "  63.04025627 114.82267372  64.7754158  173.9768334   77.88165365\n",
      " 104.82406428  40.05964747  20.56586357  55.37789762  95.26913845\n",
      "  42.31663162  25.7379394   52.55137315]\n",
      "31-th iteration, loss: 0.16220993027932049, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.443266999754706e-07\n",
      "31-th iteration, new layer inserted. now 20 layers\n",
      "[3.71275195e+00 6.11415600e+01 1.21989143e+02 3.75096534e+01\n",
      " 9.85356401e+01 6.30402547e+01 1.14822673e+02 6.47754147e+01\n",
      " 1.73976831e+02 0.00000000e+00 1.77635684e-14 7.78816542e+01\n",
      " 1.04824064e+02 4.00596451e+01 2.05658611e+01 5.53778968e+01\n",
      " 9.52691382e+01 4.23166294e+01 2.57379380e+01 5.25513718e+01]\n",
      "32-th iteration, loss: 0.16220993027890887, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.633693338450039e-07\n",
      "32-th iteration, new layer inserted. now 20 layers\n",
      "[3.71275385e+00 6.11415595e+01 1.21989142e+02 3.75096539e+01\n",
      " 9.85356403e+01 6.30402532e+01 1.14822673e+02 6.47754137e+01\n",
      " 1.73976829e+02 0.00000000e+00 3.55271368e-15 7.78816554e+01\n",
      " 1.04824063e+02 4.00596429e+01 2.05658587e+01 5.53778962e+01\n",
      " 9.52691380e+01 4.23166273e+01 2.57379367e+01 5.25513705e+01]\n",
      "33-th iteration, loss: 0.1622099302785299, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.789273498148371e-07\n",
      "33-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71275577  61.14155899 121.98914024  37.50965438  98.53564063\n",
      "  63.04025176 114.82267227  64.77541287 173.97682753  77.88165657\n",
      " 104.82406255  40.05964079  20.56585638  55.37789574  95.26913786\n",
      "  42.31662533  25.73793547  52.55136933]\n",
      "34-th iteration, loss: 0.1622099302781827, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.02438221378904e-07\n",
      "34-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71275769  61.14155854 121.98913873  37.50965489  98.53564092\n",
      "  63.04025032 114.82267179  64.77541209 173.97682565  77.88165718\n",
      " 104.82406208  40.05963883  20.56585419  55.37789545  95.26913783\n",
      "  42.31662349  25.73793432  52.55136823]\n",
      "35-th iteration, loss: 0.16220993027785918, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.229477325561205e-07\n",
      "35-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71275962  61.14155812 121.98913722  37.5096554   98.53564123\n",
      "  63.04024892 114.82267133  64.77541139 173.9768238   77.88165781\n",
      " 104.82406165  40.05963699  20.56585208  55.37789531  95.26913787\n",
      "  42.31662175  25.73793324  52.55136721]\n",
      "36-th iteration, loss: 0.16220993027755568, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.407575051356915e-07\n",
      "36-th iteration, new layer inserted. now 20 layers\n",
      "[3.71276156e+00 6.11415577e+01 1.21989136e+02 3.75096559e+01\n",
      " 9.85356415e+01 6.30402475e+01 1.14822671e+02 6.47754108e+01\n",
      " 1.73976822e+02 0.00000000e+00 2.48689958e-14 7.78816585e+01\n",
      " 1.04824061e+02 4.00596353e+01 2.05658500e+01 5.53778953e+01\n",
      " 9.52691380e+01 4.23166201e+01 2.57379322e+01 5.25513663e+01]\n",
      "37-th iteration, loss: 0.1622099302772639, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.441729728742602e-07\n",
      "37-th iteration, new layer inserted. now 20 layers\n",
      "[3.71276350e+00 6.11415574e+01 1.21989134e+02 3.75096564e+01\n",
      " 9.85356419e+01 6.30402462e+01 1.14822670e+02 6.47754102e+01\n",
      " 1.73976820e+02 0.00000000e+00 3.55271368e-15 7.78816598e+01\n",
      " 1.04824061e+02 4.00596336e+01 2.05658481e+01 5.53778954e+01\n",
      " 9.52691381e+01 4.23166186e+01 2.57379313e+01 5.25513654e+01]\n",
      "38-th iteration, loss: 0.16220993027698613, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.458713175582125e-07\n",
      "38-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71276544  61.14155707 121.9891327   37.50965696  98.53564219\n",
      "  63.04024486 114.82266997  64.77540975 173.97681842  77.88166114\n",
      " 104.82406062  40.05963208  20.56584619  55.37789565  95.26913834\n",
      "  42.31661711  25.73793038  52.55136455]\n",
      "39-th iteration, loss: 0.16220993027672548, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.580738406678582e-07\n",
      "39-th iteration, new layer inserted. now 20 layers\n",
      "[3.71276739e+00 6.11415568e+01 1.21989131e+02 3.75096575e+01\n",
      " 9.85356425e+01 6.30402435e+01 1.14822670e+02 6.47754093e+01\n",
      " 1.73976817e+02 0.00000000e+00 2.48689958e-14 7.78816618e+01\n",
      " 1.04824060e+02 4.00596306e+01 2.05658444e+01 5.53778960e+01\n",
      " 9.52691386e+01 4.23166157e+01 2.57379295e+01 5.25513638e+01]\n",
      "40-th iteration, loss: 0.16220993027646985, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.562910333860547e-07\n",
      "40-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71276935  61.14155649 121.98912969  37.50965802  98.53564287\n",
      "  63.04024225 114.82266909  64.77540894 173.97681494  77.88166317\n",
      " 104.82406008  40.05962922  20.56584258  55.37789639  95.2691389\n",
      "  42.31661442  25.73792875  52.55136306]\n",
      "41-th iteration, loss: 0.16220993027622832, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.655645196075009e-07\n",
      "41-th iteration, new layer inserted. now 18 layers\n",
      "[  3.7127713   61.14155623 121.98912819  37.50965855  98.53564322\n",
      "  63.04024098 114.82266865  64.77540859 173.97681323  77.88166385\n",
      " 104.82405984  40.0596279   20.56584085  55.37789688  95.26913925\n",
      "  42.31661318  25.737928    52.55136238]\n",
      "42-th iteration, loss: 0.1622099302759944, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.733585210689777e-07\n",
      "42-th iteration, new layer inserted. now 20 layers\n",
      "[3.71277327e+00 6.11415560e+01 1.21989127e+02 3.75096591e+01\n",
      " 9.85356436e+01 6.30402397e+01 1.14822668e+02 6.47754083e+01\n",
      " 1.73976812e+02 0.00000000e+00 3.55271368e-15 7.78816645e+01\n",
      " 1.04824060e+02 4.00596266e+01 2.05658392e+01 5.53778974e+01\n",
      " 9.52691396e+01 4.23166120e+01 2.57379273e+01 5.25513618e+01]\n",
      "43-th iteration, loss: 0.16220993027576133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.745013425219334e-07\n",
      "43-th iteration, new layer inserted. now 20 layers\n",
      "[3.71277523e+00 6.11415558e+01 1.21989125e+02 3.75096596e+01\n",
      " 9.85356439e+01 6.30402385e+01 1.14822668e+02 6.47754080e+01\n",
      " 1.73976810e+02 7.78816659e+01 1.04824059e+02 4.00596254e+01\n",
      " 2.05658375e+01 0.00000000e+00 2.22044605e-15 5.53778981e+01\n",
      " 9.52691400e+01 4.23166109e+01 2.57379266e+01 5.25513612e+01]\n",
      "44-th iteration, loss: 0.1622099302755334, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.10307760392003e-07\n",
      "44-th iteration, new layer inserted. now 18 layers\n",
      "[  3.7127772   61.14155558 121.98912369  37.50966016  98.53564429\n",
      "  63.04023727 114.82266735  64.77540782 173.97680819  77.88166661\n",
      " 104.82405926  40.05962428  20.56583595  55.37789947  95.26914047\n",
      "  42.31660982  25.73792601  52.55136061]\n",
      "45-th iteration, loss: 0.1622099302753155, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.634001581944488e-07\n",
      "45-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71277917  61.14155539 121.98912219  37.50966071  98.53564465\n",
      "  63.04023606 114.82266692  64.77540762 173.97680653  77.88166729\n",
      " 104.8240591   40.05962316  20.56583438  55.3779002   95.26914092\n",
      "  42.31660879  25.73792542  52.55136009]\n",
      "46-th iteration, loss: 0.16220993027510208, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.112362567362483e-07\n",
      "46-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71278114  61.14155522 121.98912069  37.50966125  98.53564502\n",
      "  63.04023486 114.8226665   64.77540745 173.97680489  77.88166799\n",
      " 104.82405895  40.05962208  20.56583285  55.37790098  95.2691414\n",
      "  42.31660781  25.73792485  52.55135959]\n",
      "47-th iteration, loss: 0.16220993027489247, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.542812482829705e-07\n",
      "47-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71278312  61.14155506 121.9891192   37.5096618   98.53564539\n",
      "  63.04023368 114.82266608  64.77540732 173.97680326  77.88166868\n",
      " 104.82405881  40.05962105  20.56583135  55.3779018   95.2691419\n",
      "  42.31660687  25.73792432  52.55135913]\n",
      "48-th iteration, loss: 0.1622099302746862, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.929595567022626e-07\n",
      "48-th iteration, new layer inserted. now 20 layers\n",
      "[3.71278509e+00 6.11415549e+01 1.21989118e+02 3.75096623e+01\n",
      " 9.85356458e+01 6.30402325e+01 1.14822666e+02 6.47754072e+01\n",
      " 1.73976802e+02 7.78816694e+01 1.04824059e+02 4.00596201e+01\n",
      " 2.05658299e+01 0.00000000e+00 5.32907052e-15 5.53779027e+01\n",
      " 9.52691424e+01 4.23166060e+01 2.57379238e+01 5.25513587e+01]\n",
      "49-th iteration, loss: 0.16220993027447272, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.000965390729163e-07\n",
      "49-th iteration, new layer inserted. now 18 layers\n",
      "[  3.71278707  61.14155477 121.98911622  37.5096629   98.53564615\n",
      "  63.04023135 114.82266524  64.77540712 173.97680003  77.88167008\n",
      " 104.82405858  40.0596191   20.56582844  55.37790448  95.26914296\n",
      "  42.3166051   25.73792334  52.55135828]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010613\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.4077617     0.         1210.87364534]\n",
      "1-th iteration, loss: 0.7509319290812664, 11 gd steps\n",
      "insert gradient: -0.6377633590055645\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[ 43.62107906  62.23800861 236.26802836   0.         974.60561698]\n",
      "2-th iteration, loss: 0.6040837223740461, 13 gd steps\n",
      "insert gradient: -0.6858645253158995\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  2.92976919  77.02217371 221.11552931  41.39093476 238.67892661\n",
      "   0.         735.92669037]\n",
      "3-th iteration, loss: 0.46984585699716536, 19 gd steps\n",
      "insert gradient: -0.7298002104901758\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          59.88061067 225.71978204  56.60341419 163.49855092\n",
      "  48.0632083  630.79430603   0.         105.13238434]\n",
      "4-th iteration, loss: 0.37779017256748904, 13 gd steps\n",
      "insert gradient: -0.5075308872541772\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          51.3121746  237.55435359  66.25969797 147.24345159\n",
      "  52.91672386 216.06722123   0.         367.31427609  46.60075587\n",
      " 105.13238434]\n",
      "5-th iteration, loss: 0.2775161053980338, 73 gd steps\n",
      "insert gradient: -0.4001985576506757\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          67.49417421 232.50165068  56.77518539 112.79435815\n",
      "  60.571476   174.28519921  46.92983645 110.08229867   0.\n",
      " 235.89064     44.16352682 105.13238434]\n",
      "6-th iteration, loss: 0.23742326631423208, 50 gd steps\n",
      "insert gradient: -0.07808461445071453\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          66.15242867 228.22166014  60.78574579 110.23200355\n",
      "  65.18962009 166.10075561  54.97869502  72.4520619   34.42371319\n",
      " 109.9838606    0.          87.98708848  47.64931309 105.13238434]\n",
      "7-th iteration, loss: 0.22859653151762813, 32 gd steps\n",
      "insert gradient: -0.07874654660042117\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[9.06962509e-02 6.36462318e+01 1.18185134e+02 0.00000000e+00\n",
      " 1.18185134e+02 6.19124161e+01 1.10634857e+02 6.89096342e+01\n",
      " 1.58671701e+02 6.28991815e+01 8.28172165e+01 3.69044554e+01\n",
      " 7.34505982e+01 2.30409624e+01 5.51281432e+01 5.45581543e+01\n",
      " 1.05132384e+02]\n",
      "8-th iteration, loss: 0.22376284719590242, 20 gd steps\n",
      "insert gradient: -0.04480023982222743\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  0.          68.5217881  108.95196327  11.6867133   95.78681564\n",
      "  65.15857791 113.41818315  72.58619228 152.82715874  68.75517742\n",
      "  85.92522542  34.78419543  66.14835155  27.9361779   56.45879167\n",
      "  55.73542251 105.13238434]\n",
      "9-th iteration, loss: 0.21912466557875476, 24 gd steps\n",
      "insert gradient: -0.10333083183584899\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[1.00409822e+00 6.92550663e+01 1.13625947e+02 0.00000000e+00\n",
      " 2.48689958e-14 1.53009281e+01 9.13550990e+01 6.73298593e+01\n",
      " 1.15077288e+02 7.81046450e+01 1.53832050e+02 6.70476508e+01\n",
      " 9.72860668e+01 3.58960005e+01 4.73993710e+01 3.75613802e+01\n",
      " 6.58553121e+01 5.65884335e+01 1.05132384e+02]\n",
      "10-th iteration, loss: 0.2063942092809173, 20 gd steps\n",
      "insert gradient: -0.0737567966218283\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[6.99604036e-01 6.93174412e+01 1.15280486e+02 3.01286719e+00\n",
      " 1.62453744e-02 1.84089016e+01 9.03581470e+01 6.80223558e+01\n",
      " 1.13270792e+02 7.75009669e+01 1.56012808e+02 7.10577245e+01\n",
      " 1.01765859e+02 3.84059657e+01 3.41203991e+01 4.91094700e+01\n",
      " 7.66845851e+01 4.26918664e+01 1.05132384e+02]\n",
      "11-th iteration, loss: 0.1975282911367495, 18 gd steps\n",
      "insert gradient: -0.14090007209564984\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  1.78397757  70.13125439 122.24254115  25.11178186  87.58814505\n",
      "  64.79069715 112.93588185  71.90545045 157.79236296  84.18618376\n",
      " 102.35215033  45.52223657  26.12142539  46.98261445  90.34520557\n",
      "  47.19392414  97.62292831   0.           7.50945602]\n",
      "12-th iteration, loss: 0.18291280767247642, 20 gd steps\n",
      "insert gradient: -0.03155989939769888\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[  1.15579621  67.45293938 124.7333111   28.30516051  85.30507142\n",
      "  63.95765418 121.94560076  63.05784005 161.12915503  89.42596577\n",
      " 104.62818281  51.25439866  23.99943566  41.81289843  96.5482966\n",
      "  50.74636113  89.69914414  15.07775729   7.50945602]\n",
      "13-th iteration, loss: 0.17580625997742405, 22 gd steps\n",
      "insert gradient: -0.03855850180503566\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          74.11918316 112.85490003  32.25785284  83.51901892\n",
      "  73.52354694 109.80920782  63.18728691 155.70543078  95.77470573\n",
      " 105.42066492  56.37915326  16.98647898  42.73429143 101.55006081\n",
      "  55.87920733  74.83762691  21.34775345   7.50945602]\n",
      "14-th iteration, loss: 0.1726333473874254, 23 gd steps\n",
      "insert gradient: -0.02245404704495902\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.01305851e+01 9.42840231e+01 3.49258815e+01\n",
      " 9.23821607e+01 0.00000000e+00 1.24344979e-14 7.43084811e+01\n",
      " 1.07904674e+02 6.47095948e+01 1.52953574e+02 9.68424161e+01\n",
      " 1.04569079e+02 5.84312445e+01 1.48122022e+01 4.57241151e+01\n",
      " 1.01109839e+02 5.55041039e+01 7.30889802e+01 2.24549387e+01\n",
      " 7.50945602e+00]\n",
      "15-th iteration, loss: 0.17195048084600373, 20 gd steps\n",
      "insert gradient: -0.005712275895422578\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          81.46110071  89.36179523  36.0747875   93.71614554\n",
      "   0.16001774   0.4178275   74.96003102 106.77738234  66.08907564\n",
      " 152.56321697  97.72588018 100.34592549  59.80118505  13.80027134\n",
      "  46.6059891  102.57129538  54.8350515   72.02209549  21.6040323\n",
      "   7.50945602]\n",
      "16-th iteration, loss: 0.17159130808445505, 292 gd steps\n",
      "insert gradient: -0.00040464446286723793\n",
      "16-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.35418558e+01 8.25701159e+01 0.00000000e+00\n",
      " 8.88178420e-15 3.81508678e+01 9.55845254e+01 7.55360210e+01\n",
      " 1.06406203e+02 6.59326395e+01 1.51159208e+02 9.87173396e+01\n",
      " 1.02123107e+02 6.05634842e+01 1.32211986e+01 4.71657316e+01\n",
      " 1.01690820e+02 5.50084275e+01 6.97003031e+01 2.21620273e+01\n",
      " 7.50945602e+00]\n",
      "17-th iteration, loss: 0.17155652893536677, 203 gd steps\n",
      "insert gradient: -0.0002523179926110187\n",
      "17-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.01057229  80.66038835  38.89739236  95.73506726\n",
      "  75.47455578 106.25012224  66.13654535 150.8658894   98.81793654\n",
      " 101.86902753  60.64086787  12.75026869  47.60839279 101.70019193\n",
      "  54.97843366  68.59848652  22.52400944   7.50945602]\n",
      "18-th iteration, loss: 0.17154855562634175, 999 gd steps\n",
      "insert gradient: -0.0002006781980355697\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.11204497  80.15400727  39.07749463  95.83888044\n",
      "  75.42773019 106.22732547  66.17878186 150.89144084  98.75215614\n",
      " 101.82459313  60.57227964  12.58135692  47.81843083 101.66906461\n",
      "  55.01381605  68.06612112  22.71971486   7.50945602]\n",
      "19-th iteration, loss: 0.17154625675499963, 999 gd steps\n",
      "insert gradient: -0.00020384927140679695\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.14109996  80.0205096   39.13468853  95.87146627\n",
      "  75.41152548 106.21941908  66.18146942 150.91770612  98.73218293\n",
      " 101.8106186   60.54647456  12.52548196  47.89169883 101.6609587\n",
      "  55.02553465  67.88248016  22.77708866   7.50945602]\n",
      "20-th iteration, loss: 0.17154488382118913, 999 gd steps\n",
      "insert gradient: -0.00020461770405245992\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.41573136e+01 7.99445934e+01 3.91682134e+01\n",
      " 9.58914169e+01 7.54013999e+01 1.06215053e+02 6.61818706e+01\n",
      " 1.50937204e+02 9.87188197e+01 1.01801562e+02 6.05296696e+01\n",
      " 1.24909105e+01 0.00000000e+00 3.10862447e-15 4.79374428e+01\n",
      " 1.01656191e+02 5.50330921e+01 6.77680979e+01 2.28126836e+01\n",
      " 7.50945602e+00]\n",
      "21-th iteration, loss: 0.17154334288317463, 999 gd steps\n",
      "insert gradient: -0.00015869015421240122\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.41730909e+01 7.98672065e+01 3.92047169e+01\n",
      " 9.59140538e+01 7.53907753e+01 1.06210557e+02 6.61816588e+01\n",
      " 1.50960969e+02 9.87033195e+01 1.01789150e+02 6.05014980e+01\n",
      " 1.24447862e+01 4.80090194e+01 1.01646740e+02 5.50389098e+01\n",
      " 6.76418985e+01 0.00000000e+00 1.24344979e-14 2.28501595e+01\n",
      " 7.50945602e+00]\n",
      "22-th iteration, loss: 0.1715429447998307, 294 gd steps\n",
      "insert gradient: -0.00017761850993448395\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.41764776e+01 7.98471613e+01 3.92134054e+01\n",
      " 9.59196633e+01 7.53872663e+01 1.06209444e+02 6.61816901e+01\n",
      " 1.50967852e+02 9.86989038e+01 1.01786439e+02 6.04957608e+01\n",
      " 1.24333662e+01 0.00000000e+00 3.10862447e-15 4.80197713e+01\n",
      " 1.01645402e+02 5.50417324e+01 6.76079555e+01 2.28673275e+01\n",
      " 7.50945602e+00]\n",
      "23-th iteration, loss: 0.17154179168749398, 862 gd steps\n",
      "insert gradient: -0.00015249778719847101\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.41872698e+01 7.97921717e+01 3.92402553e+01\n",
      " 9.59369562e+01 7.53783007e+01 1.06206293e+02 6.61808449e+01\n",
      " 1.50988548e+02 9.86855914e+01 1.01777094e+02 6.04755000e+01\n",
      " 1.23986549e+01 4.80722379e+01 1.01639117e+02 5.50472810e+01\n",
      " 6.75092348e+01 0.00000000e+00 2.30926389e-14 2.28929069e+01\n",
      " 7.50945602e+00]\n",
      "24-th iteration, loss: 0.1715414475538603, 262 gd steps\n",
      "insert gradient: -0.0001711377218217615\n",
      "24-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.19007698  79.77573613  39.24767253  95.94192492\n",
      "  75.37516547 106.20535397  66.18064085 150.99525372  98.68152527\n",
      " 101.77463205  60.47055801  12.38890647  48.08134068 101.63797508\n",
      "  55.04979192  67.47905025  22.9077927    7.50945602]\n",
      "25-th iteration, loss: 0.17154074847517364, 585 gd steps\n",
      "insert gradient: -0.00018938137836493183\n",
      "25-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.19661274  79.74213828  39.26365809  95.9525767\n",
      "  75.36871428 106.20341196  66.17964662 151.00975166  98.67255475\n",
      " 101.76958873  60.46199242  12.37063488  48.10365464 101.6362262\n",
      "  55.05515291  67.41514984  22.92444306   7.50945602]\n",
      "26-th iteration, loss: 0.17154009658347952, 553 gd steps\n",
      "insert gradient: -0.0001916121164599499\n",
      "26-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42029253e+01 7.97117480e+01 3.92784893e+01\n",
      " 9.59626813e+01 7.53631050e+01 1.06201665e+02 6.61783904e+01\n",
      " 1.51023817e+02 9.86643097e+01 1.01764624e+02 6.04537454e+01\n",
      " 1.23541823e+01 0.00000000e+00 5.32907052e-15 4.81252629e+01\n",
      " 1.01634340e+02 5.50597008e+01 6.73554588e+01 2.29419865e+01\n",
      " 7.50945602e+00]\n",
      "27-th iteration, loss: 0.17153919442503066, 702 gd steps\n",
      "insert gradient: -0.00015750793091957372\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42108125e+01 7.96728833e+01 3.92983015e+01\n",
      " 9.59766297e+01 7.53565793e+01 1.06199583e+02 6.61765617e+01\n",
      " 1.51042853e+02 9.86532104e+01 1.01756610e+02 6.04370562e+01\n",
      " 1.23276349e+01 4.81679560e+01 1.01629292e+02 5.50638136e+01\n",
      " 6.72760769e+01 0.00000000e+00 2.30926389e-14 2.29649022e+01\n",
      " 7.50945602e+00]\n",
      "28-th iteration, loss: 0.17153889347507978, 234 gd steps\n",
      "insert gradient: -0.00016949848151885313\n",
      "28-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.212928    79.65978491  39.30433901  95.98106\n",
      "  75.3538672  106.19889666  66.1760881  151.04953285  98.64937621\n",
      " 101.75428909  60.43239538  12.31903802  48.17597157 101.62832245\n",
      "  55.06605327  67.24906223  22.97866542   7.50945602]\n",
      "29-th iteration, loss: 0.1715383056886766, 505 gd steps\n",
      "insert gradient: -0.0001883925961189209\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42176669e+01 7.96339100e+01 3.93169305e+01\n",
      " 9.59901693e+01 7.53482876e+01 1.06197496e+02 6.61747286e+01\n",
      " 1.51063298e+02 9.86412037e+01 1.01749791e+02 6.04246498e+01\n",
      " 1.23034506e+01 0.00000000e+00 5.32907052e-15 4.81949007e+01\n",
      " 1.01627029e+02 5.50708366e+01 6.71939957e+01 2.29931226e+01\n",
      " 7.50945602e+00]\n",
      "30-th iteration, loss: 0.17153748098703392, 655 gd steps\n",
      "insert gradient: -0.00015646281740756617\n",
      "30-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.22430218  79.60065564  39.33458031  96.00311707\n",
      "  75.34199214 106.19574916  66.17263371 151.08211966  98.63039848\n",
      " 101.74232518  60.4093263   12.27927825  48.23395573 101.6226596\n",
      "  55.07486887  67.12012779  23.01388256   7.50945602]\n",
      "31-th iteration, loss: 0.17153691693503165, 486 gd steps\n",
      "insert gradient: -0.00018245743930888746\n",
      "31-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42285505e+01 7.95772085e+01 3.93464377e+01\n",
      " 9.60119929e+01 7.53364181e+01 1.06194256e+02 6.61710855e+01\n",
      " 1.51096176e+02 9.86227254e+01 1.01737652e+02 6.04009455e+01\n",
      " 1.22641759e+01 0.00000000e+00 5.32907052e-15 4.82510025e+01\n",
      " 1.01620929e+02 5.50793807e+01 6.70672287e+01 2.30302650e+01\n",
      " 7.50945602e+00]\n",
      "32-th iteration, loss: 0.17153613394734693, 631 gd steps\n",
      "insert gradient: -0.00016290009860821808\n",
      "32-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.23426813  79.54711133  39.36251172  96.02433263\n",
      "  75.33026005 106.1927042   66.16882979 151.11495468  98.61210946\n",
      " 101.73049379  60.38627456  12.24125524  48.28747983 101.61695496\n",
      "  55.08363444  66.99657461  23.05141481   7.50945602]\n",
      "33-th iteration, loss: 0.17153557917235412, 480 gd steps\n",
      "insert gradient: -0.00017994576028710065\n",
      "33-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42380850e+01 7.95251685e+01 3.93737212e+01\n",
      " 9.60330966e+01 7.53247352e+01 1.06191369e+02 6.61671264e+01\n",
      " 1.51129356e+02 9.86042556e+01 1.01725874e+02 6.03779889e+01\n",
      " 1.22264166e+01 0.00000000e+00 3.55271368e-15 4.83040093e+01\n",
      " 1.01615382e+02 5.50882049e+01 6.69441874e+01 2.30680344e+01\n",
      " 7.50945602e+00]\n",
      "34-th iteration, loss: 0.1715348079418679, 626 gd steps\n",
      "insert gradient: -0.00016557620013440933\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.24331027  79.49694795  39.38903564  96.04530255\n",
      "  75.31859505 106.18999873  66.16467747 151.14859139  98.5934133\n",
      " 101.71880252  60.36355531  12.20393274  48.33959838 101.61165985\n",
      "  55.09252976  66.87406989  23.08932226   7.50945602]\n",
      "35-th iteration, loss: 0.17153425081731824, 483 gd steps\n",
      "insert gradient: -0.00017845694445525127\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.24686645  79.4759868   39.399925    96.05413451\n",
      "  75.31301104 106.1887675   66.16280221 151.16356547  98.5852689\n",
      " 101.71414079  60.35522836  12.18911937  48.35604319 101.61020483\n",
      "  55.09717196  66.82114829  23.10622934   7.50945602]\n",
      "36-th iteration, loss: 0.1715336992291848, 483 gd steps\n",
      "insert gradient: -0.0001833313251917788\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.25053441  79.45573183  39.41056087  96.0629281\n",
      "  75.30783833 106.18776031  66.16078037 151.17852395  98.57696995\n",
      " 101.70962347  60.34765603  12.17520094  48.37370547 101.60910296\n",
      "  55.10169267  66.76860905  23.12302986   7.50945602]\n",
      "37-th iteration, loss: 0.17153314479298737, 486 gd steps\n",
      "insert gradient: -0.00018644976858846624\n",
      "37-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42542072e+01 7.94358042e+01 3.94210837e+01\n",
      " 9.60717872e+01 7.53028351e+01 1.06186859e+02 6.61586495e+01\n",
      " 1.51193694e+02 9.85685545e+01 1.01705038e+02 6.03399367e+01\n",
      " 1.21612495e+01 0.00000000e+00 1.77635684e-15 4.83918202e+01\n",
      " 1.01608004e+02 5.51060151e+01 6.67157150e+01 2.31397726e+01\n",
      " 7.50945602e+00]\n",
      "38-th iteration, loss: 0.17153233406298204, 655 gd steps\n",
      "insert gradient: -0.00016647501401828612\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.25916991  79.40882078  39.43627534  96.08467794\n",
      "  75.29663087 106.18574536  66.15564636 151.21493439  98.55671902\n",
      " 101.69742102  60.32424073  12.13768583  48.42993224 101.6042166\n",
      "  55.11018769  66.64163109  23.16203352   7.50945602]\n",
      "39-th iteration, loss: 0.1715317527692264, 504 gd steps\n",
      "insert gradient: -0.00018158507260253513\n",
      "39-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42624214e+01 7.93888773e+01 3.94470335e+01\n",
      " 9.60939332e+01 7.52907860e+01 1.06184610e+02 6.61534092e+01\n",
      " 1.51231328e+02 9.85479200e+01 1.01692433e+02 6.03152110e+01\n",
      " 1.21223125e+01 0.00000000e+00 3.10862447e-15 4.84473504e+01\n",
      " 1.01602830e+02 5.51149765e+01 6.65861634e+01 2.31797343e+01\n",
      " 7.50945602e+00]\n",
      "40-th iteration, loss: 0.17153091417938932, 681 gd steps\n",
      "insert gradient: -0.00016964238760612728\n",
      "40-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.26706457  79.3622979   39.4623763   96.10728168\n",
      "  75.28397712 106.18341441  66.15017546 151.25393772  98.5354366\n",
      " 101.68453647  60.29898239  12.09809492  48.48589786 101.59908253\n",
      "  55.11971029  66.50931605  23.20320816   7.50945602]\n",
      "41-th iteration, loss: 0.17153029878161022, 532 gd steps\n",
      "insert gradient: -0.0001801555426112575\n",
      "41-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42702306e+01 7.93422445e+01 3.94734242e+01\n",
      " 9.61170843e+01 7.52777255e+01 1.06182263e+02 6.61476814e+01\n",
      " 1.51271705e+02 9.85259198e+01 1.01679254e+02 6.02895032e+01\n",
      " 1.20820488e+01 0.00000000e+00 3.55271368e-15 4.85040826e+01\n",
      " 1.01597732e+02 5.51248950e+01 6.64504998e+01 2.32221299e+01\n",
      " 7.50945602e+00]\n",
      "42-th iteration, loss: 0.1715294046877887, 725 gd steps\n",
      "insert gradient: -0.00017194088159057217\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.27482915  79.31531784  39.4892906   96.13131534\n",
      "  75.27040925 106.18106274  66.14405841 151.29637808  98.51235737\n",
      " 101.67082296  60.27227313  12.05652252  48.54452964 101.59388359\n",
      "  55.13004005  66.36837338  23.24734219   7.50945602]\n",
      "43-th iteration, loss: 0.17152873939330124, 572 gd steps\n",
      "insert gradient: -0.00017972598179763903\n",
      "43-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.27800946  79.29476252  39.50084945  96.14190194\n",
      "  75.26366179 106.17988945  66.14122328 151.3159732   98.50188159\n",
      " 101.66511191  60.26212538  12.03947842  48.56398425 101.59255233\n",
      "  55.13568996  66.30474351  23.26788963   7.50945602]\n",
      "44-th iteration, loss: 0.1715280587880149, 590 gd steps\n",
      "insert gradient: -0.00018487863704610524\n",
      "44-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.28147163  79.27429351  39.51248871  96.1527755\n",
      "  75.25728327 106.17895059  66.13814204 151.33607797  98.49093923\n",
      " 101.65936886  60.25244244  12.02288896  48.58546709 101.59152481\n",
      "  55.14124759  66.23959848  23.28881784   7.50945602]\n",
      "45-th iteration, loss: 0.17152735179240203, 611 gd steps\n",
      "insert gradient: -0.00018862723351598955\n",
      "45-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.28505387  79.25352957  39.5243805   96.16406658\n",
      "  75.25088497 106.17807867  66.13484946 151.35703479  98.47954818\n",
      " 101.6533479   60.24220827  12.00571094  48.60823512 101.59044872\n",
      "  55.14674246  66.17199193  23.3103132    7.50945602]\n",
      "46-th iteration, loss: 0.1715266133331564, 637 gd steps\n",
      "insert gradient: -0.00019181023677044968\n",
      "46-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          84.28870163  79.23235561  39.53663979  96.17584615\n",
      "  75.2442675  106.17720444  66.13135283 151.37900213  98.46765223\n",
      " 101.64699934  60.23128137  11.98776425  48.63235195 101.58931273\n",
      "  55.15235054  66.10153034  23.33261771   7.50945602]\n",
      "47-th iteration, loss: 0.17152583689970535, 668 gd steps\n",
      "insert gradient: -0.00019471499784089747\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42924060e+01 7.92106340e+01 3.95493722e+01\n",
      " 9.61882127e+01 7.52373282e+01 1.06176302e+02 6.61276341e+01\n",
      " 1.51402173e+02 9.84551465e+01 1.01640268e+02 6.02195472e+01\n",
      " 1.19688819e+01 0.00000000e+00 1.11022302e-15 4.86579909e+01\n",
      " 1.01588099e+02 5.51581633e+01 6.60276499e+01 2.33559700e+01\n",
      " 7.50945602e+00]\n",
      "48-th iteration, loss: 0.17152456705148666, 999 gd steps\n",
      "insert gradient: -0.00017995980472891747\n",
      "48-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.42976773e+01 7.91779132e+01 3.95700827e+01\n",
      " 9.62082117e+01 7.52272035e+01 1.06174846e+02 6.61219884e+01\n",
      " 0.00000000e+00 2.30926389e-14 1.51438284e+02 9.84356845e+01\n",
      " 1.01627896e+02 6.01933961e+01 1.19328755e+01 4.87163167e+01\n",
      " 1.01582555e+02 5.51647384e+01 6.59124658e+01 2.33909978e+01\n",
      " 7.50945602e+00]\n",
      "49-th iteration, loss: 0.17152288440287897, 999 gd steps\n",
      "insert gradient: -0.0001953711291249595\n",
      "49-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 8.43045625e+01 7.91366724e+01 3.95961243e+01\n",
      " 9.62337855e+01 7.52111001e+01 1.06172092e+02 6.61059594e+01\n",
      " 1.51517993e+02 9.84017400e+01 1.01612959e+02 6.01688893e+01\n",
      " 1.18918770e+01 0.00000000e+00 1.55431223e-15 4.87658650e+01\n",
      " 1.01580342e+02 5.51773671e+01 6.57606814e+01 2.34401059e+01\n",
      " 7.50945602e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5306748252245272\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.31591693    0.         1235.09111824]\n",
      "1-th iteration, loss: 0.7524863030498403, 11 gd steps\n",
      "insert gradient: -0.6502284442411668\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.27764823   62.12834152  120.49669446    0.         1114.59442378]\n",
      "2-th iteration, loss: 0.5091933956204695, 45 gd steps\n",
      "insert gradient: -0.6617265189498799\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.60575063  53.70385439  99.02017526 100.34994578 227.46824975\n",
      "   0.         887.12617403]\n",
      "3-th iteration, loss: 0.43572521232026473, 13 gd steps\n",
      "insert gradient: -0.5778361374269031\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[1.38609481e-01 6.69143182e+01 9.45631300e+01 9.30683047e+01\n",
      " 1.12092930e+02 0.00000000e+00 9.43940466e+01 4.97943348e+01\n",
      " 8.87126174e+02]\n",
      "4-th iteration, loss: 0.34017166851030534, 51 gd steps\n",
      "insert gradient: -0.3129603124580483\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.57788156  84.6496637  124.58184287  78.27121854  97.94653786\n",
      "  44.75891346  66.51262978  47.90082217 525.70439942   0.\n",
      " 361.4217746 ]\n",
      "5-th iteration, loss: 0.30037612520196805, 26 gd steps\n",
      "insert gradient: -0.2878563346534128\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.40900272  79.05724379 132.36597797  83.58060715  99.47734429\n",
      "  46.50371573  65.83146913  43.38359387 185.06053463   0.\n",
      " 323.85593561  42.39475886 361.4217746 ]\n",
      "6-th iteration, loss: 0.2754276006128998, 26 gd steps\n",
      "insert gradient: -0.1369189924092703\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[1.51063247e-01 7.30167809e+01 1.17355251e+02 9.38771427e+01\n",
      " 9.80979446e+01 4.80998358e+01 6.80957257e+01 3.65069993e+01\n",
      " 1.58659575e+02 2.32880954e+01 3.04514726e+02 4.82101409e+01\n",
      " 3.61421775e+02]\n",
      "7-th iteration, loss: 0.23406484729972582, 47 gd steps\n",
      "insert gradient: -0.053396824671956035\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  0.89731532  68.20258013 130.93224066  86.96329529 104.34117726\n",
      "  48.64121495  72.82852725  34.88195463  89.52471745  84.80509984\n",
      " 262.52468206  53.3412569  120.47392487   0.         240.94784974]\n",
      "8-th iteration, loss: 0.22539862619777962, 75 gd steps\n",
      "insert gradient: -0.05679626949501211\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  1.03434792  65.04712083 140.22917079  80.46950861 117.85887881\n",
      "  53.06977947  74.33202024  34.97846766  82.24688154  93.38568685\n",
      " 236.47083112  69.33492031  82.90540774  18.85475889 135.53316548\n",
      "   0.         105.41468426]\n",
      "9-th iteration, loss: 0.1974817801490092, 79 gd steps\n",
      "insert gradient: -0.1489883407121201\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.11900554  66.4787607  134.67496311  63.47140713 143.99066138\n",
      "  63.753722    83.63449834  19.78903051  81.36555314  98.26071405\n",
      " 121.1877259    0.         121.1877259   54.01805181  97.66881468\n",
      "  41.04593959  65.73200883  49.11942659 105.41468426]\n",
      "10-th iteration, loss: 0.16341462121935535, 23 gd steps\n",
      "insert gradient: -0.032908066699318776\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.67237814  67.75722366 134.73467411  68.55491229 139.08566074\n",
      "  70.7647354   94.85042879  18.74460624  79.22499936  99.73032735\n",
      "  96.98780307  31.89984041  78.01359569  54.69143412  64.06395885\n",
      "   0.          45.75997061  46.25339989  56.1562838   47.33212275\n",
      " 105.41468426]\n",
      "11-th iteration, loss: 0.15111087570658985, 133 gd steps\n",
      "insert gradient: -0.03047003255643387\n",
      "11-th iteration, new layer inserted. now 20 layers\n",
      "[  2.00046692  73.44044706 136.85888984  69.92946426 135.82883526\n",
      "  76.19824335 106.78770022  22.93278022  60.27460732 108.283235\n",
      "  93.92160114  48.56210241  65.98220653  44.63430149 121.2182028\n",
      "  60.88913157  51.25331183  44.49382631 105.41468426   0.        ]\n",
      "12-th iteration, loss: 0.13087688683173465, 85 gd steps\n",
      "insert gradient: -0.02938900365009945\n",
      "12-th iteration, new layer inserted. now 22 layers\n",
      "[  2.31373339  66.98485303 140.69716472  71.19474526 132.84165447\n",
      "  80.97072378 110.33314812  40.11337858  29.08514048  73.40442393\n",
      "   0.          36.70221196  89.78606873  46.13608042  86.48241389\n",
      "  36.13686649  86.66423375  72.01356636  96.45185729  26.17117672\n",
      "  89.11139463  52.45483416]\n",
      "13-th iteration, loss: 0.11021430845390004, 60 gd steps\n",
      "insert gradient: -0.01032731700101049\n",
      "13-th iteration, new layer inserted. now 22 layers\n",
      "[  3.11864044  70.87617336 141.04667372  73.6962045  120.76988014\n",
      "  86.32262341 118.68129544  62.40270176  35.46737883  40.39089054\n",
      "  97.76388682   0.          97.76388682  51.61904397  87.7287611\n",
      "  40.16268905  72.49912675  74.12690481 106.12044119  42.09743806\n",
      "  65.2065052   49.88829632]\n",
      "14-th iteration, loss: 0.10930329693904138, 34 gd steps\n",
      "insert gradient: -0.002761547541772383\n",
      "14-th iteration, new layer inserted. now 22 layers\n",
      "[  2.20101106  71.4792396  140.51726675  75.12298025 119.65709341\n",
      "  88.0147097  117.81972911  66.6574443   35.12565521  39.57080892\n",
      "  93.794138     4.16426084  91.23349874  51.46839105  90.1440748\n",
      "  41.56160325  68.79754366  74.92729685 108.04251562  43.94220976\n",
      "  64.81001709  48.73754796]\n",
      "15-th iteration, loss: 0.10921460803057598, 24 gd steps\n",
      "insert gradient: -0.0016564878045152128\n",
      "15-th iteration, new layer inserted. now 22 layers\n",
      "[  2.11376829  71.65392266 140.53816856  75.41056115 119.55597238\n",
      "  88.07887198 117.68020718  67.52801816  35.12333266  39.90876798\n",
      "  93.91733429   5.34034661  87.20775041  51.70896798  90.38093897\n",
      "  41.65297782  68.42780001  75.27889951 108.03850351  44.22305373\n",
      "  64.60273911  48.97125739]\n",
      "16-th iteration, loss: 0.10919349404590137, 17 gd steps\n",
      "insert gradient: -0.0047960200149983504\n",
      "16-th iteration, new layer inserted. now 22 layers\n",
      "[  2.04691672  71.66310979 140.68736151  75.76051163 119.48854171\n",
      "  87.75257603 118.00855751  67.51839651  35.8269195   39.88977727\n",
      "  96.05843575   6.17726825  81.86696837  52.17726569  90.35992121\n",
      "  41.69777198  68.32350909  75.56913194 107.2781447   44.75391841\n",
      "  64.02944042  49.4221613 ]\n",
      "17-th iteration, loss: 0.10916836620571861, 29 gd steps\n",
      "insert gradient: -0.00018927402806425797\n",
      "17-th iteration, new layer inserted. now 24 layers\n",
      "[2.13372141e+00 7.17800278e+01 1.40457929e+02 7.56070604e+01\n",
      " 1.19456339e+02 8.81344613e+01 1.18000197e+02 6.74487645e+01\n",
      " 0.00000000e+00 8.88178420e-15 3.62532054e+01 3.95934732e+01\n",
      " 9.65866937e+01 6.20486772e+00 8.18449826e+01 5.21774593e+01\n",
      " 9.05909160e+01 4.17994508e+01 6.80208999e+01 7.56083840e+01\n",
      " 1.07418397e+02 4.48221062e+01 6.40270200e+01 4.93783439e+01]\n",
      "18-th iteration, loss: 0.10916661400644653, 18 gd steps\n",
      "insert gradient: -0.0003229314788676348\n",
      "18-th iteration, new layer inserted. now 22 layers\n",
      "[  2.1435764   71.7241848  140.4364164   75.67199766 119.43967695\n",
      "  88.20862066 118.04210674  67.34358553  36.70546392  39.2545236\n",
      "  97.32570242   6.16594665  81.51771304  52.15867965  90.60853816\n",
      "  41.84995606  67.89536789  75.60130134 107.49202082  44.86261126\n",
      "  64.02995504  49.35520622]\n",
      "19-th iteration, loss: 0.10916583066341713, 47 gd steps\n",
      "insert gradient: -5.4314300756073795e-05\n",
      "19-th iteration, new layer inserted. now 22 layers\n",
      "[  2.16864918  71.80878749 140.45166549  75.60610114 119.45198643\n",
      "  88.18967984 118.05881957  67.31359737  36.78174088  39.23181258\n",
      "  97.59559179   6.17587801  81.21031197  52.1856768   90.60228086\n",
      "  41.83603931  67.92143978  75.6001243  107.50260526  44.86032574\n",
      "  64.0340586   49.34834298]\n",
      "20-th iteration, loss: 0.10916564111873958, 25 gd steps\n",
      "insert gradient: -3.580079780462069e-05\n",
      "20-th iteration, new layer inserted. now 24 layers\n",
      "[2.17275175e+00 7.18036376e+01 1.40448838e+02 0.00000000e+00\n",
      " 1.77635684e-14 7.56101722e+01 1.19443418e+02 8.81974172e+01\n",
      " 1.18080273e+02 6.72979104e+01 3.68478034e+01 3.91804961e+01\n",
      " 9.78217748e+01 6.17765181e+00 8.09963431e+01 5.22037812e+01\n",
      " 9.05994527e+01 4.18344872e+01 6.79174597e+01 7.55894830e+01\n",
      " 1.07525685e+02 4.48655095e+01 6.40237154e+01 4.93550811e+01]\n",
      "21-th iteration, loss: 0.10916558411515133, 210 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540722128872869e-06\n",
      "21-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503177  71.80683043 140.44712568  75.62217252 119.44008328\n",
      "  88.20229165 118.08492849  67.28137873  36.91940368  39.14544271\n",
      "  97.97916703   6.18529505  80.8386942   52.20494708  90.60352401\n",
      "  41.84051371  67.9114263   75.57434656 107.56021279  44.87357672\n",
      "  64.01848391  49.35126323]\n",
      "22-th iteration, loss: 0.10916558411496824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540570715629089e-06\n",
      "22-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503183  71.80683044 140.44712526  75.6221726  119.44008322\n",
      "  88.20229175 118.08492847  67.28137836  36.91940478  39.14544214\n",
      "  97.97916888   6.18529511  80.83869215  52.20494727  90.60352396\n",
      "  41.84051372  67.91142631  75.57434625 107.56021337  44.87357676\n",
      "  64.01848379  49.35126322]\n",
      "23-th iteration, loss: 0.1091655841147852, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540463062659935e-06\n",
      "23-th iteration, new layer inserted. now 22 layers\n",
      "[  2.1750319   71.80683044 140.44712484  75.62217269 119.44008316\n",
      "  88.20229185 118.08492846  67.281378    36.91940588  39.14544157\n",
      "  97.97917074   6.18529518  80.8386901   52.20494745  90.60352391\n",
      "  41.84051373  67.91142632  75.57434593 107.56021394  44.87357679\n",
      "  64.01848367  49.35126321]\n",
      "24-th iteration, loss: 0.10916558411460221, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540394531207086e-06\n",
      "24-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503197  71.80683045 140.44712442  75.62217278 119.4400831\n",
      "  88.20229195 118.08492844  67.28137764  36.91940697  39.145441\n",
      "  97.97917259   6.18529524  80.83868805  52.20494763  90.60352386\n",
      "  41.84051374  67.91142632  75.57434561 107.56021451  44.87357683\n",
      "  64.01848354  49.3512632 ]\n",
      "25-th iteration, loss: 0.10916558411441926, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.854036082560839e-06\n",
      "25-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503203  71.80683045 140.447124    75.62217287 119.44008304\n",
      "  88.20229205 118.08492842  67.28137728  36.91940807  39.14544043\n",
      "  97.97917445   6.1852953   80.838686    52.20494782  90.60352381\n",
      "  41.84051375  67.91142633  75.57434529 107.56021508  44.87357686\n",
      "  64.01848342  49.35126319]\n",
      "26-th iteration, loss: 0.10916558411423634, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540357962250505e-06\n",
      "26-th iteration, new layer inserted. now 22 layers\n",
      "[  2.1750321   71.80683046 140.44712358  75.62217296 119.44008299\n",
      "  88.20229216 118.0849284   67.28137691  36.91940916  39.14543986\n",
      "  97.9791763    6.18529537  80.83868395  52.204948    90.60352376\n",
      "  41.84051377  67.91142634  75.57434497 107.56021565  44.87357689\n",
      "  64.0184833   49.35126318]\n",
      "27-th iteration, loss: 0.10916558411405348, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540382254002853e-06\n",
      "27-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503217  71.80683047 140.44712317  75.62217306 119.44008293\n",
      "  88.20229226 118.08492839  67.28137655  36.91941026  39.14543928\n",
      "  97.97917815   6.18529543  80.8386819   52.20494819  90.60352372\n",
      "  41.84051378  67.91142635  75.57434465 107.56021622  44.87357693\n",
      "  64.01848317  49.35126317]\n",
      "28-th iteration, loss: 0.10916558411387066, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.854043028868057e-06\n",
      "28-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503223  71.80683048 140.44712275  75.62217315 119.44008287\n",
      "  88.20229236 118.08492837  67.28137619  36.91941135  39.14543871\n",
      "  97.97918001   6.1852955   80.83867985  52.20494837  90.60352367\n",
      "  41.84051379  67.91142636  75.57434433 107.56021679  44.87357696\n",
      "  64.01848305  49.35126316]\n",
      "29-th iteration, loss: 0.10916558411368787, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8540498914945508e-06\n",
      "29-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503230e+00 7.18068305e+01 1.40447122e+02 7.56221732e+01\n",
      " 1.19440083e+02 8.82022925e+01 1.18084928e+02 6.72813758e+01\n",
      " 3.69194124e+01 3.91454381e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.79791819e+01 6.18529556e+00 8.08386778e+01 5.22049486e+01\n",
      " 9.06035236e+01 4.18405138e+01 6.79114264e+01 7.55743440e+01\n",
      " 1.07560217e+02 4.48735770e+01 6.40184829e+01 4.93512631e+01]\n",
      "30-th iteration, loss: 0.10916558411344224, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8492969099158041e-06\n",
      "30-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503237e+00 7.18068305e+01 1.40447122e+02 7.56221733e+01\n",
      " 1.19440083e+02 8.82022926e+01 1.18084928e+02 6.72813755e+01\n",
      " 3.69194135e+01 3.91454376e+01 0.00000000e+00 8.88178420e-16\n",
      " 9.79791856e+01 6.18529562e+00 8.08386758e+01 5.22049487e+01\n",
      " 9.06035236e+01 4.18405138e+01 6.79114264e+01 7.55743437e+01\n",
      " 1.07560218e+02 4.48735770e+01 6.40184828e+01 4.93512631e+01]\n",
      "31-th iteration, loss: 0.10916558411319698, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.844749183181237e-06\n",
      "31-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503244  71.80683052 140.44712151  75.62217345 119.44008271\n",
      "  88.20229268 118.08492833  67.2813751   36.91941463  39.14543699\n",
      "  97.97918927   6.18529567  80.8386737   52.20494891  90.60352352\n",
      "  41.84051381  67.91142638  75.57434336 107.5602185   44.87357705\n",
      "  64.01848268  49.35126313]\n",
      "32-th iteration, loss: 0.10916558411301429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.845135799964823e-06\n",
      "32-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503251  71.80683054 140.4471211   75.62217355 119.44008266\n",
      "  88.20229279 118.08492831  67.28137474  36.91941572  39.14543641\n",
      "  97.97919111   6.18529571  80.83867164  52.20494908  90.60352346\n",
      "  41.84051381  67.91142639  75.57434304 107.56021907  44.87357709\n",
      "  64.01848255  49.35126312]\n",
      "33-th iteration, loss: 0.10916558411283163, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8455053138748314e-06\n",
      "33-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503258  71.80683056 140.44712069  75.62217366 119.44008261\n",
      "  88.2022929  118.0849283   67.28137437  36.9194168   39.14543582\n",
      "  97.97919296   6.18529576  80.83866958  52.20494925  90.60352341\n",
      "  41.84051382  67.9114264   75.57434272 107.56021964  44.87357712\n",
      "  64.01848243  49.35126311]\n",
      "34-th iteration, loss: 0.10916558411264901, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8458584695709085e-06\n",
      "34-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503265e+00 7.18068306e+01 1.40447120e+02 7.56221738e+01\n",
      " 1.19440083e+02 8.82022930e+01 1.18084928e+02 6.72813740e+01\n",
      " 3.69194179e+01 3.91454352e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.79791948e+01 6.18529580e+00 8.08386675e+01 5.22049494e+01\n",
      " 9.06035234e+01 4.18405138e+01 6.79114264e+01 7.55743424e+01\n",
      " 1.07560220e+02 4.48735771e+01 6.40184823e+01 4.93512631e+01]\n",
      "35-th iteration, loss: 0.10916558411240411, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8414537776369677e-06\n",
      "35-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503272  71.8068306  140.44711987  75.62217388 119.4400825\n",
      "  88.20229311 118.08492827  67.28137364  36.91941898  39.14543465\n",
      "  97.97919849   6.18529585  80.83866547  52.2049496   90.6035233\n",
      "  41.84051382  67.91142641  75.57434207 107.56022078  44.87357718\n",
      "  64.01848218  49.35126309]\n",
      "36-th iteration, loss: 0.10916558411222156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.841968032665728e-06\n",
      "36-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503279  71.80683062 140.44711947  75.62217399 119.44008245\n",
      "  88.20229322 118.08492826  67.28137328  36.91942006  39.14543407\n",
      "  97.97920034   6.18529588  80.83866341  52.20494976  90.60352324\n",
      "  41.84051382  67.91142641  75.57434175 107.56022135  44.87357721\n",
      "  64.01848206  49.35126307]\n",
      "37-th iteration, loss: 0.10916558411203904, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8424586703327323e-06\n",
      "37-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503286  71.80683064 140.44711906  75.6221741  119.44008241\n",
      "  88.20229333 118.08492824  67.28137291  36.91942114  39.14543348\n",
      "  97.97920218   6.18529592  80.83866135  52.20494993  90.60352319\n",
      "  41.84051382  67.91142642  75.57434143 107.56022192  44.87357724\n",
      "  64.01848194  49.35126306]\n",
      "38-th iteration, loss: 0.10916558411185656, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8429267897298843e-06\n",
      "38-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503293e+00 7.18068307e+01 1.40447119e+02 7.56221742e+01\n",
      " 1.19440082e+02 8.82022934e+01 1.18084928e+02 6.72813725e+01\n",
      " 3.69194222e+01 3.91454329e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.79792040e+01 6.18529596e+00 8.08386593e+01 5.22049501e+01\n",
      " 9.06035231e+01 4.18405138e+01 6.79114264e+01 7.55743411e+01\n",
      " 1.07560222e+02 4.48735773e+01 6.40184818e+01 4.93512631e+01]\n",
      "39-th iteration, loss: 0.10916558411161198, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8386382126362797e-06\n",
      "39-th iteration, new layer inserted. now 22 layers\n",
      "[  2.175033    71.80683069 140.44711825  75.62217433 119.44008231\n",
      "  88.20229356 118.08492822  67.28137218  36.91942331  39.1454323\n",
      "  97.97920771   6.185296    80.83865723  52.20495026  90.60352307\n",
      "  41.84051383  67.91142643  75.57434079 107.56022306  44.87357731\n",
      "  64.01848169  49.35126304]\n",
      "40-th iteration, loss: 0.10916558411142956, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.839255817223072e-06\n",
      "40-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503307  71.80683072 140.44711785  75.62217445 119.44008226\n",
      "  88.20229367 118.08492821  67.28137181  36.91942439  39.14543171\n",
      "  97.97920954   6.18529603  80.83865517  52.20495043  90.60352302\n",
      "  41.84051383  67.91142644  75.57434047 107.56022363  44.87357734\n",
      "  64.01848157  49.35126303]\n",
      "41-th iteration, loss: 0.10916558411124717, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8398446056375787e-06\n",
      "41-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503314e+00 7.18068307e+01 1.40447117e+02 7.56221746e+01\n",
      " 1.19440082e+02 8.82022938e+01 1.18084928e+02 6.72813714e+01\n",
      " 3.69194255e+01 3.91454311e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.79792114e+01 6.18529606e+00 8.08386531e+01 5.22049506e+01\n",
      " 9.06035230e+01 4.18405138e+01 6.79114264e+01 7.55743401e+01\n",
      " 1.07560224e+02 4.48735774e+01 6.40184814e+01 4.93512630e+01]\n",
      "42-th iteration, loss: 0.10916558411100288, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8356780693263843e-06\n",
      "42-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503321e+00 7.18068308e+01 1.40447117e+02 7.56221747e+01\n",
      " 1.19440082e+02 8.82022939e+01 1.18084928e+02 6.72813711e+01\n",
      " 3.69194265e+01 3.91454305e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.79792151e+01 6.18529609e+00 8.08386510e+01 5.22049508e+01\n",
      " 9.06035229e+01 4.18405138e+01 6.79114264e+01 7.55743398e+01\n",
      " 1.07560225e+02 4.48735774e+01 6.40184813e+01 4.93512630e+01]\n",
      "43-th iteration, loss: 0.10916558411075886, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8316864350261176e-06\n",
      "43-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503328  71.80683081 140.44711665  75.6221748  119.44008212\n",
      "  88.20229401 118.08492817  67.28137071  36.91942762  39.14542993\n",
      "  97.97921873   6.18529611  80.83864898  52.20495091  90.60352285\n",
      "  41.84051382  67.91142645  75.5743395  107.56022533  44.87357743\n",
      "  64.0184812   49.351263  ]\n",
      "44-th iteration, loss: 0.10916558411057653, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8325694227163335e-06\n",
      "44-th iteration, new layer inserted. now 24 layers\n",
      "[2.17503336e+00 7.18068308e+01 1.40447116e+02 7.56221749e+01\n",
      " 1.19440082e+02 8.82022941e+01 1.18084928e+02 6.72813703e+01\n",
      " 3.69194287e+01 3.91454293e+01 0.00000000e+00 3.55271368e-15\n",
      " 9.79792206e+01 6.18529612e+00 8.08386469e+01 5.22049511e+01\n",
      " 9.06035228e+01 4.18405138e+01 6.79114265e+01 7.55743392e+01\n",
      " 1.07560226e+02 4.48735775e+01 6.40184811e+01 4.93512630e+01]\n",
      "45-th iteration, loss: 0.10916558411033275, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.828700076702102e-06\n",
      "45-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503343  71.80683087 140.44711585  75.62217505 119.44008203\n",
      "  88.20229423 118.08492815  67.28136997  36.91942977  39.14542872\n",
      "  97.97922423   6.18529613  80.83864484  52.20495122  90.60352272\n",
      "  41.84051381  67.91142646  75.57433886 107.56022647  44.8735775\n",
      "  64.01848095  49.35126298]\n",
      "46-th iteration, loss: 0.10916558411015044, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8296921509339675e-06\n",
      "46-th iteration, new layer inserted. now 22 layers\n",
      "[  2.1750335   71.80683091 140.44711546  75.62217518 119.44008199\n",
      "  88.20229435 118.08492814  67.2813696   36.91943084  39.14542812\n",
      "  97.97922606   6.18529614  80.83864277  52.20495137  90.60352266\n",
      "  41.84051381  67.91142646  75.57433855 107.56022704  44.87357753\n",
      "  64.01848083  49.35126297]\n",
      "47-th iteration, loss: 0.10916558410996816, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.830637296794462e-06\n",
      "47-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503358  71.80683094 140.44711506  75.6221753  119.44008194\n",
      "  88.20229446 118.08492813  67.28136923  36.91943192  39.14542751\n",
      "  97.97922789   6.18529615  80.8386407   52.20495152  90.6035226\n",
      "  41.8405138   67.91142647  75.57433823 107.56022761  44.87357756\n",
      "  64.01848071  49.35126296]\n",
      "48-th iteration, loss: 0.10916558410978595, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8315378466481185e-06\n",
      "48-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503365  71.80683098 140.44711467  75.62217543 119.4400819\n",
      "  88.20229458 118.08492812  67.28136886  36.91943299  39.14542691\n",
      "  97.97922972   6.18529617  80.83863864  52.20495167  90.60352254\n",
      "  41.8405138   67.91142647  75.57433791 107.56022818  44.8735776\n",
      "  64.01848059  49.35126296]\n",
      "49-th iteration, loss: 0.10916558410960382, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8323960073629067e-06\n",
      "49-th iteration, new layer inserted. now 22 layers\n",
      "[  2.17503372  71.80683101 140.44711427  75.62217556 119.44008186\n",
      "  88.20229469 118.08492811  67.28136849  36.91943406  39.14542631\n",
      "  97.97923155   6.18529618  80.83863657  52.20495183  90.60352248\n",
      "  41.8405138   67.91142648  75.57433759 107.56022875  44.87357763\n",
      "  64.01848046  49.35126295]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.528844833774348\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.97695304    0.         1264.55571028]\n",
      "1-th iteration, loss: 0.7443002030476201, 11 gd steps\n",
      "insert gradient: -0.6257887890247421\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.14454085   62.35383811  231.32116651    0.         1033.23454376]\n",
      "2-th iteration, loss: 0.6043299233497633, 13 gd steps\n",
      "insert gradient: -0.5881786576160809\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  4.04227937  77.97973772 217.53795917  42.02146053 126.51851556\n",
      "   0.         906.7160282 ]\n",
      "3-th iteration, loss: 0.47124001199354004, 24 gd steps\n",
      "insert gradient: -0.7451619874175202\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[1.43313580e-01 9.65870596e+01 1.87576575e+02 5.88611811e+01\n",
      " 9.97978011e+01 5.52756656e+01 3.88592584e+02 0.00000000e+00\n",
      " 5.18123445e+02]\n",
      "4-th iteration, loss: 0.3728590488213322, 36 gd steps\n",
      "insert gradient: -0.2511562329427523\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          92.8961593  194.93667922  55.6438842  102.20078546\n",
      "  59.49123182 243.15888659   0.         102.38268909  46.40530966\n",
      " 518.12344469]\n",
      "5-th iteration, loss: 0.2976159188171495, 28 gd steps\n",
      "insert gradient: -0.28003659487406357\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[1.76832087e-01 7.43527529e+01 2.08305565e+02 6.67490840e+01\n",
      " 1.11795121e+02 5.68448877e+01 1.76182154e+02 4.82097867e+01\n",
      " 7.41080382e+01 4.45516601e+01 2.35510657e+02 0.00000000e+00\n",
      " 2.82612788e+02]\n",
      "6-th iteration, loss: 0.23759026752128312, 32 gd steps\n",
      "insert gradient: -0.078042392252569\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          65.94436042 229.63478318  60.9291074  110.43005273\n",
      "  65.30282174 166.20700105  55.35439762  74.11676729  33.38977216\n",
      " 111.18715888   0.          88.9497271   46.92170204 282.61278801]\n",
      "7-th iteration, loss: 0.22796740331633292, 51 gd steps\n",
      "insert gradient: -0.09600837135449565\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.53900129  63.78960926 239.36758294  62.62258195 110.17992611\n",
      "  69.78783478 158.28200164  64.47514186  86.25826059  36.33473367\n",
      "  69.47196039  25.85502712  55.9183781   53.21738526 229.62289026\n",
      "   0.          52.98989775]\n",
      "8-th iteration, loss: 0.19765966491195266, 162 gd steps\n",
      "insert gradient: -0.16901544855111503\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  0.84746607  60.36287875 127.276005     0.         127.276005\n",
      "  57.629564   107.27971661  72.73320694 153.86677394  65.80390862\n",
      " 100.88155069  23.97856038  41.26996747  51.49655765  53.98399123\n",
      "  42.04070777 196.75408006  53.00826716  52.98989775]\n",
      "9-th iteration, loss: 0.17717982472412688, 20 gd steps\n",
      "insert gradient: -0.07884258567715276\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[  1.26440552  67.22410867  98.38392094  27.35942052  83.10399581\n",
      "  64.27056605 112.42409057  76.95461104 149.77186978  68.59235804\n",
      " 104.91128332  16.44381707  42.51552537  57.62361381  60.74691642\n",
      "  37.99602282 119.4201051    0.          85.30007507  49.7771553\n",
      "  52.98989775]\n",
      "10-th iteration, loss: 0.16373138531785478, 22 gd steps\n",
      "insert gradient: -0.06561138548329742\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  3.44429523  66.41447157 107.44552927  37.59300903  65.98523549\n",
      "  65.03965286 108.51468187  79.4430719  143.78195588  71.62062323\n",
      " 122.74924839  16.99605059  26.22819072  60.57527157  91.1773432\n",
      "  36.8568572   71.19289146  28.42513038  54.58933075  51.80500427\n",
      "  52.98989775]\n",
      "11-th iteration, loss: 0.14115327406720754, 32 gd steps\n",
      "insert gradient: -0.05199904833885833\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[1.61665643e+00 6.49664489e+01 1.16698666e+02 5.07805180e+01\n",
      " 6.48819326e+01 0.00000000e+00 2.30926389e-14 6.00089513e+01\n",
      " 1.17365030e+02 7.34139709e+01 1.63038943e+02 7.26634551e+01\n",
      " 1.24402689e+02 1.07320311e+02 1.01580354e+02 4.45589464e+01\n",
      " 5.07732506e+01 3.89671006e+01 6.65897791e+01 4.65386159e+01\n",
      " 5.29898978e+01]\n",
      "12-th iteration, loss: 0.12947564292735472, 20 gd steps\n",
      "insert gradient: -0.04806352450644627\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[1.69160516e+00 6.72233389e+01 1.02913649e+02 5.69153625e+01\n",
      " 6.88630211e+01 2.53969360e+00 4.05894341e-01 6.37087318e+01\n",
      " 1.08246863e+02 7.52026260e+01 1.72398538e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.47976898e+01 1.37042526e+02 1.08189061e+02\n",
      " 9.71320509e+01 4.37173006e+01 4.01702975e+01 4.05667345e+01\n",
      " 9.33610165e+01 4.41276270e+01 5.29898978e+01]\n",
      "13-th iteration, loss: 0.12416807761736545, 23 gd steps\n",
      "insert gradient: -0.042532591275089675\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[1.37155885e-01 7.04956621e+01 9.95872444e+01 5.51598108e+01\n",
      " 8.13089007e+01 6.47525473e+01 1.06820460e+02 7.79194157e+01\n",
      " 1.77190577e+02 6.95340255e+01 1.38959620e+02 1.09588547e+02\n",
      " 9.70133304e+01 4.63597452e+01 4.37528123e+01 0.00000000e+00\n",
      " 6.21724894e-15 3.63445747e+01 9.15874541e+01 4.56057896e+01\n",
      " 5.29898978e+01]\n",
      "14-th iteration, loss: 0.12092678235708663, 23 gd steps\n",
      "insert gradient: -0.021219500974128024\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          70.1928442   99.48664256  51.98693793  87.1442385\n",
      "  67.94372315 106.93825483  78.8858834  176.98993574  65.81535081\n",
      " 140.16154373 111.02228979  97.23095214  48.1590095   47.97888295\n",
      "  37.02923929  92.84463119  46.82944566  52.98989775]\n",
      "15-th iteration, loss: 0.12002038666750904, 23 gd steps\n",
      "insert gradient: -0.007771152673329641\n",
      "15-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          70.14410745 100.21108313  49.99796922  90.93088695\n",
      "  67.75553138 107.24022709  80.26797087 178.52252628  65.69315099\n",
      " 141.3352389  112.49967878  96.61764102  47.81805858  48.99902808\n",
      "  36.32177829  92.94575045  47.06090591  52.98989775]\n",
      "16-th iteration, loss: 0.11969847749635983, 21 gd steps\n",
      "insert gradient: -0.0045921657581085755\n",
      "16-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 7.03004860e+01 1.01048556e+02 4.84741758e+01\n",
      " 9.46890174e+01 0.00000000e+00 1.24344979e-14 6.75308582e+01\n",
      " 1.07367926e+02 8.05289106e+01 1.79388740e+02 6.51302275e+01\n",
      " 1.41832361e+02 1.12885156e+02 9.62346411e+01 4.77707212e+01\n",
      " 5.04922268e+01 3.60136552e+01 9.33847956e+01 4.78183714e+01\n",
      " 5.29898978e+01]\n",
      "17-th iteration, loss: 0.11959738874766954, 15 gd steps\n",
      "insert gradient: -0.004973096781431187\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[0.00000000e+00 7.06450344e+01 1.01578698e+02 4.67049275e+01\n",
      " 9.88889380e+01 6.73311824e+01 1.07582603e+02 8.04066909e+01\n",
      " 1.79969986e+02 6.51184767e+01 1.42020631e+02 1.13102580e+02\n",
      " 9.58236147e+01 4.76293612e+01 5.13195353e+01 0.00000000e+00\n",
      " 1.24344979e-14 3.55745872e+01 9.36953937e+01 4.86375294e+01\n",
      " 5.29898978e+01]\n",
      "18-th iteration, loss: 0.11955525980437852, 19 gd steps\n",
      "insert gradient: -0.003791288175079276\n",
      "18-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 7.04251371e+01 1.01699250e+02 4.67687513e+01\n",
      " 9.88606658e+01 6.72620258e+01 1.07658702e+02 8.08150882e+01\n",
      " 1.80293403e+02 6.45242105e+01 1.42401429e+02 6.17884164e+01\n",
      " 0.00000000e+00 5.14903470e+01 9.57529207e+01 4.76871428e+01\n",
      " 5.14446782e+01 1.03242165e-01 5.44848770e-03 3.57837430e+01\n",
      " 9.39011247e+01 4.86875778e+01 5.29898978e+01]\n",
      "19-th iteration, loss: 0.11928394154533485, 25 gd steps\n",
      "insert gradient: -0.004829649436150575\n",
      "19-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          71.11759179 101.06923817  47.07133004  99.81012049\n",
      "  67.00577596 107.55210827  80.40522465 180.68323244  65.04655874\n",
      " 143.04660899  61.44744432   4.92005657  48.00643606  97.48247176\n",
      "  47.85070807  52.07068589  34.88649241  94.89701668  50.36996138\n",
      "  52.98989775]\n",
      "20-th iteration, loss: 0.11890095449715507, 33 gd steps\n",
      "insert gradient: -0.006973270402647311\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 7.17521271e+01 1.01003828e+02 4.70032743e+01\n",
      " 1.01220667e+02 6.70697764e+01 1.07945581e+02 8.06445976e+01\n",
      " 1.81484457e+02 6.49076962e+01 1.43719480e+02 6.12037070e+01\n",
      " 9.84335408e+00 0.00000000e+00 1.11022302e-15 4.52978425e+01\n",
      " 9.67318913e+01 4.91079989e+01 5.24557336e+01 3.40648998e+01\n",
      " 9.57389972e+01 5.23238758e+01 5.29898978e+01]\n",
      "21-th iteration, loss: 0.11840676502016483, 25 gd steps\n",
      "insert gradient: -0.008066260924780952\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          72.34902143  99.93219236  47.08046045 103.5814945\n",
      "  66.8059426  108.3335837   80.69582958 182.18340969  64.92849842\n",
      " 144.85224316  59.94661475  16.09425874  43.22648782  97.5307003\n",
      "  50.33616027  54.3311646   31.9546502   96.90030201  54.88506005\n",
      "  52.98989775]\n",
      "22-th iteration, loss: 0.11818735533093223, 19 gd steps\n",
      "insert gradient: -0.005785310379422359\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          72.72805084  99.63062207  47.07039109 104.8128843\n",
      "  66.91149386 108.50441579  80.55374399 182.93019115  65.02689936\n",
      " 145.16211033  59.30089133  18.23443073  42.42649054  97.32658811\n",
      "  50.68668054  55.51746775  31.21977562  97.33891672  55.95348398\n",
      "  52.98989775]\n",
      "23-th iteration, loss: 0.11787206887307328, 31 gd steps\n",
      "insert gradient: -0.003781895307008749\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 7.33685147e+01 9.89200985e+01 4.68849635e+01\n",
      " 1.07253866e+02 6.67751585e+01 1.09282872e+02 0.00000000e+00\n",
      " 7.10542736e-15 8.05771685e+01 1.83673453e+02 6.49686957e+01\n",
      " 1.45734231e+02 5.88828403e+01 2.21557327e+01 4.08201919e+01\n",
      " 9.74452503e+01 5.12115971e+01 5.83052047e+01 2.93553012e+01\n",
      " 9.84161129e+01 5.74385042e+01 5.29898978e+01]\n",
      "24-th iteration, loss: 0.11768618887411232, 40 gd steps\n",
      "insert gradient: -0.0028313819672160143\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          73.4606125   98.83777514  47.00158718 107.86256629\n",
      "  67.35664213 109.56735947  80.7681107  184.25908711  64.84920887\n",
      " 146.25548656  58.00689731  25.51918697  39.73896108  97.71428285\n",
      "  51.61264249  60.22898306  28.20800573  99.1410504   58.4797114\n",
      "  52.98989775]\n",
      "25-th iteration, loss: 0.11761799251521776, 24 gd steps\n",
      "insert gradient: -0.0021967089988317655\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          73.59786812  98.9394636   46.86943627 108.3540722\n",
      "  67.29514631 109.79065823  80.81320498 184.57374086  64.97962739\n",
      " 146.6510793   57.47289143  27.81009418  38.93003918  97.93547941\n",
      "  51.66498555  62.03151782  26.96961105  99.55209935  58.93529072\n",
      "  52.98989775]\n",
      "26-th iteration, loss: 0.11757684825941071, 23 gd steps\n",
      "insert gradient: -0.0013875634380897\n",
      "26-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          73.6650722   99.00522184  46.71187723 108.66305008\n",
      "  67.6794335  109.72846124  80.91215347 184.67925909  64.70709121\n",
      " 147.01922416  57.38862765  29.02408352  38.42653009  97.99853658\n",
      "  51.97815501  63.20807898  26.31828458  99.86374655  59.00690313\n",
      "  52.98989775]\n",
      "27-th iteration, loss: 0.11756180450209583, 21 gd steps\n",
      "insert gradient: -0.001062820160084021\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          73.68852243  99.02569845  46.63003658 108.70455598\n",
      "  67.8347832  109.75559158  80.97727499 184.85687383  64.5717659\n",
      " 147.18854572  57.32766128  29.59984209  38.30503909  97.73135979\n",
      "  52.19684088  63.68745504  25.98778633  99.98805306  59.08325075\n",
      "  52.98989775]\n",
      "28-th iteration, loss: 0.11754785622264964, 16 gd steps\n",
      "insert gradient: -0.0014806442385173577\n",
      "28-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 7.39141317e+01 9.90445961e+01 4.65077422e+01\n",
      " 1.08962534e+02 6.78731507e+01 1.09734839e+02 8.09982312e+01\n",
      " 1.85061022e+02 0.00000000e+00 2.13162821e-14 6.43573032e+01\n",
      " 1.47225565e+02 5.74817815e+01 3.05740051e+01 3.76652044e+01\n",
      " 9.79846690e+01 5.22030304e+01 6.51050721e+01 2.51646692e+01\n",
      " 1.00381343e+02 5.90171222e+01 5.29898978e+01]\n",
      "29-th iteration, loss: 0.11753437385096999, 25 gd steps\n",
      "insert gradient: -0.001368487440690183\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          73.94833943  99.00863509  46.42265124 109.04749077\n",
      "  68.16560711 109.64818391  81.14799482 185.13782879  64.27321186\n",
      " 147.54436964  57.16002463  31.10214404  37.56554748  97.93632868\n",
      "  52.50863772  65.67943451  24.88391072 100.43174074  58.96963683\n",
      "  52.98989775]\n",
      "30-th iteration, loss: 0.11752684209637912, 19 gd steps\n",
      "insert gradient: -0.0006681406091028903\n",
      "30-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          74.01452161  99.01491883  46.3907995  109.05808727\n",
      "  68.20082114 109.57401537  81.23466621 185.26268854  64.24572227\n",
      " 147.65313946  57.0858319   31.35180858  37.56837028  97.92475725\n",
      "  52.56716569  65.90956942  24.69555767 100.47820586  58.8863094\n",
      "  52.98989775]\n",
      "31-th iteration, loss: 0.11750891200820344, 24 gd steps\n",
      "insert gradient: -0.0009875946538604476\n",
      "31-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          74.2215594   98.89162159  46.24845373 109.24459713\n",
      "  68.29761025 109.37706391  81.37204831 185.45792799  64.04857685\n",
      " 147.87437169  57.10674777  32.12668481  37.13626076  98.16247119\n",
      "  52.79010614  67.15050668  23.91558504 100.76256724  58.52947955\n",
      "  52.98989775]\n",
      "32-th iteration, loss: 0.11732015983398938, 26 gd steps\n",
      "insert gradient: -0.0089188615370411\n",
      "32-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          75.31749978  97.79692784  45.84139358 110.44470868\n",
      "  68.39055676 108.62712098  82.08393683 186.09792847  63.55090715\n",
      " 148.99379423  57.41993036  33.99311524  36.12092324  98.85554558\n",
      "  54.38144222  70.39895797  22.06817693 100.18566826  56.73307424\n",
      "  52.98989775]\n",
      "33-th iteration, loss: 0.11686765468661071, 21 gd steps\n",
      "insert gradient: -0.0076523591556566265\n",
      "33-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          76.64456541  96.212767    46.1410369  111.65647958\n",
      "  68.67209242 107.76584624  82.98811203 118.80418281   0.\n",
      "  67.88810446  63.19629937 149.15510802  57.76192929  34.0978739\n",
      "  35.92301059  99.52658173  55.82867825  72.7089023   21.77598627\n",
      "  98.07106493  55.82373971  52.98989775]\n",
      "34-th iteration, loss: 0.11616538132905435, 26 gd steps\n",
      "insert gradient: -0.007881781322822412\n",
      "34-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          79.70354517  89.35541708  47.16804241 117.15714656\n",
      "  66.71155703 109.46711634  82.21998775 114.85860074   3.36591777\n",
      "  64.22389532  65.2572476  148.58700813  58.80133021  40.3423952\n",
      "  32.14832881 101.18206716  56.36328243  81.01740914  19.48229756\n",
      "  93.76433327  56.83542182  52.98989775]\n",
      "35-th iteration, loss: 0.11592322136861181, 19 gd steps\n",
      "insert gradient: -0.005037235487096606\n",
      "35-th iteration, new layer inserted. now 25 layers\n",
      "[0.00000000e+00 7.98233574e+01 8.97728581e+01 4.71007701e+01\n",
      " 1.17572117e+02 0.00000000e+00 1.06581410e-14 6.75927030e+01\n",
      " 1.08831605e+02 8.26427591e+01 1.14811502e+02 4.19324416e+00\n",
      " 6.24148718e+01 6.58611714e+01 1.48081018e+02 5.94178816e+01\n",
      " 4.09919285e+01 3.18124431e+01 1.00626006e+02 5.71437444e+01\n",
      " 8.15237409e+01 1.95232179e+01 9.35335714e+01 5.68231368e+01\n",
      " 5.29898978e+01]\n",
      "36-th iteration, loss: 0.11570846337038074, 18 gd steps\n",
      "insert gradient: -0.00446184771780954\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[0.00000000e+00 8.03029979e+01 8.94263675e+01 4.72183656e+01\n",
      " 1.18572572e+02 0.00000000e+00 1.77635684e-14 6.77015684e+01\n",
      " 1.08954059e+02 8.23737196e+01 1.15781116e+02 5.99782604e+00\n",
      " 5.78614229e+01 6.65256683e+01 1.47793207e+02 5.99402153e+01\n",
      " 4.39258881e+01 3.03959638e+01 1.01239438e+02 5.74925200e+01\n",
      " 8.36849883e+01 1.91760375e+01 9.15855616e+01 5.69181299e+01\n",
      " 5.29898978e+01]\n",
      "37-th iteration, loss: 0.11559835290886722, 15 gd steps\n",
      "insert gradient: -0.002958165252583725\n",
      "37-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          80.51437951  89.31944255  47.39419902 118.51395989\n",
      "  67.93457511 109.61961284  82.33482654 115.86300634   6.72270458\n",
      "  55.75057724  67.11623954 147.97561243  59.39540678  44.83714167\n",
      "  30.07183696 101.53516078  57.59239603  84.39167444  19.24379785\n",
      "  91.22981065  57.03529695  52.98989775]\n",
      "38-th iteration, loss: 0.11543840288252065, 31 gd steps\n",
      "insert gradient: -0.005167386577650773\n",
      "38-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          80.48470982  89.13266013  47.17831593 118.59727324\n",
      "  68.81484332 109.68994689  82.38827938 116.41817606   8.35589769\n",
      "  51.49939093  67.39954    147.53837015  59.83670071  47.18221596\n",
      "  28.72348426 101.32924813  58.16401197  85.82261227  19.34677969\n",
      "  90.09970643  57.26832525  52.98989775]\n",
      "39-th iteration, loss: 0.11497029299791113, 57 gd steps\n",
      "insert gradient: -0.0028719924577319082\n",
      "39-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          80.89490453  88.85908137  46.63424192 119.15625772\n",
      "  68.83382876 109.44321742  80.99430522 117.31640913  14.49443359\n",
      "  37.711643    69.0456675  147.28638692  59.80931507  57.12483326\n",
      "  24.30836988 100.35527187  58.94852906  89.80473817  18.87710544\n",
      "  85.27496702  57.822263    52.98989775]\n",
      "40-th iteration, loss: 0.11452551701133334, 23 gd steps\n",
      "insert gradient: -0.003551231580888004\n",
      "40-th iteration, new layer inserted. now 25 layers\n",
      "[0.00000000e+00 8.14807110e+01 8.88180829e+01 4.60145649e+01\n",
      " 1.18986293e+02 6.92807536e+01 1.09397457e+02 7.98349363e+01\n",
      " 1.18413673e+02 0.00000000e+00 1.42108547e-14 1.90605389e+01\n",
      " 2.90985994e+01 6.90481699e+01 1.47055920e+02 6.13405883e+01\n",
      " 6.25328455e+01 2.13119924e+01 1.00278432e+02 5.92320872e+01\n",
      " 9.20175780e+01 1.97931262e+01 8.15369615e+01 5.68850317e+01\n",
      " 5.29898978e+01]\n",
      "41-th iteration, loss: 0.11400051937371786, 17 gd steps\n",
      "insert gradient: -0.013050184664639012\n",
      "41-th iteration, new layer inserted. now 25 layers\n",
      "[0.00000000e+00 8.09230012e+01 8.81033503e+01 4.56891554e+01\n",
      " 1.18916557e+02 7.12832289e+01 1.07791432e+02 7.82877992e+01\n",
      " 1.16127070e+02 0.00000000e+00 1.77635684e-14 2.72114465e+01\n",
      " 1.74393093e+01 6.94729939e+01 1.47341481e+02 5.96096468e+01\n",
      " 7.29795426e+01 1.80298358e+01 9.59553845e+01 5.97790755e+01\n",
      " 9.61658938e+01 2.10905232e+01 7.61421307e+01 5.70855539e+01\n",
      " 5.29898978e+01]\n",
      "42-th iteration, loss: 0.11264854854702676, 41 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "42-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          81.92638467  88.14697437  44.57804978 117.59221702\n",
      "  70.90293546 107.80186657  77.97313232 113.23498165  37.59980051\n",
      "   5.97509122  69.2573382  147.66632335  61.25381733  75.51271208\n",
      "  17.85266332  94.6516062   59.66751931  95.85854651  22.96391279\n",
      "  70.38741623  56.44695352  52.98989775]\n",
      "43-th iteration, loss: 0.11241378146257269, 23 gd steps\n",
      "insert gradient: -0.002907209822135853\n",
      "43-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          82.32873376  88.19636484  44.61945474 117.33292861\n",
      "  70.72760302 107.79891372  77.67576807 112.43875135  38.70833933\n",
      "   3.48823959  69.47927906 147.42250467  61.3070961   76.50051367\n",
      "  17.65520022  93.79399325  59.65801998  96.29731856  23.4472503\n",
      "  69.01741534  56.43600178  52.98989775]\n",
      "44-th iteration, loss: 0.11214251378198553, 20 gd steps\n",
      "insert gradient: -0.0034285721049637013\n",
      "44-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          82.67432661  87.8863804   44.35104634 117.11488783\n",
      "  70.98832316 107.082737    77.8672931  110.08832777 111.70327865\n",
      " 147.56912752  61.07186459  79.19350442  17.40612229  91.53939496\n",
      "  59.72741697  97.36932386  24.75639163  65.58047799  56.59668184\n",
      "  52.98989775]\n",
      "45-th iteration, loss: 0.11208945402779512, 20 gd steps\n",
      "insert gradient: -0.0005659208226417192\n",
      "45-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          82.74774907  87.77105474  44.20859239 116.97338763\n",
      "  70.81444312 107.33014883  77.779114   110.38086299 111.65600217\n",
      " 147.43436228  61.35908637  79.84798924  17.29705276  91.29656382\n",
      "  59.53882083  97.41998671  25.0333946   64.73673149  56.50429825\n",
      "  52.98989775]\n",
      "46-th iteration, loss: 0.11202687893940301, 32 gd steps\n",
      "insert gradient: -0.0004703968129997002\n",
      "46-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          83.05513193  87.34181115  43.90460898 116.75987136\n",
      "  70.98831651 107.33397685  77.70378696 110.80133166 111.4209879\n",
      " 147.5146939   61.32638019  83.05619905  16.74246767  89.83298994\n",
      "  59.36843077  97.87560029  26.96358028  60.6382925   56.83409257\n",
      "  52.98989775]\n",
      "47-th iteration, loss: 0.11202329004409585, 22 gd steps\n",
      "insert gradient: -0.0004117898091659888\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          83.15831348  87.11199208  43.89750564 116.68094463\n",
      "  71.12192888 107.347353    77.75557689 110.80665385 111.46930958\n",
      " 147.33538794  61.42062063  83.15763679  16.73704517  89.72922335\n",
      "  59.32810711  98.03198657  27.30963903  60.10838917  56.7560046\n",
      "  52.98989775]\n",
      "48-th iteration, loss: 0.11202249212199175, 16 gd steps\n",
      "insert gradient: -0.00011881312236960988\n",
      "48-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 8.31643691e+01 8.70819074e+01 4.38961592e+01\n",
      " 1.16653181e+02 7.11504152e+01 1.07348081e+02 7.77685616e+01\n",
      " 1.10790122e+02 1.11479383e+02 1.47353243e+02 6.14360117e+01\n",
      " 8.32137833e+01 1.67540588e+01 8.97200517e+01 5.93321709e+01\n",
      " 9.80271654e+01 0.00000000e+00 3.55271368e-15 2.73363790e+01\n",
      " 6.00145492e+01 5.67278503e+01 5.29898978e+01]\n",
      "49-th iteration, loss: 0.11202203754391393, 20 gd steps\n",
      "insert gradient: -6.74599546353559e-05\n",
      "49-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 8.31716813e+01 8.70478511e+01 4.39027688e+01\n",
      " 1.16620789e+02 7.11873653e+01 1.07351526e+02 7.77843198e+01\n",
      " 1.10778576e+02 1.11488849e+02 1.47360355e+02 6.14394438e+01\n",
      " 0.00000000e+00 8.88178420e-15 8.32784951e+01 1.67490699e+01\n",
      " 8.96977935e+01 5.93311997e+01 9.80306200e+01 2.74119945e+01\n",
      " 5.99034577e+01 5.67272500e+01 5.29898978e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5334898343038135\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.78420214    0.         1288.87408932]\n",
      "1-th iteration, loss: 0.7459714673681438, 11 gd steps\n",
      "insert gradient: -0.6286686086337188\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.72789646   62.36837176  235.76965049    0.         1053.10443884]\n",
      "2-th iteration, loss: 0.6056736245890192, 13 gd steps\n",
      "insert gradient: -0.6495701444152857\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.71250252  77.52257436 221.07769301  40.95588881 236.41120056\n",
      "   0.         816.69323828]\n",
      "3-th iteration, loss: 0.4618575141878406, 26 gd steps\n",
      "insert gradient: -0.7001578573676189\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[3.99269427e-02 6.01946803e+01 2.25553060e+02 4.54676520e+01\n",
      " 1.58482075e+02 5.63062171e+01 6.06686406e+02 0.00000000e+00\n",
      " 2.10006833e+02]\n",
      "4-th iteration, loss: 0.36762106707602926, 18 gd steps\n",
      "insert gradient: -0.4170149712362924\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.79049119  51.41546228 115.8932774    0.         107.61518616\n",
      "  64.47650196 138.33729022  59.01323754 569.22151662  45.87291968\n",
      " 210.0068327 ]\n",
      "5-th iteration, loss: 0.31344209116025007, 213 gd steps\n",
      "insert gradient: -0.37698028722599114\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.96908923  53.63216451  63.36778846  26.4197308   93.43410158\n",
      "  68.77839069 119.74200531  71.24351304 168.67469072   0.\n",
      " 361.44576582  77.66397463 210.0068327 ]\n",
      "6-th iteration, loss: 0.23850989645873724, 36 gd steps\n",
      "insert gradient: -0.09919558858595118\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.91651277  51.69422899  87.18610807  26.52227689  80.55319508\n",
      "  60.06325985 139.68855947  66.26759289 140.91621883  60.54998252\n",
      " 298.09147111  84.97757095 198.33978644   0.          11.66704626]\n",
      "7-th iteration, loss: 0.22294800063788436, 122 gd steps\n",
      "insert gradient: -0.0764125398410963\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.58204923  51.83862143  82.71133147  30.12581901  77.47772904\n",
      "  61.31175786 133.5949017   67.94089053 137.11305699  74.13671187\n",
      " 189.34660677   0.          86.06663944  89.6359386  166.20805318\n",
      "  33.63060413  11.66704626]\n",
      "8-th iteration, loss: 0.21369474767907526, 16 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  0.          44.6814249   94.65778708  31.81342498  70.38860474\n",
      "  61.49974523 135.1646161   66.21225086 139.26067318  69.5023358\n",
      " 162.15391106  24.69859236  50.94483519  96.48152425 159.4474304\n",
      "  50.40012164  11.66704626]\n",
      "9-th iteration, loss: 0.19446322908717328, 35 gd steps\n",
      "insert gradient: -0.10331512016644294\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[4.78868902e+00 5.29091832e+01 9.12469546e+01 0.00000000e+00\n",
      " 1.24344979e-14 3.12071933e+01 7.03084884e+01 5.40333646e+01\n",
      " 1.42401375e+02 6.60991941e+01 1.42758758e+02 6.10404500e+01\n",
      " 1.47730711e+02 5.82177595e+01 2.06425736e+00 1.05774299e+02\n",
      " 1.48128266e+02 7.70736597e+01 1.16670463e+01]\n",
      "10-th iteration, loss: 0.19128710805813068, 225 gd steps\n",
      "insert gradient: -0.02146307922632203\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  3.95200917  51.49423886  88.98617243  34.07490294  71.14813115\n",
      "  54.64637068 142.36949281  63.94258997 142.37473617  66.02731312\n",
      " 148.62348311  56.9898653    0.60700783  67.20285298   0.\n",
      "  37.33491832 146.45248037  76.50198328  11.66704626]\n",
      "11-th iteration, loss: 0.18918706364973706, 20 gd steps\n",
      "insert gradient: -0.011205312224209311\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  2.73397408  51.62382806  88.39285034  33.96677011  73.09454951\n",
      "  56.00678344 142.25855537  64.09076578 140.58609217  67.82634428\n",
      " 145.99751337 116.3347289   15.66453109  33.17315544 115.95055654\n",
      "   0.          31.62287906  78.87077098  11.66704626]\n",
      "12-th iteration, loss: 0.18853158902075928, 22 gd steps\n",
      "insert gradient: -0.0100880349193526\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[3.51094418e+00 5.18986051e+01 8.67686558e+01 3.54250619e+01\n",
      " 7.41473896e+01 0.00000000e+00 1.24344979e-14 5.46331412e+01\n",
      " 1.42711041e+02 6.47012625e+01 1.42173017e+02 6.75753935e+01\n",
      " 1.46810752e+02 1.16288197e+02 1.95996742e+01 3.14944197e+01\n",
      " 1.15944247e+02 2.38704885e+00 3.12484923e+01 7.68550575e+01\n",
      " 1.16670463e+01]\n",
      "13-th iteration, loss: 0.18827818932752613, 22 gd steps\n",
      "insert gradient: -0.004069092936568847\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[3.28445964e+00 5.20924562e+01 8.69127481e+01 3.52161273e+01\n",
      " 7.48042245e+01 2.45859781e-01 3.96899862e-02 5.49473361e+01\n",
      " 1.43528026e+02 6.43164239e+01 1.42650668e+02 6.73891514e+01\n",
      " 1.46986259e+02 1.15799774e+02 2.21138392e+01 3.09328670e+01\n",
      " 1.16695729e+02 3.02285495e+00 3.16265680e+01 7.59080580e+01\n",
      " 1.16670463e+01]\n",
      "14-th iteration, loss: 0.18399866774414142, 268 gd steps\n",
      "insert gradient: -0.008163895982077065\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  3.88202819  49.43581767  85.00891712  38.3110371   79.84080315\n",
      "  57.7800549  143.8911671   66.22986018 141.75831643  74.46973313\n",
      " 137.53699522 100.07350857  49.25273075  17.03314567 103.8275123\n",
      "   0.          20.76550246   4.8850596   48.33907947  92.18602071\n",
      "  11.66704626]\n",
      "15-th iteration, loss: 0.17531271408873703, 164 gd steps\n",
      "insert gradient: -0.010059689737276395\n",
      "15-th iteration, new layer inserted. now 19 layers\n",
      "[  1.77363933  48.02576652  83.36828352  42.13169557  81.60492738\n",
      "  75.48871627 121.98714197  77.10747505 129.45536271  81.37962245\n",
      " 130.75447046  84.94809599 190.22798958  24.85950611  29.40576519\n",
      "  76.22699566   0.          30.49079826  11.66704626]\n",
      "16-th iteration, loss: 0.17265454137613778, 18 gd steps\n",
      "insert gradient: -0.011608471590251314\n",
      "16-th iteration, new layer inserted. now 21 layers\n",
      "[1.51475846e+00 4.86274369e+01 8.37938456e+01 4.23555797e+01\n",
      " 8.49264716e+01 7.72600306e+01 1.19097939e+02 7.87526538e+01\n",
      " 1.26916313e+02 8.20246386e+01 1.32361232e+02 8.33313475e+01\n",
      " 1.76674926e+02 3.29027810e+01 1.56086731e+01 7.58060621e+01\n",
      " 0.00000000e+00 2.13162821e-14 9.61777782e+00 2.93396545e+01\n",
      " 1.16670463e+01]\n",
      "17-th iteration, loss: 0.16208128005154843, 45 gd steps\n",
      "insert gradient: -0.03520274825477133\n",
      "17-th iteration, new layer inserted. now 17 layers\n",
      "[  2.02824612  46.42089607  85.27297518  45.5397012   84.8469726\n",
      "  83.4250885  108.91551708  82.16057138 122.84250513  84.42370916\n",
      " 138.78064102  79.18751522 159.51645514 105.46020893  46.6327559\n",
      "  10.54417815  11.66704626]\n",
      "18-th iteration, loss: 0.159919674480937, 71 gd steps\n",
      "insert gradient: -0.00332797232503472\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[  2.33437138  47.8862103   86.08701631  45.68025795  84.13208568\n",
      "  85.06715801 104.15124329  83.95196175 121.14507091  84.72278663\n",
      " 125.03156765   0.          20.83859461  76.71523777 157.17721378\n",
      "  94.96814302  49.70254684   0.58956457  11.66704626]\n",
      "19-th iteration, loss: 0.159879528437872, 17 gd steps\n",
      "insert gradient: -0.004826909843837993\n",
      "19-th iteration, new layer inserted. now 19 layers\n",
      "[  2.21095653  48.00536851  86.36465625  45.65070925  84.12249529\n",
      "  85.18629798 104.17972801  84.07425503 120.99516158  85.05197809\n",
      " 124.61024425   1.35742221  20.38706323  75.49034084 157.44458651\n",
      "  94.90919033  49.63714572   0.49264602  11.66704626]\n",
      "20-th iteration, loss: 0.15987173029537088, 18 gd steps\n",
      "insert gradient: -0.0021186155852991506\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[  2.23286881  48.03487027  86.33221232  45.7560405   84.17499486\n",
      "  85.22917498 104.18977612  84.08238542 120.95658529  85.009672\n",
      " 124.60231577   1.53900565  20.35723607  75.34157262 157.57253237\n",
      "  71.24205592   0.          23.74735197  49.62362955   0.42862559\n",
      "  11.66704626]\n",
      "21-th iteration, loss: 0.15982600032469602, 18 gd steps\n",
      "insert gradient: -0.002653939161232786\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[  2.16407593  48.10269396  86.32225669  45.80020932  84.04866771\n",
      "  85.31278107 104.41545667  84.04846191 121.05060766  85.24790155\n",
      " 124.44825883   2.20035241  19.91158172  74.53489216 158.13297422\n",
      "  70.51897629   2.077436    22.98762644  61.24781552]\n",
      "22-th iteration, loss: 0.15981993818347226, 19 gd steps\n",
      "insert gradient: -0.0006557202040046703\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[2.21529209e+00 4.82178884e+01 8.63921677e+01 4.57822633e+01\n",
      " 8.41317875e+01 8.53093393e+01 1.04390933e+02 8.40704836e+01\n",
      " 1.20990330e+02 8.52030375e+01 1.24490304e+02 0.00000000e+00\n",
      " 3.55271368e-14 2.34256990e+00 1.98730329e+01 7.43919093e+01\n",
      " 1.58313263e+02 7.06002627e+01 2.23365178e+00 2.30568418e+01\n",
      " 6.12478155e+01]\n",
      "23-th iteration, loss: 0.1598127352141357, 83 gd steps\n",
      "insert gradient: -0.00047129381439014827\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[  2.1809773   48.29977925  86.41972572  45.78577221  84.15999704\n",
      "  85.39123452 104.41544525  84.14744621 120.86550509  85.36597094\n",
      " 125.12663693   0.26125231   0.38011199   2.76037336  18.60353691\n",
      "  73.50723919 158.90705881  70.60734018   2.49715119  22.96051706\n",
      "  61.24781552]\n",
      "24-th iteration, loss: 0.15981235744997752, 32 gd steps\n",
      "insert gradient: -8.118142102586744e-05\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[  2.17891183  48.27714298  86.41627032  45.8063875   84.17731722\n",
      "  85.38466689 104.41139607  84.14880677 120.86253029  85.36709683\n",
      " 125.19328373   0.24685291   0.41177332   2.79401241  18.49657007\n",
      "  73.49649986 158.91829875  70.60654125   2.51014419  22.95906373\n",
      "  61.24781552]\n",
      "25-th iteration, loss: 0.15981225725426496, 12 gd steps\n",
      "insert gradient: -0.0003065842470809012\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[  2.18417476  48.29318626  86.41437887  45.79090815  84.17343121\n",
      "  85.3899855  104.41101925  84.14977271 120.86056394  85.36552515\n",
      " 125.25944344   0.2298226    0.44621855   2.82511798  18.40245878\n",
      "  73.47872299 158.92348148  70.60478466   2.51387366  22.9574063\n",
      "  61.24781552]\n",
      "26-th iteration, loss: 0.1598115743720636, 45 gd steps\n",
      "insert gradient: -0.0003024100845371025\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[2.18341466e+00 4.82881545e+01 8.64140211e+01 0.00000000e+00\n",
      " 1.24344979e-14 4.57910599e+01 8.41721299e+01 8.53908608e+01\n",
      " 1.04412275e+02 8.41476270e+01 1.20860534e+02 8.53576407e+01\n",
      " 1.25508372e+02 1.31752024e-01 5.88723697e-01 2.97208474e+00\n",
      " 1.80357331e+01 7.34170682e+01 1.58935664e+02 7.06009584e+01\n",
      " 2.52164728e+00 2.29572940e+01 6.12478155e+01]\n",
      "27-th iteration, loss: 0.15981147103078247, 19 gd steps\n",
      "insert gradient: -5.234316982049563e-05\n",
      "27-th iteration, new layer inserted. now 23 layers\n",
      "[2.17752459e+00 4.82685260e+01 8.64132973e+01 4.58091986e+01\n",
      " 8.41756926e+01 8.53866030e+01 1.04411042e+02 8.41478135e+01\n",
      " 1.20860826e+02 8.53579441e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.25522892e+02 1.22607305e-01 5.98363739e-01 2.98311109e+00\n",
      " 1.80121403e+01 7.34145869e+01 1.58936836e+02 7.06008151e+01\n",
      " 2.52183995e+00 2.29575803e+01 6.12478155e+01]\n",
      "28-th iteration, loss: 0.15981119143491315, 17 gd steps\n",
      "insert gradient: -5.6535207792698385e-05\n",
      "28-th iteration, new layer inserted. now 23 layers\n",
      "[2.17750425e+00 4.82645791e+01 8.64183158e+01 4.58081282e+01\n",
      " 8.41802082e+01 0.00000000e+00 1.24344979e-14 8.53777668e+01\n",
      " 1.04411061e+02 8.41424524e+01 1.20860642e+02 8.53617840e+01\n",
      " 1.25684221e+02 5.66273342e-02 6.49499808e-01 3.05765296e+00\n",
      " 1.78394483e+01 7.33895834e+01 1.58935782e+02 7.06017590e+01\n",
      " 2.52007032e+00 2.29617687e+01 6.12478155e+01]\n",
      "29-th iteration, loss: 0.15981106243378018, 17 gd steps\n",
      "insert gradient: -5.200520602915475e-05\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[2.17946977e+00 4.82688371e+01 8.64072483e+01 4.58085341e+01\n",
      " 8.41733003e+01 8.53825849e+01 1.04411201e+02 8.41462672e+01\n",
      " 1.20859769e+02 8.53485550e+01 1.25720849e+02 2.66983855e-02\n",
      " 6.79249071e-01 3.10398920e+00 1.77634601e+01 7.33810035e+01\n",
      " 1.58938967e+02 7.05987455e+01 2.52110608e+00 2.29597672e+01\n",
      " 6.12478155e+01]\n",
      "30-th iteration, loss: 0.15981091030891, 22 gd steps\n",
      "insert gradient: -6.907252717753534e-05\n",
      "30-th iteration, new layer inserted. now 21 layers\n",
      "[2.17915246e+00 4.82670526e+01 8.64068778e+01 4.58182294e+01\n",
      " 8.41777171e+01 8.53850523e+01 1.04412359e+02 8.41451428e+01\n",
      " 1.20860770e+02 0.00000000e+00 1.42108547e-14 8.53484586e+01\n",
      " 1.26532628e+02 3.17134965e+00 1.76221652e+01 7.33407521e+01\n",
      " 1.58943040e+02 7.05977281e+01 2.52409551e+00 2.29624042e+01\n",
      " 6.12478155e+01]\n",
      "31-th iteration, loss: 0.15981087433329969, 11 gd steps\n",
      "insert gradient: -4.071108424731097e-05\n",
      "31-th iteration, new layer inserted. now 21 layers\n",
      "[2.18005729e+00 4.82718394e+01 8.64085317e+01 4.58079416e+01\n",
      " 8.41748694e+01 8.53834439e+01 1.04411821e+02 8.41458281e+01\n",
      " 1.20861160e+02 4.99565127e-04 2.85172748e-04 8.53489646e+01\n",
      " 1.26536238e+02 3.17194413e+00 1.76176624e+01 7.33399368e+01\n",
      " 1.58944614e+02 7.05979440e+01 2.52402428e+00 2.29628379e+01\n",
      " 6.12478155e+01]\n",
      "32-th iteration, loss: 0.15981079225637865, 15 gd steps\n",
      "insert gradient: -3.9964549159487916e-05\n",
      "32-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17663989  48.27027127  86.42200055  45.79921635  84.17896343\n",
      "  85.3835081  104.41491104  84.14373773 120.86552024  85.34820839\n",
      " 126.61417848   3.18678325  17.5337278   73.32403081 158.95633216\n",
      "  70.59469419   2.52662097  22.96303983  61.24781552]\n",
      "33-th iteration, loss: 0.15981067963663922, 28 gd steps\n",
      "insert gradient: -6.882797003199383e-05\n",
      "33-th iteration, new layer inserted. now 21 layers\n",
      "[2.17634647e+00 0.00000000e+00 7.21644966e-16 4.82644697e+01\n",
      " 8.64136110e+01 4.58052267e+01 8.41782603e+01 8.53826773e+01\n",
      " 1.04413394e+02 8.41436636e+01 1.20864999e+02 8.53464859e+01\n",
      " 1.26737898e+02 3.20588042e+00 1.74163393e+01 7.33011559e+01\n",
      " 1.58962420e+02 7.05901065e+01 2.52884435e+00 2.29653488e+01\n",
      " 6.12478155e+01]\n",
      "34-th iteration, loss: 0.15981067162851265, 12 gd steps\n",
      "insert gradient: -5.1350117647967236e-05\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[  2.1770883   48.26471121  86.41349637  45.80582039  84.17669727\n",
      "  85.38275976 104.41371429  84.14384857 120.8652308   85.34621177\n",
      " 126.74685092   3.20661906  17.40729526  73.30010188 158.9630241\n",
      "  70.5899314    2.52881592  22.9657575   61.24781552]\n",
      "35-th iteration, loss: 0.15981066018833903, 12 gd steps\n",
      "insert gradient: -7.940111643559461e-05\n",
      "35-th iteration, new layer inserted. now 21 layers\n",
      "[2.17730528e+00 0.00000000e+00 7.21644966e-16 4.82635549e+01\n",
      " 8.64123972e+01 4.58068700e+01 8.41756801e+01 8.53826021e+01\n",
      " 1.04414030e+02 8.41435982e+01 1.20865277e+02 8.53460017e+01\n",
      " 1.26762482e+02 3.20844934e+00 1.73922873e+01 7.32981725e+01\n",
      " 1.58963841e+02 7.05893767e+01 2.52862747e+00 2.29662070e+01\n",
      " 6.12478155e+01]\n",
      "36-th iteration, loss: 0.15981063938345874, 26 gd steps\n",
      "insert gradient: -3.072988581887559e-05\n",
      "36-th iteration, new layer inserted. now 23 layers\n",
      "[2.17853827e+00 2.88474589e-03 8.52369913e-04 4.82665680e+01\n",
      " 8.64129633e+01 4.58029933e+01 8.41738554e+01 0.00000000e+00\n",
      " 1.24344979e-14 8.53809136e+01 1.04413739e+02 8.41436575e+01\n",
      " 1.20865539e+02 8.53457202e+01 1.26786227e+02 3.21123363e+00\n",
      " 1.73711437e+01 7.32952715e+01 1.58964826e+02 7.05889741e+01\n",
      " 2.52828671e+00 2.29673865e+01 6.12478155e+01]\n",
      "37-th iteration, loss: 0.15981060981502387, 27 gd steps\n",
      "insert gradient: -8.035689353538968e-05\n",
      "37-th iteration, new layer inserted. now 23 layers\n",
      "[2.17842703e+00 4.82659126e+01 8.64123468e+01 0.00000000e+00\n",
      " 1.77635684e-15 4.58023913e+01 8.41739734e+01 4.17806541e-05\n",
      " 1.15534241e-04 8.53819119e+01 1.04413914e+02 8.41433925e+01\n",
      " 1.20865789e+02 8.53443581e+01 1.26825055e+02 3.21554135e+00\n",
      " 1.73346149e+01 7.32899563e+01 1.58965222e+02 7.05873108e+01\n",
      " 2.52908066e+00 2.29682494e+01 6.12478155e+01]\n",
      "38-th iteration, loss: 0.1598105582571333, 37 gd steps\n",
      "insert gradient: -2.7140414449569168e-05\n",
      "38-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17862697  48.26744103  86.41279091  45.80234311  84.17407284\n",
      "  85.38184537 104.41325243  84.14295817 120.86638641  85.34220436\n",
      " 126.89597036   3.22294373  17.27004569  73.28013943 158.96583065\n",
      "  70.58469247   2.52969059  22.97043668  61.24781552]\n",
      "39-th iteration, loss: 0.15981050743772954, 15 gd steps\n",
      "insert gradient: -0.0003215165493466335\n",
      "39-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17250935  48.24643459  86.41497592  45.81582345  84.17632309\n",
      "  85.37647559 104.41230459  84.14275908 120.86688908  85.33942452\n",
      " 127.08354248   3.24618702  17.10535262  73.24739343 158.96987737\n",
      "  70.57836281   2.52773671  22.97819413  61.24781552]\n",
      "40-th iteration, loss: 0.15981044034285724, 28 gd steps\n",
      "insert gradient: -4.847180922286806e-05\n",
      "40-th iteration, new layer inserted. now 19 layers\n",
      "[  2.18012495  48.26990847  86.4124143   45.79954165  84.17267828\n",
      "  85.37990987 104.41337976  84.14211976 120.86655892  85.33753933\n",
      " 127.10037391   3.24650859  17.08500252  73.24881292 158.97004041\n",
      "  70.57690042   2.52886355  22.97813641  61.24781552]\n",
      "41-th iteration, loss: 0.15981043322242122, 15 gd steps\n",
      "insert gradient: -3.418906250267895e-05\n",
      "41-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17877933  48.26453017  86.41179196  45.80315127  84.17384041\n",
      "  85.38090151 104.41355936  84.1418495  120.86649549  85.33714848\n",
      " 127.10943913   3.24699492  17.07481126  73.24839612 158.96976898\n",
      "  70.57592885   2.52934074  22.97793681  61.24781552]\n",
      "42-th iteration, loss: 0.15981042841076243, 12 gd steps\n",
      "insert gradient: -8.548071784534082e-05\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17798575  48.26106909  86.41164224  45.80552273  84.17404644\n",
      "  85.37977297 104.41322042  84.14161892 120.8666011   85.33684791\n",
      " 127.12640424   3.24859394  17.05916206  73.24729857 158.96974049\n",
      "  70.57508728   2.52968102  22.9785522   61.24781552]\n",
      "43-th iteration, loss: 0.15981041911107374, 16 gd steps\n",
      "insert gradient: -1.5930420924870827e-05\n",
      "43-th iteration, new layer inserted. now 21 layers\n",
      "[2.17951816e+00 4.82664073e+01 8.64119794e+01 4.58013908e+01\n",
      " 8.41727318e+01 8.53792809e+01 1.04413056e+02 8.41416624e+01\n",
      " 1.20866785e+02 8.53364579e+01 0.00000000e+00 1.77635684e-15\n",
      " 1.27137778e+02 3.24942452e+00 1.70491789e+01 7.32463648e+01\n",
      " 1.58969751e+02 7.05747236e+01 2.52965712e+00 2.29791751e+01\n",
      " 6.12478155e+01]\n",
      "44-th iteration, loss: 0.1598104010581689, 17 gd steps\n",
      "insert gradient: -3.217165466418222e-05\n",
      "44-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17913994  48.26397494  86.41131101  45.80307313  84.1741286\n",
      "  85.38111209 104.4125417   84.14102433 120.86579157  85.33618525\n",
      " 127.18362192   3.25058697  17.01375952  73.24190392 158.96786116\n",
      "  70.57343302   2.53112692  22.98049803  61.24781552]\n",
      "45-th iteration, loss: 0.15981039723331242, 11 gd steps\n",
      "insert gradient: -4.8413652005432336e-05\n",
      "45-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17874037  48.26281844  86.41153827  45.8035735   84.17349539\n",
      "  85.38004023 104.41240632  84.14193067 120.86602561  85.33504986\n",
      " 127.19099922   3.25238903  17.00375662  73.24085197 158.96804004\n",
      "  70.57238419   2.53038739  22.98021933  61.24781552]\n",
      "46-th iteration, loss: 0.15981038553694374, 19 gd steps\n",
      "insert gradient: -1.8853089785602754e-05\n",
      "46-th iteration, new layer inserted. now 19 layers\n",
      "[  2.17939039  48.26434387  86.41151304  45.80210346  84.17302814\n",
      "  85.37993685 104.41250897  84.14129535 120.86679467  85.33408378\n",
      " 127.21474903   3.2558629   16.97975437  73.2376651  158.96875729\n",
      "  70.5712511    2.53022597  22.9813418   61.24781552]\n",
      "47-th iteration, loss: 0.15981038072571985, 12 gd steps\n",
      "insert gradient: -7.048164863754281e-05\n",
      "47-th iteration, new layer inserted. now 21 layers\n",
      "[2.17851494e+00 0.00000000e+00 7.21644966e-16 4.82609707e+01\n",
      " 8.64113513e+01 4.58044627e+01 8.41732362e+01 8.53788270e+01\n",
      " 1.04412532e+02 8.41410464e+01 1.20867124e+02 8.53337599e+01\n",
      " 1.27233645e+02 3.25814340e+00 1.69623895e+01 7.32350431e+01\n",
      " 1.58969334e+02 7.05704334e+01 2.52994232e+00 2.29823301e+01\n",
      " 6.12478155e+01]\n",
      "48-th iteration, loss: 0.1598103723209217, 34 gd steps\n",
      "insert gradient: -2.860951645865988e-05\n",
      "48-th iteration, new layer inserted. now 21 layers\n",
      "[2.17966712e+00 4.82649595e+01 8.64112980e+01 0.00000000e+00\n",
      " 1.24344979e-14 4.58007499e+01 8.41723485e+01 8.53792780e+01\n",
      " 1.04412383e+02 8.41410425e+01 1.20867141e+02 8.53332195e+01\n",
      " 1.27247766e+02 3.25965390e+00 1.69492140e+01 7.32332198e+01\n",
      " 1.58969520e+02 7.05697477e+01 2.53001517e+00 2.29830170e+01\n",
      " 6.12478155e+01]\n",
      "49-th iteration, loss: 0.15981035072460395, 16 gd steps\n",
      "insert gradient: -1.5928239117675205e-05\n",
      "49-th iteration, new layer inserted. now 21 layers\n",
      "[2.18079532e+00 4.82682689e+01 8.64144969e+01 4.57993583e+01\n",
      " 8.41712922e+01 0.00000000e+00 5.32907052e-15 8.53797500e+01\n",
      " 1.04412398e+02 8.41405808e+01 1.20868252e+02 8.53305300e+01\n",
      " 1.27320156e+02 3.26758491e+00 1.68851446e+01 7.32223267e+01\n",
      " 1.58969935e+02 7.05661056e+01 2.53000731e+00 2.29864538e+01\n",
      " 6.12478155e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224615\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.59145123    0.         1313.19246837]\n",
      "1-th iteration, loss: 0.7475736523080359, 11 gd steps\n",
      "insert gradient: -0.6221099386503453\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.31049111   62.35738846  240.21813446    0.         1072.97433391]\n",
      "2-th iteration, loss: 0.6075210412153301, 13 gd steps\n",
      "insert gradient: -0.7009566293552942\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.36306164  76.96920446 224.5743563   39.35481856 240.87178924\n",
      "   0.         832.10254466]\n",
      "3-th iteration, loss: 0.47468700001903746, 20 gd steps\n",
      "insert gradient: -0.6849030872309699\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          60.88385296 230.93252166  54.11347654 170.24328705\n",
      "  44.13062296 380.3897347    0.         451.71280996]\n",
      "4-th iteration, loss: 0.39774334827671387, 13 gd steps\n",
      "insert gradient: -0.5631001765114151\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          62.43159598 226.83361069  66.60393593 150.43843063\n",
      "  47.19162132 223.75593157   0.         111.87796579  46.81345973\n",
      " 451.71280996]\n",
      "5-th iteration, loss: 0.29843058421696567, 23 gd steps\n",
      "insert gradient: -0.2963636736553526\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.30097995  74.79130083 217.5380494   63.02884806 116.90858161\n",
      "  63.45057372 178.28053873  44.21675105  82.64472252  46.74645249\n",
      " 225.85640498   0.         225.85640498]\n",
      "6-th iteration, loss: 0.23847508016888133, 21 gd steps\n",
      "insert gradient: -0.08992076864239483\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[4.05122626e-02 6.61783080e+01 2.33291348e+02 5.99005295e+01\n",
      " 1.11075810e+02 6.78820293e+01 1.67834254e+02 5.37802588e+01\n",
      " 7.68707185e+01 3.20478748e+01 1.12465479e+02 0.00000000e+00\n",
      " 8.99723833e+01 4.79219396e+01 2.25856405e+02]\n",
      "7-th iteration, loss: 0.22896245926115058, 30 gd steps\n",
      "insert gradient: -0.08767070263069543\n",
      "7-th iteration, new layer inserted. now 16 layers\n",
      "[1.31355846e-01 6.38181107e+01 2.35843966e+02 6.27369561e+01\n",
      " 1.10060204e+02 6.75337482e+01 1.60067752e+02 6.30897635e+01\n",
      " 8.25520704e+01 3.51300286e+01 7.90744359e+01 2.00337451e+01\n",
      " 6.00380657e+01 5.32522025e+01 2.25856405e+02 0.00000000e+00]\n",
      "8-th iteration, loss: 0.19766260186441464, 67 gd steps\n",
      "insert gradient: -0.171299625675673\n",
      "8-th iteration, new layer inserted. now 18 layers\n",
      "[  0.73571668  60.40918961 135.78131567   0.         118.80865121\n",
      "  57.76039555 107.14603184  72.8442646  153.85099403  65.81321781\n",
      " 101.05036928  23.90432143  41.38826926  51.4096069   54.37295151\n",
      "  41.83810718 197.31469278  52.61580592]\n",
      "9-th iteration, loss: 0.17654912372328332, 21 gd steps\n",
      "insert gradient: -0.06208683704975355\n",
      "9-th iteration, new layer inserted. now 20 layers\n",
      "[  1.67097176  67.09221951 104.66305231  24.72945748  84.00246102\n",
      "  64.38222617 111.9537144   75.89530521 148.48263369  68.6905698\n",
      " 104.25621256  16.83573774  42.68708907  57.18088217  61.23676341\n",
      "  38.52590979 109.86118572   0.          94.16673062  49.94018577]\n",
      "10-th iteration, loss: 0.16243899972680723, 26 gd steps\n",
      "insert gradient: -0.052217873483132504\n",
      "10-th iteration, new layer inserted. now 20 layers\n",
      "[  1.43357533  64.99107839 109.12600181  39.5352407   62.93357609\n",
      "  65.17023324 111.47524578  80.5102012  143.65322499  69.18607185\n",
      " 123.31928611  14.88413407  24.50705077  65.29466824  88.38698012\n",
      "  34.58477225  70.78028751  28.22507386  57.02427809  55.86247131]\n",
      "11-th iteration, loss: 0.13725761103990886, 204 gd steps\n",
      "insert gradient: -0.014004813449707675\n",
      "11-th iteration, new layer inserted. now 18 layers\n",
      "[  3.07007028  61.47303903 117.82457547  53.96708385  62.79450928\n",
      "  57.50667877 117.22471702  69.97578906 166.47152196  74.28088645\n",
      " 124.35582512 104.9141547   98.29513828  43.43806759  41.78455001\n",
      "  42.56218449  69.64377016  47.07711496]\n",
      "12-th iteration, loss: 0.13158048530487892, 34 gd steps\n",
      "insert gradient: -0.004952640653060178\n",
      "12-th iteration, new layer inserted. now 20 layers\n",
      "[4.16791998e+00 5.94497338e+01 1.17599399e+02 5.40546035e+01\n",
      " 7.40761582e+01 5.32902055e+01 1.22481457e+02 6.46818091e+01\n",
      " 1.75662574e+02 7.21823434e+01 1.27659120e+02 0.00000000e+00\n",
      " 1.06581410e-14 1.05032472e+02 9.63341766e+01 4.51690771e+01\n",
      " 3.16184934e+01 4.41098118e+01 9.38139024e+01 3.99096169e+01]\n",
      "13-th iteration, loss: 0.13136820727531784, 53 gd steps\n",
      "insert gradient: -0.0009420338492586033\n",
      "13-th iteration, new layer inserted. now 20 layers\n",
      "[4.45800740e+00 5.90909334e+01 1.17852587e+02 5.36932176e+01\n",
      " 7.56952603e+01 5.25825928e+01 1.23805647e+02 6.45552252e+01\n",
      " 1.76717034e+02 7.02349189e+01 0.00000000e+00 8.88178420e-15\n",
      " 1.31570688e+02 1.05144071e+02 9.63443805e+01 4.53316002e+01\n",
      " 3.24679758e+01 4.37139753e+01 9.34021044e+01 4.10144000e+01]\n",
      "14-th iteration, loss: 0.13128787341253756, 118 gd steps\n",
      "insert gradient: -0.0004094801306279771\n",
      "14-th iteration, new layer inserted. now 18 layers\n",
      "[  4.6537175   58.86310924 117.88754582  53.69212672  76.74050284\n",
      "  52.10162466 124.67286973  64.34485193 177.63665755  68.65521966\n",
      " 134.58188083 105.10601251  96.49020677  45.66094344  33.09542398\n",
      "  43.1249702   93.59873031  41.68560507]\n",
      "15-th iteration, loss: 0.13127450157409737, 48 gd steps\n",
      "insert gradient: -0.00046046923975299434\n",
      "15-th iteration, new layer inserted. now 18 layers\n",
      "[  4.77376852  58.75702919 117.85838342  53.76564322  77.25300482\n",
      "  51.79807303 124.96276262  64.3070139  177.97783533  68.04846712\n",
      " 135.83381953 105.06030895  96.6212648   45.82976007  33.4267642\n",
      "  42.74158211  93.73162638  42.05897453]\n",
      "16-th iteration, loss: 0.13127162120163277, 67 gd steps\n",
      "insert gradient: -9.883224663853471e-05\n",
      "16-th iteration, new layer inserted. now 18 layers\n",
      "[  4.79908697  58.72190574 117.85913846  53.77863477  77.34993086\n",
      "  51.77730076 125.11735282  64.30472659 178.13388227  67.71988443\n",
      " 136.31391753 105.14386775  96.55020672  45.85482906  33.5663382\n",
      "  42.70156888  93.78248127  42.1773626 ]\n",
      "17-th iteration, loss: 0.1312714288294837, 34 gd steps\n",
      "insert gradient: -0.00012928840985607627\n",
      "17-th iteration, new layer inserted. now 20 layers\n",
      "[4.80310422e+00 5.87057471e+01 1.17865151e+02 5.37890590e+01\n",
      " 7.73788827e+01 5.17607977e+01 1.25147884e+02 6.42961920e+01\n",
      " 1.78172967e+02 6.76610429e+01 1.36431464e+02 1.05152465e+02\n",
      " 9.65432493e+01 0.00000000e+00 8.88178420e-15 4.58630425e+01\n",
      " 3.35998384e+01 4.26769197e+01 9.37911852e+01 4.22135822e+01]\n",
      "18-th iteration, loss: 0.13127136756784638, 32 gd steps\n",
      "insert gradient: -7.74929571479428e-05\n",
      "18-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80573567  58.70302958 117.86218923  53.79325096  77.39492483\n",
      "  51.75592266 125.17094498  64.29405689 178.19120337  67.6160223\n",
      " 136.49704887 105.15847048  96.54432312  45.87026283  33.62508599\n",
      "  42.66803097  93.80151149  42.23329475]\n",
      "19-th iteration, loss: 0.13127135303374102, 21 gd steps\n",
      "insert gradient: -2.605068279384397e-05\n",
      "19-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80601783  58.70076394 117.86349038  53.79483051  77.39940884\n",
      "  51.75208043 125.17824138  64.28991131 178.19873651  67.61129795\n",
      " 136.51740234 105.15706316  96.54359507  45.8737849   33.6276566\n",
      "  42.66033679  93.80169837  42.23616369]\n",
      "20-th iteration, loss: 0.13127134898880483, 23 gd steps\n",
      "insert gradient: -1.4464754437304218e-05\n",
      "20-th iteration, new layer inserted. now 20 layers\n",
      "[4.80673956e+00 5.87004100e+01 1.17862685e+02 5.37938101e+01\n",
      " 7.74033930e+01 5.17502004e+01 1.25181214e+02 6.42913354e+01\n",
      " 1.78202268e+02 6.76043044e+01 1.36530563e+02 1.05157768e+02\n",
      " 9.65445380e+01 4.58744103e+01 0.00000000e+00 8.88178420e-16\n",
      " 3.36296793e+01 4.26571561e+01 9.38034052e+01 4.22398226e+01]\n",
      "21-th iteration, loss: 0.13127134767608445, 13 gd steps\n",
      "insert gradient: -3.146806420795122e-05\n",
      "21-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80692263  58.69974761 117.86274942  53.79507093  77.40558104\n",
      "  51.74829978 125.18360854  64.29051326 178.20387549  67.60019597\n",
      " 136.53802153 105.15587376  96.54483759  45.87806419  33.63206634\n",
      "  42.6544472   93.80427082  42.24336083]\n",
      "22-th iteration, loss: 0.13127134445559432, 73 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.633374568280305e-06\n",
      "22-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747948e+00 5.86976535e+01 1.17863200e+02 5.37964777e+01\n",
      " 7.74101177e+01 5.17463559e+01 1.25189263e+02 6.42887999e+01\n",
      " 1.78210122e+02 6.75915370e+01 1.36555282e+02 1.05158591e+02\n",
      " 9.65438539e+01 4.58775490e+01 0.00000000e+00 6.21724894e-15\n",
      " 3.36380559e+01 4.26515215e+01 9.38057991e+01 4.22473893e+01]\n",
      "23-th iteration, loss: 0.13127134445507904, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.580226191152993e-06\n",
      "23-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747950e+00 5.86976532e+01 1.17863201e+02 5.37964782e+01\n",
      " 7.74101181e+01 5.17463563e+01 1.25189264e+02 6.42887997e+01\n",
      " 1.78210122e+02 6.75915341e+01 1.36555283e+02 1.05158592e+02\n",
      " 9.65438532e+01 4.58775494e+01 2.58838878e-06 4.37269580e-07\n",
      " 0.00000000e+00 1.05879118e-22 3.36380585e+01 4.26515238e+01\n",
      " 9.38057993e+01 4.22473890e+01]\n",
      "24-th iteration, loss: 0.13127134445448238, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.512423754207429e-06\n",
      "24-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747951e+00 5.86976529e+01 1.17863201e+02 5.37964787e+01\n",
      " 7.74101185e+01 5.17463566e+01 1.25189265e+02 6.42887994e+01\n",
      " 1.78210123e+02 6.75915313e+01 1.36555284e+02 1.05158594e+02\n",
      " 9.65438524e+01 4.58775498e+01 5.11121488e-06 7.79693144e-07\n",
      " 2.52365289e-06 3.42423564e-07 3.36380610e+01 4.26515259e+01\n",
      " 9.38057995e+01 4.22473887e+01]\n",
      "25-th iteration, loss: 0.13127134445390923, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.4493856138606876e-06\n",
      "25-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747952e+00 5.86976526e+01 1.17863202e+02 5.37964791e+01\n",
      " 7.74101189e+01 5.17463569e+01 1.25189266e+02 6.42887991e+01\n",
      " 1.78210124e+02 6.75915284e+01 1.36555285e+02 1.05158595e+02\n",
      " 9.65438516e+01 4.58775500e+01 7.56864606e-06 1.02834554e-06\n",
      " 4.98255830e-06 5.86304260e-07 3.36380635e+01 4.26515279e+01\n",
      " 9.38057997e+01 4.22473883e+01]\n",
      "26-th iteration, loss: 0.13127134445335673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3905562535397095e-06\n",
      "26-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747953e+00 5.86976523e+01 1.17863202e+02 5.37964796e+01\n",
      " 7.74101193e+01 5.17463572e+01 1.25189267e+02 6.42887988e+01\n",
      " 1.78210124e+02 6.75915255e+01 1.36555286e+02 1.05158596e+02\n",
      " 9.65438508e+01 4.58775502e+01 9.96533987e-06 1.19435063e-06\n",
      " 7.38119651e-06 7.42888375e-07 3.36380659e+01 4.26515299e+01\n",
      " 9.38057999e+01 4.22473880e+01]\n",
      "27-th iteration, loss: 0.13127134445282246, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.3354595132887865e-06\n",
      "27-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747952e+00 5.86976519e+01 1.17863202e+02 5.37964800e+01\n",
      " 7.74101197e+01 5.17463575e+01 1.25189268e+02 6.42887985e+01\n",
      " 1.78210125e+02 6.75915227e+01 1.36555288e+02 1.05158598e+02\n",
      " 9.65438500e+01 4.58775503e+01 1.23054255e-05 1.28726894e-06\n",
      " 9.72354038e-06 8.21850382e-07 3.36380682e+01 4.26515318e+01\n",
      " 9.38058001e+01 4.22473877e+01]\n",
      "28-th iteration, loss: 0.13127134445230465, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.283686494285104e-06\n",
      "28-th iteration, new layer inserted. now 24 layers\n",
      "[4.80747952e+00 5.86976515e+01 1.17863203e+02 5.37964804e+01\n",
      " 7.74101201e+01 5.17463578e+01 1.25189270e+02 6.42887982e+01\n",
      " 1.78210125e+02 6.75915198e+01 1.36555289e+02 1.05158599e+02\n",
      " 9.65438491e+01 4.58775504e+01 1.45925782e-05 1.31533451e-06\n",
      " 1.20131271e-05 8.31530780e-07 0.00000000e+00 1.05879118e-22\n",
      " 3.36380705e+01 4.26515336e+01 9.38058002e+01 4.22473873e+01]\n",
      "29-th iteration, loss: 0.13127134445172728, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.2218434816332665e-06\n",
      "29-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747951e+00 5.86976511e+01 1.17863203e+02 5.37964808e+01\n",
      " 7.74101205e+01 5.17463581e+01 1.25189271e+02 6.42887979e+01\n",
      " 1.78210126e+02 6.75915169e+01 1.36555290e+02 1.05158600e+02\n",
      " 9.65438482e+01 4.58775503e+01 1.68200376e-05 1.27368276e-06\n",
      " 1.42430734e-05 7.67164734e-07 3.36380750e+01 4.26515354e+01\n",
      " 9.38058004e+01 4.22473870e+01]\n",
      "30-th iteration, loss: 0.13127134445124086, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1765356153661643e-06\n",
      "30-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747950e+00 5.86976506e+01 1.17863203e+02 5.37964811e+01\n",
      " 7.74101209e+01 5.17463584e+01 1.25189272e+02 6.42887977e+01\n",
      " 1.78210126e+02 6.75915141e+01 1.36555291e+02 1.05158601e+02\n",
      " 9.65438473e+01 4.58775503e+01 1.89984898e-05 1.17698008e-06\n",
      " 1.64239339e-05 6.43531400e-07 3.36380772e+01 4.26515371e+01\n",
      " 9.38058005e+01 4.22473866e+01]\n",
      "31-th iteration, loss: 0.1312713444507666, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.1335389935587756e-06\n",
      "31-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747949e+00 5.86976502e+01 1.17863203e+02 5.37964815e+01\n",
      " 7.74101212e+01 5.17463587e+01 1.25189273e+02 6.42887974e+01\n",
      " 1.78210127e+02 6.75915113e+01 1.36555293e+02 1.05158603e+02\n",
      " 9.65438464e+01 4.58775502e+01 2.11337015e-05 1.03591612e-06\n",
      " 1.85613710e-05 4.71413252e-07 3.36380793e+01 4.26515387e+01\n",
      " 9.38058006e+01 4.22473863e+01]\n",
      "32-th iteration, loss: 0.1312713444503036, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0926397355422564e-06\n",
      "32-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747947e+00 5.86976497e+01 1.17863204e+02 5.37964818e+01\n",
      " 7.74101216e+01 5.17463589e+01 1.25189274e+02 6.42887972e+01\n",
      " 1.78210128e+02 6.75915085e+01 1.36555294e+02 1.05158604e+02\n",
      " 9.65438455e+01 4.58775500e+01 2.32279867e-05 8.54868206e-07\n",
      " 2.06576150e-05 2.55269718e-07 3.36380814e+01 4.26515403e+01\n",
      " 9.38058007e+01 4.22473860e+01]\n",
      "33-th iteration, loss: 0.13127134444985117, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0536434796194246e-06\n",
      "33-th iteration, new layer inserted. now 22 layers\n",
      "[4.80747945e+00 5.86976493e+01 1.17863204e+02 5.37964822e+01\n",
      " 7.74101220e+01 5.17463592e+01 1.25189276e+02 6.42887970e+01\n",
      " 1.78210128e+02 6.75915057e+01 1.36555295e+02 1.05158605e+02\n",
      " 9.65438445e+01 4.58775499e+01 2.52834528e-05 6.37663130e-07\n",
      " 0.00000000e+00 6.61744490e-23 3.36381062e+01 4.26515418e+01\n",
      " 9.38058008e+01 4.22473856e+01]\n",
      "34-th iteration, loss: 0.1312713444494098, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -2.0140208478337883e-06\n",
      "34-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747943e+00 5.86976488e+01 1.17863204e+02 5.37964825e+01\n",
      " 7.74101224e+01 5.17463595e+01 1.25189277e+02 6.42887968e+01\n",
      " 1.78210129e+02 6.75915030e+01 1.36555297e+02 1.05158606e+02\n",
      " 9.65438436e+01 4.58775497e+01 2.73017752e-05 3.87227673e-07\n",
      " 3.36381102e+01 4.26515433e+01 9.38058009e+01 4.22473853e+01]\n",
      "35-th iteration, loss: 0.1312713444490374, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.987790755344423e-06\n",
      "35-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747941e+00 5.86976483e+01 1.17863205e+02 5.37964829e+01\n",
      " 7.74101227e+01 5.17463598e+01 1.25189278e+02 6.42887966e+01\n",
      " 1.78210130e+02 6.75915003e+01 1.36555298e+02 1.05158608e+02\n",
      " 9.65438427e+01 4.58775494e+01 2.92899809e-05 1.09194639e-07\n",
      " 3.36381122e+01 4.26515448e+01 9.38058010e+01 4.22473850e+01]\n",
      "36-th iteration, loss: 0.13127134444867136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9607070247557946e-06\n",
      "36-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747939e+00 5.86976478e+01 1.17863205e+02 5.37964832e+01\n",
      " 7.74101231e+01 5.17463601e+01 1.25189279e+02 6.42887964e+01\n",
      " 1.78210130e+02 6.75914976e+01 1.36555299e+02 1.05158609e+02\n",
      " 9.65438418e+01 4.58775492e+01 0.00000000e+00 8.88178420e-16\n",
      " 3.36381454e+01 4.26515462e+01 9.38058011e+01 4.22473846e+01]\n",
      "37-th iteration, loss: 0.1312713444483115, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9333836127278053e-06\n",
      "37-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747937  58.69764735 117.86320506  53.7964835   77.41012345\n",
      "  51.74636036 125.18928057  64.2887963  178.21013085  67.59149496\n",
      " 136.55530056 105.15861023  96.54384086  45.87754892  33.63814928\n",
      "  42.6515476   93.80580117  42.24738433]\n",
      "38-th iteration, loss: 0.1312713444480123, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9182428097865983e-06\n",
      "38-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747934  58.69764686 117.86320532  53.79648381  77.41012381\n",
      "  51.74636066 125.18928181  64.28879617 178.21013154  67.59149233\n",
      " 136.55530191 105.1586115   96.54383994  45.87754865  33.6381512\n",
      "  42.65154897  93.80580125  42.24738401]\n",
      "39-th iteration, loss: 0.13127134444771596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.9032263438871954e-06\n",
      "39-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747932  58.69764637 117.86320557  53.79648411  77.41012417\n",
      "  51.74636096 125.18928306  64.28879605 178.21013223  67.59148973\n",
      " 136.55530328 105.15861278  96.54383903  45.87754837  33.6381531\n",
      "  42.65155033  93.80580133  42.24738369]\n",
      "40-th iteration, loss: 0.13127134444742242, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.88835129408103e-06\n",
      "40-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747929  58.69764587 117.86320582  53.79648441  77.41012453\n",
      "  51.74636126 125.18928432  64.28879594 178.21013292  67.59148715\n",
      " 136.55530465 105.15861407  96.54383812  45.87754808  33.63815499\n",
      "  42.65155166  93.8058014   42.24738338]\n",
      "41-th iteration, loss: 0.13127134444713154, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8736318861792544e-06\n",
      "41-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747927e+00 5.86976454e+01 1.17863206e+02 5.37964847e+01\n",
      " 7.74101249e+01 5.17463616e+01 1.25189286e+02 6.42887958e+01\n",
      " 1.78210134e+02 6.75914846e+01 1.36555306e+02 1.05158615e+02\n",
      " 9.65438372e+01 4.58775478e+01 0.00000000e+00 8.88178420e-16\n",
      " 3.36381569e+01 4.26515530e+01 9.38058015e+01 4.22473831e+01]\n",
      "42-th iteration, loss: 0.13127134444679137, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.847933440461872e-06\n",
      "42-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747924  58.69764488 117.86320631  53.79648499  77.41012525\n",
      "  51.74636187 125.18928683  64.28879576 178.21013434  67.59148207\n",
      " 136.55530743 105.15861665  96.54383632  45.87754748  33.63816056\n",
      "  42.65155428  93.80580155  42.24738276]\n",
      "43-th iteration, loss: 0.13127134444650643, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.8339402711193997e-06\n",
      "43-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747921e+00 5.86976444e+01 1.17863207e+02 5.37964853e+01\n",
      " 7.74101256e+01 5.17463622e+01 1.25189288e+02 6.42887957e+01\n",
      " 1.78210135e+02 6.75914796e+01 1.36555309e+02 1.05158618e+02\n",
      " 9.65438354e+01 4.58775472e+01 0.00000000e+00 4.44089210e-15\n",
      " 3.36381624e+01 4.26515555e+01 9.38058016e+01 4.22473824e+01]\n",
      "44-th iteration, loss: 0.1312713444461741, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.809180289546784e-06\n",
      "44-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747918  58.69764387 117.86320678  53.79648554  77.41012595\n",
      "  51.74636247 125.18928935  64.28879561 178.21013579  67.59147705\n",
      " 136.55531025 105.15861924  96.54383451  45.87754682  33.63816601\n",
      "  42.6515568   93.80580168  42.24738214]\n",
      "45-th iteration, loss: 0.13127134444589458, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7958813479496004e-06\n",
      "45-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747915e+00 5.86976434e+01 1.17863207e+02 5.37964858e+01\n",
      " 7.74101263e+01 5.17463628e+01 1.25189291e+02 6.42887956e+01\n",
      " 1.78210137e+02 6.75914746e+01 1.36555312e+02 1.05158621e+02\n",
      " 9.65438336e+01 4.58775465e+01 0.00000000e+00 8.88178420e-16\n",
      " 3.36381678e+01 4.26515580e+01 9.38058017e+01 4.22473818e+01]\n",
      "46-th iteration, loss: 0.13127134444556962, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7720188211969222e-06\n",
      "46-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747912e+00 5.86976428e+01 1.17863207e+02 5.37964861e+01\n",
      " 7.74101267e+01 5.17463631e+01 1.25189292e+02 6.42887955e+01\n",
      " 1.78210137e+02 6.75914721e+01 1.36555313e+02 1.05158622e+02\n",
      " 9.65438327e+01 4.58775461e+01 0.00000000e+00 8.88178420e-16\n",
      " 3.36381714e+01 4.26515592e+01 9.38058018e+01 4.22473815e+01]\n",
      "47-th iteration, loss: 0.13127134444524874, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.748809842399705e-06\n",
      "47-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747909  58.69764232 117.86320746  53.79648633  77.410127\n",
      "  51.74636338 125.18929314  64.28879545 178.210138    67.59146967\n",
      " 136.55531453 105.15862311  96.54383181  45.87754572  33.63817485\n",
      "  42.6515604   93.80580186  42.24738124]\n",
      "48-th iteration, loss: 0.13127134444497707, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7366682962689918e-06\n",
      "48-th iteration, new layer inserted. now 20 layers\n",
      "[4.80747906e+00 5.86976418e+01 1.17863208e+02 5.37964866e+01\n",
      " 7.74101273e+01 5.17463637e+01 1.25189294e+02 6.42887954e+01\n",
      " 1.78210139e+02 6.75914672e+01 1.36555316e+02 1.05158624e+02\n",
      " 9.65438309e+01 4.58775453e+01 0.00000000e+00 1.24344979e-14\n",
      " 3.36381766e+01 4.26515616e+01 9.38058019e+01 4.22473809e+01]\n",
      "49-th iteration, loss: 0.13127134444466276, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7142660754238893e-06\n",
      "49-th iteration, new layer inserted. now 18 layers\n",
      "[  4.80747903  58.69764128 117.8632079   53.79648683  77.41012768\n",
      "  51.74636398 125.18929568  64.28879539 178.2101395   67.59146484\n",
      " 136.55531742 105.15862568  96.54383     45.87754493  33.63818002\n",
      "  42.65156268  93.80580196  42.24738065]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536853803235571\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.39870033    0.         1337.51084741]\n",
      "1-th iteration, loss: 0.7491078933829632, 11 gd steps\n",
      "insert gradient: -0.6165738060587627\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.89280919   62.32063131  228.35551053    0.         1109.15533688]\n",
      "2-th iteration, loss: 0.5277319925653697, 30 gd steps\n",
      "insert gradient: -0.42384123268487656\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  1.7702564   54.66715977 176.9365139   55.40344422 316.90152482\n",
      "   0.         792.25381205]\n",
      "3-th iteration, loss: 0.44982741733303916, 28 gd steps\n",
      "insert gradient: -0.49425650716856157\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  1.61960139  58.62961699 171.03109516  53.7705063  269.25550518\n",
      "  50.10883856 135.81493921   0.         656.43887285]\n",
      "4-th iteration, loss: 0.370711518645387, 48 gd steps\n",
      "insert gradient: -0.48234708985852603\n",
      "4-th iteration, new layer inserted. now 10 layers\n",
      "[  2.21654151  49.64103327 185.29715483  50.37984303 236.4634404\n",
      "  65.20322953 100.64249581  64.1154882  656.43887285   0.        ]\n",
      "5-th iteration, loss: 0.29210224134257123, 14 gd steps\n",
      "insert gradient: -0.3605448672750713\n",
      "5-th iteration, new layer inserted. now 12 layers\n",
      "[  0.          36.94110563 203.23950428  52.4450014   96.54379278\n",
      "   0.         135.1613099   66.26308129 106.66923965  65.45347903\n",
      " 593.97781684  75.94826987]\n",
      "6-th iteration, loss: 0.2497925868568524, 35 gd steps\n",
      "insert gradient: -0.15625003749453295\n",
      "6-th iteration, new layer inserted. now 14 layers\n",
      "[  2.77955079  53.32578666 194.51697559  51.41607143  61.10716443\n",
      "  26.24545919  99.66341014  69.26454804 107.13401762  66.41763315\n",
      " 411.20989767   0.         176.23281329  66.97301705]\n",
      "7-th iteration, loss: 0.22673550786880192, 18 gd steps\n",
      "insert gradient: -0.03211971642158994\n",
      "7-th iteration, new layer inserted. now 16 layers\n",
      "[  2.18596296  59.22992135 187.5409865   50.33195798  62.32857848\n",
      "  28.20700569  99.50823636  66.62473471 113.31968153  68.07000182\n",
      " 257.55769458   0.         140.48601523  29.47967298 130.78306182\n",
      "  70.68722757]\n",
      "8-th iteration, loss: 0.22478157816827454, 15 gd steps\n",
      "insert gradient: -0.03972172904452207\n",
      "8-th iteration, new layer inserted. now 18 layers\n",
      "[  2.65926962  58.55001986 184.69227723  52.06759091  62.97396995\n",
      "  28.04374945  99.15423989  67.24179036 111.43377284  67.12582546\n",
      " 203.24805421   0.          50.81201355   3.13332037 137.16192627\n",
      "  32.52171356 129.20691592  68.76852497]\n",
      "9-th iteration, loss: 0.19649193793701297, 44 gd steps\n",
      "insert gradient: -0.04995481617985302\n",
      "9-th iteration, new layer inserted. now 16 layers\n",
      "[  1.32964181  59.01339648 178.65621752  54.67869857  74.1588759\n",
      "  28.77770095  92.52703583  64.71926762 120.7706698   60.3900671\n",
      " 174.09018765  84.17174053  95.05349636  28.50217688 135.7924137\n",
      "  70.19510028]\n",
      "10-th iteration, loss: 0.19510579216823934, 20 gd steps\n",
      "insert gradient: -0.023213871470010798\n",
      "10-th iteration, new layer inserted. now 18 layers\n",
      "[7.74906450e-01 6.08159950e+01 1.76666611e+02 5.63024839e+01\n",
      " 7.62767918e+01 2.98834024e+01 9.09325550e+01 6.37070273e+01\n",
      " 1.22001533e+02 0.00000000e+00 3.19744231e-14 6.19852621e+01\n",
      " 1.71399994e+02 8.69028158e+01 8.85069601e+01 3.22778150e+01\n",
      " 1.35892279e+02 6.83837462e+01]\n",
      "11-th iteration, loss: 0.1945336125805485, 19 gd steps\n",
      "insert gradient: -0.029274036925921378\n",
      "11-th iteration, new layer inserted. now 16 layers\n",
      "[  0.87191661  62.00502728 176.80681753  57.38333759  74.84061068\n",
      "  30.75710201  90.97731495  63.38543584 123.6015761   61.59829183\n",
      " 171.42893919  89.21057143  87.05100308  33.58836164 136.90982974\n",
      "  66.84396021]\n",
      "12-th iteration, loss: 0.1941906374023801, 18 gd steps\n",
      "insert gradient: -0.013479089599013755\n",
      "12-th iteration, new layer inserted. now 16 layers\n",
      "[  0.47168589  61.89558424 176.91087313  57.63185863  75.15616668\n",
      "  31.1886627   90.79123094  63.85744262 123.72024866  62.2650275\n",
      " 171.45500266  89.00086674  87.56376978  33.80003605 137.50741855\n",
      "  66.78745427]\n",
      "13-th iteration, loss: 0.19401195073230784, 21 gd steps\n",
      "insert gradient: -0.013408041166206188\n",
      "13-th iteration, new layer inserted. now 18 layers\n",
      "[6.65322416e-01 6.19734114e+01 1.77099710e+02 5.80636337e+01\n",
      " 7.53599373e+01 3.13055899e+01 9.07771330e+01 6.41311886e+01\n",
      " 1.24310964e+02 6.24581125e+01 1.71707646e+02 0.00000000e+00\n",
      " 1.06581410e-14 8.89905370e+01 8.88085785e+01 3.37047287e+01\n",
      " 1.38738866e+02 6.60708456e+01]\n",
      "14-th iteration, loss: 0.19374604530987674, 19 gd steps\n",
      "insert gradient: -0.01387929737097747\n",
      "14-th iteration, new layer inserted. now 18 layers\n",
      "[4.33219286e-01 6.15988411e+01 1.77260414e+02 0.00000000e+00\n",
      " 7.10542736e-14 5.84615587e+01 7.55514689e+01 3.17376834e+01\n",
      " 9.06047963e+01 6.37759980e+01 1.24625528e+02 6.19949073e+01\n",
      " 1.72295258e+02 8.95509027e+01 8.93479725e+01 3.38654810e+01\n",
      " 1.39300583e+02 6.53988630e+01]\n",
      "15-th iteration, loss: 0.19355586525868665, 18 gd steps\n",
      "insert gradient: -0.012252747870204424\n",
      "15-th iteration, new layer inserted. now 18 layers\n",
      "[5.10998092e-01 6.16577121e+01 1.77604951e+02 5.89068147e+01\n",
      " 7.56934143e+01 3.17974857e+01 9.05517878e+01 6.37119647e+01\n",
      " 1.25015643e+02 6.19121050e+01 1.72650061e+02 0.00000000e+00\n",
      " 4.61852778e-14 8.96196316e+01 9.00695304e+01 3.40156786e+01\n",
      " 1.40027728e+02 6.48807434e+01]\n",
      "16-th iteration, loss: 0.19341226842337356, 16 gd steps\n",
      "insert gradient: -0.015376498718018326\n",
      "16-th iteration, new layer inserted. now 16 layers\n",
      "[  1.54200228  62.43263431 177.9451032   58.5979224   75.31708292\n",
      "  32.09650618  91.41615853  63.36677078 124.81668352  62.10669794\n",
      " 172.68337756  89.84536445  92.74771882  33.07314056 141.46298803\n",
      "  63.98051801]\n",
      "17-th iteration, loss: 0.1932386979448407, 16 gd steps\n",
      "insert gradient: -0.01745283190190241\n",
      "17-th iteration, new layer inserted. now 16 layers\n",
      "[  0.69405536  62.22035025 178.18416936  59.40646086  75.91627759\n",
      "  32.24640175  90.49972526  63.49427963 125.26690745  62.44962899\n",
      " 172.97014141  89.72413862  92.53363182  33.75795225 142.11159854\n",
      "  63.42009455]\n",
      "18-th iteration, loss: 0.1930382838075221, 18 gd steps\n",
      "insert gradient: -0.012704219231355878\n",
      "18-th iteration, new layer inserted. now 16 layers\n",
      "[  0.45810489  61.14453452 178.71280155  60.03886193  76.15403572\n",
      "  32.12554954  90.61873769  63.88392869 126.00797609  61.7350395\n",
      " 173.41992451  90.10286958  92.83966892  33.93687206 142.53333034\n",
      "  63.08344571]\n",
      "19-th iteration, loss: 0.19289237729387443, 17 gd steps\n",
      "insert gradient: -0.0189520667412186\n",
      "19-th iteration, new layer inserted. now 16 layers\n",
      "[  0.65208985  60.89668434 179.22694518  60.18189013  76.5442816\n",
      "  32.25422681  90.88770346  63.5693604  126.09374846  61.76219283\n",
      " 173.49989016  90.38129521  93.9586058   33.89520214 143.57537088\n",
      "  62.03020832]\n",
      "20-th iteration, loss: 0.1927326009189064, 18 gd steps\n",
      "insert gradient: -0.017356990202240193\n",
      "20-th iteration, new layer inserted. now 16 layers\n",
      "[  0.57905289  60.99014069 179.50171511  60.65747275  76.69722064\n",
      "  32.24375508  90.45557595  64.12788527 126.54585808  62.14699327\n",
      " 173.8642539   89.98915863  94.39397312  34.22858326 144.39910551\n",
      "  61.52672112]\n",
      "21-th iteration, loss: 0.19250146405667926, 22 gd steps\n",
      "insert gradient: -0.022278544966091298\n",
      "21-th iteration, new layer inserted. now 18 layers\n",
      "[  0.73855841  60.72950615 179.97653917  60.71038443  76.99460499\n",
      "  32.4180874   91.03480175  63.82671345 127.01729097  62.22103374\n",
      " 173.89000898  90.52340645  95.35980575  34.2408611   86.91110757\n",
      "   0.          57.94073838  60.82274556]\n",
      "22-th iteration, loss: 0.19145898982230822, 24 gd steps\n",
      "insert gradient: -0.03387473779706999\n",
      "22-th iteration, new layer inserted. now 18 layers\n",
      "[  0.9822796   59.45500323 180.52842968  61.40320849  77.26284894\n",
      "  33.39387469  92.23298605  61.92938033 128.83034906  62.10255325\n",
      " 174.13386169  89.02823802 103.53243557  39.04277608  76.37722322\n",
      "   5.95652948  47.51437845  60.69168449]\n",
      "23-th iteration, loss: 0.19039902048694296, 21 gd steps\n",
      "insert gradient: -0.021916502396934323\n",
      "23-th iteration, new layer inserted. now 18 layers\n",
      "[  0.72434014  59.01967316 181.74535828  61.94135604  78.64705508\n",
      "  33.55883774  90.82728027  63.24646271 128.95514607  62.72652859\n",
      " 174.08083297  88.65580115 104.98977839  41.7249236   75.19499498\n",
      "   7.35386344  46.95796606  57.58373785]\n",
      "24-th iteration, loss: 0.1889660045197348, 22 gd steps\n",
      "insert gradient: -0.02723106756483084\n",
      "24-th iteration, new layer inserted. now 18 layers\n",
      "[  1.48639327  57.99951776 182.38391637  62.18511349  82.98870457\n",
      "  33.06503509  91.47811326  61.67417256 132.40196028  62.59655659\n",
      " 173.47172575  88.03166009 109.68817876  47.9659471   63.02383946\n",
      "  10.65462444  46.66535886  54.55460655]\n",
      "25-th iteration, loss: 0.1718492660661303, 73 gd steps\n",
      "insert gradient: -0.07105866058834667\n",
      "25-th iteration, new layer inserted. now 18 layers\n",
      "[  1.62694656  55.24247909  89.42174216   0.         102.19627675\n",
      "  62.01972197  90.0042126   40.11826571  84.68770693  58.44818355\n",
      " 142.9268334   55.94968069 174.17744855  94.77842753 109.38485823\n",
      "  72.7918762  141.13599685  36.50946754]\n",
      "26-th iteration, loss: 0.1656128219899123, 21 gd steps\n",
      "insert gradient: -0.05753952708424845\n",
      "26-th iteration, new layer inserted. now 18 layers\n",
      "[  0.          51.44008984  91.66254862   2.39364712 102.71257883\n",
      "  62.38783753  87.39098214  43.47703836  83.96069866  57.6757973\n",
      " 143.66386015  60.85874239 172.93086689  96.80563548 105.73307281\n",
      "  80.31214894 123.12884154  48.27783884]\n",
      "27-th iteration, loss: 0.1612495124423806, 20 gd steps\n",
      "insert gradient: -0.037858087250913545\n",
      "27-th iteration, new layer inserted. now 20 layers\n",
      "[  0.6462311   53.3500159   91.19492931   2.92343214 101.41413654\n",
      "  61.49443996  87.7899492   44.81240915  85.9128342   58.19853493\n",
      " 143.66111839  55.25118814  93.42970557   0.          80.08260477\n",
      " 100.75477101 106.23423569  84.00112563 120.70979713  49.75814583]\n",
      "28-th iteration, loss: 0.15747241966722, 19 gd steps\n",
      "insert gradient: -0.03651552473671128\n",
      "28-th iteration, new layer inserted. now 18 layers\n",
      "[  0.75433627  54.53322127  89.0637727    7.15532581  96.34737507\n",
      "  61.72942572  88.16082434  46.65766417  88.545133    58.13833079\n",
      " 143.84730509  53.73743779 178.91342373 106.20272972 100.80304431\n",
      "  89.34318806 113.57154945  58.19698163]\n",
      "29-th iteration, loss: 0.1326673750053232, 34 gd steps\n",
      "insert gradient: -0.014956684070149568\n",
      "29-th iteration, new layer inserted. now 20 layers\n",
      "[9.12294126e-02 4.91459254e+01 8.40663018e+01 0.00000000e+00\n",
      " 1.24344979e-14 1.40687142e+01 8.53866471e+01 5.50074020e+01\n",
      " 9.15860174e+01 5.10433758e+01 9.27752475e+01 6.31340114e+01\n",
      " 1.29327895e+02 6.05393611e+01 1.67536753e+02 1.05889378e+02\n",
      " 1.05069705e+02 8.93315243e+01 1.04371677e+02 1.08301239e+02]\n",
      "30-th iteration, loss: 0.12990075227608902, 45 gd steps\n",
      "insert gradient: -0.012253693374030403\n",
      "30-th iteration, new layer inserted. now 20 layers\n",
      "[  1.79171478  55.22430193  80.44854065  15.09641457  86.80933747\n",
      "  52.48631858  93.22537875  50.97272053  95.05181951  65.01053584\n",
      " 129.26867549  56.46795965 172.34351336  58.64383817   0.\n",
      "  50.266147   100.47266686  90.53802285  99.95395478 103.08410784]\n",
      "31-th iteration, loss: 0.12278834147187508, 50 gd steps\n",
      "insert gradient: -0.009758476421063698\n",
      "31-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.53780412e+01 8.38253279e+01 1.98189805e+01\n",
      " 8.15845396e+01 5.19486603e+01 8.73385685e+01 5.08857080e+01\n",
      " 9.87251442e+01 6.74720049e+01 1.31228683e+02 6.14420602e+01\n",
      " 1.57488825e+02 6.37671914e+01 0.00000000e+00 1.77635684e-14\n",
      " 3.26479088e+01 2.72453516e+01 1.05470982e+02 9.43863163e+01\n",
      " 1.03637088e+02 9.54200058e+01]\n",
      "32-th iteration, loss: 0.11864067187756705, 93 gd steps\n",
      "insert gradient: -0.008242982905028673\n",
      "32-th iteration, new layer inserted. now 22 layers\n",
      "[9.70670248e-02 0.00000000e+00 2.77555756e-17 6.55564742e+01\n",
      " 8.47899354e+01 2.54171942e+01 8.04811238e+01 5.06928908e+01\n",
      " 8.24469107e+01 5.12503132e+01 1.03533499e+02 6.94860324e+01\n",
      " 1.31118200e+02 6.17780487e+01 1.56023171e+02 5.95433363e+01\n",
      " 6.69005094e+01 1.35949249e+01 1.02961370e+02 9.92757966e+01\n",
      " 1.04488732e+02 9.55540095e+01]\n",
      "33-th iteration, loss: 0.1181113291433524, 999 gd steps\n",
      "insert gradient: -0.0007452521299370975\n",
      "33-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65829656e+01 8.60087027e+01 2.58704740e+01\n",
      " 7.97362555e+01 5.03812655e+01 8.24236615e+01 5.17107106e+01\n",
      " 1.02915369e+02 6.88981990e+01 1.32285511e+02 6.13220596e+01\n",
      " 1.57248587e+02 5.90286569e+01 0.00000000e+00 1.77635684e-14\n",
      " 7.60115082e+01 1.06940659e+01 9.88035038e+01 1.01487229e+02\n",
      " 1.02546846e+02 9.59296391e+01]\n",
      "34-th iteration, loss: 0.11808303195387693, 19 gd steps\n",
      "insert gradient: -0.0005098994093543306\n",
      "34-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65633578e+01 8.60775014e+01 0.00000000e+00\n",
      " 1.77635684e-15 2.60038825e+01 7.97995205e+01 5.04105999e+01\n",
      " 8.21564828e+01 5.17461312e+01 1.03070263e+02 6.87876763e+01\n",
      " 1.32556761e+02 6.12924320e+01 1.57374647e+02 5.86527915e+01\n",
      " 7.85690051e+01 1.01555718e+01 9.77872836e+01 1.01702210e+02\n",
      " 1.02468024e+02 9.59693915e+01]\n",
      "35-th iteration, loss: 0.11808045230697052, 130 gd steps\n",
      "insert gradient: -0.00018828342837545598\n",
      "35-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65843735e+01 8.60890138e+01 2.00566979e-03\n",
      " 1.01367864e-02 2.60084464e+01 7.98102195e+01 5.04107422e+01\n",
      " 8.21225023e+01 5.17466476e+01 1.03038033e+02 6.87512607e+01\n",
      " 1.32595831e+02 6.12629918e+01 1.57376835e+02 5.86855952e+01\n",
      " 7.87291596e+01 1.00910002e+01 9.75399617e+01 1.01797918e+02\n",
      " 1.02412429e+02 9.59988415e+01]\n",
      "36-th iteration, loss: 0.11808042561218635, 66 gd steps\n",
      "insert gradient: -0.00018824091655726607\n",
      "36-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65843067e+01 8.60890156e+01 1.89651476e-03\n",
      " 1.01284332e-02 2.60083901e+01 7.98105032e+01 5.04110901e+01\n",
      " 8.21221301e+01 5.17465755e+01 1.03037530e+02 6.87509910e+01\n",
      " 1.32596372e+02 6.12628866e+01 1.57376451e+02 5.86858620e+01\n",
      " 7.87321011e+01 1.00904943e+01 9.75363716e+01 1.01798901e+02\n",
      " 1.02411629e+02 9.59988436e+01]\n",
      "37-th iteration, loss: 0.11808039983848123, 64 gd steps\n",
      "insert gradient: -0.0001881631237153543\n",
      "37-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65842488e+01 8.60890212e+01 1.79372972e-03\n",
      " 1.01246526e-02 2.60083387e+01 7.98107804e+01 5.04114283e+01\n",
      " 8.21217736e+01 5.17465153e+01 1.03037051e+02 6.87507374e+01\n",
      " 1.32596899e+02 6.12627916e+01 1.57376077e+02 5.86861166e+01\n",
      " 7.87349631e+01 1.00900131e+01 9.75328945e+01 1.01799851e+02\n",
      " 1.02410858e+02 9.59988471e+01]\n",
      "38-th iteration, loss: 0.11808037489493035, 63 gd steps\n",
      "insert gradient: -0.00018804644529372204\n",
      "38-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65841983e+01 8.60890296e+01 1.69524513e-03\n",
      " 1.01243670e-02 2.60082904e+01 7.98110506e+01 5.04117548e+01\n",
      " 8.21214295e+01 5.17464636e+01 1.03036594e+02 6.87504971e+01\n",
      " 1.32597412e+02 6.12627050e+01 1.57375711e+02 5.86863591e+01\n",
      " 7.87377530e+01 1.00895534e+01 9.75295190e+01 1.01800772e+02\n",
      " 1.02410114e+02 9.59988523e+01]\n",
      "39-th iteration, loss: 0.11808035074201165, 61 gd steps\n",
      "insert gradient: -0.00018790161620255082\n",
      "39-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65841540e+01 8.60890403e+01 1.60050038e-03\n",
      " 1.01270298e-02 2.60082445e+01 7.98113138e+01 5.04120687e+01\n",
      " 8.21210963e+01 5.17464184e+01 1.03036156e+02 6.87502686e+01\n",
      " 1.32597913e+02 6.12626255e+01 1.57375353e+02 5.86865898e+01\n",
      " 7.87404729e+01 1.00891131e+01 9.75262404e+01 1.01801665e+02\n",
      " 1.02409396e+02 9.59988589e+01]\n",
      "40-th iteration, loss: 0.11808032730571817, 60 gd steps\n",
      "insert gradient: -0.00018773782675404567\n",
      "40-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65841152e+01 8.60890529e+01 1.50911437e-03\n",
      " 1.01322575e-02 2.60082010e+01 7.98115705e+01 5.04123705e+01\n",
      " 8.21207726e+01 5.17463782e+01 1.03035735e+02 6.87500503e+01\n",
      " 1.32598402e+02 6.12625521e+01 1.57375003e+02 5.86868092e+01\n",
      " 7.87431290e+01 1.00886899e+01 9.75230496e+01 1.01802531e+02\n",
      " 1.02408700e+02 9.59988670e+01]\n",
      "41-th iteration, loss: 0.11808030453945258, 59 gd steps\n",
      "insert gradient: -0.00018756169117032571\n",
      "41-th iteration, new layer inserted. now 24 layers\n",
      "[0.00000000e+00 6.65840810e+01 8.60890672e+01 1.42091086e-03\n",
      " 1.01397501e-02 2.60081596e+01 7.98118211e+01 5.04126604e+01\n",
      " 8.21204573e+01 5.17463420e+01 1.03035330e+02 6.87498411e+01\n",
      " 1.32598879e+02 6.12624841e+01 1.57374660e+02 5.86870180e+01\n",
      " 0.00000000e+00 5.32907052e-15 7.87457251e+01 1.00882822e+01\n",
      " 9.75199408e+01 1.01803373e+02 1.02408026e+02 9.59988765e+01]\n",
      "42-th iteration, loss: 0.1180802002782022, 195 gd steps\n",
      "insert gradient: -0.00017097601630340083\n",
      "42-th iteration, new layer inserted. now 24 layers\n",
      "[0.00000000e+00 6.65841300e+01 8.60892562e+01 1.41021870e-03\n",
      " 1.03062361e-02 2.60083130e+01 7.98127510e+01 5.04135067e+01\n",
      " 8.21192982e+01 5.17462128e+01 1.03034043e+02 6.87492023e+01\n",
      " 1.32600617e+02 6.12621869e+01 1.57373363e+02 5.86871656e+01\n",
      " 0.00000000e+00 8.88178420e-15 7.87627503e+01 1.00859501e+01\n",
      " 9.75088242e+01 1.01805834e+02 1.02405588e+02 9.59988996e+01]\n",
      "43-th iteration, loss: 0.11808011674311747, 166 gd steps\n",
      "insert gradient: -0.0001665042528199139\n",
      "43-th iteration, new layer inserted. now 24 layers\n",
      "[0.00000000e+00 6.65842020e+01 8.60894391e+01 1.40995616e-03\n",
      " 1.04703470e-02 2.60084509e+01 7.98134721e+01 5.04140085e+01\n",
      " 8.21182707e+01 5.17460832e+01 1.03033062e+02 6.87487482e+01\n",
      " 1.32602127e+02 6.12619811e+01 1.57372289e+02 5.86869807e+01\n",
      " 0.00000000e+00 8.88178420e-15 7.87761729e+01 1.00835573e+01\n",
      " 9.74995270e+01 1.01807705e+02 1.02403582e+02 9.59989891e+01]\n",
      "44-th iteration, loss: 0.11808004469254464, 147 gd steps\n",
      "insert gradient: -0.0001638696482688426\n",
      "44-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65842603e+01 8.60896028e+01 1.38872199e-03\n",
      " 1.06176167e-02 2.60085533e+01 7.98140882e+01 5.04143986e+01\n",
      " 8.21173558e+01 5.17459682e+01 1.03032223e+02 6.87483509e+01\n",
      " 1.32603465e+02 6.12618223e+01 1.57371362e+02 5.86867353e+01\n",
      " 7.87877751e+01 1.00813882e+01 9.74913653e+01 1.01809365e+02\n",
      " 1.02401860e+02 9.59991285e+01]\n",
      "45-th iteration, loss: 0.11808002642232288, 49 gd steps\n",
      "insert gradient: -0.00016935866274371493\n",
      "45-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65842627e+01 8.60896444e+01 1.34552714e-03\n",
      " 1.06538196e-02 2.60085518e+01 7.98142816e+01 5.04145269e+01\n",
      " 8.21170542e+01 5.17459329e+01 1.03031942e+02 6.87482154e+01\n",
      " 1.32603908e+02 6.12617852e+01 1.57371069e+02 5.86866877e+01\n",
      " 7.87897345e+01 1.00807422e+01 9.74886828e+01 1.01809965e+02\n",
      " 1.02401307e+02 9.59991905e+01]\n",
      "46-th iteration, loss: 0.11808000923391838, 47 gd steps\n",
      "insert gradient: -0.00017235862456488248\n",
      "46-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65842477e+01 8.60896696e+01 1.26905454e-03\n",
      " 1.06741175e-02 2.60085152e+01 7.98144578e+01 5.04146600e+01\n",
      " 8.21167736e+01 5.17458999e+01 1.03031667e+02 6.87480765e+01\n",
      " 1.32604321e+02 6.12617596e+01 1.57370803e+02 5.86866967e+01\n",
      " 7.87916521e+01 1.00802310e+01 9.74861702e+01 1.01810588e+02\n",
      " 1.02400800e+02 9.59992573e+01]\n",
      "47-th iteration, loss: 0.1180799926491726, 46 gd steps\n",
      "insert gradient: -0.0001741770855051704\n",
      "47-th iteration, new layer inserted. now 24 layers\n",
      "[0.00000000e+00 6.65842277e+01 8.60896907e+01 1.18769174e-03\n",
      " 1.06906911e-02 2.60084728e+01 7.98146315e+01 5.04148050e+01\n",
      " 8.21165093e+01 5.17458723e+01 1.03031395e+02 6.87479337e+01\n",
      " 1.32604712e+02 6.12617371e+01 1.57370548e+02 5.86867349e+01\n",
      " 0.00000000e+00 7.10542736e-15 7.87935455e+01 1.00797961e+01\n",
      " 9.74837545e+01 1.01811221e+02 1.02400318e+02 9.59993246e+01]\n",
      "48-th iteration, loss: 0.11807993252679308, 126 gd steps\n",
      "insert gradient: -0.00016450947535499272\n",
      "48-th iteration, new layer inserted. now 22 layers\n",
      "[0.00000000e+00 6.65842428e+01 8.60898157e+01 1.12831523e-03\n",
      " 1.08041477e-02 2.60085216e+01 7.98151735e+01 5.04152268e+01\n",
      " 8.21157904e+01 5.17458144e+01 1.03030675e+02 6.87475412e+01\n",
      " 1.32605789e+02 6.12616296e+01 1.57369806e+02 5.86866757e+01\n",
      " 7.88037000e+01 1.00783497e+01 9.74770084e+01 1.01812816e+02\n",
      " 1.02398956e+02 9.59994909e+01]\n",
      "49-th iteration, loss: 0.11807991669689527, 44 gd steps\n",
      "insert gradient: -0.0001687028685837331\n",
      "49-th iteration, new layer inserted. now 24 layers\n",
      "[0.00000000e+00 6.65842459e+01 8.60898582e+01 1.10113610e-03\n",
      " 1.08427538e-02 2.60085326e+01 7.98153578e+01 5.04153601e+01\n",
      " 8.21155358e+01 5.17457967e+01 1.03030432e+02 6.87474119e+01\n",
      " 1.32606173e+02 6.12616004e+01 1.57369551e+02 5.86866448e+01\n",
      " 0.00000000e+00 7.10542736e-15 7.88054626e+01 1.00778346e+01\n",
      " 9.74746513e+01 1.01813370e+02 1.02398483e+02 9.59995521e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 8 gd steps\n",
      "insert gradient: -2.535590565167535\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.20594943    0.         1361.82922645]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6329352862294932\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  43.47534951   62.25781836  232.50742891    0.         1129.32179755]\n",
      "2-th iteration, loss: 0.6031035685735551, 13 gd steps\n",
      "insert gradient: -0.6055532244298907\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.1961044   77.57071065 218.12625321  42.42126087 391.8055216\n",
      "   0.         737.51627595]\n",
      "3-th iteration, loss: 0.5063650694217351, 17 gd steps\n",
      "insert gradient: -0.6164654267792954\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          69.53620447 204.12219855  52.37398746 335.60762008\n",
      "  47.41332135 358.22219118   0.         379.29408477]\n",
      "4-th iteration, loss: 0.4451739430697683, 16 gd steps\n",
      "insert gradient: -0.28272644201260483\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.99704662  69.68540185 195.32560175  60.34635617 155.71461164\n",
      "   0.         167.69265869  42.96960729 343.56560823  45.82605816\n",
      " 379.29408477]\n",
      "5-th iteration, loss: 0.31190790844784544, 20 gd steps\n",
      "insert gradient: -0.2730422916154678\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.0515369   44.26115291 118.37992075   0.          98.64993396\n",
      "  71.49124424 119.94149298  66.57290628 133.04811859  26.67493241\n",
      " 349.93852852  54.28807023 379.29408477]\n",
      "6-th iteration, loss: 0.2643437286251285, 15 gd steps\n",
      "insert gradient: -0.17373591133172955\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  2.33346447  58.49575595  87.00158126  23.39997212  73.53377767\n",
      "  57.15448679 149.95803323  69.30903179 123.73484756  46.28130834\n",
      " 317.85786197  72.20426564 379.29408477]\n",
      "7-th iteration, loss: 0.23747535487325488, 22 gd steps\n",
      "insert gradient: -0.09850259472239353\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  3.96510184  53.61203637  86.50535454  26.09547753  79.63471113\n",
      "  59.65495517 135.72162144  66.82652301 138.14555148  63.35661125\n",
      " 291.7183832   85.80255439 189.64704239   0.         189.64704239]\n",
      "8-th iteration, loss: 0.2233295832032649, 47 gd steps\n",
      "insert gradient: -0.07493371934225972\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  2.46954897  52.23148237  82.06701787  30.19218789  78.48868644\n",
      "  61.46084787 134.43354844  67.63647822 137.84906598  73.49294221\n",
      " 190.82306872   0.          86.73775851  89.0279826  173.53043101\n",
      "  27.01890424 189.64704239]\n",
      "9-th iteration, loss: 0.21369715955125168, 18 gd steps\n",
      "insert gradient: -0.10444620434470503\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  0.3097648   48.54174477  89.71764683  31.03153959  74.50284702\n",
      "  62.45967405 136.98310608  66.84593364 139.44872187  68.67370117\n",
      " 168.72784661  19.42314566  60.70007064  94.91075016 167.26933474\n",
      "  37.74304491 189.64704239]\n",
      "10-th iteration, loss: 0.1942997221768742, 37 gd steps\n",
      "insert gradient: -0.078485778415399\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[3.35359653e+00 5.20522438e+01 8.74267493e+01 3.65318919e+01\n",
      " 7.17767043e+01 5.44668836e+01 1.42497935e+02 6.74303618e+01\n",
      " 1.41194237e+02 6.47950939e+01 1.50158004e+02 5.40758625e+01\n",
      " 4.10610960e+00 1.03152798e+02 1.54302928e+02 7.48783245e+01\n",
      " 1.89647042e+02 0.00000000e+00 5.32907052e-14]\n",
      "11-th iteration, loss: 0.18853593210364972, 14 gd steps\n",
      "insert gradient: -0.11236679601575685\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[3.38221255e+00 5.17478685e+01 8.67611040e+01 3.28456161e+01\n",
      " 7.17223698e+01 5.68626948e+01 1.42938001e+02 6.29109567e+01\n",
      " 1.41180740e+02 6.78493822e+01 1.49581953e+02 5.54052915e+01\n",
      " 2.14529616e+00 1.04379362e+02 1.50734421e+02 7.49541119e+01\n",
      " 1.89615631e+02 1.09559441e+01 5.58659056e-14]\n",
      "12-th iteration, loss: 0.18571290355454576, 245 gd steps\n",
      "insert gradient: -0.029277544684077283\n",
      "12-th iteration, new layer inserted. now 19 layers\n",
      "[3.01210384e+00 5.26067776e+01 8.75389401e+01 3.48193726e+01\n",
      " 7.15966886e+01 5.46613302e+01 1.42131202e+02 6.42999490e+01\n",
      " 1.41200787e+02 6.74732434e+01 1.49207034e+02 1.27901435e+02\n",
      " 0.00000000e+00 3.48822096e+01 1.39307172e+02 8.12932951e+01\n",
      " 1.83795287e+02 1.54553586e+01 5.48908807e-14]\n",
      "13-th iteration, loss: 0.181483526503424, 16 gd steps\n",
      "insert gradient: -0.016516313346173803\n",
      "13-th iteration, new layer inserted. now 19 layers\n",
      "[4.34841101e+00 5.42221657e+01 8.12495320e+01 3.62695833e+01\n",
      " 7.53429284e+01 5.70570150e+01 1.41444551e+02 6.40752695e+01\n",
      " 1.40334145e+02 6.82119908e+01 1.47083377e+02 1.13819498e+02\n",
      " 2.61872716e+01 2.68020582e+01 1.48077294e+02 8.37203251e+01\n",
      " 1.78914823e+02 1.80977170e+01 4.05729038e-14]\n",
      "14-th iteration, loss: 0.1797164677400586, 21 gd steps\n",
      "insert gradient: -0.018897258750059136\n",
      "14-th iteration, new layer inserted. now 19 layers\n",
      "[2.14345188e+00 5.15433241e+01 8.73201801e+01 3.59932309e+01\n",
      " 7.40268327e+01 5.52049574e+01 1.41955956e+02 6.52808470e+01\n",
      " 1.40862359e+02 6.73405609e+01 1.48244227e+02 1.13616808e+02\n",
      " 3.18393127e+01 2.29280725e+01 1.49245986e+02 8.44747965e+01\n",
      " 1.79529573e+02 1.99925000e+01 4.54435664e-14]\n",
      "15-th iteration, loss: 0.1762203183078903, 30 gd steps\n",
      "insert gradient: -0.03611875744917972\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[2.01446395e+00 5.14898060e+01 8.64802282e+01 0.00000000e+00\n",
      " 8.88178420e-15 3.71496548e+01 7.56320642e+01 5.44694142e+01\n",
      " 1.42521203e+02 6.50169117e+01 1.42503959e+02 6.65853301e+01\n",
      " 1.48069628e+02 1.02821295e+02 5.56494955e+01 1.09209211e+01\n",
      " 1.60913324e+02 8.85633400e+01 1.79318426e+02 2.42240022e+01\n",
      " 5.78582659e-14]\n",
      "16-th iteration, loss: 0.17056353600853194, 32 gd steps\n",
      "insert gradient: -0.022128301364424275\n",
      "16-th iteration, new layer inserted. now 19 layers\n",
      "[2.55572622e+00 5.08391902e+01 8.54800587e+01 3.94084884e+01\n",
      " 7.70217658e+01 5.40414390e+01 1.43117447e+02 6.54136484e+01\n",
      " 1.43312254e+02 6.66546487e+01 1.47298042e+02 9.11182868e+01\n",
      " 1.78167513e+02 0.00000000e+00 7.12670053e+01 9.50351403e+01\n",
      " 1.79010692e+02 2.89676944e+01 5.78516073e-14]\n",
      "17-th iteration, loss: 0.1641271944505204, 22 gd steps\n",
      "insert gradient: -0.04082153593267288\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[1.27254376e+00 5.26060190e+01 8.84344901e+01 4.21843584e+01\n",
      " 7.51988576e+01 5.22847492e+01 1.45835876e+02 6.39268795e+01\n",
      " 1.47221455e+02 6.59887864e+01 1.51054584e+02 7.70882596e+01\n",
      " 3.92951332e+01 0.00000000e+00 1.17885400e+02 2.05661791e+01\n",
      " 5.30640536e+01 1.04414549e+02 1.77037325e+02 4.26754111e+01\n",
      " 9.95298411e-14]\n",
      "18-th iteration, loss: 0.1595079957743828, 21 gd steps\n",
      "insert gradient: -0.0664832025614726\n",
      "18-th iteration, new layer inserted. now 21 layers\n",
      "[4.04181229e-01 5.15926999e+01 9.19289178e+01 4.14926298e+01\n",
      " 7.24888657e+01 5.19283126e+01 1.49187776e+02 6.33163242e+01\n",
      " 1.48040544e+02 6.77124105e+01 1.51687760e+02 6.58162208e+01\n",
      " 3.65193804e+01 2.88996982e+00 1.18029623e+02 3.53783657e+01\n",
      " 2.19219682e+01 1.13160104e+02 1.71678545e+02 5.63049899e+01\n",
      " 1.34572254e-13]\n",
      "19-th iteration, loss: 0.15809236808305122, 14 gd steps\n",
      "insert gradient: -0.025092888777685455\n",
      "19-th iteration, new layer inserted. now 23 layers\n",
      "[1.13107801e+00 5.26782042e+01 9.16013824e+01 4.29370015e+01\n",
      " 7.38642274e+01 5.23807904e+01 1.49909438e+02 6.31236004e+01\n",
      " 1.47818600e+02 6.65820016e+01 1.51703452e+02 6.62248042e+01\n",
      " 3.72105194e+01 3.60713629e+00 1.18577356e+02 3.51489402e+01\n",
      " 2.14092659e+01 1.13142008e+02 1.40033878e+02 0.00000000e+00\n",
      " 3.11186396e+01 5.73644720e+01 1.35289609e-13]\n",
      "20-th iteration, loss: 0.15678606748408616, 13 gd steps\n",
      "insert gradient: -0.016960428193305243\n",
      "20-th iteration, new layer inserted. now 25 layers\n",
      "[2.27499619e+00 5.37426720e+01 8.93660382e+01 4.22814516e+01\n",
      " 7.52152831e+01 5.09108353e+01 1.51515302e+02 6.26391874e+01\n",
      " 1.48532730e+02 6.56733712e+01 1.52687661e+02 6.62904222e+01\n",
      " 3.84463164e+01 3.21122525e+00 1.19529314e+02 3.56405809e+01\n",
      " 1.99570058e+01 6.76962153e+01 0.00000000e+00 4.51308102e+01\n",
      " 1.37585822e+02 3.48586924e+00 2.85569713e+01 5.91428900e+01\n",
      " 1.39051733e-13]\n",
      "21-th iteration, loss: 0.1528645876722251, 16 gd steps\n",
      "insert gradient: -0.03251719698422132\n",
      "21-th iteration, new layer inserted. now 23 layers\n",
      "[1.50592538e+00 5.20729735e+01 8.98890073e+01 4.56409741e+01\n",
      " 7.63377871e+01 4.87760852e+01 1.57204693e+02 6.15278288e+01\n",
      " 1.50279944e+02 6.44911133e+01 1.52262539e+02 6.50733966e+01\n",
      " 1.68705570e+02 4.96654617e+01 1.41250974e+01 5.47592686e+01\n",
      " 2.39114511e+01 2.72371057e+01 1.33610776e+02 6.82290209e+00\n",
      " 2.69787830e+01 6.54410860e+01 1.65949980e-13]\n",
      "22-th iteration, loss: 0.14669865753506162, 45 gd steps\n",
      "insert gradient: -0.01588694177903569\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[1.68969922e+00 5.24301270e+01 8.80783874e+01 4.45005024e+01\n",
      " 8.16538203e+01 5.56547260e+01 1.48797540e+02 6.52772348e+01\n",
      " 1.47911157e+02 6.93121425e+01 1.54446833e+02 6.54902507e+01\n",
      " 1.69286850e+02 5.49636377e+01 1.39183228e+01 5.07107997e+01\n",
      " 3.70316521e+01 2.19751255e+01 1.26326076e+02 1.59405535e+01\n",
      " 2.58423985e+01 5.83626471e+01 1.77717932e-13]\n",
      "23-th iteration, loss: 0.14531181230253443, 16 gd steps\n",
      "insert gradient: -0.023197474773734465\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[1.45979085e+00 5.07169537e+01 8.63770590e+01 4.62301329e+01\n",
      " 8.56168580e+01 5.49801792e+01 1.49177698e+02 6.50284240e+01\n",
      " 1.46053044e+02 7.10022914e+01 1.52670193e+02 6.74603281e+01\n",
      " 1.69186386e+02 6.16052706e+01 1.65522966e+01 4.18729419e+01\n",
      " 4.78364866e+01 1.83082767e+01 1.20216274e+02 1.96993580e+01\n",
      " 2.64673086e+01 5.50791832e+01 1.88822974e-13]\n",
      "24-th iteration, loss: 0.14415233773409017, 17 gd steps\n",
      "insert gradient: -0.012493540122712673\n",
      "24-th iteration, new layer inserted. now 23 layers\n",
      "[1.35991957e+00 5.09723921e+01 8.73067974e+01 4.53052528e+01\n",
      " 8.40081544e+01 5.63192255e+01 1.49469980e+02 6.54379450e+01\n",
      " 1.45771837e+02 7.17937401e+01 1.53216802e+02 6.84551802e+01\n",
      " 1.69684875e+02 6.19139942e+01 1.76146007e+01 4.06684231e+01\n",
      " 5.04338940e+01 1.78491178e+01 1.19247133e+02 2.07700597e+01\n",
      " 2.66663440e+01 5.46708369e+01 1.93226490e-13]\n",
      "25-th iteration, loss: 0.14300876335936927, 29 gd steps\n",
      "insert gradient: -0.006270280874800072\n",
      "25-th iteration, new layer inserted. now 23 layers\n",
      "[1.34095528e+00 5.13151181e+01 8.76909393e+01 4.63546272e+01\n",
      " 8.44079552e+01 5.59476440e+01 1.50627842e+02 6.54768621e+01\n",
      " 1.45523790e+02 7.30594836e+01 1.52697845e+02 7.00113000e+01\n",
      " 1.70469066e+02 6.31821764e+01 2.17954504e+01 3.58176614e+01\n",
      " 5.81743870e+01 1.64385719e+01 1.15800899e+02 2.28592272e+01\n",
      " 2.70975210e+01 5.28569803e+01 1.97708716e-13]\n",
      "26-th iteration, loss: 0.14150034596273536, 37 gd steps\n",
      "insert gradient: -0.016135231556501078\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[6.01820459e-01 4.93399557e+01 9.05088499e+01 4.92942082e+01\n",
      " 8.55248863e+01 5.35076763e+01 1.53731870e+02 6.61197238e+01\n",
      " 1.46097635e+02 7.36320298e+01 1.21567410e+02 0.00000000e+00\n",
      " 3.03918524e+01 7.21121651e+01 1.69308793e+02 6.49960614e+01\n",
      " 3.28351553e+01 2.34329229e+01 8.75995240e+01 1.27088326e+01\n",
      " 1.00693147e+02 2.83348191e+01 2.90050174e+01 4.65797033e+01\n",
      " 2.32613394e-13]\n",
      "27-th iteration, loss: 0.1400963387059732, 21 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[1.59775697e+00 5.22793056e+01 8.86868442e+01 4.70140779e+01\n",
      " 8.62956845e+01 5.64538592e+01 1.54189899e+02 6.47314061e+01\n",
      " 1.46685688e+02 7.41358972e+01 1.21615335e+02 5.64658655e-01\n",
      " 3.04031598e+01 7.25443568e+01 1.69873259e+02 6.54877633e+01\n",
      " 3.37011744e+01 2.26267704e+01 8.90015356e+01 1.29688685e+01\n",
      " 1.10093676e+01 0.00000000e+00 8.80749406e+01 2.83258410e+01\n",
      " 2.97397637e+01 4.58522821e+01 2.38607612e-13]\n",
      "28-th iteration, loss: 0.13819653223492387, 16 gd steps\n",
      "insert gradient: -0.019827035007548836\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[1.93447604e+00 5.08350734e+01 9.12429250e+01 4.59209786e+01\n",
      " 9.03947048e+01 5.78872738e+01 1.56417537e+02 6.20031196e+01\n",
      " 1.48703837e+02 7.41712108e+01 1.22073508e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.30264844e-01 2.92691403e+01 7.32935561e+01\n",
      " 1.66766326e+02 6.87883573e+01 4.19740641e+01 1.49012746e+01\n",
      " 1.04915723e+02 1.95765250e+01 6.87828351e+01 3.43942212e+01\n",
      " 3.63227457e+01 3.75824672e+01 3.30709153e-13]\n",
      "29-th iteration, loss: 0.13507139342022526, 26 gd steps\n",
      "insert gradient: -0.021680492408993727\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[1.67076099e+00 5.27192024e+01 9.01603388e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.86275483e+01 8.87311450e+01 5.64595729e+01\n",
      " 1.58479765e+02 6.44209974e+01 1.49597940e+02 7.59676544e+01\n",
      " 1.21808360e+02 1.59367302e+00 6.23965357e-02 2.24333970e+00\n",
      " 2.84420530e+01 7.21004003e+01 1.69084507e+02 6.81067337e+01\n",
      " 4.45227617e+01 1.33647464e+01 1.07469858e+02 2.47689735e+01\n",
      " 5.88879462e+01 3.55406157e+01 4.25363145e+01 3.20393520e+01\n",
      " 3.37580093e-13]\n",
      "30-th iteration, loss: 0.13101666285341676, 42 gd steps\n",
      "insert gradient: -0.016428659511122892\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[1.18960343e+00 5.16833144e+01 9.12087736e+01 5.16896472e+01\n",
      " 9.08639016e+01 5.66736929e+01 1.61651311e+02 6.33295314e+01\n",
      " 1.49314066e+02 7.80740389e+01 1.19609390e+02 9.17524431e+00\n",
      " 2.58472624e+01 6.88146358e+01 1.14478399e+02 0.00000000e+00\n",
      " 5.72391996e+01 6.84945217e+01 5.17146161e+01 8.76374792e+00\n",
      " 1.10876106e+02 3.43605968e+01 4.44940983e+01 3.62049250e+01\n",
      " 5.34585766e+01 2.45140731e+01 3.52439782e-13]\n",
      "31-th iteration, loss: 0.12793314274895348, 23 gd steps\n",
      "insert gradient: -0.02180672404574759\n",
      "31-th iteration, new layer inserted. now 29 layers\n",
      "[1.02257119e+00 5.14907050e+01 9.20720418e+01 5.22918370e+01\n",
      " 9.18909818e+01 5.67760766e+01 1.63680453e+02 6.23164492e+01\n",
      " 1.27379488e+02 0.00000000e+00 1.81970697e+01 8.10573869e+01\n",
      " 1.17064083e+02 1.28760490e+01 2.29606091e+01 6.73155054e+01\n",
      " 1.11928485e+02 3.73990885e+00 5.39171477e+01 6.64729703e+01\n",
      " 5.84253166e+01 4.92948807e+00 1.13803071e+02 4.40630702e+01\n",
      " 3.49949721e+01 3.60507522e+01 6.21085077e+01 2.03581763e+01\n",
      " 3.49152552e-13]\n",
      "32-th iteration, loss: 0.11560938281077074, 121 gd steps\n",
      "insert gradient: -0.009017677109436345\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[1.39148157e+00 5.37185896e+01 9.40106765e+01 5.28487098e+01\n",
      " 9.27119997e+01 5.38228995e+01 1.26398868e+02 0.00000000e+00\n",
      " 3.15997170e+01 7.10376989e+01 1.20823953e+02 9.60020082e+01\n",
      " 1.12217680e+02 1.02551609e+02 1.06207318e+02 1.88195534e+01\n",
      " 3.32369996e+01 5.77457065e+01 1.91164400e+02 6.13232256e+01\n",
      " 3.61452638e+01 2.73478113e+01 7.71481293e+01 1.85364544e+01\n",
      " 3.62748295e-13]\n",
      "33-th iteration, loss: 0.10733826960750281, 60 gd steps\n",
      "insert gradient: -0.053121570342199956\n",
      "33-th iteration, new layer inserted. now 25 layers\n",
      "[3.90636845e+00 5.56062675e+01 9.76570781e+01 5.27078264e+01\n",
      " 9.46545350e+01 5.52654620e+01 1.05715066e+02 1.01305869e+02\n",
      " 1.04247501e+02 1.03779222e+02 1.11466599e+02 1.04192785e+02\n",
      " 9.96127099e+01 3.32749286e+01 1.39473494e+01 6.00947619e+01\n",
      " 1.14457345e+02 0.00000000e+00 7.63048967e+01 4.97308804e+01\n",
      " 6.78669413e+01 9.88019332e+00 1.07217515e+02 1.83242847e+01\n",
      " 4.52333079e-13]\n",
      "34-th iteration, loss: 0.09480369552366558, 103 gd steps\n",
      "insert gradient: -0.0029024677025384967\n",
      "34-th iteration, new layer inserted. now 25 layers\n",
      "[1.40643450e+00 5.67942904e+01 9.80694241e+01 5.49254658e+01\n",
      " 9.55093326e+01 5.70182679e+01 1.03168034e+02 1.05751220e+02\n",
      " 1.03160627e+02 1.04139907e+02 1.16836483e+02 7.38458454e+01\n",
      " 0.00000000e+00 3.16482194e+01 1.04862862e+02 1.09974896e+02\n",
      " 9.52198650e+01 3.40561269e+01 2.41316562e+01 5.43341263e+01\n",
      " 9.60008010e+01 1.26781508e+01 7.42862196e+01 3.03249849e+01\n",
      " 2.32077705e-13]\n",
      "35-th iteration, loss: 0.09378879808113509, 33 gd steps\n",
      "insert gradient: -0.003560152831585737\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[1.50531289e+00 5.70462411e+01 9.85701112e+01 5.50524280e+01\n",
      " 9.53771335e+01 5.67707850e+01 1.04165472e+02 1.06041773e+02\n",
      " 1.02693393e+02 1.03987628e+02 1.17969203e+02 7.12984169e+01\n",
      " 9.07431592e+00 2.74647432e+01 1.05986935e+02 6.06505950e+01\n",
      " 0.00000000e+00 4.85204760e+01 9.93116485e+01 3.85462286e+01\n",
      " 1.98543367e+01 5.25313780e+01 9.82768036e+01 1.72242620e+01\n",
      " 5.86973865e+01 3.61277172e+01 1.76152820e-13]\n",
      "36-th iteration, loss: 0.09179146856337514, 131 gd steps\n",
      "insert gradient: -0.006665533133536711\n",
      "36-th iteration, new layer inserted. now 26 layers\n",
      "[  1.35732504  57.5149956   99.28615408  55.47120858  95.98551471\n",
      "  56.98446718 104.92869065 106.94007081 103.7776399  102.17105085\n",
      " 126.42448132  69.06923492  25.37512898  18.8395712  109.61115895\n",
      "  60.46770698  20.1003338   31.2117227  100.78915066  53.95944103\n",
      "  13.49100458  44.83035431 101.42891075  28.36041237  31.83011687\n",
      "  48.76961806]\n",
      "37-th iteration, loss: 0.08875614025764338, 116 gd steps\n",
      "insert gradient: -0.012983324672113352\n",
      "37-th iteration, new layer inserted. now 26 layers\n",
      "[  1.54939654  56.90179987  98.97944475  55.10299124  95.61974396\n",
      "  56.54718626 105.56165675 105.01616823 105.81334518  91.2475654\n",
      " 147.99574024  64.96756007  52.45344514   7.56540563 113.70992907\n",
      "  59.45092451  43.64695271  17.78503292 102.61514656  58.31442853\n",
      "  27.32313808  28.76221321 100.95918419  45.42348828  12.16678093\n",
      "  53.3452571 ]\n",
      "38-th iteration, loss: 0.08408159110533776, 174 gd steps\n",
      "insert gradient: -0.0008976268953456241\n",
      "38-th iteration, new layer inserted. now 26 layers\n",
      "[1.42165037e+00 5.73782022e+01 1.00342612e+02 5.56708372e+01\n",
      " 9.59860984e+01 5.72745743e+01 1.03954195e+02 1.04691798e+02\n",
      " 1.08847656e+02 7.67622364e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.71933560e+02 5.95620835e+01 1.93913152e+02 5.39184834e+01\n",
      " 7.14773743e+01 1.02290709e+01 9.61652095e+01 5.64928551e+01\n",
      " 4.46028455e+01 1.95337972e+01 9.71656612e+01 6.12543326e+01\n",
      " 7.72512176e+00 4.30458326e+01]\n",
      "39-th iteration, loss: 0.08362210907549704, 75 gd steps\n",
      "insert gradient: -0.00027064467663490957\n",
      "39-th iteration, new layer inserted. now 26 layers\n",
      "[1.32573906e+00 5.75052344e+01 1.00629485e+02 0.00000000e+00\n",
      " 3.19744231e-14 5.57460761e+01 9.56070760e+01 5.71992856e+01\n",
      " 1.04582411e+02 1.04117571e+02 1.11109791e+02 7.52159979e+01\n",
      " 1.75522632e+02 5.87996527e+01 1.94762287e+02 5.19991053e+01\n",
      " 8.23193866e+01 7.48270424e+00 9.50204650e+01 5.43144589e+01\n",
      " 5.52038548e+01 1.52043429e+01 9.89634420e+01 5.94107435e+01\n",
      " 1.59141827e+01 3.56445602e+01]\n",
      "40-th iteration, loss: 0.08359650279724473, 39 gd steps\n",
      "insert gradient: -0.00018311268691710984\n",
      "40-th iteration, new layer inserted. now 24 layers\n",
      "[  1.33641103  57.5486572  100.68868051  55.73094481  95.48229876\n",
      "  57.15973945 104.72619485 104.02414685 111.38167993  75.24900723\n",
      " 175.59537801  58.83008673 194.94525343  51.55366984  84.42223511\n",
      "   6.99922884  94.68462152  53.7506136   57.72349338  14.22835233\n",
      "  99.4582387   58.84489601  17.68468748  34.2930619 ]\n",
      "41-th iteration, loss: 0.08359345107253625, 34 gd steps\n",
      "insert gradient: -0.00026273793247246765\n",
      "41-th iteration, new layer inserted. now 26 layers\n",
      "[1.34073568e+00 5.75747172e+01 1.00695957e+02 0.00000000e+00\n",
      " 1.42108547e-14 5.56719673e+01 9.54261856e+01 5.72492552e+01\n",
      " 1.04792159e+02 1.03953288e+02 1.11457285e+02 7.52979768e+01\n",
      " 1.75591873e+02 5.87746784e+01 1.94988218e+02 5.15197119e+01\n",
      " 8.49199627e+01 6.93061886e+00 9.44814266e+01 5.35413866e+01\n",
      " 5.85176722e+01 1.39743752e+01 9.95434657e+01 5.87275379e+01\n",
      " 1.81964464e+01 3.39009183e+01]\n",
      "42-th iteration, loss: 0.08359196326164187, 431 gd steps\n",
      "insert gradient: -3.275061056757443e-05\n",
      "42-th iteration, new layer inserted. now 26 layers\n",
      "[  1.32164602  57.51988496 100.7166486   55.75608975  95.41528099\n",
      "  57.17854753 104.79526187 103.96015499 111.47663659  75.35243802\n",
      " 175.59789607  58.72242946 108.349445     0.          86.679556\n",
      "  51.46451699  85.14390309   6.89017253  94.40038568  53.51285808\n",
      "  58.88930682  13.83160712  99.57937915  58.68189898  18.44623253\n",
      "  33.72180503]\n",
      "43-th iteration, loss: 0.08359141398560269, 30 gd steps\n",
      "insert gradient: -0.0006198051697703902\n",
      "43-th iteration, new layer inserted. now 28 layers\n",
      "[1.32112649e+00 5.75283064e+01 1.00729521e+02 5.57539409e+01\n",
      " 9.53916930e+01 5.71855985e+01 1.04827878e+02 1.03930528e+02\n",
      " 1.11554888e+02 7.53487204e+01 1.75628083e+02 5.87167141e+01\n",
      " 9.62347001e+01 0.00000000e+00 1.20293375e+01 5.35440500e-02\n",
      " 8.66551713e+01 5.13983537e+01 8.55688522e+01 6.80463389e+00\n",
      " 9.43149779e+01 5.34102346e+01 5.93554308e+01 1.36495994e+01\n",
      " 9.97015225e+01 5.86060286e+01 1.86956015e+01 3.35340034e+01]\n",
      "44-th iteration, loss: 0.08358774375534131, 72 gd steps\n",
      "insert gradient: -7.214494845127338e-05\n",
      "44-th iteration, new layer inserted. now 28 layers\n",
      "[1.32165412e+00 5.75440060e+01 1.00742500e+02 5.57920465e+01\n",
      " 9.54329594e+01 5.72188952e+01 1.04837963e+02 1.03964080e+02\n",
      " 1.11585474e+02 7.53941484e+01 1.75691118e+02 5.87226224e+01\n",
      " 9.60789722e+01 0.00000000e+00 1.06581410e-14 2.96044463e-01\n",
      " 9.83004368e+01 5.13993570e+01 8.55834968e+01 6.81050546e+00\n",
      " 9.43289622e+01 5.34639745e+01 5.93727228e+01 1.36281145e+01\n",
      " 9.97204603e+01 5.86512109e+01 1.86981572e+01 3.34873917e+01]\n",
      "45-th iteration, loss: 0.08358462730209221, 19 gd steps\n",
      "insert gradient: -0.00022420305054702058\n",
      "45-th iteration, new layer inserted. now 30 layers\n",
      "[1.29241698e+00 0.00000000e+00 2.22044605e-16 5.75378814e+01\n",
      " 1.00775463e+02 5.58373088e+01 9.54488570e+01 5.72505342e+01\n",
      " 1.04828371e+02 1.03962155e+02 1.11740774e+02 7.53176184e+01\n",
      " 1.75884346e+02 5.89447391e+01 9.55544168e+01 2.35649155e-01\n",
      " 7.43652948e-03 5.31693618e-01 9.72157669e+01 5.14993221e+01\n",
      " 8.57037845e+01 6.76828312e+00 9.44278597e+01 5.34371625e+01\n",
      " 5.94512882e+01 1.35227790e+01 9.99293925e+01 5.89541390e+01\n",
      " 1.85682562e+01 3.33353608e+01]\n",
      "46-th iteration, loss: 0.08358294929226019, 57 gd steps\n",
      "insert gradient: -7.920172611289597e-05\n",
      "46-th iteration, new layer inserted. now 28 layers\n",
      "[1.30831319e+00 5.75972907e+01 1.00806740e+02 5.58169750e+01\n",
      " 9.54885990e+01 5.72774306e+01 1.04830773e+02 1.04008183e+02\n",
      " 1.11720858e+02 7.53136429e+01 1.76035556e+02 5.88147071e+01\n",
      " 9.59871477e+01 2.42218727e-01 7.71694396e-02 5.98488324e-01\n",
      " 9.66141710e+01 5.15253310e+01 8.57073557e+01 6.76724493e+00\n",
      " 9.44295254e+01 5.34573263e+01 5.95215148e+01 1.34810536e+01\n",
      " 9.99601170e+01 5.89751247e+01 1.85561373e+01 3.32867190e+01]\n",
      "47-th iteration, loss: 0.08357907997919742, 63 gd steps\n",
      "insert gradient: -5.3632866835099344e-05\n",
      "47-th iteration, new layer inserted. now 26 layers\n",
      "[  1.30710548  57.61844334 100.83202453  55.83505287  95.50365893\n",
      "  57.29720648 104.83921332 104.01891718 111.77722065  75.27963687\n",
      " 176.1653111   58.79367938  97.86919789   1.03875811  94.2798746\n",
      "  51.58012256  85.80937875   6.75198401  94.41663224  53.45017145\n",
      "  59.62900858  13.41596837 100.03628636  59.07050006  18.53972121\n",
      "  33.20030695]\n",
      "48-th iteration, loss: 0.08357829525255711, 58 gd steps\n",
      "insert gradient: -3.5659510517907407e-05\n",
      "48-th iteration, new layer inserted. now 26 layers\n",
      "[  1.30495021  57.62046302 100.84324645  55.83915507  95.50428705\n",
      "  57.30265318 104.84488364 104.0204235  111.80246632  75.2629835\n",
      " 176.21907532  58.79109098  98.61749909   1.11433223  93.31008948\n",
      "  51.60733543  85.85857133   6.74391409  94.40316156  53.44425491\n",
      "  59.69248737  13.38223485 100.06535991  59.10520858  18.54172318\n",
      "  33.15913553]\n",
      "49-th iteration, loss: 0.08357814296160193, 70 gd steps\n",
      "insert gradient: -3.139572553553939e-05\n",
      "49-th iteration, new layer inserted. now 28 layers\n",
      "[1.30365935e+00 5.76171160e+01 1.00845164e+02 5.58366460e+01\n",
      " 9.55150983e+01 5.73029742e+01 1.04846299e+02 1.04018288e+02\n",
      " 1.11809776e+02 7.52619606e+01 1.76226380e+02 5.87831069e+01\n",
      " 9.89832874e+01 1.12690892e+00 9.29100604e+01 0.00000000e+00\n",
      " 3.55271368e-15 5.16175083e+01 8.58815418e+01 6.74330916e+00\n",
      " 9.43882957e+01 5.34381949e+01 5.97132495e+01 1.33774707e+01\n",
      " 1.00067675e+02 5.91066724e+01 1.85511368e+01 3.31494573e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.532378848404031\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.01319852    0.         1386.1476055 ]\n",
      "1-th iteration, loss: 0.7519772466020609, 11 gd steps\n",
      "insert gradient: -0.6407793744285358\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.05862423   62.16864421  236.65934728    0.         1149.48825822]\n",
      "2-th iteration, loss: 0.6037216736995527, 13 gd steps\n",
      "insert gradient: -0.6720260403990779\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  2.70923615  76.80783071 221.3488803   41.3947929  234.58944045\n",
      "   0.         914.89881776]\n",
      "3-th iteration, loss: 0.4471375866893936, 32 gd steps\n",
      "insert gradient: -0.6208029767403809\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          52.9765482  221.5683606   51.669082   150.27431135\n",
      "  53.50674584 601.2192231    0.         313.67959466]\n",
      "4-th iteration, loss: 0.35381788748350124, 20 gd steps\n",
      "insert gradient: -0.518349850507396\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.7595183   50.55051645 105.68889742   0.         113.81881261\n",
      "  56.72860831 137.32454465  55.42532402 559.09669815  46.48489775\n",
      " 313.67959466]\n",
      "5-th iteration, loss: 0.31903054375365336, 27 gd steps\n",
      "insert gradient: -0.3313634366697644\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.86363128  54.01680458  82.01015189  17.67091109  89.38033552\n",
      "  60.6149     130.04216754  62.56312407 171.34426425   0.\n",
      " 367.16628054  61.44469018 313.67959466]\n",
      "6-th iteration, loss: 0.23649965891171096, 151 gd steps\n",
      "insert gradient: -0.11359967008722305\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  3.14985639  51.91211135  82.81620231  26.8049239   81.51852149\n",
      "  61.27059902 135.0304906   66.70919445 136.16722065  70.39937668\n",
      " 280.12425231  87.21872753 191.69308563   0.         121.98650904]\n",
      "7-th iteration, loss: 0.22350645298455019, 30 gd steps\n",
      "insert gradient: -0.0747262361884789\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.83185623  52.90845232  81.65479986  29.8771374   78.57030031\n",
      "  61.91461918 134.2791675   68.40330921 136.49806561  74.87547663\n",
      " 172.76943592   0.         103.66166155  88.19979972 173.60691676\n",
      "  29.76603953 121.98650904]\n",
      "8-th iteration, loss: 0.2167295200415124, 15 gd steps\n",
      "insert gradient: -0.052862376035114664\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[1.94387816e+00 0.00000000e+00 4.99600361e-16 5.24614455e+01\n",
      " 8.75167221e+01 3.20694459e+01 7.46909578e+01 5.49289802e+01\n",
      " 1.41717965e+02 6.55550738e+01 1.42550850e+02 7.03514047e+01\n",
      " 1.65238984e+02 1.37361663e+01 8.06181403e+01 8.92143843e+01\n",
      " 1.70893572e+02 3.59954094e+01 1.21986509e+02]\n",
      "9-th iteration, loss: 0.19352133642041822, 68 gd steps\n",
      "insert gradient: -0.0276278191012874\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.63922782  51.64801772  88.51106461  33.50129802  71.34728767\n",
      "  56.60696916 138.28835627  66.45750859 138.22136273  67.98034404\n",
      " 153.26401866  45.41008004  13.57501318  58.07983309   0.\n",
      "  43.55987482 156.07484753  70.90048126 121.98650904]\n",
      "10-th iteration, loss: 0.18755540594539472, 173 gd steps\n",
      "insert gradient: -0.01854711280506615\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  3.50258359  52.38092807  87.35637496  36.46139955  75.63606382\n",
      "  54.4298501  146.02856643  63.52423925 145.48620442  67.34676385\n",
      " 147.59783093  64.19331375   4.38882309  38.07415527  43.5779141\n",
      "  21.88952546 124.55349622   0.          41.51783207  75.85162168\n",
      " 121.98650904]\n",
      "11-th iteration, loss: 0.18684935771106959, 30 gd steps\n",
      "insert gradient: -0.008116393416061027\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  3.37534196  52.65785065  87.36213471  36.91559359  76.50521362\n",
      "  54.33381212 147.17371042  63.76451219 145.89918297  67.74710974\n",
      " 147.5281419   65.88116088   6.94008459  33.90688475  49.74019362\n",
      "  19.80191664 122.03414345   4.20454383  37.95698162  74.74179871\n",
      "  33.26904792   0.          88.71746112]\n",
      "12-th iteration, loss: 0.1853881489779991, 36 gd steps\n",
      "insert gradient: -0.018515006660236387\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[3.05074622e+00 5.55079684e+01 8.80419390e+01 3.78109501e+01\n",
      " 7.75238265e+01 0.00000000e+00 7.10542736e-15 5.32744700e+01\n",
      " 1.49719497e+02 6.43363077e+01 1.49281109e+02 6.71502479e+01\n",
      " 1.51266365e+02 6.70887610e+01 1.45731217e+01 2.58781648e+01\n",
      " 6.56443404e+01 1.60975527e+01 1.18451284e+02 8.47617156e+00\n",
      " 3.49362713e+01 6.95697619e+01 3.05144736e+01 3.93647130e+00\n",
      " 8.87174611e+01]\n",
      "13-th iteration, loss: 0.18283329090963277, 32 gd steps\n",
      "insert gradient: -0.01448447152023427\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  1.37801169  55.28175446  88.61994352  38.98280312  80.24480268\n",
      "  55.46019821 151.02516698  64.18058144 150.99270847  66.6932643\n",
      " 154.71410541  67.93167132  21.82013999  21.82100094  78.87014417\n",
      "  15.07900888 112.89642659  13.11124924  32.45942533  63.66571147\n",
      "  32.25205507   3.52232545  88.71746112]\n",
      "14-th iteration, loss: 0.178672962147428, 22 gd steps\n",
      "insert gradient: -0.03309350616407077\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  1.74014115  53.64360831  86.85985256  40.98584486  82.63618305\n",
      "  57.66945136 149.7832222   66.32089693 149.05462386  67.04673408\n",
      " 155.79563814  68.88502723  33.09771662  14.47308303 100.43222879\n",
      "  14.11077283  99.86968872  21.16823014  28.07685289  54.05955383\n",
      " 125.28211656]\n",
      "15-th iteration, loss: 0.14750492535422813, 55 gd steps\n",
      "insert gradient: -0.05034884946316194\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[  1.44958528  52.7932832   92.18954372  49.11961371  89.32093665\n",
      "  59.48928296 156.97126915  68.87335381 154.5479578   73.14741005\n",
      " 169.09775104  59.11331005 183.17696106  59.38446584  23.27061431\n",
      "  33.61191849  71.82316277  24.36901822  93.96158742   0.\n",
      "  31.32052914]\n",
      "16-th iteration, loss: 0.13005813459987703, 68 gd steps\n",
      "insert gradient: -0.013543443554927013\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[  1.30818056  53.31190352  92.62814748  51.43718598  91.49836823\n",
      "  57.2599939  165.66358803  63.37950301 160.3953515   71.16877331\n",
      " 125.71838581   0.          47.14439468  57.39635752 182.42229653\n",
      "  54.42602862  41.8789606   15.30376854 103.665976    42.125698\n",
      "  21.94932795  37.21042416  31.32052914]\n",
      "17-th iteration, loss: 0.12335757377111871, 303 gd steps\n",
      "insert gradient: -0.026221807823675166\n",
      "17-th iteration, new layer inserted. now 23 layers\n",
      "[  1.38574945  54.38155405  94.90305723  53.01608152  93.4612615\n",
      "  54.93946394 161.27722422  70.3843569  108.58956461 101.8206069\n",
      " 100.66848386  27.04091051  19.96759017  61.20570209 116.3577387\n",
      "   0.          66.4901364   57.10573749 184.51949489  59.53445312\n",
      "  30.99542772  20.78147723  31.32052914]\n",
      "18-th iteration, loss: 0.11350138246880627, 82 gd steps\n",
      "insert gradient: -0.030385366996034138\n",
      "18-th iteration, new layer inserted. now 21 layers\n",
      "[  2.29096692  52.89639007  97.67583186  53.55370136  92.50203593\n",
      "  52.2610478  123.99275167  92.73538209 108.05567139 102.91738106\n",
      "  97.20118379 104.47092622  96.94377117  27.28770332  24.73885899\n",
      "  57.12163381 188.25922091  56.32503019  41.26082217  10.30394324\n",
      "  31.32052914]\n",
      "19-th iteration, loss: 0.11157256806627427, 129 gd steps\n",
      "insert gradient: -0.01489863574205234\n",
      "19-th iteration, new layer inserted. now 23 layers\n",
      "[  3.06499636  54.48954301  98.40631584  53.11861549  92.83374297\n",
      "  52.23596589 109.92225781 100.04195397 107.73581313 104.01935408\n",
      "  97.15840348 104.78986003  96.10499969  30.28375116  20.58402218\n",
      "  57.00348512 116.737563     0.          66.70717886  57.09127605\n",
      "  38.17268097  11.19750969  31.32052914]\n",
      "20-th iteration, loss: 0.10488089806534193, 153 gd steps\n",
      "insert gradient: -0.0030909188066065804\n",
      "20-th iteration, new layer inserted. now 25 layers\n",
      "[  1.24335134  57.16504765  96.92200507  53.98882542  96.27814329\n",
      "  58.24211231 108.87539813 102.94006986 110.12850952 102.41486878\n",
      " 120.30986856  64.56733378   0.          43.04488919  98.80074833\n",
      "  50.10566044  13.05790478  47.43799666  95.31115907  30.8791975\n",
      "  30.00319175  52.24076974  81.43507213   8.03944455  31.32052914]\n",
      "21-th iteration, loss: 0.10426530391512209, 61 gd steps\n",
      "insert gradient: -0.00282295384840133\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[  1.155337    57.99041539  97.36862235  54.29932101  95.95631782\n",
      "  58.4301111  109.97316998 103.29755907 111.26741144 101.91028996\n",
      " 124.52745594  65.5474799   13.51570896  30.49623635 101.63781949\n",
      "  54.60352099  13.7231325   43.33743405  96.25030007  37.50914367\n",
      "  22.09000566  52.66433653  89.15440173   7.34508965  31.32052914]\n",
      "22-th iteration, loss: 0.1042133015247301, 28 gd steps\n",
      "insert gradient: -0.000617938044126631\n",
      "22-th iteration, new layer inserted. now 25 layers\n",
      "[  1.07737514  57.98441289  97.54448029  54.50909592  96.29698221\n",
      "  58.63016934 110.348559   103.41251417 111.17035496 101.96577972\n",
      " 125.36810323  65.34740134  14.75855935  30.04943873 101.54136255\n",
      "  54.94247126  14.01582899  43.11070473  96.52958778  38.25934934\n",
      "  21.66096943  52.72396442  89.79185386   7.408462    31.32052914]\n",
      "23-th iteration, loss: 0.1041758073188396, 19 gd steps\n",
      "insert gradient: -0.0024260680062943097\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[  0.98570491  57.84093861  97.53193503  54.74131822  96.14475831\n",
      "  58.59727536 110.61485095 103.50687269 111.18439093 101.65380778\n",
      " 126.50829612  64.64162157  16.84577659  29.00506989 101.91885079\n",
      "  55.616468    15.06449991  41.45844178  96.74769208  40.06947207\n",
      "  19.79732403  52.6094605   92.23374372   7.41525561  31.32052914]\n",
      "24-th iteration, loss: 0.10412363808963662, 48 gd steps\n",
      "insert gradient: -0.0009698068418244114\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[  1.06298378  58.33714349  97.8038883   54.47640899  96.26930198\n",
      "  58.67712279 111.12095146 103.46299491 111.27967707 101.59361242\n",
      " 127.75097075  64.31510178  18.92357278  27.73525483 102.53684849\n",
      "  56.35479079  16.52927179  39.69558077  97.01432888  42.27290988\n",
      "  18.185162    52.38980166  94.37453499   7.60401633  31.32052914]\n",
      "25-th iteration, loss: 0.10408538936073379, 59 gd steps\n",
      "insert gradient: -0.00038112593183495616\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[9.17833269e-01 5.83568488e+01 9.80533706e+01 5.45663294e+01\n",
      " 9.61935425e+01 5.86879680e+01 1.11527326e+02 1.03401284e+02\n",
      " 1.11556044e+02 1.01120236e+02 0.00000000e+00 1.77635684e-14\n",
      " 1.29296766e+02 6.47111921e+01 2.10230134e+01 2.61070806e+01\n",
      " 1.03071762e+02 5.69728333e+01 1.80107207e+01 3.81311230e+01\n",
      " 9.73400185e+01 4.41876620e+01 1.71269572e+01 5.18113446e+01\n",
      " 9.59707147e+01 8.31355910e+00 3.13205291e+01]\n",
      "26-th iteration, loss: 0.10406790794506722, 36 gd steps\n",
      "insert gradient: -0.0003712030001304468\n",
      "26-th iteration, new layer inserted. now 27 layers\n",
      "[9.41720243e-01 5.84202647e+01 9.79934184e+01 5.46547240e+01\n",
      " 9.60787567e+01 5.87560794e+01 1.11765123e+02 1.03351039e+02\n",
      " 1.11660405e+02 0.00000000e+00 3.19744231e-14 1.00766014e+02\n",
      " 1.30424224e+02 6.46841935e+01 2.22324115e+01 2.53016081e+01\n",
      " 1.03456103e+02 5.73494425e+01 1.89081757e+01 3.71021798e+01\n",
      " 9.75809465e+01 4.53356834e+01 1.65397698e+01 5.14671584e+01\n",
      " 9.66599342e+01 8.76114250e+00 3.13205291e+01]\n",
      "27-th iteration, loss: 0.10405608053545781, 29 gd steps\n",
      "insert gradient: -0.0005832576991602666\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[  0.92214059  58.42316132  98.03051354  54.67668977  96.02430815\n",
      "  58.76353355 111.91470093 103.27761996 111.73811485 100.56820039\n",
      " 131.02223277  64.70711497  23.06795398  24.71631276 103.70213741\n",
      "  57.60098989  19.60318984  36.33989564  97.73808205  46.14948954\n",
      "  16.18533467  51.15421592  97.13726201   9.06971858  31.32052914]\n",
      "28-th iteration, loss: 0.10403267459748038, 37 gd steps\n",
      "insert gradient: -0.0007247777746196707\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[  0.92361459  58.41366912  97.97515048  54.57702953  95.93346532\n",
      "  58.72212982 112.23037839 103.22583253 111.79872123  88.92028994\n",
      "   0.          11.11503624 132.19605511  64.6009459   24.54006516\n",
      "  23.71827615 104.28807569  57.98476089  20.84284378  34.88853223\n",
      "  98.11742764  47.5828847   15.59433334  50.4516464   97.94220764\n",
      "   9.62237609  31.32052914]\n",
      "29-th iteration, loss: 0.10355005641745772, 66 gd steps\n",
      "insert gradient: -0.004557699750017526\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[1.12601436e+00 0.00000000e+00 2.22044605e-16 5.76956137e+01\n",
      " 9.80749306e+01 5.46176330e+01 9.49120193e+01 5.93071158e+01\n",
      " 1.12360477e+02 1.02486059e+02 1.12354662e+02 8.50077439e+01\n",
      " 7.38013727e+00 9.46224930e+00 1.34066921e+02 6.53900732e+01\n",
      " 3.44536684e+01 1.76976420e+01 1.06447361e+02 5.86419302e+01\n",
      " 2.88578060e+01 2.80976140e+01 9.98828213e+01 5.41693813e+01\n",
      " 1.50121947e+01 4.54354829e+01 1.01946313e+02 1.22739722e+01\n",
      " 3.13205291e+01]\n",
      "30-th iteration, loss: 0.0938571579271344, 120 gd steps\n",
      "insert gradient: -0.0037079975453694573\n",
      "30-th iteration, new layer inserted. now 25 layers\n",
      "[1.61838576e+00 5.75570781e+01 1.03061052e+02 5.42294577e+01\n",
      " 9.31832414e+01 5.79709040e+01 1.07865796e+02 1.02890452e+02\n",
      " 1.13388770e+02 7.95262006e+01 1.72579747e+02 5.79673520e+01\n",
      " 1.97415256e+02 5.12758907e+01 7.04109552e+01 8.43861559e+00\n",
      " 1.06078268e+02 5.65337505e+01 0.00000000e+00 5.32907052e-15\n",
      " 3.99926063e+01 2.29535084e+01 1.13364624e+02 2.38374827e+01\n",
      " 3.13205291e+01]\n",
      "31-th iteration, loss: 0.0914438235852093, 92 gd steps\n",
      "insert gradient: -0.005310151190335665\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[1.33414915e+00 5.82294941e+01 1.02326297e+02 5.54313448e+01\n",
      " 9.23736197e+01 5.71701272e+01 1.09079810e+02 1.02555645e+02\n",
      " 1.14280000e+02 7.96767838e+01 1.74825200e+02 5.74781476e+01\n",
      " 1.97601428e+02 4.95691938e+01 8.31868462e+01 6.47596579e+00\n",
      " 1.01633315e+02 5.10010885e+01 6.56345569e+01 1.20034108e+01\n",
      " 1.13702759e+02 3.05875944e+01 3.13205291e+01 0.00000000e+00\n",
      " 8.88178420e-16]\n",
      "32-th iteration, loss: 0.08473728171410991, 33 gd steps\n",
      "insert gradient: -0.001661394082219859\n",
      "32-th iteration, new layer inserted. now 24 layers\n",
      "[  1.01151519  58.45239161 102.17501747  56.07020746  94.23354336\n",
      "  57.51520657 104.49267193 103.59077763 114.55179551  75.80749495\n",
      " 179.97531674  56.12355647 199.10311027  49.56984401  88.8456488\n",
      "   7.03832999  93.12563655  49.69395116  75.89642297   7.06016763\n",
      " 107.85980426  50.87197173  39.05849927  20.77262809]\n",
      "33-th iteration, loss: 0.08363132107034132, 78 gd steps\n",
      "insert gradient: -0.002636419303335167\n",
      "33-th iteration, new layer inserted. now 26 layers\n",
      "[1.28006800e+00 5.74488762e+01 1.01003076e+02 5.58691574e+01\n",
      " 9.51448571e+01 5.71720424e+01 1.05199954e+02 1.03862325e+02\n",
      " 1.11737247e+02 7.58199242e+01 1.75743378e+02 5.82102398e+01\n",
      " 1.95512325e+02 0.00000000e+00 6.39488462e-14 5.09664445e+01\n",
      " 8.80873478e+01 6.51324425e+00 9.32978967e+01 5.24188547e+01\n",
      " 6.46222552e+01 1.20102190e+01 9.95414315e+01 5.79596400e+01\n",
      " 2.12816662e+01 3.15775696e+01]\n",
      "34-th iteration, loss: 0.083594569626078, 999 gd steps\n",
      "insert gradient: -5.0478493195523125e-05\n",
      "34-th iteration, new layer inserted. now 26 layers\n",
      "[  1.31793901  57.52907744 100.76189128  55.76728655  95.32986024\n",
      "  57.17709553 104.90941101 103.88988699 111.65351915  75.45781051\n",
      " 175.65320884  58.63451525 108.46890229   0.          86.77512184\n",
      "  51.18956089  86.59791385   6.66878424  93.89857652  53.08078591\n",
      "  61.04174824  13.08660413  99.81488279  58.38061296  19.66299514\n",
      "  32.8008168 ]\n",
      "35-th iteration, loss: 0.08359118252820923, 50 gd steps\n",
      "insert gradient: -0.00045673143366465626\n",
      "35-th iteration, new layer inserted. now 28 layers\n",
      "[  1.31754518  57.53401794 100.74915683  55.773061    95.40281111\n",
      "  57.19438532 104.84341855 103.94678063 111.57619337  75.39067032\n",
      " 175.67012723  58.69795537  95.84905792   0.          11.98113224\n",
      "   0.17572051  86.76432084  51.45143543  85.55008636   6.82925015\n",
      "  94.28246436  53.39510378  59.57634494  13.57829127  99.68203362\n",
      "  58.66134145  18.82193321  33.36746685]\n",
      "36-th iteration, loss: 0.08358769727817106, 77 gd steps\n",
      "insert gradient: -7.654778403602779e-05\n",
      "36-th iteration, new layer inserted. now 26 layers\n",
      "[  1.32038978  57.54850401 100.75430248  55.79016621  95.42281326\n",
      "  57.218402   104.84913878 103.95931272 111.59753327  75.40755482\n",
      " 175.71137074  58.7208857   95.74939127   0.3188294   98.53460798\n",
      "  51.42865156  85.57760447   6.81857157  94.30016468  53.42721532\n",
      "  59.5698377   13.56398753  99.7053086   58.67817397  18.8040748\n",
      "  33.3620189 ]\n",
      "37-th iteration, loss: 0.08358381940906887, 43 gd steps\n",
      "insert gradient: -9.579757944338783e-05\n",
      "37-th iteration, new layer inserted. now 28 layers\n",
      "[1.30769780e+00 5.75903242e+01 1.00800536e+02 5.58188208e+01\n",
      " 9.54913661e+01 5.72850941e+01 1.04830985e+02 1.03990265e+02\n",
      " 1.11736839e+02 7.53114576e+01 1.76000981e+02 5.88471218e+01\n",
      " 0.00000000e+00 1.59872116e-14 9.56054287e+01 8.81455103e-01\n",
      " 9.69254170e+01 5.15242295e+01 8.57534276e+01 6.74272923e+00\n",
      " 9.44891276e+01 5.34304403e+01 5.95677977e+01 1.34654265e+01\n",
      " 9.99930007e+01 5.89596815e+01 1.85720050e+01 3.32897490e+01]\n",
      "38-th iteration, loss: 0.08358283334821281, 48 gd steps\n",
      "insert gradient: -2.6054738682077994e-05\n",
      "38-th iteration, new layer inserted. now 28 layers\n",
      "[1.30789195e+00 5.76003174e+01 1.00808278e+02 5.58220229e+01\n",
      " 9.54916045e+01 5.72767940e+01 1.04830987e+02 1.04007361e+02\n",
      " 1.11727352e+02 7.53058567e+01 1.76057011e+02 5.87906062e+01\n",
      " 0.00000000e+00 1.59872116e-14 9.61036054e+01 8.47139064e-01\n",
      " 9.66083903e+01 5.15056787e+01 8.57338868e+01 6.76134871e+00\n",
      " 9.44491057e+01 5.34421224e+01 5.95804492e+01 1.34533869e+01\n",
      " 9.99911237e+01 5.89870568e+01 1.85704619e+01 3.32614209e+01]\n",
      "39-th iteration, loss: 0.08358212109193608, 145 gd steps\n",
      "insert gradient: -2.2521755335551613e-05\n",
      "39-th iteration, new layer inserted. now 28 layers\n",
      "[1.30740978e+00 5.76016373e+01 1.00813300e+02 5.58218664e+01\n",
      " 9.54894724e+01 5.72806393e+01 1.04831745e+02 1.04009025e+02\n",
      " 1.11731277e+02 7.53013523e+01 1.76073234e+02 5.87936657e+01\n",
      " 0.00000000e+00 7.10542736e-15 9.63947832e+01 8.66669499e-01\n",
      " 9.62552050e+01 5.15194984e+01 8.57357488e+01 6.76552296e+00\n",
      " 9.44301224e+01 5.34419765e+01 5.95927375e+01 1.34489263e+01\n",
      " 9.99895115e+01 5.89934803e+01 1.85700620e+01 3.32527930e+01]\n",
      "40-th iteration, loss: 0.0835812654447311, 63 gd steps\n",
      "insert gradient: -2.10302732734202e-05\n",
      "40-th iteration, new layer inserted. now 26 layers\n",
      "[  1.30676279  57.603315   100.81682719  55.82802653  95.49109079\n",
      "  57.2767669  104.83322169 104.00727645 111.74409351  75.29554223\n",
      " 176.09779537  58.80193964  96.74498169   0.90424938  95.78633792\n",
      "  51.53205319  85.75301217   6.76055781  94.4228381   53.44508043\n",
      "  59.60026178  13.43955094 100.00062764  59.01368263  18.56296162\n",
      "  33.24092429]\n",
      "41-th iteration, loss: 0.08357919296177424, 38 gd steps\n",
      "insert gradient: -9.156092678531058e-05\n",
      "41-th iteration, new layer inserted. now 28 layers\n",
      "[1.30874269e+00 5.76242538e+01 1.00827742e+02 0.00000000e+00\n",
      " 2.48689958e-14 5.58240044e+01 9.55019299e+01 5.72918831e+01\n",
      " 1.04846677e+02 1.04004359e+02 1.11807943e+02 7.52749151e+01\n",
      " 1.76143623e+02 5.88299048e+01 9.76924171e+01 1.08730019e+00\n",
      " 9.42773174e+01 5.15987929e+01 8.58216256e+01 6.73997255e+00\n",
      " 9.44426367e+01 5.34508485e+01 5.96238049e+01 1.34126239e+01\n",
      " 1.00050677e+02 5.90682849e+01 1.85294728e+01 3.32128485e+01]\n",
      "42-th iteration, loss: 0.08357910564384062, 14 gd steps\n",
      "insert gradient: -2.6917461512119634e-05\n",
      "42-th iteration, new layer inserted. now 28 layers\n",
      "[1.30411489e+00 5.76111907e+01 1.00832544e+02 5.58423298e+01\n",
      " 9.55036820e+01 0.00000000e+00 3.55271368e-15 5.72918083e+01\n",
      " 1.04840341e+02 1.04011357e+02 1.11798733e+02 7.52759740e+01\n",
      " 1.76162443e+02 5.88222288e+01 9.77544786e+01 1.08361415e+00\n",
      " 9.42342556e+01 5.15956092e+01 8.58195646e+01 6.74409659e+00\n",
      " 9.44377587e+01 5.34490923e+01 5.96293291e+01 1.34125810e+01\n",
      " 1.00051991e+02 5.90756177e+01 1.85317360e+01 3.32042731e+01]\n",
      "43-th iteration, loss: 0.08357833854132787, 986 gd steps\n",
      "insert gradient: -1.2144964557576991e-05\n",
      "43-th iteration, new layer inserted. now 26 layers\n",
      "[  1.30531993  57.62143401 100.84282427  55.83802258  95.50702655\n",
      "  57.30125127 104.84464583 104.01982051 111.80391305  75.26354604\n",
      " 176.21549794  58.79336225  98.54012404   1.11410702  93.38743555\n",
      "  51.6073477   85.85726144   6.74563113  94.40363721  53.44273867\n",
      "  59.69446493  13.38315909 100.06516038  59.10412776  18.54352819\n",
      "  33.15944822]\n",
      "44-th iteration, loss: 0.08357811446440751, 51 gd steps\n",
      "insert gradient: -1.4676600809499084e-05\n",
      "44-th iteration, new layer inserted. now 26 layers\n",
      "[  1.3063675   57.62586545 100.84906381  55.83309473  95.50225776\n",
      "  57.30585849 104.85265682 104.01663973 111.81255833  75.25732812\n",
      " 176.22987857  58.78161404  99.45394039   1.13126214  92.43031138\n",
      "  51.62301647  85.9049373    6.74146262  94.37064712  53.43606905\n",
      "  59.73503245  13.36973246 100.06585947  59.1070214   18.56321022\n",
      "  33.13618795]\n",
      "45-th iteration, loss: 0.08357810910785199, 47 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.576699140752855e-06\n",
      "45-th iteration, new layer inserted. now 28 layers\n",
      "[1.30557895e+00 5.76218908e+01 1.00847213e+02 5.58396196e+01\n",
      " 9.55050570e+01 5.73009077e+01 1.04850367e+02 1.04018087e+02\n",
      " 1.11813685e+02 7.52583515e+01 1.76231081e+02 5.87784584e+01\n",
      " 9.94376112e+01 0.00000000e+00 1.77635684e-14 1.13197591e+00\n",
      " 9.24467416e+01 5.16218634e+01 8.59029430e+01 6.74105336e+00\n",
      " 9.43738765e+01 5.34373719e+01 5.97354857e+01 1.33690805e+01\n",
      " 1.00066500e+02 5.91067054e+01 1.85630994e+01 3.31361038e+01]\n",
      "46-th iteration, loss: 0.08357810910028184, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.063877376361477e-06\n",
      "46-th iteration, new layer inserted. now 28 layers\n",
      "[1.30557868e+00 5.76218896e+01 1.00847211e+02 5.58396149e+01\n",
      " 9.55050543e+01 5.73009038e+01 1.04850366e+02 1.04018087e+02\n",
      " 1.11813685e+02 7.52583520e+01 1.76231082e+02 5.87784616e+01\n",
      " 9.94376117e+01 7.42044414e-06 4.40436404e-07 1.13198333e+00\n",
      " 9.24467471e+01 5.16218681e+01 8.59029456e+01 6.74106028e+00\n",
      " 9.43738795e+01 5.34373753e+01 5.97354875e+01 1.33690840e+01\n",
      " 1.00066502e+02 5.91067067e+01 1.85631003e+01 3.31361049e+01]\n",
      "47-th iteration, loss: 0.08357810909368288, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.588446111943495e-06\n",
      "47-th iteration, new layer inserted. now 30 layers\n",
      "[1.30557844e+00 5.76218885e+01 1.00847210e+02 5.58396106e+01\n",
      " 9.55050518e+01 5.73009002e+01 1.04850364e+02 1.04018088e+02\n",
      " 1.11813685e+02 7.52583524e+01 1.76231083e+02 5.87784644e+01\n",
      " 9.94376119e+01 1.43394555e-05 6.47228711e-07 0.00000000e+00\n",
      " 1.32348898e-22 1.13199025e+00 9.24467524e+01 5.16218724e+01\n",
      " 8.59029480e+01 6.74106680e+00 9.43738824e+01 5.34373785e+01\n",
      " 5.97354892e+01 1.33690872e+01 1.00066503e+02 5.91067079e+01\n",
      " 1.85631011e+01 3.31361059e+01]\n",
      "48-th iteration, loss: 0.08357810908700167, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.044275866996637e-06\n",
      "48-th iteration, new layer inserted. now 28 layers\n",
      "[1.30557822e+00 5.76218875e+01 1.00847208e+02 5.58396065e+01\n",
      " 9.55050494e+01 5.73008968e+01 1.04850363e+02 1.04018088e+02\n",
      " 1.11813685e+02 7.52583529e+01 1.76231084e+02 5.87784669e+01\n",
      " 9.94376119e+01 2.07562943e-05 6.25785857e-07 1.13200308e+00\n",
      " 9.24467574e+01 5.16218762e+01 8.59029501e+01 6.74107294e+00\n",
      " 9.43738851e+01 5.34373814e+01 5.97354907e+01 1.33690903e+01\n",
      " 1.00066504e+02 5.91067090e+01 1.85631019e+01 3.31361068e+01]\n",
      "49-th iteration, loss: 0.08357810908207279, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.641577136154449e-06\n",
      "49-th iteration, new layer inserted. now 30 layers\n",
      "[1.30557803e+00 5.76218866e+01 1.00847206e+02 5.58396027e+01\n",
      " 9.55050472e+01 5.73008936e+01 1.04850362e+02 1.04018088e+02\n",
      " 1.11813685e+02 7.52583532e+01 1.76231085e+02 5.87784692e+01\n",
      " 9.94376116e+01 2.66779507e-05 3.78562034e-07 0.00000000e+00\n",
      " 3.97046694e-23 1.13200900e+00 9.24467622e+01 5.16218797e+01\n",
      " 8.59029521e+01 6.74107870e+00 9.43738876e+01 5.34373840e+01\n",
      " 5.97354922e+01 1.33690931e+01 1.00066505e+02 5.91067101e+01\n",
      " 1.85631027e+01 3.31361077e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5272322322663734\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.82044762    0.         1410.46598454]\n",
      "1-th iteration, loss: 0.7533148147060021, 11 gd steps\n",
      "insert gradient: -0.6588940805364314\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.64315757   62.05278155  120.40563283    0.         1290.06035171]\n",
      "2-th iteration, loss: 0.509117293032358, 59 gd steps\n",
      "insert gradient: -0.6241131504590243\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   4.52839687   53.43030277   98.98408885   99.43131751  236.94986052\n",
      "    0.         1053.1104912 ]\n",
      "3-th iteration, loss: 0.4311901973172266, 30 gd steps\n",
      "insert gradient: -0.44111464509509274\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.20883823  59.9194741  100.84233686  94.55220095 207.38163574\n",
      "  44.46769026 601.77742354   0.         451.33306766]\n",
      "4-th iteration, loss: 0.36770584972599885, 13 gd steps\n",
      "insert gradient: -0.5982392986291682\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.84772527  63.94064428 110.28229911  84.00147234 109.46181958\n",
      "   0.         101.64311818  45.97415524 581.72493219  43.30644173\n",
      " 451.33306766]\n",
      "5-th iteration, loss: 0.3257557448696964, 32 gd steps\n",
      "insert gradient: -0.16864975226736478\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.5225213   67.65491581 124.29912589  69.54477756  95.93149913\n",
      "  25.41145849  68.30185067  53.24737748 259.25357523   0.\n",
      " 311.10429028  47.1248156  451.33306766]\n",
      "6-th iteration, loss: 0.2957951036008253, 21 gd steps\n",
      "insert gradient: -0.2771557867617264\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          64.28353127 120.07809348  78.51123234 107.94110891\n",
      "  36.19055582  40.66450157  49.63303288 124.47699698   0.\n",
      "  99.58159758  48.90613493 283.2092553   40.31522872 451.33306766]\n",
      "7-th iteration, loss: 0.28071925611804815, 18 gd steps\n",
      "insert gradient: -0.11141441012065444\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  1.61382344  68.09585543 118.29600994  73.3424325  109.29113742\n",
      "  38.01028659  34.72102981  53.35132863 112.93835432  11.34163913\n",
      "  87.46885912  50.9926529  192.83078601   0.          87.65035728\n",
      "  42.22843114 451.33306766]\n",
      "8-th iteration, loss: 0.234431885450308, 32 gd steps\n",
      "insert gradient: -0.044502985556676485\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  1.57751147  64.66858051 123.28441964  72.59817373 113.37351451\n",
      "  55.46764864  34.68347727  47.87177728  96.70680046  51.20246364\n",
      "  21.43229254  53.5533008  169.30861651  42.84729177  29.16204937\n",
      "  48.56246126 451.33306766]\n",
      "9-th iteration, loss: 0.2133555436972995, 139 gd steps\n",
      "insert gradient: -0.12031361170085487\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.56899412  70.75088618 126.10145308  87.88756891 116.74850533\n",
      "  66.03165723  78.6595102   33.1629936   92.55788187  54.42799684\n",
      "  63.9331146   13.04562949 141.39792498   0.          78.55440277\n",
      "  45.80507892 104.32985035  19.90280466 451.33306766]\n",
      "10-th iteration, loss: 0.17261635118597776, 152 gd steps\n",
      "insert gradient: -0.04324927655723926\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  1.46800111  69.11990596 127.1987272   99.46710228 117.00458421\n",
      "  70.74104423 102.22186352  40.33766949  73.76937511  46.19852354\n",
      " 198.49249031  51.35322876  54.24117555  47.64850194 122.77231711\n",
      "  37.36124497 354.61883887   0.          96.71422878]\n",
      "11-th iteration, loss: 0.16203143987037427, 24 gd steps\n",
      "insert gradient: -0.032354308761861746\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  0.89675452  68.61090222 123.65317898 102.49472209 111.5541597\n",
      "  69.47244426 104.772526    44.19835589  64.15019752  46.7108168\n",
      " 189.0257785   56.70027071  68.96028009  30.7006659  121.98099991\n",
      "  49.58130893 263.13413117   0.          87.71137706  21.12585396\n",
      "  96.71422878]\n",
      "12-th iteration, loss: 0.1498205273963379, 60 gd steps\n",
      "insert gradient: -0.06453511911932637\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  1.56425704  69.32022275 120.14704367 104.27801198 109.80582487\n",
      "  67.07485934 106.77874293  49.03252787  57.37964762  45.24373881\n",
      " 119.10772473   0.          68.06155699  51.02641049  96.82237109\n",
      "  30.0742294   70.3783429   78.39220315 191.66017603  31.72556632\n",
      "  43.42376064  54.16212894  96.71422878]\n",
      "13-th iteration, loss: 0.1407428172447303, 43 gd steps\n",
      "insert gradient: -0.018077431179346695\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  0.92500051  67.92987787 121.35949746 100.49957169 118.17833293\n",
      "  67.44902595 111.07731735  50.84013781  61.47713282  48.1060476\n",
      " 103.71184541  20.78098842  33.87329707  54.59960411  96.78218618\n",
      "  29.42237881  65.51099217  79.75469436 190.83572987  38.91541057\n",
      "  33.66572661  55.55715203  96.71422878]\n",
      "14-th iteration, loss: 0.13395051623440662, 62 gd steps\n",
      "insert gradient: -0.028723490645776232\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  0.55507864  70.86779958 124.66211502 102.18929556 118.51769805\n",
      "  67.37359524 115.06905002  54.440588    69.91892096  44.7301612\n",
      "  92.94787487  49.19899392   9.00716579  54.83534096  90.92910735\n",
      "  36.41130622  50.34308965  83.7389045  136.34050476   0.\n",
      "  58.4316449   46.33120239  33.78291698  51.03071409  96.71422878]\n",
      "15-th iteration, loss: 0.12980931171746316, 31 gd steps\n",
      "insert gradient: -0.00822811036969596\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[7.59143820e-01 7.29173214e+01 1.25838044e+02 1.02010556e+02\n",
      " 1.19919754e+02 6.79126427e+01 1.16254573e+02 5.55503329e+01\n",
      " 7.12273727e+01 4.46765392e+01 9.39798020e+01 5.27319710e+01\n",
      " 8.98304988e+00 5.22986858e+01 9.17375617e+01 3.91165482e+01\n",
      " 5.13472045e+01 7.69515031e+01 1.29413565e+02 0.00000000e+00\n",
      " 3.19744231e-14 1.39578879e+01 4.76576961e+01 5.13149020e+01\n",
      " 3.40631637e+01 4.45876691e+01 9.67142288e+01]\n",
      "16-th iteration, loss: 0.11777748312582939, 116 gd steps\n",
      "insert gradient: -0.020613156959223663\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[  1.54251245  76.02679414 132.12977515 101.75551611 127.06351326\n",
      "  71.44796921 117.76312639  62.19000744  85.54598531  44.2961103\n",
      "  93.72723963  55.18090988  59.05246009  18.1884348   98.09838214\n",
      "  52.09491356  59.05123904  38.68524622 155.1372807   63.55045714\n",
      "  19.45420927  45.01959065  57.78875463  24.86558632  96.71422878]\n",
      "17-th iteration, loss: 0.1075966849966162, 37 gd steps\n",
      "insert gradient: -0.02411579285844512\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          74.56993083 136.59805773 105.51485317 128.6113707\n",
      "  71.44948175 126.39732416  66.17885748  96.52717604  45.00448311\n",
      "  86.83508859  54.83382207  94.01242677  14.19112746  80.16924722\n",
      "  52.95857399  83.95325315  24.6705664  101.85166909   0.\n",
      "  50.92583454  66.47052906  59.17361392  31.11808218  60.40659546\n",
      "  29.96506543  96.71422878]\n",
      "18-th iteration, loss: 0.1007283687859882, 24 gd steps\n",
      "insert gradient: -0.013368186012850563\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          75.6722665  138.2891551  106.64120956 129.8899032\n",
      "  70.39437819 124.83908812  71.71249608 101.29542629  45.53413854\n",
      "  84.50849758  55.92771139 100.34797781  18.52226396  65.56595414\n",
      "  55.95832791  94.40077552  29.09438848  86.16352203  14.09688652\n",
      "  31.53487291  57.131327    71.54632663  32.4897378   53.62297112\n",
      "  35.79058956  96.71422878]\n",
      "19-th iteration, loss: 0.08782295547835083, 28 gd steps\n",
      "insert gradient: -0.007302055731258231\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[1.94031628e+00 7.57655301e+01 1.42360097e+02 1.02890156e+02\n",
      " 1.36589765e+02 7.21216429e+01 1.21602688e+02 7.24473915e+01\n",
      " 1.10360745e+02 5.02968309e+01 8.44024706e+01 5.28688304e+01\n",
      " 1.05880797e+02 4.19112074e+01 3.70764150e+01 4.88776291e+01\n",
      " 1.00042824e+02 4.23366686e+01 6.16137229e+01 4.20580470e+01\n",
      " 1.33874741e+01 3.91561369e+01 8.77070293e+01 4.59583990e+01\n",
      " 3.37035055e+01 3.58978813e+01 9.67142288e+01 0.00000000e+00\n",
      " 1.06581410e-14]\n",
      "20-th iteration, loss: 0.06923675171261386, 53 gd steps\n",
      "insert gradient: -0.010525833989527682\n",
      "20-th iteration, new layer inserted. now 29 layers\n",
      "[3.21726268e-01 6.96666335e+01 1.38651878e+02 8.13643886e+01\n",
      " 1.82898825e+02 7.49789533e+01 1.23932508e+02 7.82146625e+01\n",
      " 1.22291504e+02 5.72016767e+01 8.61316525e+01 5.11282809e+01\n",
      " 1.00921379e+02 6.09919604e+01 7.30053447e+01 1.89068418e+01\n",
      " 9.40129482e+01 5.40093060e+01 7.77918706e+01 3.41237323e+01\n",
      " 5.84213966e+01 9.75481368e+00 8.35470383e+01 5.33186804e+01\n",
      " 9.03768151e+01 2.09610253e+01 7.30637074e+01 3.31066469e+01\n",
      " 3.73124202e-14]\n",
      "21-th iteration, loss: 0.050001833282761726, 143 gd steps\n",
      "insert gradient: -0.002114856721230452\n",
      "21-th iteration, new layer inserted. now 29 layers\n",
      "[4.73852718e-01 8.08239988e+01 1.33785352e+02 8.49953108e+01\n",
      " 1.74472800e+02 8.27970540e+01 1.20504893e+02 7.87282145e+01\n",
      " 1.23897320e+02 7.62280590e+01 9.95385848e+01 4.57411407e+01\n",
      " 9.46245898e+01 6.61389346e+01 1.03780817e+02 4.64201376e+01\n",
      " 6.96763546e+01 3.82925886e+01 8.79603529e+01 4.31203486e+01\n",
      " 6.51877811e+01 5.16862300e+01 4.05534548e+01 1.17583694e+01\n",
      " 9.98430338e+01 6.75530655e+01 7.68062723e+01 2.71699464e+01\n",
      " 2.93618154e-13]\n",
      "22-th iteration, loss: 0.04980347222477621, 16 gd steps\n",
      "insert gradient: -0.0008899433259469871\n",
      "22-th iteration, new layer inserted. now 29 layers\n",
      "[5.32057147e-01 8.15356896e+01 1.33477975e+02 8.49505618e+01\n",
      " 1.73996460e+02 8.28679679e+01 1.20396308e+02 7.85091304e+01\n",
      " 1.23693227e+02 7.62523789e+01 9.94175600e+01 4.50683293e+01\n",
      " 9.44243006e+01 6.59765221e+01 1.03951101e+02 4.74220288e+01\n",
      " 6.98004411e+01 3.78745215e+01 8.76503589e+01 4.30245467e+01\n",
      " 6.53129674e+01 5.18394771e+01 4.22409313e+01 1.02718861e+01\n",
      " 1.00433611e+02 6.81520687e+01 7.63286091e+01 2.76924079e+01\n",
      " 2.97613496e-13]\n",
      "23-th iteration, loss: 0.0489845721374633, 25 gd steps\n",
      "insert gradient: -0.001970205173798227\n",
      "23-th iteration, new layer inserted. now 29 layers\n",
      "[5.48480243e-02 8.26681380e+01 1.27494807e+02 8.60720978e+01\n",
      " 1.72667502e+02 8.40082207e+01 1.20470282e+02 7.95163847e+01\n",
      " 1.20093339e+02 7.72905369e+01 1.00471115e+02 4.47881079e+01\n",
      " 9.37485890e+01 6.68096124e+01 1.03943715e+02 4.72584188e+01\n",
      " 7.17505435e+01 3.69483163e+01 8.68799373e+01 4.51447545e+01\n",
      " 6.58091036e+01 4.81304476e+01 1.03431338e+02 0.00000000e+00\n",
      " 6.20588026e+01 7.25959367e+01 6.73043247e+01 3.11218878e+01\n",
      " 3.03049523e-13]\n",
      "24-th iteration, loss: 0.0488946143216357, 76 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.40086142e+01 1.27392868e+02 8.48811279e+01\n",
      " 1.73320689e+02 8.31763983e+01 1.21136124e+02 7.98810456e+01\n",
      " 1.20762047e+02 7.56724744e+01 1.00213518e+02 4.49713600e+01\n",
      " 9.36234433e+01 6.65432230e+01 1.04526638e+02 4.75468824e+01\n",
      " 7.10004730e+01 3.73432935e+01 8.69156589e+01 4.38034066e+01\n",
      " 6.77250606e+01 4.66835699e+01 1.04668477e+02 5.27015005e-01\n",
      " 1.56136113e+01 0.00000000e+00 4.68408340e+01 7.37468510e+01\n",
      " 6.20873100e+01 3.29261999e+01 3.51033195e-13]\n",
      "25-th iteration, loss: 0.04888975561186436, 140 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "25-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.40484979e+01 1.27339475e+02 8.49037500e+01\n",
      " 1.73271544e+02 8.31713295e+01 1.21121032e+02 7.99030828e+01\n",
      " 1.20707794e+02 7.57028311e+01 1.00194285e+02 4.49401239e+01\n",
      " 9.36285325e+01 6.65891612e+01 1.04550506e+02 4.75472276e+01\n",
      " 7.10305882e+01 3.73544490e+01 8.68819725e+01 4.38269424e+01\n",
      " 6.76661368e+01 4.67235540e+01 1.20137939e+02 7.83409534e-01\n",
      " 5.83984272e+00 0.00000000e+00 4.08788991e+01 7.36861181e+01\n",
      " 6.19209959e+01 3.30128599e+01 3.43958915e-13]\n",
      "26-th iteration, loss: 0.04810562227345835, 90 gd steps\n",
      "insert gradient: -0.0024525356278494543\n",
      "26-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.74886887e+01 1.18256554e+02 8.92377343e+01\n",
      " 1.71388673e+02 8.36420484e+01 1.26120064e+02 8.09806706e+01\n",
      " 1.13257004e+02 7.67569225e+01 1.03634045e+02 4.42729235e+01\n",
      " 9.14660771e+01 7.05441762e+01 1.06446396e+02 4.72952187e+01\n",
      " 7.28732199e+01 3.65401053e+01 8.77038801e+01 4.42805080e+01\n",
      " 7.48468593e+01 4.84215097e+01 1.12451767e+02 2.57202117e+01\n",
      " 1.64769869e+01 6.67355347e+01 7.28468666e+01 2.61120134e+01\n",
      " 1.39985818e-13]\n",
      "27-th iteration, loss: 0.04759841052705162, 54 gd steps\n",
      "insert gradient: -0.0009806733220076244\n",
      "27-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.75364388e+01 1.17285940e+02 8.98541572e+01\n",
      " 1.71537561e+02 8.55393987e+01 1.23244715e+02 8.22728699e+01\n",
      " 1.13140180e+02 7.76030638e+01 1.03017370e+02 4.47991383e+01\n",
      " 9.07531496e+01 7.29096599e+01 1.03891457e+02 4.78060703e+01\n",
      " 7.52656250e+01 3.44624139e+01 8.86405903e+01 4.60364624e+01\n",
      " 8.00311174e+01 4.57172489e+01 1.00333914e+02 3.87462408e+01\n",
      " 7.13223060e+00 6.36945663e+01 8.70789940e+01 2.00416678e+01\n",
      " 2.44306080e-13]\n",
      "28-th iteration, loss: 0.04754380409482073, 23 gd steps\n",
      "insert gradient: -0.00026925178319871864\n",
      "28-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.69973834e+01 1.17300107e+02 9.03161004e+01\n",
      " 1.72006908e+02 8.52064748e+01 1.22901055e+02 8.24272046e+01\n",
      " 1.13463800e+02 7.78058555e+01 1.02941912e+02 4.42960754e+01\n",
      " 9.08626932e+01 7.31489689e+01 1.03993222e+02 4.77401082e+01\n",
      " 7.52959431e+01 3.44756891e+01 8.87497779e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.64291674e+01 8.00808328e+01 4.55436163e+01\n",
      " 1.00075744e+02 3.90550231e+01 7.12723886e+00 6.35828512e+01\n",
      " 8.75919549e+01 2.04426261e+01 2.41155003e-13]\n",
      "29-th iteration, loss: 0.04748562026047493, 48 gd steps\n",
      "insert gradient: -8.3872361489667e-05\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.71716450e+01 1.16794206e+02 9.06180155e+01\n",
      " 1.73173576e+02 8.51676227e+01 1.22312675e+02 8.27078131e+01\n",
      " 1.13391323e+02 7.83661692e+01 1.02766210e+02 4.43066552e+01\n",
      " 9.08932267e+01 7.35815079e+01 1.03543096e+02 4.76787190e+01\n",
      " 7.60425096e+01 3.39416471e+01 8.89482178e+01 4.75548785e+01\n",
      " 8.10422874e+01 4.50658130e+01 9.74499318e+01 4.18396573e+01\n",
      " 6.66949257e+00 6.09767459e+01 9.27728601e+01 1.96951722e+01\n",
      " 1.94731781e-13]\n",
      "30-th iteration, loss: 0.047461273933322404, 999 gd steps\n",
      "insert gradient: -3.131094526385991e-05\n",
      "30-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73258974e+01 1.16469145e+02 9.08206157e+01\n",
      " 1.73358873e+02 8.54270942e+01 1.21920942e+02 8.29649054e+01\n",
      " 1.13074761e+02 7.85211769e+01 1.02838447e+02 4.43511984e+01\n",
      " 9.08635994e+01 7.38060953e+01 1.03225918e+02 4.78979216e+01\n",
      " 7.62713637e+01 3.35432035e+01 8.94139528e+01 4.77546843e+01\n",
      " 8.21184725e+01 4.46909225e+01 9.61252334e+01 4.36583861e+01\n",
      " 7.50986070e+00 5.76723362e+01 9.85886472e+01 1.85294819e+01\n",
      " 2.39362236e-13]\n",
      "31-th iteration, loss: 0.047461268951810964, 214 gd steps\n",
      "insert gradient: -3.127790153087832e-05\n",
      "31-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73259816e+01 1.16468955e+02 9.08207246e+01\n",
      " 1.73359026e+02 8.54271204e+01 1.21920889e+02 8.29650273e+01\n",
      " 1.13074627e+02 7.85212767e+01 1.02838387e+02 4.43512428e+01\n",
      " 9.08636319e+01 7.38061864e+01 1.03225760e+02 4.78979766e+01\n",
      " 7.62716331e+01 3.35429746e+01 8.94141362e+01 4.77548682e+01\n",
      " 8.21188521e+01 4.46907315e+01 9.61247977e+01 4.36583542e+01\n",
      " 7.51055371e+00 5.76713350e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.85915990e+01 1.85290088e+01 2.39429375e-13]\n",
      "32-th iteration, loss: 0.047460897362286536, 999 gd steps\n",
      "insert gradient: -2.5424440467029603e-05\n",
      "32-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73268711e+01 1.16459613e+02 9.08248651e+01\n",
      " 1.73366500e+02 8.54289341e+01 1.21918889e+02 8.29722838e+01\n",
      " 1.13067586e+02 7.85257062e+01 1.02835685e+02 4.43524747e+01\n",
      " 9.08639221e+01 7.38108406e+01 1.03217802e+02 4.79010839e+01\n",
      " 7.62837229e+01 3.35306261e+01 8.94244529e+01 4.77684571e+01\n",
      " 8.21369903e+01 4.46752839e+01 9.60974696e+01 4.36489999e+01\n",
      " 7.54550069e+00 5.76131033e+01 9.88424392e+01 1.84929875e+01\n",
      " 2.38560526e-13]\n",
      "33-th iteration, loss: 0.0474608957438636, 79 gd steps\n",
      "insert gradient: -2.5523284945693457e-05\n",
      "33-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73268854e+01 1.16459545e+02 9.08249052e+01\n",
      " 1.73366563e+02 8.54289536e+01 1.21918867e+02 8.29723364e+01\n",
      " 1.13067527e+02 7.85257472e+01 1.02835675e+02 4.43524991e+01\n",
      " 9.08639245e+01 7.38108743e+01 1.03217740e+02 4.79011215e+01\n",
      " 7.62838136e+01 3.35305246e+01 8.94245327e+01 4.77685757e+01\n",
      " 8.21371388e+01 4.46751760e+01 9.60972542e+01 4.36489315e+01\n",
      " 7.54579836e+00 5.76126437e+01 9.88433340e+01 1.84926738e+01\n",
      " 2.38540252e-13]\n",
      "34-th iteration, loss: 0.047460894171420916, 77 gd steps\n",
      "insert gradient: -2.5595385002062798e-05\n",
      "34-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73269017e+01 1.16459480e+02 9.08249462e+01\n",
      " 1.73366624e+02 8.54289744e+01 1.21918846e+02 8.29723890e+01\n",
      " 1.13067472e+02 7.85257914e+01 1.02835668e+02 4.43525300e+01\n",
      " 9.08639296e+01 7.38109084e+01 1.03217683e+02 4.79011634e+01\n",
      " 7.62839033e+01 3.35304266e+01 8.94246107e+01 4.77686931e+01\n",
      " 8.21372864e+01 4.46750797e+01 9.60970496e+01 4.36488723e+01\n",
      " 7.54609143e+00 5.76122029e+01 9.88442102e+01 1.84923713e+01\n",
      " 2.38531897e-13]\n",
      "35-th iteration, loss: 0.04746089263003509, 76 gd steps\n",
      "insert gradient: -2.5654254439259777e-05\n",
      "35-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73269202e+01 1.16459416e+02 9.08249884e+01\n",
      " 1.73366685e+02 8.54289959e+01 1.21918825e+02 8.29724410e+01\n",
      " 1.13067419e+02 7.85258367e+01 1.02835663e+02 4.43525627e+01\n",
      " 9.08639351e+01 7.38109411e+01 1.03217627e+02 4.79012058e+01\n",
      " 7.62839915e+01 3.35303297e+01 8.94246869e+01 4.77688088e+01\n",
      " 8.21374331e+01 4.46749912e+01 9.60968524e+01 4.36488197e+01\n",
      " 7.54638160e+00 5.76117750e+01 9.88450743e+01 1.84920769e+01\n",
      " 2.38506897e-13]\n",
      "36-th iteration, loss: 0.047460891115730784, 75 gd steps\n",
      "insert gradient: -2.570507750071359e-05\n",
      "36-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73269404e+01 1.16459355e+02 9.08250315e+01\n",
      " 1.73366745e+02 8.54290177e+01 1.21918805e+02 8.29724917e+01\n",
      " 1.13067367e+02 7.85258821e+01 1.02835659e+02 4.43525953e+01\n",
      " 9.08639404e+01 7.38109720e+01 1.03217572e+02 4.79012475e+01\n",
      " 7.62840779e+01 3.35302332e+01 8.94247611e+01 4.77689225e+01\n",
      " 8.21375788e+01 4.46749086e+01 9.60966613e+01 4.36487723e+01\n",
      " 7.54666899e+00 5.76113579e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.88459276e+01 1.84917897e+01 2.38474299e-13]\n",
      "37-th iteration, loss: 0.04746081999793831, 999 gd steps\n",
      "insert gradient: -2.473607966190275e-05\n",
      "37-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73274748e+01 1.16457457e+02 9.08262225e+01\n",
      " 1.73368485e+02 8.54295686e+01 1.21918073e+02 8.29738801e+01\n",
      " 1.13065631e+02 7.85269612e+01 1.02835321e+02 4.43531390e+01\n",
      " 9.08639353e+01 7.38118091e+01 1.03215753e+02 4.79021534e+01\n",
      " 7.62865280e+01 3.35272992e+01 8.94269304e+01 4.77721262e+01\n",
      " 8.21417489e+01 4.46720055e+01 9.60906671e+01 4.36469779e+01\n",
      " 7.55513536e+00 5.75984929e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.88958626e+01 1.84830918e+01 2.38420391e-13]\n",
      "38-th iteration, loss: 0.04746075628423303, 999 gd steps\n",
      "insert gradient: -2.4077650700750484e-05\n",
      "38-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73279804e+01 1.16455696e+02 9.08273087e+01\n",
      " 1.73370057e+02 8.54301187e+01 1.21917347e+02 8.29752184e+01\n",
      " 1.13064002e+02 7.85279587e+01 1.02835045e+02 4.43536526e+01\n",
      " 9.08639351e+01 7.38126299e+01 1.03214066e+02 4.79030274e+01\n",
      " 7.62887671e+01 3.35246198e+01 8.94289670e+01 4.77751105e+01\n",
      " 8.21456215e+01 4.46693063e+01 9.60850263e+01 4.36452524e+01\n",
      " 7.56309603e+00 5.75863608e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.89411229e+01 1.84749048e+01 2.38239205e-13]\n",
      "39-th iteration, loss: 0.04746070424173583, 999 gd steps\n",
      "insert gradient: -2.356524677035885e-05\n",
      "39-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73284424e+01 1.16454221e+02 9.08282422e+01\n",
      " 1.73371363e+02 8.54306134e+01 1.21916688e+02 8.29763458e+01\n",
      " 1.13062612e+02 7.85287969e+01 1.02834841e+02 4.43541005e+01\n",
      " 9.08639373e+01 7.38133201e+01 1.03212637e+02 4.79037799e+01\n",
      " 7.62906179e+01 3.35223698e+01 8.94306760e+01 4.77776058e+01\n",
      " 8.21488952e+01 4.46671102e+01 9.60802830e+01 4.36438482e+01\n",
      " 7.56989838e+00 5.75761007e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.89785670e+01 1.84680099e+01 2.38184868e-13]\n",
      "40-th iteration, loss: 0.04746066398685335, 999 gd steps\n",
      "insert gradient: -2.317807428613981e-05\n",
      "40-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73288313e+01 1.16453059e+02 9.08289922e+01\n",
      " 1.73372384e+02 8.54310237e+01 1.21916136e+02 8.29772372e+01\n",
      " 1.13061499e+02 7.85294566e+01 1.02834695e+02 4.43544625e+01\n",
      " 9.08639418e+01 7.38138673e+01 1.03211500e+02 4.79043847e+01\n",
      " 7.62920627e+01 3.35205954e+01 8.94320262e+01 4.77795618e+01\n",
      " 8.21514930e+01 4.46654182e+01 9.60765333e+01 4.36427715e+01\n",
      " 7.57534436e+00 5.75679472e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.90078539e+01 1.84625661e+01 2.38036287e-13]\n",
      "41-th iteration, loss: 0.04746063278979754, 999 gd steps\n",
      "insert gradient: -2.2881956377810574e-05\n",
      "41-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73291511e+01 1.16452145e+02 9.08295894e+01\n",
      " 1.73373180e+02 8.54313583e+01 1.21915683e+02 8.29779405e+01\n",
      " 1.13060615e+02 7.85299739e+01 1.02834589e+02 4.43547510e+01\n",
      " 9.08639476e+01 7.38143003e+01 1.03210600e+02 4.79048664e+01\n",
      " 7.62931901e+01 3.35192014e+01 8.94330902e+01 4.77810905e+01\n",
      " 8.21535446e+01 4.46641108e+01 9.60735796e+01 4.36419445e+01\n",
      " 7.57967502e+00 5.75614952e+01 9.90307588e+01 1.84582867e+01\n",
      " 2.37859634e-13]\n",
      "42-th iteration, loss: 0.047460632074496155, 40 gd steps\n",
      "insert gradient: -2.2931212837468185e-05\n",
      "42-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73291630e+01 1.16452112e+02 9.08296111e+01\n",
      " 1.73373208e+02 8.54313707e+01 1.21915666e+02 8.29779661e+01\n",
      " 1.13060584e+02 7.85299932e+01 1.02834586e+02 4.43547631e+01\n",
      " 9.08639486e+01 7.38143164e+01 1.03210568e+02 4.79048850e+01\n",
      " 7.62932307e+01 3.35191518e+01 8.94331285e+01 4.77811455e+01\n",
      " 8.21536187e+01 4.46640657e+01 9.60734745e+01 4.36419163e+01\n",
      " 7.57983134e+00 5.75612644e+01 0.00000000e+00 1.06581410e-14\n",
      " 9.90311689e+01 1.84581338e+01 2.37858455e-13]\n",
      "43-th iteration, loss: 0.04746060832668978, 839 gd steps\n",
      "insert gradient: -2.2671630840536337e-05\n",
      "43-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73294248e+01 1.16451410e+02 9.08300803e+01\n",
      " 1.73373818e+02 8.54316370e+01 1.21915304e+02 8.29785062e+01\n",
      " 1.13059899e+02 7.85303932e+01 1.02834512e+02 4.43549919e+01\n",
      " 9.08639558e+01 7.38146480e+01 1.03209872e+02 4.79052607e+01\n",
      " 7.62940925e+01 3.35180781e+01 8.94339463e+01 4.77823125e+01\n",
      " 8.21552063e+01 4.46630912e+01 9.60712113e+01 4.36413102e+01\n",
      " 7.58319775e+00 5.75562919e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.90487573e+01 1.84548573e+01 2.37763750e-13]\n",
      "44-th iteration, loss: 0.04746058939761565, 687 gd steps\n",
      "insert gradient: -2.2488717772473624e-05\n",
      "44-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73296356e+01 1.16450844e+02 9.08304559e+01\n",
      " 1.73374304e+02 8.54318533e+01 1.21915006e+02 8.29789414e+01\n",
      " 1.13059344e+02 7.85307101e+01 1.02834452e+02 4.43551723e+01\n",
      " 9.08639618e+01 7.38149180e+01 1.03209311e+02 4.79055622e+01\n",
      " 7.62947820e+01 3.35172180e+01 8.94346060e+01 4.77832442e+01\n",
      " 8.21564815e+01 4.46623040e+01 9.60693831e+01 4.36408202e+01\n",
      " 7.58591619e+00 5.75522683e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.90628515e+01 1.84522219e+01 2.37723245e-13]\n",
      "45-th iteration, loss: 0.0474605735015708, 588 gd steps\n",
      "insert gradient: -2.2337781099312458e-05\n",
      "45-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73298160e+01 1.16450365e+02 9.08307742e+01\n",
      " 1.73374712e+02 8.54320394e+01 1.21914750e+02 8.29793117e+01\n",
      " 1.13058874e+02 7.85309776e+01 1.02834404e+02 4.43553252e+01\n",
      " 9.08639677e+01 7.38151480e+01 1.03208835e+02 4.79058188e+01\n",
      " 7.62953633e+01 3.35164924e+01 8.94351656e+01 4.77840298e+01\n",
      " 8.21575613e+01 4.46616421e+01 9.60678348e+01 4.36404092e+01\n",
      " 7.58822557e+00 5.75488525e+01 9.90747434e+01 1.84499943e+01\n",
      " 2.37613615e-13]\n",
      "46-th iteration, loss: 0.04746057290578965, 34 gd steps\n",
      "insert gradient: -2.2379993097251285e-05\n",
      "46-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 8.73298267e+01 1.16450338e+02 9.08307929e+01\n",
      " 1.73374736e+02 8.54320505e+01 1.21914736e+02 8.29793336e+01\n",
      " 1.13058847e+02 7.85309938e+01 1.02834401e+02 4.43553353e+01\n",
      " 9.08639686e+01 7.38151618e+01 1.03208808e+02 4.79058347e+01\n",
      " 7.62953973e+01 3.35164505e+01 8.94351983e+01 4.77840757e+01\n",
      " 8.21576245e+01 4.46616047e+01 9.60677453e+01 4.36403862e+01\n",
      " 7.58836044e+00 5.75486543e+01 9.90750893e+01 1.84498651e+01\n",
      " 2.37615253e-13]\n",
      "47-th iteration, loss: 0.047460572314621335, 34 gd steps\n",
      "insert gradient: -2.2416051736632658e-05\n",
      "47-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73298377e+01 1.16450310e+02 9.08308116e+01\n",
      " 1.73374759e+02 8.54320619e+01 1.21914721e+02 8.29793557e+01\n",
      " 1.13058820e+02 7.85310109e+01 1.02834400e+02 4.43553474e+01\n",
      " 9.08639703e+01 7.38151762e+01 1.03208782e+02 4.79058520e+01\n",
      " 7.62954316e+01 3.35164094e+01 8.94352309e+01 4.77841219e+01\n",
      " 8.21576879e+01 4.46615694e+01 9.60676575e+01 4.36403649e+01\n",
      " 7.58849504e+00 5.75484588e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.90754339e+01 1.84497374e+01 2.37609446e-13]\n",
      "48-th iteration, loss: 0.04746055853523857, 517 gd steps\n",
      "insert gradient: -2.2238886336974656e-05\n",
      "48-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73300062e+01 1.16449895e+02 9.08310974e+01\n",
      " 1.73375116e+02 8.54322294e+01 1.21914494e+02 8.29796779e+01\n",
      " 1.13058411e+02 7.85312488e+01 1.02834363e+02 4.43554879e+01\n",
      " 9.08639777e+01 7.38153735e+01 1.03208367e+02 4.79060799e+01\n",
      " 7.62959360e+01 3.35157751e+01 8.94357169e+01 4.77848031e+01\n",
      " 8.21586362e+01 4.46610197e+01 9.60663227e+01 4.36400331e+01\n",
      " 7.59052428e+00 5.75454941e+01 0.00000000e+00 8.88178420e-15\n",
      " 9.90858134e+01 1.84478121e+01 2.37556973e-13]\n",
      "49-th iteration, loss: 0.047460546390705585, 462 gd steps\n",
      "insert gradient: -2.211443933476014e-05\n",
      "49-th iteration, new layer inserted. now 31 layers\n",
      "[0.00000000e+00 8.73301524e+01 1.16449526e+02 9.08313470e+01\n",
      " 1.73375428e+02 8.54323764e+01 1.21914289e+02 8.29799617e+01\n",
      " 1.13058044e+02 7.85314535e+01 1.02834327e+02 4.43556058e+01\n",
      " 9.08639829e+01 7.38155500e+01 1.03207997e+02 4.79062775e+01\n",
      " 7.62963809e+01 3.35152145e+01 8.94361484e+01 4.77854013e+01\n",
      " 8.21594725e+01 4.46605214e+01 9.60651310e+01 4.36397302e+01\n",
      " 7.59232318e+00 5.75428490e+01 0.00000000e+00 5.32907052e-15\n",
      " 9.90949877e+01 1.84461040e+01 2.37479491e-13]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.526591713269497\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.67423463    0.         1440.73782567]\n",
      "1-th iteration, loss: 0.7436554430853707, 11 gd steps\n",
      "insert gradient: -0.6190377688545543\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  40.92549008   62.34185727  228.40965529    0.         1212.32817038]\n",
      "2-th iteration, loss: 0.5272095421162547, 37 gd steps\n",
      "insert gradient: -0.3818100379323004\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[8.51639248e-01 5.34843789e+01 1.73269273e+02 5.43433816e+01\n",
      " 2.96896695e+02 0.00000000e+00 9.15431476e+02]\n",
      "3-th iteration, loss: 0.4557322095240005, 13 gd steps\n",
      "insert gradient: -0.5353111946660188\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.18209612  62.82199383 164.23549248  50.66181293 278.53511583\n",
      "  43.51842699 130.77592509   0.         784.65555051]\n",
      "4-th iteration, loss: 0.3704936247171973, 70 gd steps\n",
      "insert gradient: -0.4810233379667217\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.4331826   50.98830832 183.69183587  50.21056033 234.45641888\n",
      "  65.24417544 100.74545311  63.5530298  639.34896708   0.\n",
      " 145.30658343]\n",
      "5-th iteration, loss: 0.2909576979433926, 14 gd steps\n",
      "insert gradient: -0.43834852017158\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          44.68825166 201.84602811  46.18994776 104.10695594\n",
      "   0.         124.92834713  68.3432308  107.70369065  63.27106547\n",
      " 585.24285662  84.06235848 145.30658343]\n",
      "6-th iteration, loss: 0.2496060947367972, 68 gd steps\n",
      "insert gradient: -0.18451236141742727\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  3.0234614   53.48454419 192.75614084  52.03972005  59.24472434\n",
      "  26.00253419 100.43743244  68.04885691 108.10787048  64.59143655\n",
      " 421.04164635   0.         161.93909475  68.95046703 145.30658343]\n",
      "7-th iteration, loss: 0.22475229395424307, 62 gd steps\n",
      "insert gradient: -0.04049731150132568\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[1.82571871e+00 5.63027566e+01 1.82305798e+02 5.51002823e+01\n",
      " 6.61147669e+01 2.58036792e+01 9.85450511e+01 6.66191155e+01\n",
      " 1.12187235e+02 6.62930051e+01 3.93479839e+02 3.53327744e+01\n",
      " 1.23538927e+02 7.05942467e+01 1.45306583e+02 0.00000000e+00\n",
      " 1.77635684e-14]\n",
      "8-th iteration, loss: 0.21796348769091467, 27 gd steps\n",
      "insert gradient: -0.09995674159388286\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[2.88722996e+00 5.51657270e+01 1.83836577e+02 5.30347261e+01\n",
      " 7.67721770e+01 2.29149795e+01 9.50910238e+01 6.91676177e+01\n",
      " 1.09863063e+02 6.94609855e+01 2.78512452e+02 0.00000000e+00\n",
      " 1.11404981e+02 3.36607248e+01 1.28085493e+02 6.57358063e+01\n",
      " 1.53289679e+02 1.98430791e+01 3.29106656e-14]\n",
      "9-th iteration, loss: 0.21267727970011302, 18 gd steps\n",
      "insert gradient: -0.08196318948194418\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[2.29532855e+00 5.74720230e+01 1.84739772e+02 0.00000000e+00\n",
      " 2.48689958e-14 5.08767565e+01 7.87601154e+01 2.54938011e+01\n",
      " 8.70146034e+01 7.02123018e+01 1.10784572e+02 7.11856912e+01\n",
      " 2.69954832e+02 5.90242190e+00 1.02677118e+02 3.67081050e+01\n",
      " 1.34440566e+02 5.92178216e+01 1.44643481e+02 3.55620397e+01\n",
      " 3.93575534e-14]\n",
      "10-th iteration, loss: 0.21078091555492248, 11 gd steps\n",
      "insert gradient: -0.09025867333347816\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[1.68293247e+00 5.48534119e+01 1.85010377e+02 5.47127314e+01\n",
      " 7.88905190e+01 2.46504033e+01 8.74024450e+01 7.06226337e+01\n",
      " 1.11220671e+02 7.12377128e+01 2.02150311e+02 0.00000000e+00\n",
      " 6.73834371e+01 6.62664252e+00 1.02582531e+02 3.65537631e+01\n",
      " 1.34406169e+02 5.90463354e+01 1.43946600e+02 3.73336590e+01\n",
      " 3.95898392e-14]\n",
      "11-th iteration, loss: 0.13596699272599383, 40 gd steps\n",
      "insert gradient: -0.09598021988327551\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[2.04484939e+00 6.12574298e+01 1.02778376e+02 0.00000000e+00\n",
      " 1.02778376e+02 5.70537064e+01 8.48560791e+01 2.51440221e+01\n",
      " 7.59998116e+01 5.57661644e+01 1.42498845e+02 6.39409626e+01\n",
      " 1.88958236e+02 8.25333548e+01 1.02829472e+02 5.16565146e+01\n",
      " 1.26891980e+02 5.55504943e+01 1.02911215e+02 7.36689813e+01\n",
      " 1.19213624e-13]\n",
      "12-th iteration, loss: 0.12469974499040319, 40 gd steps\n",
      "insert gradient: -0.023066585239849818\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[3.76241586e+00 6.81941126e+01 9.97183381e+01 1.06543457e+01\n",
      " 9.89254695e+01 5.06335656e+01 9.40559136e+01 2.61541304e+01\n",
      " 6.27517059e+01 5.33476372e+01 1.62292933e+02 5.80045264e+01\n",
      " 5.18401111e+01 0.00000000e+00 1.38240296e+02 7.56687016e+01\n",
      " 1.08981758e+02 5.91672025e+01 1.11010308e+02 5.83762388e+01\n",
      " 1.04031989e+02 6.72365704e+01 1.62900081e-13]\n",
      "13-th iteration, loss: 0.10950061213927528, 26 gd steps\n",
      "insert gradient: -0.03097715292003799\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[3.96351342e-01 6.42953084e+01 9.26037726e+01 1.10549439e+01\n",
      " 9.98847629e+01 5.45406108e+01 9.18773493e+01 1.88021251e+01\n",
      " 7.79228142e+01 5.53370316e+01 1.60799334e+02 6.12352299e+01\n",
      " 8.60408673e+00 0.00000000e+00 1.11022302e-15 3.67838292e+01\n",
      " 1.09598062e+02 7.58788019e+01 1.02688627e+02 6.40708518e+01\n",
      " 1.08874518e+02 5.72746365e+01 1.08428793e+02 7.14619255e+01\n",
      " 2.45341477e-13]\n",
      "14-th iteration, loss: 0.10592246200014438, 33 gd steps\n",
      "insert gradient: -0.0022523388796771558\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[5.91113458e-01 6.16498097e+01 8.98694773e+01 1.08580017e+01\n",
      " 1.01359346e+02 5.45101028e+01 8.83442285e+01 0.00000000e+00\n",
      " 1.42108547e-14 1.93925231e+01 7.92206053e+01 5.60543560e+01\n",
      " 1.57968070e+02 6.10479022e+01 1.48828345e+00 4.59515258e+01\n",
      " 1.08719499e+02 7.63725243e+01 1.01353189e+02 6.68185794e+01\n",
      " 1.08006982e+02 5.16167409e+01 1.11377391e+02 7.04250205e+01\n",
      " 2.53734362e-13]\n",
      "15-th iteration, loss: 0.10386528662490938, 83 gd steps\n",
      "insert gradient: -0.0016411062453056792\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[1.87787331e+00 6.03022625e+01 6.79534365e+01 1.92379689e+01\n",
      " 1.06897117e+02 5.66543061e+01 8.53989286e+01 1.92392122e+01\n",
      " 8.45651655e+01 5.90922991e+01 1.54179074e+02 1.11444141e+02\n",
      " 1.09725092e+02 7.83059047e+01 9.98644640e+01 7.63438413e+01\n",
      " 9.45710438e+01 5.13451098e+01 1.26948392e+02 6.27967688e+01\n",
      " 3.78138022e-13]\n",
      "16-th iteration, loss: 0.10345332553654826, 28 gd steps\n",
      "insert gradient: -0.0022928407912742492\n",
      "16-th iteration, new layer inserted. now 21 layers\n",
      "[1.30080552e+00 6.25397791e+01 5.21664878e+01 2.66936494e+01\n",
      " 1.05237112e+02 5.65236529e+01 8.33689492e+01 1.83997867e+01\n",
      " 8.97794539e+01 5.97548569e+01 1.54492881e+02 1.11058085e+02\n",
      " 1.10624561e+02 7.83239589e+01 9.96404938e+01 7.76110593e+01\n",
      " 9.24401222e+01 5.03680449e+01 1.30254608e+02 6.19178065e+01\n",
      " 5.38494554e-13]\n",
      "17-th iteration, loss: 0.10338699322807045, 19 gd steps\n",
      "insert gradient: -0.0020049702578703974\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[1.34636107e+00 6.23756898e+01 5.09162821e+01 2.71865417e+01\n",
      " 1.06367255e+02 5.63827014e+01 8.33240842e+01 1.89008695e+01\n",
      " 8.88442750e+01 6.02087688e+01 1.53933417e+02 1.11515636e+02\n",
      " 1.10529940e+02 7.86747878e+01 1.00037305e+02 7.77783727e+01\n",
      " 9.21447724e+01 5.04293506e+01 1.30289743e+02 6.21633055e+01\n",
      " 5.81620505e-13]\n",
      "18-th iteration, loss: 0.10332426688924685, 38 gd steps\n",
      "insert gradient: -0.001878128313153905\n",
      "18-th iteration, new layer inserted. now 21 layers\n",
      "[1.23137269e+00 6.24637067e+01 4.77458666e+01 2.92927600e+01\n",
      " 1.06172198e+02 5.64851695e+01 8.28599367e+01 1.91754710e+01\n",
      " 8.97050618e+01 6.02885938e+01 1.54314215e+02 1.11823067e+02\n",
      " 1.10810077e+02 7.90441163e+01 1.00058598e+02 7.83657284e+01\n",
      " 9.19711100e+01 5.03084427e+01 1.30634422e+02 6.22714902e+01\n",
      " 6.94524575e-13]\n",
      "19-th iteration, loss: 0.10330814411580659, 33 gd steps\n",
      "insert gradient: -0.0006772489593492115\n",
      "19-th iteration, new layer inserted. now 23 layers\n",
      "[1.30345029e+00 6.24905136e+01 4.72012988e+01 2.98872304e+01\n",
      " 1.05949932e+02 5.66057090e+01 8.27427650e+01 1.91835991e+01\n",
      " 8.99790972e+01 6.01044332e+01 1.54604806e+02 0.00000000e+00\n",
      " 1.77635684e-14 1.11876941e+02 1.10524386e+02 7.91617349e+01\n",
      " 1.00224370e+02 7.85251939e+01 9.21309410e+01 5.02961964e+01\n",
      " 1.30770643e+02 6.23173480e+01 7.36157425e-13]\n",
      "20-th iteration, loss: 0.10330363829119743, 32 gd steps\n",
      "insert gradient: -0.0005460025708958883\n",
      "20-th iteration, new layer inserted. now 21 layers\n",
      "[1.31793560e+00 6.25075403e+01 4.63488304e+01 3.05007842e+01\n",
      " 1.05760390e+02 5.66406731e+01 8.25661216e+01 1.91979711e+01\n",
      " 9.03710092e+01 6.00184494e+01 1.54788921e+02 1.11942966e+02\n",
      " 1.10536941e+02 7.92897728e+01 1.00356969e+02 7.85515502e+01\n",
      " 9.21156083e+01 5.02367834e+01 1.30900285e+02 6.23951494e+01\n",
      " 8.00636636e-13]\n",
      "21-th iteration, loss: 0.1033024845832652, 17 gd steps\n",
      "insert gradient: -0.00018493089378102514\n",
      "21-th iteration, new layer inserted. now 21 layers\n",
      "[1.30331224e+00 6.24935464e+01 4.62710384e+01 3.05486714e+01\n",
      " 1.05798977e+02 5.66459072e+01 8.25707444e+01 1.92348999e+01\n",
      " 9.03423181e+01 6.00578343e+01 1.54809448e+02 1.11961352e+02\n",
      " 1.10516732e+02 7.92653993e+01 1.00335727e+02 7.86404040e+01\n",
      " 9.21147101e+01 5.02180521e+01 1.30905277e+02 6.23656221e+01\n",
      " 8.03557543e-13]\n",
      "22-th iteration, loss: 0.10330209462333778, 18 gd steps\n",
      "insert gradient: -0.00014253514172477192\n",
      "22-th iteration, new layer inserted. now 21 layers\n",
      "[1.29022035e+00 6.24939512e+01 4.61170169e+01 3.06467496e+01\n",
      " 1.05817112e+02 5.66411581e+01 8.25569107e+01 1.92546447e+01\n",
      " 9.03380561e+01 6.00736413e+01 1.54818985e+02 1.11972457e+02\n",
      " 1.10506320e+02 7.92672234e+01 1.00347064e+02 7.86741530e+01\n",
      " 9.20990726e+01 5.02077380e+01 1.30922674e+02 6.23634441e+01\n",
      " 8.18669177e-13]\n",
      "23-th iteration, loss: 0.10330182171285313, 24 gd steps\n",
      "insert gradient: -7.464447320285763e-05\n",
      "23-th iteration, new layer inserted. now 21 layers\n",
      "[1.28732084e+00 6.24940648e+01 4.59967891e+01 3.07372738e+01\n",
      " 1.05789595e+02 5.66499616e+01 8.25352411e+01 1.92615764e+01\n",
      " 9.03836488e+01 6.00700069e+01 1.54845605e+02 1.11987063e+02\n",
      " 1.10510553e+02 7.92819037e+01 1.00353538e+02 7.86978360e+01\n",
      " 9.20937660e+01 5.02028980e+01 1.30927807e+02 6.23717768e+01\n",
      " 8.37795090e-13]\n",
      "24-th iteration, loss: 0.10330167371066172, 21 gd steps\n",
      "insert gradient: -0.00010301309318407265\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[1.28319288e+00 6.25077933e+01 4.57934710e+01 3.08781644e+01\n",
      " 1.05741470e+02 5.66501953e+01 8.24864401e+01 1.92599327e+01\n",
      " 9.04610720e+01 6.00727555e+01 1.54872955e+02 1.11982719e+02\n",
      " 1.10511145e+02 7.92876195e+01 1.00352421e+02 7.87162063e+01\n",
      " 9.20758770e+01 5.01924160e+01 1.30955226e+02 6.23842256e+01\n",
      " 9.01263946e-13]\n",
      "25-th iteration, loss: 0.1033016254443074, 23 gd steps\n",
      "insert gradient: -3.667286364122843e-05\n",
      "25-th iteration, new layer inserted. now 21 layers\n",
      "[1.28182961e+00 6.25047693e+01 4.57739225e+01 3.08898639e+01\n",
      " 1.05752159e+02 5.66536517e+01 8.24883294e+01 1.92711208e+01\n",
      " 9.04567432e+01 6.00655500e+01 1.54873380e+02 1.11993446e+02\n",
      " 1.10510387e+02 7.92956827e+01 1.00361506e+02 7.87257997e+01\n",
      " 9.20734934e+01 5.01892565e+01 1.30952594e+02 6.23804111e+01\n",
      " 9.17587938e-13]\n",
      "26-th iteration, loss: 0.10330159638320922, 63 gd steps\n",
      "insert gradient: -1.505688042434674e-05\n",
      "26-th iteration, new layer inserted. now 21 layers\n",
      "[1.28118864e+00 6.25045454e+01 4.56997639e+01 3.09442555e+01\n",
      " 1.05745200e+02 5.66555776e+01 8.24765214e+01 1.92772325e+01\n",
      " 9.04767416e+01 6.00624641e+01 1.54891598e+02 1.11998803e+02\n",
      " 1.10504838e+02 7.93042496e+01 1.00368666e+02 7.87371852e+01\n",
      " 9.20720941e+01 5.01856056e+01 1.30958996e+02 6.23857574e+01\n",
      " 1.06154091e-12]\n",
      "27-th iteration, loss: 0.10330159516590871, 26 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.500312932361444e-06\n",
      "27-th iteration, new layer inserted. now 21 layers\n",
      "[1.28110863e+00 6.25039325e+01 4.56967719e+01 3.09482557e+01\n",
      " 1.05745176e+02 5.66565834e+01 8.24761577e+01 1.92775339e+01\n",
      " 9.04777421e+01 6.00616395e+01 1.54892982e+02 1.11999398e+02\n",
      " 1.10504557e+02 7.93051393e+01 1.00369122e+02 7.87392140e+01\n",
      " 9.20728707e+01 5.01861043e+01 1.30959863e+02 6.23862846e+01\n",
      " 1.06781301e-12]\n",
      "28-th iteration, loss: 0.10330159516162102, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.359336942431483e-06\n",
      "28-th iteration, new layer inserted. now 21 layers\n",
      "[1.28110864e+00 6.25039323e+01 4.56967701e+01 3.09482566e+01\n",
      " 1.05745177e+02 5.66565862e+01 8.24761600e+01 1.92775397e+01\n",
      " 9.04777457e+01 6.00616455e+01 1.54892986e+02 1.11999406e+02\n",
      " 1.10504560e+02 7.93051444e+01 1.00369123e+02 7.87392150e+01\n",
      " 9.20728704e+01 5.01861022e+01 1.30959861e+02 6.23862835e+01\n",
      " 1.06782440e-12]\n",
      "29-th iteration, loss: 0.1033015951574876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.221843491285504e-06\n",
      "29-th iteration, new layer inserted. now 21 layers\n",
      "[1.28110864e+00 6.25039321e+01 4.56967684e+01 3.09482576e+01\n",
      " 1.05745177e+02 5.66565889e+01 8.24761623e+01 1.92775453e+01\n",
      " 9.04777492e+01 6.00616512e+01 1.54892991e+02 1.11999413e+02\n",
      " 1.10504563e+02 7.93051494e+01 1.00369125e+02 7.87392161e+01\n",
      " 9.20728700e+01 5.01861003e+01 1.30959860e+02 6.23862825e+01\n",
      " 1.06783271e-12]\n",
      "30-th iteration, loss: 0.10330159515350022, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.087745399717207e-06\n",
      "30-th iteration, new layer inserted. now 23 layers\n",
      "[1.28110864e+00 6.25039319e+01 4.56967666e+01 3.09482586e+01\n",
      " 1.05745177e+02 5.66565916e+01 8.24761646e+01 1.92775509e+01\n",
      " 9.04777526e+01 6.00616569e+01 1.54892995e+02 0.00000000e+00\n",
      " 1.77635684e-14 1.11999420e+02 1.10504566e+02 7.93051544e+01\n",
      " 1.00369127e+02 7.87392173e+01 9.20728698e+01 5.01860985e+01\n",
      " 1.30959859e+02 6.23862815e+01 1.06783835e-12]\n",
      "31-th iteration, loss: 0.10330159514838883, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.88732380077828e-06\n",
      "31-th iteration, new layer inserted. now 23 layers\n",
      "[1.28110864e+00 6.25039316e+01 4.56967649e+01 3.09482596e+01\n",
      " 1.05745178e+02 5.66565942e+01 8.24761667e+01 1.92775563e+01\n",
      " 9.04777560e+01 6.00616623e+01 1.54892999e+02 6.97693434e-06\n",
      " 4.21456197e-06 1.11999427e+02 1.10504569e+02 7.93051594e+01\n",
      " 1.00369129e+02 7.87392186e+01 9.20728695e+01 5.01860968e+01\n",
      " 1.30959858e+02 6.23862806e+01 1.06784236e-12]\n",
      "32-th iteration, loss: 0.10330159514352173, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.693548209181388e-06\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[1.28110863e+00 6.25039314e+01 4.56967631e+01 3.09482607e+01\n",
      " 1.05745178e+02 5.66565968e+01 8.24761688e+01 1.92775616e+01\n",
      " 9.04777593e+01 6.00616676e+01 1.54893003e+02 1.37486663e-05\n",
      " 8.32427756e-06 0.00000000e+00 1.27054942e-21 1.11999434e+02\n",
      " 1.10504572e+02 7.93051643e+01 1.00369131e+02 7.87392201e+01\n",
      " 9.20728694e+01 5.01860951e+01 1.30959857e+02 6.23862797e+01\n",
      " 1.06785126e-12]\n",
      "33-th iteration, loss: 0.10330159513777386, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.44092522258284e-06\n",
      "33-th iteration, new layer inserted. now 27 layers\n",
      "[1.28110862e+00 6.25039311e+01 4.56967614e+01 3.09482618e+01\n",
      " 1.05745178e+02 5.66565994e+01 8.24761709e+01 1.92775667e+01\n",
      " 9.04777625e+01 6.00616727e+01 1.54893007e+02 2.02924945e-05\n",
      " 1.23182889e-05 6.55822775e-06 3.99401137e-06 0.00000000e+00\n",
      " 7.41153829e-22 1.11999440e+02 1.10504575e+02 7.93051692e+01\n",
      " 1.00369132e+02 7.87392216e+01 9.20728693e+01 5.01860936e+01\n",
      " 1.30959856e+02 6.23862788e+01 1.06786127e-12]\n",
      "34-th iteration, loss: 0.10330159513137062, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.136709382570984e-06\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[1.28110861e+00 6.25039308e+01 4.56967597e+01 3.09482630e+01\n",
      " 1.05745179e+02 5.66566019e+01 8.24761729e+01 1.92775717e+01\n",
      " 9.04777655e+01 6.00616775e+01 1.54893011e+02 2.65459275e-05\n",
      " 1.61719589e-05 1.28329692e-05 7.83633678e-06 6.28165036e-06\n",
      " 3.84232540e-06 1.11999447e+02 1.10504578e+02 7.93051739e+01\n",
      " 1.00369134e+02 7.87392232e+01 9.20728693e+01 5.01860922e+01\n",
      " 1.30959855e+02 6.23862779e+01 1.06787100e-12]\n",
      "35-th iteration, loss: 0.10330159512545871, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.8482733802642726e-06\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[1.28110861e+00 6.25039306e+01 4.56967580e+01 3.09482643e+01\n",
      " 1.05745179e+02 5.66566045e+01 8.24761748e+01 1.92775765e+01\n",
      " 9.04777685e+01 6.00616821e+01 1.54893015e+02 3.24814499e-05\n",
      " 1.98771848e-05 1.87964663e-05 1.15193639e-05 1.22587030e-05\n",
      " 7.51448637e-06 1.11999453e+02 1.10504580e+02 7.93051786e+01\n",
      " 1.00369136e+02 7.87392250e+01 9.20728694e+01 5.01860909e+01\n",
      " 1.30959854e+02 6.23862772e+01 1.06787080e-12]\n",
      "36-th iteration, loss: 0.10330159511998983, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.574800202599623e-06\n",
      "36-th iteration, new layer inserted. now 29 layers\n",
      "[1.28110860e+00 6.25039303e+01 4.56967564e+01 3.09482657e+01\n",
      " 1.05745179e+02 5.66566070e+01 8.24761767e+01 1.92775812e+01\n",
      " 9.04777714e+01 6.00616864e+01 1.54893018e+02 3.81153536e-05\n",
      " 2.34415988e-05 2.44647541e-05 1.50512632e-05 1.79469173e-05\n",
      " 1.10251802e-05 0.00000000e+00 2.11758237e-21 1.11999458e+02\n",
      " 1.10504583e+02 7.93051832e+01 1.00369138e+02 7.87392270e+01\n",
      " 9.20728695e+01 5.01860897e+01 1.30959853e+02 6.23862764e+01\n",
      " 1.06787602e-12]\n",
      "37-th iteration, loss: 0.10330159511417415, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.261983398009848e-06\n",
      "37-th iteration, new layer inserted. now 31 layers\n",
      "[1.28110859e+00 6.25039301e+01 4.56967547e+01 3.09482671e+01\n",
      " 1.05745180e+02 5.66566096e+01 8.24761785e+01 1.92775858e+01\n",
      " 9.04777742e+01 6.00616905e+01 1.54893022e+02 4.34389250e-05\n",
      " 2.68607353e-05 2.98288754e-05 1.84280799e-05 2.33370747e-05\n",
      " 1.43709518e-05 5.40922912e-06 3.34577161e-06 0.00000000e+00\n",
      " 5.29395592e-22 1.11999464e+02 1.10504585e+02 7.93051877e+01\n",
      " 1.00369140e+02 7.87392290e+01 9.20728697e+01 5.01860887e+01\n",
      " 1.30959852e+02 6.23862757e+01 1.06788216e-12]\n",
      "38-th iteration, loss: 0.10330159510821271, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.918272243745465e-06\n",
      "38-th iteration, new layer inserted. now 31 layers\n",
      "[1.28110858e+00 6.25039298e+01 4.56967531e+01 3.09482687e+01\n",
      " 1.05745180e+02 5.66566122e+01 8.24761803e+01 1.92775902e+01\n",
      " 9.04777769e+01 6.00616943e+01 1.54893025e+02 4.84115083e-05\n",
      " 3.01191679e-05 3.48479232e-05 2.16349136e-05 2.83880000e-05\n",
      " 1.75374163e-05 1.04850137e-05 6.50287905e-06 5.08157224e-06\n",
      " 3.15710744e-06 1.11999469e+02 1.10504587e+02 7.93051922e+01\n",
      " 1.00369142e+02 7.87392312e+01 9.20728700e+01 5.01860878e+01\n",
      " 1.30959852e+02 6.23862751e+01 1.06789138e-12]\n",
      "39-th iteration, loss: 0.10330159510285274, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.598976456456436e-06\n",
      "39-th iteration, new layer inserted. now 33 layers\n",
      "[1.28110858e+00 6.25039296e+01 4.56967516e+01 3.09482704e+01\n",
      " 1.05745181e+02 5.66566149e+01 8.24761820e+01 1.92775946e+01\n",
      " 9.04777795e+01 6.00616978e+01 1.54893028e+02 5.30228131e-05\n",
      " 3.32156067e-05 3.95113291e-05 2.46710713e-05 3.30888307e-05\n",
      " 2.05244676e-05 1.52161812e-05 9.47179295e-06 9.82398859e-06\n",
      " 6.11723108e-06 0.00000000e+00 2.11758237e-22 1.11999474e+02\n",
      " 1.10504589e+02 7.93051965e+01 1.00369144e+02 7.87392335e+01\n",
      " 9.20728704e+01 5.01860870e+01 1.30959851e+02 6.23862745e+01\n",
      " 1.06788895e-12]\n",
      "40-th iteration, loss: 0.10330159509752221, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.281295025453301e-06\n",
      "40-th iteration, new layer inserted. now 35 layers\n",
      "[1.28110857e+00 6.25039294e+01 4.56967500e+01 3.09482722e+01\n",
      " 1.05745182e+02 5.66566175e+01 8.24761837e+01 1.92775987e+01\n",
      " 9.04777820e+01 6.00617010e+01 1.54893031e+02 5.72784913e-05\n",
      " 3.61519345e-05 4.38244648e-05 2.75390511e-05 3.74446432e-05\n",
      " 2.33352092e-05 1.96074977e-05 1.22562131e-05 1.42316897e-05\n",
      " 8.88465730e-06 4.41828290e-06 2.76742622e-06 1.11999478e+02\n",
      " 1.10504591e+02 0.00000000e+00 1.06581410e-14 7.93052007e+01\n",
      " 1.00369145e+02 7.87392360e+01 9.20728709e+01 5.01860864e+01\n",
      " 1.30959851e+02 6.23862739e+01 1.06789900e-12]\n",
      "41-th iteration, loss: 0.10330159509238988, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.1472012422686635e-06\n",
      "41-th iteration, new layer inserted. now 37 layers\n",
      "[1.28110857e+00 6.25039291e+01 4.56967485e+01 3.09482741e+01\n",
      " 1.05745182e+02 5.66566203e+01 8.24761854e+01 1.92776028e+01\n",
      " 9.04777844e+01 6.00617040e+01 1.54893034e+02 6.11650916e-05\n",
      " 3.89231280e-05 4.77736020e-05 3.02344355e-05 4.14414184e-05\n",
      " 2.59658204e-05 2.36446390e-05 1.48529066e-05 1.82900323e-05\n",
      " 1.14567323e-05 8.49199450e-06 5.33185831e-06 1.11999482e+02\n",
      " 1.10504593e+02 4.12534628e-06 1.83822164e-06 0.00000000e+00\n",
      " 4.23516474e-22 7.93052049e+01 1.00369147e+02 7.87392386e+01\n",
      " 9.20728714e+01 5.01860858e+01 1.30959850e+02 6.23862734e+01\n",
      " 1.06791169e-12]\n",
      "42-th iteration, loss: 0.10330159508745143, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.014574231193571e-06\n",
      "42-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110857e+00 6.25039289e+01 4.56967471e+01 3.09482761e+01\n",
      " 1.05745183e+02 5.66566230e+01 8.24761870e+01 0.00000000e+00\n",
      " 3.55271368e-15 1.92776067e+01 9.04777868e+01 6.00617067e+01\n",
      " 1.54893037e+02 6.47000947e-05 4.15345671e-05 5.13759358e-05\n",
      " 3.27632340e-05 4.50960529e-05 2.84229320e-05 2.73441901e-05\n",
      " 1.72691167e-05 2.20152765e-05 1.38413036e-05 1.22370571e-05\n",
      " 7.70173988e-06 1.11999486e+02 1.10504595e+02 8.09782109e-06\n",
      " 3.52942477e-06 3.97556704e-06 1.69120313e-06 7.93052088e+01\n",
      " 1.00369149e+02 7.87392413e+01 9.20728721e+01 5.01860854e+01\n",
      " 1.30959850e+02 6.23862729e+01 1.06792178e-12]\n",
      "43-th iteration, loss: 0.10330159508269611, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.885093421207349e-06\n",
      "43-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110858e+00 6.25039288e+01 4.56967457e+01 3.09482783e+01\n",
      " 1.05745184e+02 5.66566259e+01 8.24761886e+01 3.84911673e-06\n",
      " 1.59425281e-06 1.92776106e+01 9.04777890e+01 6.00617092e+01\n",
      " 1.54893040e+02 6.79039066e-05 4.39946196e-05 5.46515957e-05\n",
      " 3.51344144e-05 4.84283879e-05 3.07161035e-05 3.07256922e-05\n",
      " 1.95149872e-05 2.54266514e-05 1.60490913e-05 1.56723752e-05\n",
      " 9.88835934e-06 1.11999489e+02 1.10504596e+02 1.19061952e-05\n",
      " 5.07521590e-06 7.78987825e-06 3.23030661e-06 7.93052127e+01\n",
      " 1.00369151e+02 7.87392440e+01 9.20728727e+01 5.01860852e+01\n",
      " 1.30959850e+02 6.23862725e+01 1.06792595e-12]\n",
      "44-th iteration, loss: 0.10330159507840443, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.763292416448667e-06\n",
      "44-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110859e+00 6.25039286e+01 4.56967443e+01 3.09482806e+01\n",
      " 1.05745185e+02 5.66566287e+01 8.24761901e+01 7.57369355e-06\n",
      " 3.14542229e-06 1.92776143e+01 9.04777912e+01 6.00617114e+01\n",
      " 1.54893042e+02 7.08040590e-05 4.63189047e-05 5.76278517e-05\n",
      " 3.73641611e-05 5.14654208e-05 3.28620768e-05 3.38158591e-05\n",
      " 2.16078100e-05 2.85505761e-05 1.80979299e-05 1.88240623e-05\n",
      " 1.19100873e-05 1.11999492e+02 1.10504598e+02 1.55888829e-05\n",
      " 6.49386135e-06 1.14811034e-05 4.63584806e-06 7.93052164e+01\n",
      " 1.00369153e+02 7.87392469e+01 9.20728735e+01 5.01860850e+01\n",
      " 1.30959850e+02 6.23862721e+01 1.06793106e-12]\n",
      "45-th iteration, loss: 0.10330159507450572, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.648659255966956e-06\n",
      "45-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110861e+00 6.25039285e+01 4.56967430e+01 3.09482830e+01\n",
      " 1.05745185e+02 5.66566317e+01 8.24761916e+01 1.11809996e-05\n",
      " 4.65511314e-06 1.92776179e+01 9.04777932e+01 6.00617134e+01\n",
      " 1.54893044e+02 7.34262885e-05 4.85189795e-05 6.03302055e-05\n",
      " 3.94645489e-05 5.42324087e-05 3.48734377e-05 3.66396931e-05\n",
      " 2.35606747e-05 3.14117882e-05 2.00014068e-05 2.17165809e-05\n",
      " 1.37810015e-05 1.11999495e+02 1.10504599e+02 1.91552140e-05\n",
      " 7.79535700e-06 1.50583584e-05 5.91803038e-06 7.93052199e+01\n",
      " 1.00369155e+02 7.87392499e+01 9.20728743e+01 5.01860850e+01\n",
      " 1.30959850e+02 6.23862717e+01 1.06793431e-12]\n",
      "46-th iteration, loss: 0.10330159507094065, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5407163004490826e-06\n",
      "46-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110862e+00 6.25039283e+01 4.56967417e+01 3.09482855e+01\n",
      " 1.05745186e+02 5.66566346e+01 8.24761931e+01 1.46778244e-05\n",
      " 6.12487433e-06 1.92776214e+01 9.04777952e+01 6.00617152e+01\n",
      " 1.54893047e+02 7.57941530e-05 5.06054190e-05 6.27820001e-05\n",
      " 4.14466267e-05 5.67524706e-05 3.67617020e-05 3.92200806e-05\n",
      " 2.53855582e-05 3.40329320e-05 2.17719527e-05 2.43723240e-05\n",
      " 1.55139812e-05 1.11999498e+02 1.10504600e+02 2.26136762e-05\n",
      " 8.98884671e-06 1.85299338e-05 7.08618919e-06 7.93052234e+01\n",
      " 1.00369157e+02 7.87392530e+01 9.20728752e+01 5.01860850e+01\n",
      " 1.30959850e+02 6.23862714e+01 1.06793458e-12]\n",
      "47-th iteration, loss: 0.10330159506765924, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.43901797973221e-06\n",
      "47-th iteration, new layer inserted. now 39 layers\n",
      "[1.28110864e+00 6.25039282e+01 4.56967404e+01 3.09482881e+01\n",
      " 1.05745187e+02 5.66566376e+01 8.24761946e+01 1.80705096e-05\n",
      " 7.55619873e-06 1.92776248e+01 9.04777971e+01 6.00617168e+01\n",
      " 1.54893049e+02 7.79292159e-05 5.25878994e-05 6.50046023e-05\n",
      " 4.33205043e-05 5.90467690e-05 3.85374067e-05 4.15779710e-05\n",
      " 2.70934183e-05 3.64347354e-05 2.34209412e-05 2.68117894e-05\n",
      " 1.71208093e-05 1.11999501e+02 1.10504602e+02 2.59719931e-05\n",
      " 1.00826961e-05 2.19033716e-05 8.14886783e-06 7.93052268e+01\n",
      " 1.00369159e+02 7.87392562e+01 9.20728761e+01 5.01860852e+01\n",
      " 1.30959850e+02 6.23862711e+01 1.06793852e-12]\n",
      "48-th iteration, loss: 0.10330159506461963, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.453717746862872e-06\n",
      "48-th iteration, new layer inserted. now 41 layers\n",
      "[1.28110866e+00 6.25039281e+01 4.56967391e+01 3.09482908e+01\n",
      " 1.05745188e+02 5.66566406e+01 8.24761960e+01 2.13649788e-05\n",
      " 8.95052308e-06 1.92776281e+01 9.04777990e+01 6.00617183e+01\n",
      " 1.54893051e+02 7.98512150e-05 5.44752750e-05 6.70175702e-05\n",
      " 4.50954321e-05 6.11346745e-05 4.02101929e-05 4.37325402e-05\n",
      " 2.86942811e-05 3.86361720e-05 2.49587776e-05 2.90537406e-05\n",
      " 1.86122655e-05 1.11999503e+02 1.10504603e+02 2.92371944e-05\n",
      " 1.10845596e-05 2.51855338e-05 9.11388597e-06 7.93052301e+01\n",
      " 1.00369160e+02 0.00000000e+00 7.10542736e-15 7.87392595e+01\n",
      " 9.20728771e+01 5.01860854e+01 1.30959850e+02 6.23862708e+01\n",
      " 1.06794680e-12]\n",
      "49-th iteration, loss: 0.10330159506150206, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.476379546540624e-06\n",
      "49-th iteration, new layer inserted. now 41 layers\n",
      "[1.28110868e+00 6.25039279e+01 4.56967379e+01 3.09482935e+01\n",
      " 1.05745189e+02 5.66566436e+01 8.24761974e+01 2.45680240e-05\n",
      " 1.03109795e-05 1.92776313e+01 9.04778008e+01 6.00617196e+01\n",
      " 1.54893053e+02 8.15726029e-05 5.62714110e-05 6.88331917e-05\n",
      " 4.67756383e-05 6.30283041e-05 4.17846456e-05 4.56957271e-05\n",
      " 3.01930833e-05 4.06489955e-05 2.63907452e-05 3.11097390e-05\n",
      " 1.99939746e-05 1.11999505e+02 1.10504604e+02 3.23936503e-05\n",
      " 1.19949288e-05 2.83606360e-05 9.98188851e-06 7.93052333e+01\n",
      " 1.00369162e+02 3.31232421e-06 1.86306704e-06 7.87392628e+01\n",
      " 9.20728782e+01 5.01860857e+01 1.30959850e+02 6.23862706e+01\n",
      " 1.06794698e-12]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5314150056394804\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.38057759    0.         1465.15711086]\n",
      "1-th iteration, loss: 0.745144539899422, 11 gd steps\n",
      "insert gradient: -0.6271255992432505\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.43634346   62.36427952  232.28100538    0.         1232.87610548]\n",
      "2-th iteration, loss: 0.6045224007822917, 13 gd steps\n",
      "insert gradient: -0.5988403982624378\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[  3.92563605  77.89197424 218.27817007  41.87757628 251.60736846\n",
      "   0.         981.26873701]\n",
      "3-th iteration, loss: 0.43923635935498645, 30 gd steps\n",
      "insert gradient: -0.6335745085270758\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[4.03675612e-01 4.42358847e+01 1.07644330e+02 0.00000000e+00\n",
      " 1.13976350e+02 7.62862252e+01 1.06652235e+02 7.23910555e+01\n",
      " 9.81268737e+02]\n",
      "4-th iteration, loss: 0.36328123960364733, 98 gd steps\n",
      "insert gradient: -0.44623703601399434\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          44.4282839   55.17203393  48.92407824 106.83736778\n",
      "  76.81211257 123.1088368   80.14409768 181.71643278   0.\n",
      " 799.55230423]\n",
      "5-th iteration, loss: 0.2484960022902374, 26 gd steps\n",
      "insert gradient: -0.20678308300644135\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.26458622  41.87037417  77.32338643  45.39309804  97.03717606\n",
      "  79.97217841 119.72246665  79.03064321 145.87638835  83.16411772\n",
      " 181.71643278   0.         617.83587145]\n",
      "6-th iteration, loss: 0.18908278448041319, 53 gd steps\n",
      "insert gradient: -0.07034136690820121\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  3.55308755  49.56687231  84.42577655  41.51187438  85.42469619\n",
      "  82.92328908 110.56854377  83.82704047 124.20807055  81.90919375\n",
      " 158.83241876  72.0208737  205.94529048   0.         411.89058097]\n",
      "7-th iteration, loss: 0.1775798635191305, 57 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.07179714  51.85924575  86.82559525  44.74279937  82.57637277\n",
      "  87.48671679 105.14391024  86.65173598 116.51619368  88.63650165\n",
      " 150.48113392  71.90882819 180.51438885  32.19062669  51.48632262\n",
      "   0.         360.40425835]\n",
      "8-th iteration, loss: 0.16814009445395947, 23 gd steps\n",
      "insert gradient: -0.06165580248392507\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[2.01112342e-02 4.70703922e+01 8.65666231e+01 5.01119447e+01\n",
      " 8.65640918e+01 8.78583989e+01 1.03929186e+02 8.76074551e+01\n",
      " 1.15518109e+02 8.90275776e+01 1.49087898e+02 7.30298503e+01\n",
      " 1.75566390e+02 4.26229161e+01 3.61193175e+01 1.92943405e+01\n",
      " 3.60404258e+02]\n",
      "9-th iteration, loss: 0.16175666762975008, 37 gd steps\n",
      "insert gradient: -0.041735333967609\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.35248108  47.35575717  86.43203005  47.35623232  85.06093962\n",
      "  87.6777299  102.23642884  85.8867034  117.49633567  86.13800544\n",
      " 149.25346517  74.10000005 164.54473459  53.23466461  12.65446903\n",
      "  28.34798575 231.68845179   0.         128.71580655]\n",
      "10-th iteration, loss: 0.15942104365902732, 14 gd steps\n",
      "insert gradient: -0.0217597610804887\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[1.52097078e+00 0.00000000e+00 6.66133815e-16 4.78129454e+01\n",
      " 8.65035185e+01 4.72144746e+01 8.49903988e+01 8.73106040e+01\n",
      " 1.02617642e+02 8.54897438e+01 1.18142631e+02 8.66267704e+01\n",
      " 1.48469917e+02 7.51111566e+01 1.63504457e+02 5.43237142e+01\n",
      " 1.10221427e+01 2.90276965e+01 2.31360894e+02 5.57325798e+00\n",
      " 1.28715807e+02]\n",
      "11-th iteration, loss: 0.15809741239475109, 38 gd steps\n",
      "insert gradient: -0.01571588645003873\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  2.03348955  47.92197724  86.56046499  46.37007521  84.79742531\n",
      "  86.74927533 102.47821106  85.11236051 118.91169137  86.13002418\n",
      " 145.55704762  77.05703779 159.48767929  60.54503093   4.80412643\n",
      "  30.03076486 221.822298     7.76025935  96.53685491   0.\n",
      "  32.17895164]\n",
      "12-th iteration, loss: 0.1574787600178623, 21 gd steps\n",
      "insert gradient: -0.009103703891372451\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  1.92214042  48.38507255  86.56844942  46.79757562  85.05387142\n",
      "  86.57857877 102.50158406  84.85173248 119.29857617  85.69782674\n",
      " 118.32061947   0.          26.29347099  78.33096407 158.93640287\n",
      "  61.37804866   4.04869006  30.23373135 220.04658759   8.99461337\n",
      "  96.17422604   3.86492652  32.17895164]\n",
      "13-th iteration, loss: 0.15733527924708257, 15 gd steps\n",
      "insert gradient: -0.006085258954225375\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  1.86341268  48.0993493   86.39354314  46.63795053  85.00025997\n",
      "  86.52850102 102.58366372  84.89694879 119.50865816  85.74182103\n",
      " 117.76213      0.76106653  25.71519946  78.36415819 158.69829044\n",
      "  61.58319367   3.82462811  30.21006719 175.45254978   0.\n",
      "  43.86313744   9.03026226  95.80398226   4.06361593  32.17895164]\n",
      "14-th iteration, loss: 0.15645947218762876, 29 gd steps\n",
      "insert gradient: -0.008932544983899572\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  1.85181525  48.11098621  86.80097726  47.04268472  84.67617818\n",
      "  87.18279045 102.15094786  84.76070916 119.97914392  86.57209406\n",
      " 115.93868457   3.91503789  22.62207648  75.50222529 158.45016473\n",
      "  63.93836251   1.1162803   29.96426585 170.74115311   7.22596456\n",
      "  39.87233673  12.83974991  49.21977584   0.          39.37582068\n",
      "   3.3362427   32.17895164]\n",
      "15-th iteration, loss: 0.15601435676220346, 14 gd steps\n",
      "insert gradient: -0.014198296126446014\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[1.63595019e+00 0.00000000e+00 3.33066907e-16 4.76467424e+01\n",
      " 8.67112899e+01 4.71311555e+01 8.50511965e+01 8.69163784e+01\n",
      " 1.01835482e+02 8.47986751e+01 1.19672047e+02 8.62467136e+01\n",
      " 1.16306847e+02 3.99237532e+00 2.24751523e+01 7.49859405e+01\n",
      " 1.58344531e+02 6.39029771e+01 1.04506391e+00 2.98226348e+01\n",
      " 1.69558867e+02 8.49035504e+00 3.86084454e+01 1.54486866e+01\n",
      " 4.82234687e+01 3.67820151e+00 3.83477288e+01 3.80017039e+00\n",
      " 3.21789516e+01]\n",
      "16-th iteration, loss: 0.15564698854627151, 19 gd steps\n",
      "insert gradient: -0.0060243841774963395\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  1.85817709  48.14327832  86.83438752  46.96938761  84.95980939\n",
      "  86.82497833 101.80399989  84.67262616 119.66111822  86.16894622\n",
      " 116.57589364   3.89526088  22.36278972  74.72450462 158.12318362\n",
      "  63.64977785   0.7748914   29.50906184 168.30034395   9.06270665\n",
      "  37.04953924  17.15110643  47.64896176   5.50208205  37.49175475\n",
      "   3.03973996  32.17895164]\n",
      "17-th iteration, loss: 0.15339830580019603, 399 gd steps\n",
      "insert gradient: -0.003175110690520319\n",
      "17-th iteration, new layer inserted. now 25 layers\n",
      "[  1.96071755  47.99579104  86.75347855  46.78279413  84.78790664\n",
      "  86.673478   101.0127915   84.43666291 119.55436576  85.93129175\n",
      " 117.43946994   4.5227541   20.82423738  74.44819708 157.08505396\n",
      "  62.77014259   0.32133951  28.48072089 158.11440565  16.11231845\n",
      "  27.11790638  32.60184177  44.93381266  11.94159046  68.11592024]\n",
      "18-th iteration, loss: 0.15192069843147837, 19 gd steps\n",
      "insert gradient: -0.028341799606960913\n",
      "18-th iteration, new layer inserted. now 25 layers\n",
      "[  2.28929305  49.68461868  86.61161039  45.23247669  84.56939763\n",
      "  87.12486948  99.99391008  84.28419079 119.66055416  85.33604608\n",
      " 119.90605716   5.45908661  17.95637303  73.56393059 157.65616373\n",
      "  61.28639906   3.03125745  25.37340537 142.0958832   18.32391881\n",
      "  13.73460716  61.29689699  30.49775343  14.2149152   68.11592024]\n",
      "19-th iteration, loss: 0.15137705700598347, 31 gd steps\n",
      "insert gradient: -0.0035511710847640873\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[  1.90035871  47.85054928  86.79935374  46.94625021  84.32885214\n",
      "  86.73936176 100.29387005  84.0457041  119.90336721  85.52404741\n",
      " 120.09960271   5.74166426  17.43039486  73.28049377 157.96564114\n",
      "  61.85316041   3.40798707  24.52448026 141.13613074  17.54128833\n",
      "  13.92182012  62.82308535  30.75549632  13.89585147  68.11592024]\n",
      "20-th iteration, loss: 0.15132400818784864, 32 gd steps\n",
      "insert gradient: -0.0023986833440628897\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[1.91152843e+00 4.79985145e+01 8.68310455e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.67934278e+01 8.43205768e+01 8.66980436e+01\n",
      " 1.00437327e+02 8.39962745e+01 1.20028031e+02 8.55120745e+01\n",
      " 1.20522609e+02 5.99778216e+00 1.69275417e+01 7.30340722e+01\n",
      " 1.58219868e+02 6.26403275e+01 3.97547610e+00 2.34625365e+01\n",
      " 1.39794917e+02 1.70333735e+01 1.40635898e+01 6.43687064e+01\n",
      " 3.07484005e+01 1.38224613e+01 6.81159202e+01]\n",
      "21-th iteration, loss: 0.15130356978340834, 29 gd steps\n",
      "insert gradient: -0.0013530198268116706\n",
      "21-th iteration, new layer inserted. now 27 layers\n",
      "[1.93482234e+00 4.81918674e+01 8.68915323e+01 0.00000000e+00\n",
      " 2.84217094e-14 4.67023238e+01 8.43393607e+01 8.67193352e+01\n",
      " 1.00493147e+02 8.39813400e+01 1.20086613e+02 8.54830677e+01\n",
      " 1.20754444e+02 6.09287626e+00 1.66462462e+01 7.29148668e+01\n",
      " 1.58302023e+02 6.30619340e+01 4.30936962e+00 2.28422590e+01\n",
      " 1.39057288e+02 1.68547225e+01 1.41273505e+01 6.50414958e+01\n",
      " 3.07506526e+01 1.37709506e+01 6.81159202e+01]\n",
      "22-th iteration, loss: 0.15125218023756354, 57 gd steps\n",
      "insert gradient: -0.0008596300666013118\n",
      "22-th iteration, new layer inserted. now 25 layers\n",
      "[  1.90174196  48.12488608  86.88089207  46.83554468  84.29662871\n",
      "  86.67661479 100.52811658  83.94907542 120.14295547  85.5103279\n",
      " 121.4157695    6.43946899  15.97658009  72.60431251 158.56498655\n",
      "  64.31024243   5.19502407  21.27524546 137.04966386  16.52707578\n",
      "  14.40829558  66.45220951  30.84955772  13.73552817  68.11592024]\n",
      "23-th iteration, loss: 0.15124448817078226, 13 gd steps\n",
      "insert gradient: -0.0019586264539008342\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[1.92317623e+00 4.82890557e+01 8.69061651e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.67318433e+01 8.42777257e+01 8.67572843e+01\n",
      " 1.00545369e+02 8.39613053e+01 1.20151986e+02 8.55052306e+01\n",
      " 1.21602611e+02 6.54219447e+00 1.57750660e+01 7.24973412e+01\n",
      " 1.58624991e+02 6.46753305e+01 5.44460382e+00 2.08046186e+01\n",
      " 1.36438771e+02 1.64588872e+01 1.44733886e+01 6.67692769e+01\n",
      " 3.08928169e+01 1.36936282e+01 6.81159202e+01]\n",
      "24-th iteration, loss: 0.15122634070937685, 38 gd steps\n",
      "insert gradient: -0.0008967708352961134\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[1.87531079e+00 4.81510302e+01 8.69026376e+01 0.00000000e+00\n",
      " 2.84217094e-14 4.68599622e+01 8.43060490e+01 8.67705806e+01\n",
      " 1.00548697e+02 8.39786770e+01 1.20164010e+02 8.55343965e+01\n",
      " 1.21817808e+02 6.69387834e+00 1.55388378e+01 7.23825718e+01\n",
      " 1.58728512e+02 6.51181822e+01 5.81320661e+00 2.02740656e+01\n",
      " 1.35716442e+02 1.64518954e+01 1.45756134e+01 6.70984223e+01\n",
      " 3.09883376e+01 1.36786242e+01 6.81159202e+01]\n",
      "25-th iteration, loss: 0.15119973346782617, 26 gd steps\n",
      "insert gradient: -0.000494000614462763\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[1.84321140e+00 4.81683005e+01 8.69490225e+01 4.69560340e+01\n",
      " 8.43190549e+01 8.68063890e+01 1.00577834e+02 8.39759013e+01\n",
      " 1.20206772e+02 8.55394721e+01 1.22396319e+02 0.00000000e+00\n",
      " 2.48689958e-14 7.00655088e+00 1.49588574e+01 7.20560002e+01\n",
      " 1.58890847e+02 6.63032423e+01 6.52935272e+00 1.90318788e+01\n",
      " 1.33874530e+02 1.63568293e+01 1.48504708e+01 6.77658142e+01\n",
      " 3.11488954e+01 1.36314464e+01 6.81159202e+01]\n",
      "26-th iteration, loss: 0.15118708509872744, 41 gd steps\n",
      "insert gradient: -0.002953747810272551\n",
      "26-th iteration, new layer inserted. now 27 layers\n",
      "[1.75773718e+00 4.80427830e+01 8.69499797e+01 4.70895643e+01\n",
      " 8.42634473e+01 8.68159726e+01 1.00583735e+02 8.39917274e+01\n",
      " 1.20239127e+02 8.55682537e+01 1.22775474e+02 1.20318830e-01\n",
      " 3.40684622e-01 7.21014507e+00 1.43534402e+01 7.17357690e+01\n",
      " 1.59050370e+02 6.72413293e+01 7.27280165e+00 1.79755721e+01\n",
      " 1.32217032e+02 1.64472908e+01 1.50389373e+01 6.81006263e+01\n",
      " 3.13909013e+01 1.36130585e+01 6.81159202e+01]\n",
      "27-th iteration, loss: 0.1511823668206394, 10 gd steps\n",
      "insert gradient: -0.0011546413521239475\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[1.83653150e+00 4.83044855e+01 8.69617896e+01 4.68905504e+01\n",
      " 8.42332589e+01 8.68349251e+01 1.00587604e+02 8.39863220e+01\n",
      " 1.20230272e+02 8.55457370e+01 1.22774528e+02 1.11438802e-01\n",
      " 3.37401763e-01 7.20763290e+00 1.43361891e+01 7.17228363e+01\n",
      " 1.59054942e+02 6.72751438e+01 7.30940482e+00 1.79760711e+01\n",
      " 1.32187764e+02 1.64621324e+01 1.50485811e+01 6.81121642e+01\n",
      " 3.13959649e+01 1.36083484e+01 6.81159202e+01]\n",
      "28-th iteration, loss: 0.1511708870622313, 40 gd steps\n",
      "insert gradient: -0.0004001941141242315\n",
      "28-th iteration, new layer inserted. now 29 layers\n",
      "[1.83405359e+00 4.82951178e+01 8.69170295e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.69652722e+01 8.42848350e+01 8.69170179e+01\n",
      " 1.00579055e+02 8.40239049e+01 1.20231648e+02 8.55841797e+01\n",
      " 1.22991621e+02 1.05888686e-01 4.50631716e-01 7.49791390e+00\n",
      " 1.39431516e+01 7.15071690e+01 1.59187000e+02 6.79496454e+01\n",
      " 8.00384831e+00 1.71713055e+01 1.30856251e+02 1.66437012e+01\n",
      " 1.51142717e+01 6.82794903e+01 3.16596742e+01 1.35494927e+01\n",
      " 6.81159202e+01]\n",
      "29-th iteration, loss: 0.15116634118386738, 67 gd steps\n",
      "insert gradient: -0.00034344812309561974\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[1.79039621e+00 4.83117445e+01 8.69882673e+01 4.69792855e+01\n",
      " 8.42765529e+01 8.69686355e+01 1.00587972e+02 8.40435731e+01\n",
      " 1.20223573e+02 8.56012871e+01 1.23074809e+02 5.22197453e-02\n",
      " 4.77022358e-01 7.72743543e+00 1.37647265e+01 7.13601157e+01\n",
      " 1.59301390e+02 6.82942090e+01 8.48811436e+00 1.67599348e+01\n",
      " 1.30090369e+02 1.68397755e+01 1.51088048e+01 6.83264292e+01\n",
      " 3.18942695e+01 1.35008375e+01 6.81159202e+01]\n",
      "30-th iteration, loss: 0.15116418687221797, 17 gd steps\n",
      "insert gradient: -0.0008185293679702667\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[1.77080815e+00 0.00000000e+00 2.22044605e-16 4.83049539e+01\n",
      " 8.69899198e+01 4.70349405e+01 8.42307396e+01 8.69998751e+01\n",
      " 1.00581327e+02 8.40586278e+01 1.20231105e+02 8.56336396e+01\n",
      " 1.23666868e+02 7.92344306e+00 1.36424436e+01 7.12313342e+01\n",
      " 1.59369397e+02 6.85855112e+01 8.78144547e+00 1.64439937e+01\n",
      " 1.29473479e+02 1.69370184e+01 1.51423775e+01 6.83100189e+01\n",
      " 3.20224275e+01 1.34822976e+01 6.81159202e+01]\n",
      "31-th iteration, loss: 0.1511637873518851, 14 gd steps\n",
      "insert gradient: -0.0001422956293168504\n",
      "31-th iteration, new layer inserted. now 25 layers\n",
      "[  1.77910511  48.34606194  86.99380376  47.01973211  84.24721479\n",
      "  87.01084924 100.58370775  84.05808302 120.22611253  85.62252809\n",
      " 123.66183102   7.91835632  13.63097291  71.22097009 159.3736359\n",
      "  68.59846878   8.80538779  16.44281668 129.45752907  16.95133584\n",
      "  15.14526073  68.31550768  32.03024032  13.47843751  68.11592024]\n",
      "32-th iteration, loss: 0.15116207048001257, 25 gd steps\n",
      "insert gradient: -0.00012142762808397631\n",
      "32-th iteration, new layer inserted. now 27 layers\n",
      "[1.75989461e+00 0.00000000e+00 2.77555756e-16 4.83690732e+01\n",
      " 8.70079599e+01 4.70314563e+01 8.42438729e+01 8.70518166e+01\n",
      " 1.00595415e+02 8.40739786e+01 1.20229685e+02 8.56422420e+01\n",
      " 1.23734911e+02 8.04231830e+00 1.35259520e+01 7.11129294e+01\n",
      " 1.59455018e+02 6.88582757e+01 9.13862028e+00 1.61589258e+01\n",
      " 1.28851876e+02 1.70715747e+01 1.51777296e+01 6.82998358e+01\n",
      " 3.21635766e+01 1.34554077e+01 6.81159202e+01]\n",
      "33-th iteration, loss: 0.15116181012317662, 15 gd steps\n",
      "insert gradient: -0.00025739047941796944\n",
      "33-th iteration, new layer inserted. now 29 layers\n",
      "[1.78388678e+00 1.33832836e-02 5.77495566e-03 4.83819283e+01\n",
      " 8.70018507e+01 0.00000000e+00 1.77635684e-14 4.70107840e+01\n",
      " 8.43004458e+01 8.70482114e+01 1.00568906e+02 8.40891685e+01\n",
      " 1.20215483e+02 8.56591446e+01 1.23732591e+02 8.07603908e+00\n",
      " 1.35083357e+01 7.10897599e+01 1.59481192e+02 6.88756766e+01\n",
      " 9.22951105e+00 1.61024408e+01 1.28740431e+02 1.71184420e+01\n",
      " 1.51605591e+01 6.83029672e+01 3.22162971e+01 1.34499449e+01\n",
      " 6.81159202e+01]\n",
      "34-th iteration, loss: 0.1511612283789666, 187 gd steps\n",
      "insert gradient: -5.445407878291502e-05\n",
      "34-th iteration, new layer inserted. now 25 layers\n",
      "[  1.76297453  48.39563699  87.01863874  47.04376858  84.27190729\n",
      "  87.08180097 100.56023867  84.10708537 120.198513    85.67415626\n",
      " 123.72092606   8.13398309  13.48791912  71.04444929 159.53449324\n",
      "  68.90011755   9.3826514   16.02857717 128.57120177  17.20655724\n",
      "  15.12308788  68.29688069  32.31715477  13.42432539  68.11592024]\n",
      "35-th iteration, loss: 0.15116080349585587, 21 gd steps\n",
      "insert gradient: -0.0001324175068279031\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[1.75445300e+00 4.84182463e+01 8.70262339e+01 4.70511529e+01\n",
      " 8.42598270e+01 0.00000000e+00 3.55271368e-15 8.70974537e+01\n",
      " 1.00563058e+02 8.41118513e+01 1.20207124e+02 8.56896106e+01\n",
      " 1.23728166e+02 8.19453741e+00 1.34600971e+01 7.09858950e+01\n",
      " 1.59581924e+02 6.89748658e+01 9.53339275e+00 1.59249252e+01\n",
      " 1.28330111e+02 1.72872338e+01 1.51091995e+01 6.82637974e+01\n",
      " 3.24073132e+01 1.34068507e+01 6.81159202e+01]\n",
      "36-th iteration, loss: 0.15116075267079526, 12 gd steps\n",
      "insert gradient: -8.463115456420709e-05\n",
      "36-th iteration, new layer inserted. now 27 layers\n",
      "[1.75774461e+00 4.84208074e+01 8.70282404e+01 4.70511441e+01\n",
      " 8.42654673e+01 0.00000000e+00 3.55271368e-15 8.71044853e+01\n",
      " 1.00559551e+02 8.41170648e+01 1.20201865e+02 8.56908031e+01\n",
      " 1.23726042e+02 8.20263817e+00 1.34577680e+01 7.09817251e+01\n",
      " 1.59589373e+02 6.89768109e+01 9.55467678e+00 1.59148260e+01\n",
      " 1.28311103e+02 1.72991191e+01 1.51049746e+01 6.82632475e+01\n",
      " 3.24198107e+01 1.34039926e+01 6.81159202e+01]\n",
      "37-th iteration, loss: 0.1511606123747296, 31 gd steps\n",
      "insert gradient: -0.00015078924889457668\n",
      "37-th iteration, new layer inserted. now 27 layers\n",
      "[1.75231350e+00 4.84181027e+01 8.70297941e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.70578175e+01 8.42691108e+01 8.71172245e+01\n",
      " 1.00550088e+02 8.41270191e+01 1.20192434e+02 8.57013489e+01\n",
      " 1.23719252e+02 8.22694582e+00 1.34518042e+01 7.09642503e+01\n",
      " 1.59611231e+02 6.89836392e+01 9.61459638e+00 1.58854138e+01\n",
      " 1.28246748e+02 1.73349509e+01 1.50898162e+01 6.82564021e+01\n",
      " 3.24595824e+01 1.33947288e+01 6.81159202e+01]\n",
      "38-th iteration, loss: 0.15116047747353492, 33 gd steps\n",
      "insert gradient: -0.00038157810846644165\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[1.73961856e+00 0.00000000e+00 2.77555756e-16 4.84100344e+01\n",
      " 8.70339234e+01 4.70784649e+01 8.42695109e+01 8.71269991e+01\n",
      " 1.00547942e+02 8.41328461e+01 1.20192384e+02 8.57135439e+01\n",
      " 1.23711964e+02 8.26234401e+00 1.34396388e+01 7.09315790e+01\n",
      " 1.59641900e+02 6.90060346e+01 9.70447284e+00 1.58311447e+01\n",
      " 1.28127755e+02 1.73899875e+01 1.50690994e+01 6.82357296e+01\n",
      " 3.25214066e+01 1.33818796e+01 6.81159202e+01]\n",
      "39-th iteration, loss: 0.15116038976182838, 27 gd steps\n",
      "insert gradient: -4.120552934827412e-05\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[1.74578805e+00 4.84316190e+01 8.70372149e+01 4.70708905e+01\n",
      " 8.42698012e+01 8.71328773e+01 1.00546171e+02 8.41355446e+01\n",
      " 1.20188299e+02 8.57133247e+01 1.23709338e+02 8.26908178e+00\n",
      " 1.34383834e+01 7.09265479e+01 1.59648896e+02 6.90086106e+01\n",
      " 0.00000000e+00 1.24344979e-14 9.72283701e+00 1.58251707e+01\n",
      " 1.28111810e+02 1.74012427e+01 1.50658192e+01 6.82338367e+01\n",
      " 3.25325197e+01 1.33784770e+01 6.81159202e+01]\n",
      "40-th iteration, loss: 0.15115996966494305, 114 gd steps\n",
      "insert gradient: -3.246766912410096e-05\n",
      "40-th iteration, new layer inserted. now 25 layers\n",
      "[  1.72591294  48.4671201   87.05552084  47.09909177  84.26771577\n",
      "  87.18907262 100.52546296  84.16987092 120.17259343  85.76054913\n",
      " 123.6679479    8.40307943  13.40965499  70.81184404 159.7669703\n",
      "  69.08001888  10.05528583  15.64799399 127.7076491   17.60955105\n",
      "  14.9947625   68.13986303  32.75434537  13.33009883  68.11592024]\n",
      "41-th iteration, loss: 0.1511599616221712, 12 gd steps\n",
      "insert gradient: -2.073392221636157e-05\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[1.73089365e+00 0.00000000e+00 1.66533454e-16 4.84683285e+01\n",
      " 8.70571106e+01 4.70978831e+01 8.42700875e+01 8.71924618e+01\n",
      " 1.00524264e+02 8.41715289e+01 1.20169340e+02 8.57597969e+01\n",
      " 1.23666356e+02 8.40672819e+00 1.34097710e+01 7.08106731e+01\n",
      " 1.59771742e+02 6.90803649e+01 1.00633648e+01 1.56446344e+01\n",
      " 1.27702786e+02 1.76164719e+01 1.49928885e+01 6.81399887e+01\n",
      " 3.27609513e+01 1.33290499e+01 6.81159202e+01]\n",
      "42-th iteration, loss: 0.15115994539916727, 26 gd steps\n",
      "insert gradient: -4.611987893217928e-05\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[1.72831034e+00 4.84724992e+01 8.70583205e+01 0.00000000e+00\n",
      " 2.48689958e-14 4.70996335e+01 8.42692546e+01 8.71990598e+01\n",
      " 1.00519869e+02 8.41763921e+01 1.20164890e+02 8.57645348e+01\n",
      " 1.23660108e+02 8.41741420e+00 1.34103930e+01 7.08016272e+01\n",
      " 1.59782762e+02 6.90803455e+01 1.00861905e+01 1.56350545e+01\n",
      " 1.27682339e+02 1.76333965e+01 1.49845597e+01 6.81336357e+01\n",
      " 3.27799361e+01 1.33244146e+01 6.81159202e+01]\n",
      "43-th iteration, loss: 0.1511599242481924, 42 gd steps\n",
      "insert gradient: -3.965661627768777e-05\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[1.72627995e+00 0.00000000e+00 1.66533454e-16 4.84754234e+01\n",
      " 8.70610648e+01 4.71049392e+01 8.42709080e+01 8.72053562e+01\n",
      " 1.00516271e+02 8.41807971e+01 1.20162274e+02 8.57703971e+01\n",
      " 1.23650728e+02 8.43267708e+00 1.34103612e+01 7.07885380e+01\n",
      " 1.59797586e+02 6.90806716e+01 1.01208548e+01 1.56196933e+01\n",
      " 1.27650285e+02 1.76594934e+01 1.49719541e+01 6.81217628e+01\n",
      " 3.28083327e+01 1.33178425e+01 6.81159202e+01]\n",
      "44-th iteration, loss: 0.1511599125926311, 43 gd steps\n",
      "insert gradient: -2.185504493747339e-05\n",
      "44-th iteration, new layer inserted. now 29 layers\n",
      "[1.72455996e+00 2.55372578e-03 1.85032633e-05 4.84782130e+01\n",
      " 8.70636989e+01 4.71069441e+01 8.42683134e+01 0.00000000e+00\n",
      " 1.06581410e-14 8.72098434e+01 1.00513486e+02 8.41839907e+01\n",
      " 1.20159772e+02 8.57748081e+01 1.23644519e+02 8.44229635e+00\n",
      " 1.34111747e+01 7.07795905e+01 1.59807159e+02 6.90800295e+01\n",
      " 1.01418140e+01 1.56111888e+01 1.27630922e+02 1.76757864e+01\n",
      " 1.49643746e+01 6.81137721e+01 3.28260049e+01 1.33140999e+01\n",
      " 6.81159202e+01]\n",
      "45-th iteration, loss: 0.1511599110107613, 14 gd steps\n",
      "insert gradient: -1.2807951812947844e-05\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[1.72470351e+00 4.84811877e+01 8.70636615e+01 4.71073494e+01\n",
      " 8.42691570e+01 8.72111844e+01 1.00513156e+02 8.41844828e+01\n",
      " 1.20159139e+02 8.57746351e+01 1.23643593e+02 0.00000000e+00\n",
      " 2.48689958e-14 8.44364809e+00 1.34111458e+01 7.07787548e+01\n",
      " 1.59808531e+02 6.90799998e+01 1.01450959e+01 1.56103268e+01\n",
      " 1.27628924e+02 1.76780682e+01 1.49633180e+01 6.81131270e+01\n",
      " 3.28283297e+01 1.33131257e+01 6.81159202e+01]\n",
      "46-th iteration, loss: 0.15115990289673975, 28 gd steps\n",
      "insert gradient: -2.7799473126604442e-05\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[1.72323856e+00 0.00000000e+00 2.77555756e-16 4.84819992e+01\n",
      " 8.70645395e+01 4.71088955e+01 8.42697707e+01 8.72149121e+01\n",
      " 1.00512465e+02 8.41864146e+01 1.20158769e+02 8.57775348e+01\n",
      " 1.23638948e+02 8.45257099e+00 1.34107538e+01 7.07709791e+01\n",
      " 1.59816208e+02 6.90807228e+01 1.01632191e+01 1.56020794e+01\n",
      " 1.27612172e+02 1.76919075e+01 1.49568351e+01 6.81058306e+01\n",
      " 3.28428721e+01 1.33096685e+01 6.81159202e+01]\n",
      "47-th iteration, loss: 0.15115989836080745, 42 gd steps\n",
      "insert gradient: -1.09695895034717e-05\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[  1.72311261  48.48494541  87.06563462  47.10994182  84.26950203\n",
      "  87.21723116 100.51045526  84.18835166 120.15671575  85.77947887\n",
      " 123.6358927    8.45668304  13.41095071  70.76763087 159.82112852\n",
      "  69.07986228  10.17417855  15.59769698 127.60366525  17.70025206\n",
      "  14.952579    68.10252656  32.85192081  13.3075982   68.11592024]\n",
      "48-th iteration, loss: 0.15115989079247744, 35 gd steps\n",
      "insert gradient: -2.3808007046974935e-05\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[1.72175503e+00 0.00000000e+00 3.88578059e-16 4.84860173e+01\n",
      " 8.70668149e+01 4.71125581e+01 8.42698242e+01 8.72214828e+01\n",
      " 1.00509288e+02 8.41907739e+01 1.20155724e+02 8.57828742e+01\n",
      " 1.23630298e+02 8.46566056e+00 1.34107914e+01 7.07604582e+01\n",
      " 1.59829957e+02 6.90816923e+01 1.01965690e+01 1.55868122e+01\n",
      " 1.27582963e+02 1.77175497e+01 1.49448221e+01 6.80928492e+01\n",
      " 3.28696194e+01 1.33035842e+01 6.81159202e+01]\n",
      "49-th iteration, loss: 0.15115988959049323, 17 gd steps\n",
      "insert gradient: -1.0563525183589761e-05\n",
      "49-th iteration, new layer inserted. now 27 layers\n",
      "[1.72167849e+00 4.84877839e+01 8.70672772e+01 4.71124787e+01\n",
      " 8.42695668e+01 8.72221721e+01 1.00508663e+02 8.41913556e+01\n",
      " 1.20154979e+02 8.57834139e+01 1.23629362e+02 0.00000000e+00\n",
      " 2.48689958e-14 8.46693307e+00 1.34109540e+01 7.07592569e+01\n",
      " 1.59831316e+02 6.90813809e+01 1.01995546e+01 1.55857966e+01\n",
      " 1.27580716e+02 1.77198542e+01 1.49437189e+01 6.80919762e+01\n",
      " 3.28721566e+01 1.33030762e+01 6.81159202e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5347213725423434\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.08692055    0.         1489.57639604]\n",
      "1-th iteration, loss: 0.7465803216307183, 11 gd steps\n",
      "insert gradient: -0.6290973564541312\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.94643294   62.36725652  236.15235547    0.         1253.42404057]\n",
      "2-th iteration, loss: 0.6056758138893144, 13 gd steps\n",
      "insert gradient: -0.5879418905063067\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.62174586   77.45692066  221.34199269   40.89571699  230.22074215\n",
      "    0.         1023.20329842]\n",
      "3-th iteration, loss: 0.4605770108490111, 24 gd steps\n",
      "insert gradient: -0.6821622599395638\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          54.75708752 229.90455537  43.54340929 151.01809575\n",
      "  61.7026071  350.81255946   0.         672.39073896]\n",
      "4-th iteration, loss: 0.3532792452386083, 19 gd steps\n",
      "insert gradient: -0.35763578338271246\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.29899423  56.29929976 208.09036993  56.36260589 135.89207507\n",
      "  51.73022763 332.13052424  44.34756659 572.77729615   0.\n",
      "  99.61344281]\n",
      "5-th iteration, loss: 0.30890370084888596, 13 gd steps\n",
      "insert gradient: -0.2338208263110318\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.54924495  54.20896979 206.53819089  64.34783264 119.19695262\n",
      "  61.77847573 217.10090967   0.         101.31375785  41.68553363\n",
      " 560.14201581  31.86207199  99.61344281]\n",
      "6-th iteration, loss: 0.27073152669120604, 33 gd steps\n",
      "insert gradient: -0.10133966620041235\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[2.66259246e-01 6.06472546e+01 1.93205689e+02 7.25611067e+01\n",
      " 1.06703497e+02 5.75047213e+01 1.86446959e+02 2.70117512e+01\n",
      " 7.18535396e+01 5.01390239e+01 4.15596428e+02 0.00000000e+00\n",
      " 1.18741837e+02 4.47056441e+01 9.96134428e+01]\n",
      "7-th iteration, loss: 0.19038753683265267, 26 gd steps\n",
      "insert gradient: -0.08459807969604308\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.04805289  61.18840096 157.88036077  98.90640329 109.3625909\n",
      "  74.30391623 111.5232871   46.52922796  75.37572606  50.6223305\n",
      " 244.44524031   0.         146.66714418  39.44681754  66.76825484\n",
      "  52.70347001  99.61344281]\n",
      "8-th iteration, loss: 0.16690686151441675, 53 gd steps\n",
      "insert gradient: -0.0014466795278958067\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  1.11134786  67.28636367 136.60938107 105.29166526 107.84739385\n",
      "  75.39032191 105.26610727  46.03412414  71.41964572  42.37586087\n",
      " 204.46407307  40.93532925 129.16864312  35.94470504  71.8254103\n",
      "  55.4377623   99.61344281]\n",
      "9-th iteration, loss: 0.16570334068071235, 21 gd steps\n",
      "insert gradient: -0.008278546971436785\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  1.18366582  68.84552856 133.11080531 105.77979752 106.72795277\n",
      "  74.67034457 104.39462871  46.26529419  69.78104199  42.40551008\n",
      " 199.00308498  47.87402086 122.35895799  35.65569445  73.8197668\n",
      "  55.70068182  99.61344281]\n",
      "10-th iteration, loss: 0.16533418827029567, 16 gd steps\n",
      "insert gradient: -0.00538375170367673\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[1.06791925e+00 6.86359829e+01 1.31880888e+02 0.00000000e+00\n",
      " 2.48689958e-14 1.05191008e+02 1.07092972e+02 7.47584483e+01\n",
      " 1.04496045e+02 4.69615871e+01 6.93494931e+01 4.25607919e+01\n",
      " 1.96583613e+02 5.00574886e+01 1.19981552e+02 3.53980295e+01\n",
      " 7.44420575e+01 5.60553040e+01 9.96134428e+01]\n",
      "11-th iteration, loss: 0.1650896342661019, 18 gd steps\n",
      "insert gradient: -0.0014841081648908755\n",
      "11-th iteration, new layer inserted. now 17 layers\n",
      "[  1.01924871  68.86285866 131.47443703 105.49985345 107.05031058\n",
      "  74.63916319 104.24550142  46.52802508  68.95341673  42.68988758\n",
      " 195.39337175  51.10158065 118.52382686  35.18748766  74.78072349\n",
      "  56.35264795  99.61344281]\n",
      "12-th iteration, loss: 0.1649304151938072, 17 gd steps\n",
      "insert gradient: -0.0005692439812912391\n",
      "12-th iteration, new layer inserted. now 17 layers\n",
      "[  0.93996719  68.75455649 130.84187381 105.37966161 107.17078171\n",
      "  74.63688853 104.24338697  46.632373    68.72887538  42.75070855\n",
      " 194.31574113  51.8928766  117.52719092  34.98238103  75.05825345\n",
      "  56.68245209  99.61344281]\n",
      "13-th iteration, loss: 0.16479798533871745, 18 gd steps\n",
      "WARNING: positive grad everywhere, min grad: 9.218764504224788e-18\n",
      "13-th iteration, new layer inserted. now 17 layers\n",
      "[  0.86026542  68.77196436 130.08815474 105.43269933 107.25884371\n",
      "  74.76655116 104.12475512  46.51513842  68.39619042  42.89630643\n",
      " 193.09324247  52.98271113 116.34061686  34.6767841   75.42210763\n",
      "  57.06377048  99.61344281]\n",
      "14-th iteration, loss: 0.16469307484318685, 17 gd steps\n",
      "insert gradient: -0.004330234661357758\n",
      "14-th iteration, new layer inserted. now 17 layers\n",
      "[  0.60733192  68.95585357 128.43784548 105.7153716  106.65870787\n",
      "  75.34581887 103.63924554  46.737579    67.99121915  42.80986687\n",
      " 191.14495696  55.56621566 113.36708202  34.29156808  76.4737346\n",
      "  57.55492995  99.61344281]\n",
      "15-th iteration, loss: 0.1646098358125857, 18 gd steps\n",
      "insert gradient: -0.005056613674786949\n",
      "15-th iteration, new layer inserted. now 19 layers\n",
      "[  0.71685527  69.25955375 128.51069684 105.52038905 106.92028023\n",
      "  74.95601971 103.65556206  46.61861792  67.95402261  42.85019922\n",
      " 122.63407139   0.          68.13003966  55.54754827 113.20737289\n",
      "  34.21188864  76.4828928   57.61314884  99.61344281]\n",
      "16-th iteration, loss: 0.16041154541370226, 20 gd steps\n",
      "insert gradient: -0.021227667518864155\n",
      "16-th iteration, new layer inserted. now 21 layers\n",
      "[1.99313572e+00 6.66175644e+01 1.36004245e+02 0.00000000e+00\n",
      " 4.26325641e-14 1.01573922e+02 1.06927840e+02 7.56752232e+01\n",
      " 1.06460392e+02 4.37495896e+01 7.80013574e+01 4.55448020e+01\n",
      " 8.77210337e+01 1.77265666e+01 3.64808125e+01 6.78032324e+01\n",
      " 1.20167440e+02 2.82719516e+01 7.61607112e+01 5.81588382e+01\n",
      " 9.96134428e+01]\n",
      "17-th iteration, loss: 0.15662331495552378, 889 gd steps\n",
      "insert gradient: -0.0022235070397310457\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[1.33087962e+00 6.67934956e+01 1.40187862e+02 9.93334382e+01\n",
      " 1.04238433e+02 7.84036293e+01 1.03647276e+02 4.47287329e+01\n",
      " 7.68498923e+01 4.41150804e+01 9.95668009e+01 2.43893898e+01\n",
      " 7.68917804e+00 0.00000000e+00 1.55431223e-15 7.72499917e+01\n",
      " 1.24489718e+02 2.37030960e+01 7.46930329e+01 5.87432203e+01\n",
      " 9.96134428e+01]\n",
      "18-th iteration, loss: 0.15617307165046723, 15 gd steps\n",
      "insert gradient: -0.003737605789262782\n",
      "18-th iteration, new layer inserted. now 19 layers\n",
      "[  1.3568414   66.80177807 141.37041361  98.80914429 103.15873862\n",
      "  79.34093003 102.88110386  44.73239572  77.4775513   43.92465488\n",
      "  99.80886198  24.88748482   2.70059303  80.87347939 125.68374159\n",
      "  23.00353916  74.09110126  58.78111319  99.61344281]\n",
      "19-th iteration, loss: 0.15590297620768798, 51 gd steps\n",
      "insert gradient: -0.0005423840122660374\n",
      "19-th iteration, new layer inserted. now 17 layers\n",
      "[  1.31199034  66.15401235 142.19774118  98.47959619 103.20874484\n",
      "  79.72027842 102.73087252  44.72708595  77.47495207  43.92192316\n",
      " 100.35943593 107.19280423 126.2557781   22.44388281  73.69664199\n",
      "  59.15705415  99.61344281]\n",
      "20-th iteration, loss: 0.15585675681392933, 83 gd steps\n",
      "insert gradient: -0.0009344809955239808\n",
      "20-th iteration, new layer inserted. now 17 layers\n",
      "[  1.51875898  64.12757781 146.13862931  98.01655749 103.45665928\n",
      "  79.38918115 103.47160237  45.33650483  77.11842391  43.76731459\n",
      " 100.63344717 107.87169533 123.41914555  23.94861339  71.45434086\n",
      "  60.90760817  99.61344281]\n",
      "21-th iteration, loss: 0.15584746788387308, 68 gd steps\n",
      "insert gradient: -0.00031749810994768056\n",
      "21-th iteration, new layer inserted. now 19 layers\n",
      "[1.49774420e+00 6.39659575e+01 1.46578516e+02 9.80964448e+01\n",
      " 1.03315361e+02 0.00000000e+00 2.30926389e-14 7.94891833e+01\n",
      " 1.03503863e+02 4.52597781e+01 7.73879708e+01 4.38472399e+01\n",
      " 1.00643078e+02 1.08215316e+02 1.22286531e+02 2.43024811e+01\n",
      " 7.12294594e+01 6.14421487e+01 9.96134428e+01]\n",
      "22-th iteration, loss: 0.1558422150260635, 364 gd steps\n",
      "insert gradient: -7.778794437427051e-05\n",
      "22-th iteration, new layer inserted. now 17 layers\n",
      "[  1.49552106  63.72702415 147.06938432  98.16376683 103.19499048\n",
      "  79.59537217 103.47641502  45.38010176  77.38174614  43.85793195\n",
      " 100.96833108 108.47564347 121.00596236  24.84238457  70.47779933\n",
      "  62.31129414  99.61344281]\n",
      "23-th iteration, loss: 0.15584026117493116, 39 gd steps\n",
      "insert gradient: -0.00022927236001650032\n",
      "23-th iteration, new layer inserted. now 19 layers\n",
      "[1.49481397e+00 6.36542862e+01 1.47233290e+02 9.82001737e+01\n",
      " 1.03126151e+02 7.96557981e+01 1.03429192e+02 4.54603100e+01\n",
      " 7.73316109e+01 4.38570437e+01 1.01191907e+02 0.00000000e+00\n",
      " 2.30926389e-14 1.08543219e+02 1.20407446e+02 2.51756257e+01\n",
      " 6.98887520e+01 6.28238205e+01 9.96134428e+01]\n",
      "24-th iteration, loss: 0.15583918181477535, 47 gd steps\n",
      "insert gradient: -0.00010366276098057555\n",
      "24-th iteration, new layer inserted. now 21 layers\n",
      "[1.49125130e+00 6.35996285e+01 1.47369936e+02 9.82273199e+01\n",
      " 1.03079745e+02 7.96879738e+01 1.03424614e+02 4.54931937e+01\n",
      " 7.73493531e+01 0.00000000e+00 1.77635684e-15 4.38562495e+01\n",
      " 1.01278523e+02 2.37888169e-02 2.05331723e-03 1.08621470e+02\n",
      " 1.19990206e+02 2.53415864e+01 6.96780867e+01 6.31124025e+01\n",
      " 9.96134428e+01]\n",
      "25-th iteration, loss: 0.15583786894178345, 91 gd steps\n",
      "insert gradient: -0.00012584184973178755\n",
      "25-th iteration, new layer inserted. now 19 layers\n",
      "[1.48714336e+00 6.35285504e+01 1.47560121e+02 9.82856427e+01\n",
      " 1.02991555e+02 0.00000000e+00 2.66453526e-14 7.97419211e+01\n",
      " 1.03378293e+02 4.55637199e+01 7.73497563e+01 4.38603939e+01\n",
      " 1.01482684e+02 1.08753012e+02 1.19387869e+02 2.56172783e+01\n",
      " 6.92690522e+01 6.35719148e+01 9.96134428e+01]\n",
      "26-th iteration, loss: 0.15583753164686834, 45 gd steps\n",
      "insert gradient: -0.00011596858856042403\n",
      "26-th iteration, new layer inserted. now 19 layers\n",
      "[1.48344542e+00 6.34953217e+01 1.47603390e+02 9.83012626e+01\n",
      " 1.02982358e+02 7.97608280e+01 1.03371572e+02 0.00000000e+00\n",
      " 2.13162821e-14 4.55767218e+01 7.73430162e+01 4.38661735e+01\n",
      " 1.01520603e+02 1.08784621e+02 1.19218868e+02 2.56961249e+01\n",
      " 6.91375340e+01 6.37216801e+01 9.96134428e+01]\n",
      "27-th iteration, loss: 0.15583718537041158, 39 gd steps\n",
      "insert gradient: -4.528351270653621e-05\n",
      "27-th iteration, new layer inserted. now 19 layers\n",
      "[1.48359453e+00 6.34831231e+01 1.47660955e+02 9.83146051e+01\n",
      " 1.02959992e+02 7.97825670e+01 1.03356166e+02 4.55997798e+01\n",
      " 7.73444427e+01 4.38671033e+01 1.01583711e+02 1.08823656e+02\n",
      " 1.19012355e+02 2.58003010e+01 6.89736133e+01 0.00000000e+00\n",
      " 3.19744231e-14 6.38916942e+01 9.96134428e+01]\n",
      "28-th iteration, loss: 0.1558368263989348, 302 gd steps\n",
      "insert gradient: -1.915230570824571e-05\n",
      "28-th iteration, new layer inserted. now 17 layers\n",
      "[  1.48023719  63.44940276 147.72693736  98.33713636 102.92442827\n",
      "  79.81373664 103.34012107  45.62974196  77.34318953  43.86813945\n",
      " 101.64931488 108.87561791 118.76164011  25.91051235  68.80253707\n",
      "  64.10371074  99.61344281]\n",
      "29-th iteration, loss: 0.15583648852216248, 72 gd steps\n",
      "insert gradient: -0.00011130184905240676\n",
      "29-th iteration, new layer inserted. now 19 layers\n",
      "[1.47914821e+00 6.34233287e+01 1.47808957e+02 9.83629891e+01\n",
      " 1.02888558e+02 7.98371394e+01 1.03318898e+02 4.56758955e+01\n",
      " 7.73234392e+01 0.00000000e+00 8.88178420e-15 4.38590268e+01\n",
      " 1.01768584e+02 1.08915753e+02 1.18450241e+02 2.61009365e+01\n",
      " 6.84767660e+01 6.43658320e+01 9.96134428e+01]\n",
      "30-th iteration, loss: 0.15583624270415314, 999 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.525828302794489e-06\n",
      "30-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472141e+00 6.33779667e+01 1.47918381e+02 9.83983294e+01\n",
      " 1.02844223e+02 7.98822823e+01 1.03304306e+02 4.57102286e+01\n",
      " 7.73403220e+01 4.38675894e+01 1.01849220e+02 1.08999629e+02\n",
      " 1.18081865e+02 2.62794692e+01 6.82171376e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46667758e+01 9.96134428e+01]\n",
      "31-th iteration, loss: 0.1558362427004124, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.508168294556888e-06\n",
      "31-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472135  63.3779658  147.9183831   98.39833017 102.84422222\n",
      "  79.88228308 103.30430571  45.71022946  77.34032186  43.86758916\n",
      " 101.84922205 108.99963087 118.08185623  26.27947399  68.21712965\n",
      "  64.66679078  99.61344281]\n",
      "32-th iteration, loss: 0.15583624269739701, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.50787385493233e-06\n",
      "32-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472128  63.37796494 147.91838541  98.39833095 102.84422126\n",
      "  79.88228391 103.30430524  45.71023036  77.34032172  43.86758893\n",
      " 101.8492243  108.99963223 118.08184751  26.27947873  68.21712167\n",
      "  64.66679828  99.61344281]\n",
      "33-th iteration, loss: 0.15583624269438215, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.507558327014839e-06\n",
      "33-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472122e+00 6.33779641e+01 1.47918388e+02 9.83983317e+01\n",
      " 1.02844220e+02 7.98822847e+01 1.03304305e+02 4.57102313e+01\n",
      " 7.73403216e+01 4.38675887e+01 1.01849227e+02 1.08999634e+02\n",
      " 1.18081839e+02 2.62794835e+01 6.82171137e+01 0.00000000e+00\n",
      " 3.19744231e-14 6.46668058e+01 9.96134428e+01]\n",
      "34-th iteration, loss: 0.15583624269064633, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.4900666167450075e-06\n",
      "34-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472115e+00 6.33779632e+01 1.47918390e+02 9.83983325e+01\n",
      " 1.02844219e+02 7.98822856e+01 1.03304304e+02 4.57102322e+01\n",
      " 7.73403215e+01 4.38675885e+01 1.01849229e+02 1.08999635e+02\n",
      " 1.18081830e+02 2.62794882e+01 6.82171057e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46668208e+01 9.96134428e+01]\n",
      "35-th iteration, loss: 0.1558362426869143, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.472780543183945e-06\n",
      "35-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472108e+00 6.33779623e+01 1.47918392e+02 9.83983333e+01\n",
      " 1.02844218e+02 7.98822864e+01 1.03304304e+02 4.57102331e+01\n",
      " 7.73403214e+01 4.38675883e+01 1.01849231e+02 1.08999636e+02\n",
      " 1.18081821e+02 2.62794930e+01 6.82170977e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46668357e+01 9.96134428e+01]\n",
      "36-th iteration, loss: 0.15583624268318602, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.455695156849693e-06\n",
      "36-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472101e+00 6.33779615e+01 1.47918395e+02 9.83983341e+01\n",
      " 1.02844217e+02 7.98822873e+01 1.03304303e+02 4.57102341e+01\n",
      " 7.73403212e+01 4.38675881e+01 1.01849233e+02 1.08999638e+02\n",
      " 1.18081813e+02 2.62794977e+01 6.82170897e+01 0.00000000e+00\n",
      " 1.95399252e-14 6.46668507e+01 9.96134428e+01]\n",
      "37-th iteration, loss: 0.15583624267946133, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.4388057109898795e-06\n",
      "37-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472095  63.3779606  147.91839692  98.39833487 102.84421644\n",
      "  79.8822881  103.30430293  45.71023505  77.34032114  43.867588\n",
      " 101.84923566 108.99963921 118.08180393  26.27950241  68.2170817\n",
      "  64.66686556  99.61344281]\n",
      "38-th iteration, loss: 0.15583624267644858, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.439110943611246e-06\n",
      "38-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472088  63.37795973 147.91839922  98.39833566 102.84421548\n",
      "  79.88228897 103.30430248  45.71023601  77.34032105  43.86758785\n",
      " 101.84923795 108.9996406  118.0817952   26.27950709  68.21707367\n",
      "  64.66687299  99.61344281]\n",
      "39-th iteration, loss: 0.15583624267343624, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.439383093086407e-06\n",
      "39-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472081e+00 6.33779589e+01 1.47918402e+02 9.83983365e+01\n",
      " 1.02844215e+02 7.98822898e+01 1.03304302e+02 4.57102370e+01\n",
      " 7.73403210e+01 4.38675877e+01 1.01849240e+02 1.08999642e+02\n",
      " 1.18081786e+02 2.62795118e+01 6.82170656e+01 0.00000000e+00\n",
      " 2.13162821e-14 6.46668804e+01 9.96134428e+01]\n",
      "40-th iteration, loss: 0.15583624266971582, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.422619926640655e-06\n",
      "40-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472074  63.37795797 147.91840381  98.39833724 102.84421356\n",
      "  79.88229068 103.30430157  45.71023795  77.34032086  43.86758758\n",
      " 101.84924254 108.99964343 118.08177774  26.27951648  68.21705762\n",
      "  64.66689528  99.61344281]\n",
      "41-th iteration, loss: 0.1558362426667042, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.423014893411911e-06\n",
      "41-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472067e+00 6.33779571e+01 1.47918406e+02 9.83983380e+01\n",
      " 1.02844213e+02 7.98822915e+01 1.03304301e+02 4.57102389e+01\n",
      " 7.73403208e+01 4.38675875e+01 1.01849245e+02 1.08999645e+02\n",
      " 1.18081769e+02 2.62795212e+01 6.82170496e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46669027e+01 9.96134428e+01]\n",
      "42-th iteration, loss: 0.1558362426629876, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.4064094831123264e-06\n",
      "42-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472060e+00 6.33779562e+01 1.47918408e+02 9.83983388e+01\n",
      " 1.02844212e+02 7.98822924e+01 1.03304301e+02 4.57102399e+01\n",
      " 7.73403207e+01 4.38675874e+01 1.01849247e+02 1.08999646e+02\n",
      " 1.18081760e+02 2.62795259e+01 6.82170415e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46669175e+01 9.96134428e+01]\n",
      "43-th iteration, loss: 0.15583624265927437, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.389992861209061e-06\n",
      "43-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472054e+00 6.33779553e+01 1.47918411e+02 9.83983396e+01\n",
      " 1.02844211e+02 7.98822933e+01 1.03304300e+02 4.57102409e+01\n",
      " 7.73403206e+01 4.38675873e+01 1.01849249e+02 1.08999648e+02\n",
      " 1.18081752e+02 2.62795305e+01 6.82170335e+01 0.00000000e+00\n",
      " 1.24344979e-14 6.46669323e+01 9.96134428e+01]\n",
      "44-th iteration, loss: 0.1558362426555645, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.373760576623626e-06\n",
      "44-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472047  63.37795445 147.91841298  98.39834041 102.84420973\n",
      "  79.88229415 103.30429976  45.7102419   77.34032056  43.86758719\n",
      " 101.8492518  108.99964916 118.08174282  26.27953519  68.21702545\n",
      "  64.66694708  99.61344281]\n",
      "45-th iteration, loss: 0.15583624265255402, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.374566046293105e-06\n",
      "45-th iteration, new layer inserted. now 17 layers\n",
      "[  1.4747204   63.37795357 147.91841528  98.39834122 102.84420878\n",
      "  79.88229504 103.30429931  45.7102429   77.3403205   43.86758711\n",
      " 101.84925413 108.99965059 118.08173407  26.27953983  68.21701737\n",
      "  64.66695445  99.61344281]\n",
      "46-th iteration, loss: 0.15583624264954393, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.375330338086959e-06\n",
      "46-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472033  63.37795269 147.91841757  98.39834202 102.84420782\n",
      "  79.88229592 103.30429886  45.7102439   77.34032044  43.86758704\n",
      " 101.84925646 108.99965203 118.08172532  26.27954447  68.21700931\n",
      "  64.66696182  99.61344281]\n",
      "47-th iteration, loss: 0.15583624264653417, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.376055745421084e-06\n",
      "47-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472026  63.3779518  147.91841986  98.39834281 102.84420687\n",
      "  79.8822968  103.30429841  45.71024491  77.34032037  43.86758697\n",
      " 101.84925879 108.99965348 118.08171658  26.27954912  68.21700124\n",
      "  64.66696919  99.61344281]\n",
      "48-th iteration, loss: 0.15583624264352483, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.376744410639383e-06\n",
      "48-th iteration, new layer inserted. now 17 layers\n",
      "[  1.47472019  63.37795091 147.91842215  98.3983436  102.84420591\n",
      "  79.88229767 103.30429796  45.71024591  77.34032032  43.86758691\n",
      " 101.84926112 108.99965493 118.08170785  26.27955377  68.21699318\n",
      "  64.66697656  99.61344281]\n",
      "49-th iteration, loss: 0.15583624264051582, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.377398334561133e-06\n",
      "49-th iteration, new layer inserted. now 19 layers\n",
      "[1.47472012e+00 6.33779500e+01 1.47918424e+02 9.83983444e+01\n",
      " 1.02844205e+02 7.98822985e+01 1.03304298e+02 4.57102469e+01\n",
      " 7.73403203e+01 4.38675869e+01 1.01849263e+02 1.08999656e+02\n",
      " 1.18081699e+02 2.62795584e+01 6.82169851e+01 0.00000000e+00\n",
      " 2.13162821e-14 6.46669839e+01 9.96134428e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5365154209642933\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.79326351    0.         1513.99568122]\n",
      "1-th iteration, loss: 0.7479635372963357, 11 gd steps\n",
      "insert gradient: -0.6236822564482586\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.45607731   62.35062607  240.02370556    0.         1273.97197566]\n",
      "2-th iteration, loss: 0.6072394623611427, 13 gd steps\n",
      "insert gradient: -0.6505932837691593\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.30418759   76.9559106   224.37218523   39.49831897  233.99485267\n",
      "    0.         1039.97712299]\n",
      "3-th iteration, loss: 0.48673777084899666, 13 gd steps\n",
      "insert gradient: -0.6358190169139886\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           69.70511789  107.83187478    0.          114.17492623\n",
      "   39.11525261  182.68551666   47.22760227 1039.97712299]\n",
      "4-th iteration, loss: 0.42582578622330536, 22 gd steps\n",
      "insert gradient: -0.6084278812274673\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          66.99927476  78.31156058  23.50593372  79.61064225\n",
      "  59.69499116 147.46245735  55.51814918 616.28273955   0.\n",
      " 423.69438344]\n",
      "5-th iteration, loss: 0.31627464207095685, 43 gd steps\n",
      "insert gradient: -0.41194930706161487\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.22427892  52.66087434  76.35816986  22.69543878  90.91118521\n",
      "  64.54170143 122.37575722  70.50820515 170.60281261   0.\n",
      " 365.5774556   71.510542   423.69438344]\n",
      "6-th iteration, loss: 0.2547707016925907, 18 gd steps\n",
      "insert gradient: -0.22229178630565322\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  3.58823564  57.64379916  94.49221503  22.67584848  83.56464266\n",
      "  58.50491352 130.95995125  69.1620664  140.72564472  53.8226592\n",
      " 204.82223716   0.         102.41111858  80.53767699 423.69438344]\n",
      "7-th iteration, loss: 0.22937817589505718, 87 gd steps\n",
      "insert gradient: -0.14484944138097317\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54068007  58.27475626  88.20623701  29.42710256  80.23305455\n",
      "  57.72876868 144.75878648  64.13006585 145.38997056  66.55092072\n",
      " 166.68446116  30.60355032  52.95860339  98.33474745 211.84719172\n",
      "   0.         211.84719172]\n",
      "8-th iteration, loss: 0.1914826188611355, 346 gd steps\n",
      "insert gradient: -0.06358982647506846\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  3.57356423  51.75936155  86.99239272  34.29844173  73.40607981\n",
      "  54.76464443 143.01404755  64.25003225 141.26397409  67.53742929\n",
      " 146.98107123 162.19947574 153.96786797  72.49553365 198.60674224\n",
      "   0.          13.24044948]\n",
      "9-th iteration, loss: 0.18566991888851556, 57 gd steps\n",
      "insert gradient: -0.026324255173088777\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.44170556  52.73625917  87.64160366  35.14796521  70.4348411\n",
      "  54.79091797 142.15361451  64.31443237 141.25263374  67.47189617\n",
      " 149.30178016 128.45755545   0.          35.03387876 137.72219805\n",
      "  81.80740158 182.52219852  15.88909837  13.24044948]\n",
      "10-th iteration, loss: 0.17702590305486998, 46 gd steps\n",
      "insert gradient: -0.05275975978094511\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          50.89370877  85.57403657  41.68082286  71.49487754\n",
      "  50.9762726  143.43962203  64.73311979 143.96961296  68.60630435\n",
      " 146.81339858 101.04374355  63.76743065   7.15297429 161.34537772\n",
      "  88.59829857 181.31493574  24.1130438   13.24044948]\n",
      "11-th iteration, loss: 0.17222670556759692, 25 gd steps\n",
      "insert gradient: -0.02544343891347576\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  2.51112573  50.90771625  85.58052789  39.02160657  76.36368607\n",
      "  53.84695484 143.30639857  65.27065736 143.0936478   66.76307689\n",
      " 147.08561979  95.10811912  71.56550239   2.40424091  98.27238868\n",
      "   0.          70.19456335  93.20623111 180.90417349  27.5452929\n",
      "  13.24044948]\n",
      "12-th iteration, loss: 0.16990852114027774, 199 gd steps\n",
      "insert gradient: -0.014663561654905864\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[2.64199052e+00 5.11253872e+01 8.60464167e+01 3.95596226e+01\n",
      " 7.66359126e+01 5.35904591e+01 1.44011979e+02 6.48463340e+01\n",
      " 1.44099580e+02 6.58303105e+01 1.47035977e+02 9.06095994e+01\n",
      " 1.71462513e+02 3.19208509e+00 7.10704232e+01 9.52129801e+01\n",
      " 1.81275150e+02 2.96034216e+01 1.32404495e+01 0.00000000e+00\n",
      " 1.77635684e-15]\n",
      "13-th iteration, loss: 0.15200954724711352, 106 gd steps\n",
      "insert gradient: -0.011263467598411837\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[3.20749183e+00 5.07828929e+01 8.75825635e+01 4.22662167e+01\n",
      " 7.93953847e+01 5.09624924e+01 1.51435636e+02 6.03821846e+01\n",
      " 1.50246477e+02 6.52519893e+01 1.49660512e+02 8.25651785e+01\n",
      " 1.37871367e+02 2.94817220e+01 1.35180486e+01 7.68083118e+01\n",
      " 0.00000000e+00 4.38904639e+01 1.69534464e+02 7.54090378e+01\n",
      " 3.21951087e+00 1.66416103e+01 6.88483500e-14]\n",
      "14-th iteration, loss: 0.14735147952438077, 53 gd steps\n",
      "insert gradient: -0.03658460471403731\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[2.73225500e+00 5.32792762e+01 8.99361783e+01 4.41436306e+01\n",
      " 8.21175687e+01 5.09399768e+01 1.58813227e+02 5.99780005e+01\n",
      " 1.54790408e+02 6.42397302e+01 1.56438726e+02 7.52384871e+01\n",
      " 4.45315606e+01 0.00000000e+00 1.03906975e+02 4.36310015e+01\n",
      " 7.61028067e+00 5.75220329e+01 3.37454447e+01 2.55304437e+01\n",
      " 1.86332270e+02 7.24154351e+01 7.26750222e+00 2.01403389e+01\n",
      " 1.24823884e-13]\n",
      "15-th iteration, loss: 0.1452240527983237, 52 gd steps\n",
      "insert gradient: -0.02937061986595179\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[2.45972663e+00 5.31763575e+01 9.02269097e+01 4.50585099e+01\n",
      " 8.28474854e+01 5.29296047e+01 1.56529524e+02 6.28591974e+01\n",
      " 1.54261061e+02 6.67585750e+01 1.57728432e+02 6.75021284e+01\n",
      " 3.74904662e+01 5.91835845e+00 1.07438198e+02 4.82570302e+01\n",
      " 3.21816227e+00 5.92902857e+01 3.91684929e+01 2.29250743e+01\n",
      " 1.27316775e+02 0.00000000e+00 6.36583876e+01 6.53277099e+01\n",
      " 1.17787879e+01 2.29903654e+01 1.78672015e-13]\n",
      "16-th iteration, loss: 0.14254188382681007, 17 gd steps\n",
      "insert gradient: -0.011818516668682388\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[2.54622777e+00 5.15203679e+01 8.65683996e+01 4.82796697e+01\n",
      " 8.73904946e+01 5.17805619e+01 1.57161854e+02 6.48655720e+01\n",
      " 1.54903932e+02 6.62168472e+01 1.57498520e+02 6.74798415e+01\n",
      " 3.68676245e+01 4.97254431e+00 1.10283648e+02 1.13127869e+02\n",
      " 4.94295821e+01 1.68561594e+01 1.13316574e+02 1.11030825e+01\n",
      " 5.11408086e+01 6.19965013e+01 1.44067070e+01 2.56839448e+01\n",
      " 1.95949956e-13]\n",
      "17-th iteration, loss: 0.14094466888594578, 21 gd steps\n",
      "insert gradient: -0.004456439716572852\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[2.06097938e+00 5.15012927e+01 8.90298890e+01 4.68354104e+01\n",
      " 8.66567526e+01 5.59637868e+01 1.55832793e+02 6.32261974e+01\n",
      " 1.54822268e+02 6.74846209e+01 1.58829270e+02 6.83461555e+01\n",
      " 3.83709631e+01 4.41354898e+00 1.11714036e+02 1.10939991e+02\n",
      " 0.00000000e+00 2.48689958e-14 5.39568431e+01 1.57026671e+01\n",
      " 1.10786272e+02 1.36377822e+01 4.96973447e+01 6.16664967e+01\n",
      " 1.58774737e+01 2.62431863e+01 2.01581083e-13]\n",
      "18-th iteration, loss: 0.13913983193535637, 22 gd steps\n",
      "insert gradient: -0.006637769978726757\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[2.33135893e+00 5.13566996e+01 8.95182145e+01 4.74332087e+01\n",
      " 8.81175195e+01 5.64591813e+01 1.57369655e+02 6.34756093e+01\n",
      " 1.54731538e+02 6.78513174e+01 1.59779541e+02 6.75454884e+01\n",
      " 3.99552804e+01 0.00000000e+00 5.32907052e-15 3.87722750e+00\n",
      " 1.13414700e+02 1.07063878e+02 7.83683120e+01 9.68622569e+00\n",
      " 1.00349980e+02 2.08937537e+01 4.16229632e+01 5.88935958e+01\n",
      " 2.04032671e+01 2.60761017e+01 2.55995182e-13]\n",
      "19-th iteration, loss: 0.1378039446461179, 58 gd steps\n",
      "insert gradient: -0.007360279550022456\n",
      "19-th iteration, new layer inserted. now 27 layers\n",
      "[1.97784868e+00 5.11289714e+01 9.00875319e+01 4.80531023e+01\n",
      " 9.01722839e+01 5.68695715e+01 1.58231853e+02 6.23265552e+01\n",
      " 1.56180010e+02 6.84295089e+01 1.61107206e+02 6.77274257e+01\n",
      " 4.26778954e+01 0.00000000e+00 5.32907052e-15 3.59884398e+00\n",
      " 1.12921557e+02 1.04766175e+02 9.57727883e+01 9.57444799e+00\n",
      " 8.13826299e+01 2.76740921e+01 3.86862011e+01 5.23127316e+01\n",
      " 2.65606983e+01 2.60535435e+01 2.48403617e-13]\n",
      "20-th iteration, loss: 0.1374622111914922, 16 gd steps\n",
      "insert gradient: -0.01222372988561888\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[2.38893714e+00 5.25956880e+01 9.01910464e+01 4.77325415e+01\n",
      " 8.96206805e+01 5.83258735e+01 1.58636233e+02 6.16136234e+01\n",
      " 1.56501867e+02 6.85878525e+01 1.61479580e+02 6.82439249e+01\n",
      " 4.42895217e+01 0.00000000e+00 5.32907052e-15 3.27297140e+00\n",
      " 1.12672904e+02 1.04210899e+02 9.97074323e+01 1.11577955e+01\n",
      " 7.14459948e+01 3.15314596e+01 3.76263510e+01 4.89073129e+01\n",
      " 3.03903873e+01 2.54017569e+01 2.51127092e-13]\n",
      "21-th iteration, loss: 0.13677413134870228, 20 gd steps\n",
      "insert gradient: -0.01638602757241322\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[1.81340520e+00 5.02734265e+01 8.99655105e+01 4.99819613e+01\n",
      " 9.00890874e+01 5.59817676e+01 1.60282839e+02 6.27276498e+01\n",
      " 1.57078081e+02 6.84975267e+01 1.62087208e+02 6.82386020e+01\n",
      " 4.50761891e+01 3.24483692e+00 1.12601004e+02 1.04458553e+02\n",
      " 1.01591106e+02 1.22905554e+01 6.62888613e+01 3.35809881e+01\n",
      " 3.76125944e+01 4.71917790e+01 3.23309610e+01 2.53095041e+01\n",
      " 2.53303782e-13]\n",
      "22-th iteration, loss: 0.1332485578948725, 64 gd steps\n",
      "insert gradient: -0.009714703685487797\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[2.11766089e+00 5.15827667e+01 9.18405235e+01 5.01951234e+01\n",
      " 9.10060537e+01 5.56620573e+01 1.64553708e+02 6.07805658e+01\n",
      " 1.59847457e+02 6.83035146e+01 1.65042492e+02 6.58010337e+01\n",
      " 4.66548109e+01 4.74587965e+00 1.09882247e+02 6.79076870e+01\n",
      " 0.00000000e+00 3.39538435e+01 1.12446054e+02 2.00753381e+01\n",
      " 3.44209830e+01 5.31563427e+01 3.67874003e+01 3.29554758e+01\n",
      " 5.24117162e+01 1.99270235e+01 2.47381945e-13]\n",
      "23-th iteration, loss: 0.1258695343460729, 72 gd steps\n",
      "insert gradient: -0.016213378276106548\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[1.54388583e+00 5.18556771e+01 9.21250529e+01 5.14712494e+01\n",
      " 9.13678926e+01 5.60475153e+01 1.66314105e+02 6.09704898e+01\n",
      " 1.60174309e+02 7.02189653e+01 1.63174427e+02 6.65686117e+01\n",
      " 1.77916546e+02 6.05239390e+01 3.35896554e+01 1.44851049e+01\n",
      " 1.09796605e+02 4.63409129e+01 7.50781219e+00 5.84025547e+01\n",
      " 6.26845940e+01 2.09168061e+01 6.66974166e+01 2.04450290e+01\n",
      " 2.62363848e-13]\n",
      "24-th iteration, loss: 0.12553464258311053, 15 gd steps\n",
      "insert gradient: -0.021894920608077243\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[1.76009544e+00 5.28813379e+01 9.26582947e+01 5.15942604e+01\n",
      " 9.15274936e+01 5.58320984e+01 1.66305377e+02 6.04451963e+01\n",
      " 1.60183623e+02 7.08444299e+01 1.27053266e+02 0.00000000e+00\n",
      " 3.63009330e+01 6.71222341e+01 1.75752187e+02 5.98661032e+01\n",
      " 3.46024796e+01 1.39488422e+01 1.09507607e+02 4.72160430e+01\n",
      " 7.13580894e+00 5.79923751e+01 6.40831583e+01 2.09787564e+01\n",
      " 6.63263012e+01 2.04283775e+01 2.58878696e-13]\n",
      "25-th iteration, loss: 0.1246769418374243, 25 gd steps\n",
      "insert gradient: -0.018754362373341216\n",
      "25-th iteration, new layer inserted. now 29 layers\n",
      "[1.73411597e+00 5.28769641e+01 9.28119340e+01 5.18960760e+01\n",
      " 9.17598094e+01 5.58546417e+01 1.66886890e+02 6.03022145e+01\n",
      " 1.19819370e+02 0.00000000e+00 3.99397901e+01 7.13513564e+01\n",
      " 1.24917776e+02 3.69012859e+00 3.42014199e+01 6.59945227e+01\n",
      " 1.74767159e+02 6.05697504e+01 3.63698122e+01 1.32127819e+01\n",
      " 1.09443187e+02 4.87272739e+01 6.24943458e+00 5.74335751e+01\n",
      " 6.60197598e+01 2.05484654e+01 6.58018104e+01 2.07896635e+01\n",
      " 2.54785057e-13]\n",
      "26-th iteration, loss: 0.12059592362165213, 130 gd steps\n",
      "insert gradient: -0.00797051084664523\n",
      "26-th iteration, new layer inserted. now 31 layers\n",
      "[1.65868983e+00 5.35585982e+01 9.42722453e+01 5.28037364e+01\n",
      " 9.32614923e+01 5.51707664e+01 1.67482796e+02 6.38267998e+01\n",
      " 1.15283721e+02 1.23329824e+01 1.96209268e+01 7.27980368e+01\n",
      " 1.10498858e+02 1.30917625e+01 3.12096940e+01 6.42853445e+01\n",
      " 1.10190112e+02 0.00000000e+00 6.61140670e+01 6.50730643e+01\n",
      " 5.29937386e+01 5.55010959e+00 1.09640610e+02 6.34914170e+01\n",
      " 1.36966199e+01 3.51639348e+01 8.80249992e+01 2.14564574e+01\n",
      " 5.44349899e+01 2.53941306e+01 5.45461946e-14]\n",
      "27-th iteration, loss: 0.11394965812189337, 42 gd steps\n",
      "insert gradient: -0.007732773504620279\n",
      "27-th iteration, new layer inserted. now 30 layers\n",
      "[  1.52793069  53.7049734   94.6833844   53.50478507  93.42267501\n",
      "  54.29327655 123.5748128    0.          41.19160427  66.58089725\n",
      " 117.69812184  16.73500461   3.84131302  78.45227458 110.25086477\n",
      "  22.85614309  16.7205083   66.85617181 102.03128915  11.27105404\n",
      "  46.29732441  61.23928947 183.70486124  67.71215137  30.89356216\n",
      "  19.65264404 101.09013336  31.37134733  38.35551939  28.95981747]\n",
      "28-th iteration, loss: 0.10691569250227852, 21 gd steps\n",
      "insert gradient: -0.03328153223686055\n",
      "28-th iteration, new layer inserted. now 26 layers\n",
      "[  1.87339264  54.39096043  97.26064729  55.04637919  94.87725334\n",
      "  53.42077923 111.46571474  11.6935348   28.64722848  64.90844551\n",
      " 113.74035914  99.82792512 106.39851977 103.12004617 102.38425502\n",
      "  21.70877876  27.70367317  58.34500523 185.35280136  65.9328572\n",
      "  45.68355991  10.52556529 106.65559715  44.36553684  27.5002012\n",
      "  28.1677783 ]\n",
      "29-th iteration, loss: 0.10491275082816627, 20 gd steps\n",
      "insert gradient: -0.008842631693607404\n",
      "29-th iteration, new layer inserted. now 28 layers\n",
      "[1.98471561e+00 5.42627136e+01 9.64871784e+01 0.00000000e+00\n",
      " 2.48689958e-14 5.36670254e+01 9.46818395e+01 5.35026307e+01\n",
      " 1.10751754e+02 1.31679779e+01 2.74087652e+01 6.53606822e+01\n",
      " 1.13215674e+02 1.00118026e+02 1.06335712e+02 1.04331755e+02\n",
      " 1.02317003e+02 2.30789782e+01 2.68707890e+01 5.80469821e+01\n",
      " 1.84273847e+02 6.39801384e+01 4.66863717e+01 1.03443247e+01\n",
      " 1.06834646e+02 4.50356299e+01 2.65942678e+01 2.79531733e+01]\n",
      "30-th iteration, loss: 0.10066313048397392, 108 gd steps\n",
      "insert gradient: -0.024947845383425794\n",
      "30-th iteration, new layer inserted. now 24 layers\n",
      "[  3.34882539  54.44111719  97.75534243  53.40762855  94.92677657\n",
      "  52.17635708 101.75019539 102.70554495 103.40771649 103.74257292\n",
      "  99.24195854 105.08895591  95.82787325  32.1451465   16.68477823\n",
      "  58.06306053 128.81200198   0.          55.2051437   53.29393274\n",
      " 184.35732651  58.68940002  15.39977701  26.66043744]\n",
      "31-th iteration, loss: 0.09260139707606718, 132 gd steps\n",
      "insert gradient: -0.003853596340469679\n",
      "31-th iteration, new layer inserted. now 26 layers\n",
      "[  1.38342487  57.70946836  98.9072502   55.92328879  97.78671854\n",
      "  56.61915734  98.55452873 109.42262466  99.9227273  109.78144291\n",
      " 101.86883142  61.6569335    0.          49.3255468   94.42747038\n",
      "  48.5389036   11.45061775  49.24017513  97.66731712  26.19618979\n",
      "  22.6380682   55.97569683 174.87877848  52.39086355  11.46973506\n",
      "  37.46502768]\n",
      "32-th iteration, loss: 0.09180330949766317, 48 gd steps\n",
      "insert gradient: -0.0020100304806949993\n",
      "32-th iteration, new layer inserted. now 28 layers\n",
      "[  1.38299138  58.22488426  99.24564426  56.20294818  98.28165836\n",
      "  57.0391438   99.057204   109.89968692 101.04045478  73.28534356\n",
      "   0.          36.64267178 105.08092506  60.33325751  14.1099606\n",
      "  36.33699651  96.98173929  52.81207592   9.29444523  47.70605498\n",
      "  99.94295799  27.34795051  17.27064109  58.55440665 173.14100587\n",
      "  53.65643486  10.75506176  38.32487758]\n",
      "33-th iteration, loss: 0.09132933487249596, 101 gd steps\n",
      "insert gradient: -0.0008735029856302958\n",
      "33-th iteration, new layer inserted. now 30 layers\n",
      "[1.29119308e+00 5.85909794e+01 9.97484303e+01 5.66904633e+01\n",
      " 9.93072230e+01 5.81166894e+01 1.00337132e+02 1.10561827e+02\n",
      " 1.03328784e+02 7.21403127e+01 0.00000000e+00 1.06581410e-14\n",
      " 1.04613189e+01 3.02446176e+01 1.05026763e+02 6.08589903e+01\n",
      " 2.11353369e+01 3.12821690e+01 9.87716080e+01 5.70001709e+01\n",
      " 1.17986935e+01 4.11476153e+01 1.01605108e+02 2.98433393e+01\n",
      " 1.10330307e+01 6.16728114e+01 1.71893866e+02 5.63967909e+01\n",
      " 1.08051185e+01 3.75213781e+01]\n",
      "34-th iteration, loss: 0.09084799136632614, 345 gd steps\n",
      "insert gradient: -0.0003447481260985733\n",
      "34-th iteration, new layer inserted. now 28 layers\n",
      "[1.21569610e+00 5.91250263e+01 1.00493277e+02 5.72312795e+01\n",
      " 1.00438110e+02 5.96397060e+01 1.02574106e+02 1.11211472e+02\n",
      " 1.05829937e+02 6.91060588e+01 2.43992760e+01 2.38158443e+01\n",
      " 1.07046018e+02 6.04590011e+01 0.00000000e+00 7.10542736e-15\n",
      " 3.41444220e+01 2.38766210e+01 1.00990477e+02 5.98416505e+01\n",
      " 2.22641132e+01 2.90047512e+01 1.04663404e+02 1.01472188e+02\n",
      " 1.72559792e+02 6.04371920e+01 1.56711798e+01 3.25199080e+01]\n",
      "35-th iteration, loss: 0.09074832417977942, 885 gd steps\n",
      "insert gradient: -0.00027280857727991504\n",
      "35-th iteration, new layer inserted. now 28 layers\n",
      "[1.20380399e+00 5.91991199e+01 1.00668219e+02 5.73879879e+01\n",
      " 1.00678597e+02 5.98944621e+01 1.02997234e+02 1.11447634e+02\n",
      " 1.06138115e+02 6.82645432e+01 0.00000000e+00 1.59872116e-14\n",
      " 2.78317728e+01 2.21378014e+01 1.08236388e+02 6.00485626e+01\n",
      " 3.85355640e+01 2.15373039e+01 1.01880742e+02 6.01537474e+01\n",
      " 2.50682854e+01 2.65306255e+01 1.05440867e+02 1.00901369e+02\n",
      " 1.73610524e+02 6.13500561e+01 1.78274323e+01 3.07030562e+01]\n",
      "36-th iteration, loss: 0.09062149559025684, 551 gd steps\n",
      "insert gradient: -0.0002938734015989265\n",
      "36-th iteration, new layer inserted. now 28 layers\n",
      "[1.19319938e+00 5.92405864e+01 1.00871353e+02 5.75881869e+01\n",
      " 1.01020257e+02 6.02509500e+01 1.03394735e+02 1.11744069e+02\n",
      " 1.06466190e+02 6.68282275e+01 3.43767193e+01 1.92151645e+01\n",
      " 1.09229959e+02 5.97799314e+01 0.00000000e+00 1.24344979e-14\n",
      " 4.44674999e+01 1.86288934e+01 1.02451257e+02 6.04799390e+01\n",
      " 2.87902373e+01 2.33702017e+01 1.06520622e+02 9.97957310e+01\n",
      " 1.75705366e+02 6.21362505e+01 2.15017133e+01 2.81282827e+01]\n",
      "37-th iteration, loss: 0.09050234054667713, 170 gd steps\n",
      "insert gradient: -0.0004005569739100409\n",
      "37-th iteration, new layer inserted. now 26 layers\n",
      "[  1.17719778  59.2206458  100.93042722  57.73635473 101.28199332\n",
      "  60.54179715 103.61574721 111.97676165 106.56918043  66.02202432\n",
      "  38.49452951  17.38600101 110.0646713   59.21419346  49.94683166\n",
      "  16.32523248 102.56298549  60.56212778  31.79199249  21.18636456\n",
      " 106.91929207  99.07391915 177.49040761  62.60625538  24.47127362\n",
      "  26.21792316]\n",
      "38-th iteration, loss: 0.0899502502920969, 60 gd steps\n",
      "insert gradient: -0.0037828658253141905\n",
      "38-th iteration, new layer inserted. now 28 layers\n",
      "[  1.03025024  58.42173892 100.19186102  57.99871126 102.00099374\n",
      "  62.01743474 102.9619253  112.26031007 108.24174274  61.03424066\n",
      "  58.3119287   10.04943803 112.05620331  55.79505117  72.45702528\n",
      "   8.3445061  101.85577998  61.09337809  42.17727682  14.6079215\n",
      " 106.10438575  97.28679069 122.78954176   0.          61.39477088\n",
      "  61.62748272  35.6660981   21.13408879]\n",
      "39-th iteration, loss: 0.08719164250535726, 236 gd steps\n",
      "insert gradient: -0.007757885995091313\n",
      "39-th iteration, new layer inserted. now 29 layers\n",
      "[8.38440693e-01 5.87220197e+01 1.02155111e+02 6.08413869e+01\n",
      " 1.01998550e+02 6.23525891e+01 1.01632681e+02 7.22351683e+01\n",
      " 0.00000000e+00 4.33411010e+01 1.04974491e+02 5.99319906e+01\n",
      " 8.63418215e+01 2.94535831e+00 1.09268488e+02 5.41668152e+01\n",
      " 9.44106997e+01 1.00628597e+01 8.13334204e+01 5.33947263e+01\n",
      " 7.58873796e+01 4.49017315e-02 1.10361355e+02 9.70695542e+01\n",
      " 1.08982625e+02 2.61435999e+01 2.88842876e+01 5.80393425e+01\n",
      " 7.44404837e+01]\n",
      "40-th iteration, loss: 0.08428351618639077, 84 gd steps\n",
      "insert gradient: -0.0006569979758478943\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[  0.932338    59.21601861 102.76059464  61.35001508 105.51735608\n",
      "  61.10733625 105.31778073  63.55387333  29.73110735  26.02571318\n",
      " 110.48361163  62.51647256  92.1836745    1.220287   107.0299244\n",
      "  56.06538889  97.28287882  10.78207834  73.50327562  53.80603587\n",
      " 181.45626547 101.04364163 106.65298725  31.1339084   24.23593297\n",
      "  57.67465509  74.4404837 ]\n",
      "41-th iteration, loss: 0.08401584767768576, 56 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "41-th iteration, new layer inserted. now 29 layers\n",
      "[  1.32796814  59.20096157 102.45185886  61.1307808  105.33636075\n",
      "  61.51233832 103.17182497  63.72400623  33.98103191  23.42920302\n",
      " 109.75829197  62.43650455  96.28190285   0.89719281  12.91447879\n",
      "   0.          90.40135154  56.08305878  99.122096    12.21484954\n",
      "  65.57218238  56.67704286 175.83204471 102.89941651 104.02173843\n",
      "  32.1939373   22.98901465  58.36717735  74.4404837 ]\n",
      "42-th iteration, loss: 0.08395485184489036, 69 gd steps\n",
      "insert gradient: -0.00014464738383928975\n",
      "42-th iteration, new layer inserted. now 29 layers\n",
      "[1.34705654e+00 5.92336360e+01 1.02517582e+02 6.11734771e+01\n",
      " 1.05319912e+02 6.15713467e+01 1.03279845e+02 6.37266944e+01\n",
      " 3.42950697e+01 2.31984603e+01 1.09813817e+02 6.25754612e+01\n",
      " 1.08049200e+02 0.00000000e+00 1.77635684e-14 1.69239833e+00\n",
      " 8.96554805e+01 5.60065379e+01 9.92230162e+01 1.21544472e+01\n",
      " 6.55606189e+01 5.70229752e+01 1.75807170e+02 1.02811145e+02\n",
      " 1.04136841e+02 3.22894072e+01 2.27631729e+01 5.84970745e+01\n",
      " 7.44404837e+01]\n",
      "43-th iteration, loss: 0.08392481471715688, 17 gd steps\n",
      "insert gradient: -0.0011470684807449484\n",
      "43-th iteration, new layer inserted. now 29 layers\n",
      "[1.52300253e+00 5.93697717e+01 1.02707362e+02 0.00000000e+00\n",
      " 1.06581410e-14 6.10057534e+01 1.05052679e+02 6.16151203e+01\n",
      " 1.03617977e+02 6.33586359e+01 3.56589354e+01 2.23312523e+01\n",
      " 1.10391686e+02 6.34129300e+01 1.05240844e+02 3.20924425e+00\n",
      " 8.74091413e+01 5.59791322e+01 9.98245839e+01 1.21405498e+01\n",
      " 6.50176956e+01 5.71996417e+01 1.75471016e+02 1.02753748e+02\n",
      " 1.04644514e+02 3.25481812e+01 2.16316102e+01 5.93696993e+01\n",
      " 7.44404837e+01]\n",
      "44-th iteration, loss: 0.08391418378733415, 458 gd steps\n",
      "insert gradient: -1.9047561707850682e-05\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[  1.46547519  59.16050941 102.78572267  61.09684723 105.1335037\n",
      "  61.62657268 103.51435609  63.25544856  36.67638816  21.58332837\n",
      " 110.74474258  63.40744278 104.86383719   3.52859316  86.6043307\n",
      "  56.30816258 100.00816324  12.19855594  64.59227218  56.97577445\n",
      " 175.30996028 102.91231976 104.49357103  32.78055233  21.31858422\n",
      "  59.37520782  74.4404837 ]\n",
      "45-th iteration, loss: 0.08391363332135233, 69 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.520476911262148e-06\n",
      "45-th iteration, new layer inserted. now 29 layers\n",
      "[1.48752666e+00 5.91457972e+01 1.02806592e+02 6.10938085e+01\n",
      " 1.05149299e+02 0.00000000e+00 1.06581410e-14 6.16347595e+01\n",
      " 1.03546163e+02 6.31796113e+01 3.71132797e+01 2.13310145e+01\n",
      " 1.10836219e+02 6.34525428e+01 1.05253965e+02 3.67469429e+00\n",
      " 8.57862391e+01 5.63398929e+01 1.00095894e+02 1.22995136e+01\n",
      " 6.42074262e+01 5.70284761e+01 1.75188002e+02 1.02989316e+02\n",
      " 1.04416990e+02 3.29331064e+01 2.10946865e+01 5.94603150e+01\n",
      " 7.44404837e+01]\n",
      "46-th iteration, loss: 0.08391363331981429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4482077123332015e-06\n",
      "46-th iteration, new layer inserted. now 29 layers\n",
      "[1.48752515e+00 5.91457993e+01 1.02806591e+02 6.10938083e+01\n",
      " 1.05149299e+02 3.50344805e-06 4.19911511e-07 6.16347630e+01\n",
      " 1.03546165e+02 6.31796118e+01 3.71132803e+01 2.13310126e+01\n",
      " 1.10836218e+02 6.34525400e+01 1.05253964e+02 3.67469293e+00\n",
      " 8.57862376e+01 5.63398921e+01 1.00095893e+02 1.22995143e+01\n",
      " 6.42074254e+01 5.70284740e+01 1.75188002e+02 1.02989316e+02\n",
      " 1.04416989e+02 3.29331058e+01 2.10946860e+01 5.94603136e+01\n",
      " 7.44404837e+01]\n",
      "47-th iteration, loss: 0.0839136333183285, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.379140018164365e-06\n",
      "47-th iteration, new layer inserted. now 29 layers\n",
      "[1.48752363e+00 5.91458013e+01 1.02806589e+02 6.10938080e+01\n",
      " 1.05149300e+02 6.93465870e-06 7.87046692e-07 6.16347664e+01\n",
      " 1.03546167e+02 6.31796123e+01 3.71132809e+01 2.13310107e+01\n",
      " 1.10836217e+02 6.34525373e+01 1.05253963e+02 3.67469169e+00\n",
      " 8.57862362e+01 5.63398914e+01 1.00095893e+02 1.22995151e+01\n",
      " 6.42074247e+01 5.70284720e+01 1.75188002e+02 1.02989316e+02\n",
      " 1.04416988e+02 3.29331052e+01 2.10946856e+01 5.94603122e+01\n",
      " 7.44404837e+01]\n",
      "48-th iteration, loss: 0.08391363331689042, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.3131191206992714e-06\n",
      "48-th iteration, new layer inserted. now 29 layers\n",
      "[1.48752210e+00 5.91458033e+01 1.02806588e+02 6.10938077e+01\n",
      " 1.05149300e+02 1.02968883e-05 1.10381749e-06 6.16347698e+01\n",
      " 1.03546168e+02 6.31796128e+01 3.71132815e+01 2.13310088e+01\n",
      " 1.10836216e+02 6.34525346e+01 1.05253962e+02 3.67469054e+00\n",
      " 8.57862348e+01 5.63398907e+01 1.00095892e+02 1.22995160e+01\n",
      " 6.42074240e+01 5.70284699e+01 1.75188002e+02 1.02989316e+02\n",
      " 1.04416988e+02 3.29331046e+01 2.10946851e+01 5.94603108e+01\n",
      " 7.44404837e+01]\n",
      "49-th iteration, loss: 0.08391363331549587, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2499978488962075e-06\n",
      "49-th iteration, new layer inserted. now 29 layers\n",
      "[1.48752056e+00 5.91458052e+01 1.02806586e+02 6.10938073e+01\n",
      " 1.05149300e+02 1.35932361e-05 1.37251909e-06 6.16347731e+01\n",
      " 1.03546170e+02 6.31796134e+01 3.71132822e+01 2.13310070e+01\n",
      " 1.10836215e+02 6.34525320e+01 1.05253961e+02 3.67468948e+00\n",
      " 8.57862335e+01 5.63398901e+01 1.00095892e+02 1.22995169e+01\n",
      " 6.42074233e+01 5.70284679e+01 1.75188002e+02 1.02989316e+02\n",
      " 1.04416987e+02 3.29331041e+01 2.10946846e+01 5.94603094e+01\n",
      " 7.44404837e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.536802874516841\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.49960647    0.         1538.4149664 ]\n",
      "1-th iteration, loss: 0.7492949539496615, 11 gd steps\n",
      "insert gradient: -0.6097147272111854\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.96560419   62.31421067  243.89505565    0.         1294.51991075]\n",
      "2-th iteration, loss: 0.6096929582132561, 14 gd steps\n",
      "insert gradient: -0.6993360468916259\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.99639394   76.5104703   227.44281332   37.61274141  237.7689632\n",
      "    0.         1056.75094755]\n",
      "3-th iteration, loss: 0.4336787955581125, 47 gd steps\n",
      "insert gradient: -0.5971215494214315\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  3.31737773  50.54031869 215.0849043   53.87584788 135.25931004\n",
      "  57.92150997 332.12172637   0.         724.62922118]\n",
      "4-th iteration, loss: 0.3490771449043274, 14 gd steps\n",
      "insert gradient: -0.2842920626514796\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  4.23044544  58.38906601 206.98354593  52.79401133 126.43693541\n",
      "  51.12344916 318.40740062  50.09167605 375.73367024   0.\n",
      " 348.89555094]\n",
      "5-th iteration, loss: 0.3022372642466976, 27 gd steps\n",
      "insert gradient: -0.22596525209856003\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.30376488  66.20902306 195.63364904  61.67160005 109.58450878\n",
      "  54.54747836 320.44230916  44.3295479  235.99374583   0.\n",
      " 110.13041472  32.7293617  348.89555094]\n",
      "6-th iteration, loss: 0.24967318627323673, 45 gd steps\n",
      "insert gradient: -0.2797673206055671\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          70.96099457 172.42467577  75.07659001 109.81534183\n",
      "  59.74213394 137.08267203   0.         137.08267203  47.63001146\n",
      " 193.66179188  43.50942056  70.52418163  41.3446769  348.89555094]\n",
      "7-th iteration, loss: 0.18869222573115915, 294 gd steps\n",
      "insert gradient: -0.09454269331449454\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.53897795  69.88280557 133.35413303  96.9499183  115.36958391\n",
      "  66.33389357 100.00775352  40.75101584  77.06078099  49.74826028\n",
      " 187.75271032  50.11086658  59.94299964  40.98892215 174.44777547\n",
      "   0.         174.44777547]\n",
      "8-th iteration, loss: 0.16597212865961303, 78 gd steps\n",
      "insert gradient: -0.018693945139399155\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  1.26147033  68.55575707 123.63284552 101.62706869 112.44479762\n",
      "  70.65107737 102.28088913  44.02742126  66.49952379  46.15526325\n",
      " 106.90302242   0.          80.17726681  52.34068512  95.25385118\n",
      "  21.14259325 123.77806769  51.17975108 174.44777547]\n",
      "9-th iteration, loss: 0.1645391962367971, 16 gd steps\n",
      "insert gradient: -0.034720122368968216\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.96085126  65.68068589 127.27102269  99.34186441 114.27072354\n",
      "  72.42721389 104.06506109  41.78448177  70.8337753   48.82330882\n",
      "  90.57853282  10.90043396  57.54421404  55.45830119 103.77488002\n",
      "  17.67395142 118.76905825  57.58853431 174.44777547]\n",
      "10-th iteration, loss: 0.16241605324179298, 36 gd steps\n",
      "insert gradient: -0.01984154615166395\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  1.76539442  65.591656   130.60644237  97.09859763 113.68562731\n",
      "  73.12472332 104.11612858  42.37769518  74.13237405  44.4611415\n",
      " 103.43164148  17.67933391  39.52452692  53.60461281 103.07381325\n",
      "  14.35907526 121.45771725  60.70269469 174.44777547]\n",
      "11-th iteration, loss: 0.15990629932695244, 64 gd steps\n",
      "insert gradient: -0.013104158024924263\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[3.08206962e+00 6.62573650e+01 1.36168874e+02 9.20467707e+01\n",
      " 1.18829875e+02 6.87166941e+01 1.07962414e+02 4.27332492e+01\n",
      " 7.43217972e+01 0.00000000e+00 1.59872116e-14 4.53119237e+01\n",
      " 9.62443280e+01 3.00780909e+01 2.62401664e+01 5.44320424e+01\n",
      " 8.93155088e+01 1.86645921e+01 1.03715395e+02 7.57100224e+01\n",
      " 1.74447775e+02]\n",
      "12-th iteration, loss: 0.15603323236940156, 19 gd steps\n",
      "insert gradient: -0.047051675497542404\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[3.26702414e+00 6.89408210e+01 1.34860534e+02 9.28207700e+01\n",
      " 1.23208554e+02 6.71111574e+01 1.12120567e+02 4.62406023e+01\n",
      " 7.45367440e+01 0.00000000e+00 2.13162821e-14 4.48038295e+01\n",
      " 9.56376755e+01 3.39063474e+01 2.59130377e+01 5.20773941e+01\n",
      " 8.83347784e+01 2.18151137e+01 9.46839543e+01 8.01917564e+01\n",
      " 1.74447775e+02]\n",
      "13-th iteration, loss: 0.14231682770162749, 44 gd steps\n",
      "insert gradient: -0.04117067195773437\n",
      "13-th iteration, new layer inserted. now 21 layers\n",
      "[  2.12639508  73.19636166 128.54160425  97.47980431 134.49609011\n",
      "  65.70778021 114.28170752  58.35861762  73.62077744  45.98831534\n",
      "  91.9150986   51.81354231  18.09733064  46.70806642  87.61400432\n",
      "  38.37011316  57.58430497  85.97023446 130.8358316    0.\n",
      "  43.61194387]\n",
      "14-th iteration, loss: 0.13091768288070332, 41 gd steps\n",
      "insert gradient: -0.0319543902413503\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  3.14092626  69.01258919 123.16842597 101.08231838 127.47974303\n",
      "  74.76735285 115.86432792  59.84994186  81.25189004  42.56525789\n",
      "  91.10536848  52.18592432  43.1178268   23.41917476  99.36296751\n",
      "  50.02519884  32.09287816  86.31910076 121.13482859  35.94634096\n",
      "  43.61194387]\n",
      "15-th iteration, loss: 0.1294035187364745, 29 gd steps\n",
      "insert gradient: -0.019310774215734756\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  2.32856594  69.20535246 124.56629702  99.46998188 128.30913832\n",
      "  76.23684407 114.14030815  61.56772868  81.03151718  42.02254958\n",
      "  91.14179231  52.11498109  52.30986569  20.74575431  96.31597123\n",
      "  48.72561856  27.50655514  58.64472241   0.          33.51126995\n",
      " 116.00983004  41.20326377  43.61194387]\n",
      "16-th iteration, loss: 0.12210494762898103, 37 gd steps\n",
      "insert gradient: -0.023455197207749193\n",
      "16-th iteration, new layer inserted. now 25 layers\n",
      "[3.36830541e+00 6.71009558e+01 1.33494699e+02 9.75583168e+01\n",
      " 1.28384135e+02 7.97556368e+01 1.12044280e+02 6.15415752e+01\n",
      " 8.84582112e+01 4.22058650e+01 8.98913468e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.99065290e+01 7.34695324e+01 1.35462818e+01\n",
      " 9.39927839e+01 5.14258828e+01 3.81266990e+01 4.04352048e+01\n",
      " 3.58856711e+01 2.36228389e+01 9.60068634e+01 5.32866354e+01\n",
      " 4.36119439e+01]\n",
      "17-th iteration, loss: 0.1203419489290782, 15 gd steps\n",
      "insert gradient: -0.02582354174482243\n",
      "17-th iteration, new layer inserted. now 23 layers\n",
      "[  3.00405737  67.72338595 132.63510541  97.43278524 130.9121669\n",
      "  81.80158025 109.68006585  60.65840781  92.92367822  43.11837388\n",
      "  88.834234    50.62357014  78.10197543  12.13229254  91.77869645\n",
      "  50.86323911  41.85463888  35.26000424  42.96827366  24.71549682\n",
      "  92.54117995  53.45320214  43.61194387]\n",
      "18-th iteration, loss: 0.11004632930496346, 34 gd steps\n",
      "insert gradient: -0.012399528938782914\n",
      "18-th iteration, new layer inserted. now 23 layers\n",
      "[  1.93424714  73.66536025 131.15473824  99.41756508 138.71005538\n",
      "  77.23274159 117.517517    63.96647033  95.07783958  47.493417\n",
      "  90.91611448  54.85704744  91.62678901  16.74786374  74.7963967\n",
      "  51.4105819   70.43322209  33.79305529  61.03962492  15.54475467\n",
      "  67.85521512  50.02287606  43.61194387]\n",
      "19-th iteration, loss: 0.10413635947545112, 26 gd steps\n",
      "insert gradient: -0.011510383238537114\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[2.51193153e+00 7.73901194e+01 1.38094645e+02 9.80743969e+01\n",
      " 1.38685545e+02 7.45249615e+01 1.20636790e+02 6.40047654e+01\n",
      " 1.01347917e+02 4.69921070e+01 9.30749082e+01 0.00000000e+00\n",
      " 1.06581410e-14 5.44311366e+01 9.60563939e+01 2.99071118e+01\n",
      " 5.39007584e+01 4.70511041e+01 8.84672428e+01 3.47300653e+01\n",
      " 7.25566632e+01 2.47402985e+01 2.29875552e+01 5.41976043e+01\n",
      " 4.36119439e+01]\n",
      "20-th iteration, loss: 0.10004649212585638, 24 gd steps\n",
      "insert gradient: -0.00894445245943369\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[  1.82320959  77.44612969 141.75000566  93.55969238 142.77981132\n",
      "  75.97011965 121.42160698  64.9409378  102.95574458  48.59680966\n",
      "  91.28222292  56.53653427  95.85334253  41.52105437  45.07267266\n",
      "  45.52281118  83.70754079  37.6527896   69.21706683  44.01366425\n",
      "   6.31347994  45.00907255  43.61194387]\n",
      "21-th iteration, loss: 0.09895724408548762, 27 gd steps\n",
      "insert gradient: -0.007083706354497717\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[1.99013376e+00 7.74761282e+01 1.39659777e+02 9.17271125e+01\n",
      " 1.50414971e+02 7.62179544e+01 1.22638680e+02 6.72132455e+01\n",
      " 1.02813081e+02 4.96167921e+01 9.06029477e+01 5.79929705e+01\n",
      " 9.59458309e+01 0.00000000e+00 1.06581410e-14 4.36422760e+01\n",
      " 4.38426020e+01 4.49476996e+01 8.37715929e+01 3.87884275e+01\n",
      " 6.58785970e+01 5.16452176e+01 6.02031895e+00 4.01244093e+01\n",
      " 4.36119439e+01]\n",
      "22-th iteration, loss: 0.0986017314606909, 22 gd steps\n",
      "insert gradient: -0.0032462264895868724\n",
      "22-th iteration, new layer inserted. now 25 layers\n",
      "[2.14773682e+00 7.87520328e+01 1.39842018e+02 9.22717208e+01\n",
      " 1.51239641e+02 7.57600550e+01 1.22784612e+02 0.00000000e+00\n",
      " 1.77635684e-14 6.69332282e+01 1.02721755e+02 4.99186718e+01\n",
      " 9.08576664e+01 5.86120640e+01 9.63329742e+01 4.46348321e+01\n",
      " 4.40203166e+01 4.44804654e+01 8.42515653e+01 3.97151446e+01\n",
      " 6.60902507e+01 5.15231277e+01 8.05940020e+00 3.95098760e+01\n",
      " 4.36119439e+01]\n",
      "23-th iteration, loss: 0.09844510162495149, 17 gd steps\n",
      "insert gradient: -0.004902503102817237\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[  1.25865176  78.86555746 141.13804245  92.52155518 151.31532392\n",
      "  76.39077197 122.57222648  67.25228249 103.55954304  50.18752876\n",
      "  90.56149324  59.03487025  96.85075424  44.58700905  44.36633735\n",
      "  44.57268088  84.71037257  39.92976974  68.16391059  52.13679912\n",
      "  12.1495262   35.29847815  43.61194387]\n",
      "24-th iteration, loss: 0.09820185556931206, 28 gd steps\n",
      "insert gradient: -0.0029974853762324265\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[1.98323311e+00 8.00836176e+01 1.41302027e+02 9.30871066e+01\n",
      " 1.51804994e+02 7.59648600e+01 1.23057440e+02 6.76510017e+01\n",
      " 1.03752484e+02 5.09586313e+01 9.10200702e+01 0.00000000e+00\n",
      " 1.06581410e-14 5.92238035e+01 9.70663552e+01 4.58756488e+01\n",
      " 4.44083164e+01 4.39937240e+01 8.52383873e+01 4.08453882e+01\n",
      " 6.71635215e+01 5.16856381e+01 1.39888253e+01 3.43875694e+01\n",
      " 4.36119439e+01]\n",
      "25-th iteration, loss: 0.0980357731544794, 22 gd steps\n",
      "insert gradient: -0.002661501776255804\n",
      "25-th iteration, new layer inserted. now 25 layers\n",
      "[1.59595688e+00 8.02278802e+01 1.42795277e+02 9.32448659e+01\n",
      " 1.51732721e+02 7.61555155e+01 1.22693799e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.75867943e+01 1.05106946e+02 5.11161482e+01\n",
      " 9.05925208e+01 6.01085142e+01 9.77251200e+01 4.62111464e+01\n",
      " 4.53977502e+01 4.34719288e+01 8.57218251e+01 4.11377992e+01\n",
      " 6.90124424e+01 5.21545330e+01 1.77576801e+01 2.97846318e+01\n",
      " 4.36119439e+01]\n",
      "26-th iteration, loss: 0.09781450357311011, 17 gd steps\n",
      "insert gradient: -0.005747241575906256\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[1.54086213e+00 7.99738224e+01 1.44156600e+02 9.29962930e+01\n",
      " 1.51762496e+02 7.74896297e+01 1.22324607e+02 6.81498724e+01\n",
      " 1.05479860e+02 5.10417807e+01 9.13129927e+01 0.00000000e+00\n",
      " 3.55271368e-15 6.09147977e+01 9.79191775e+01 4.64022533e+01\n",
      " 4.65731526e+01 4.30532998e+01 8.52597640e+01 4.23175457e+01\n",
      " 7.04539163e+01 5.32645003e+01 2.27022143e+01 2.48741473e+01\n",
      " 4.36119439e+01]\n",
      "27-th iteration, loss: 0.08897572363001262, 133 gd steps\n",
      "insert gradient: -0.005177063242218056\n",
      "27-th iteration, new layer inserted. now 22 layers\n",
      "[  2.30627116  79.46409533 144.53752949  90.06366497 149.82152051\n",
      "  79.70208057 122.55103674  68.27693599 112.56435351  47.25555872\n",
      "  88.7370236   67.03850545 102.46633212  52.81465053  60.68563425\n",
      "  35.60601241  81.0021931   41.13649734  75.08577331  48.10813181\n",
      " 106.69727223   0.        ]\n",
      "28-th iteration, loss: 0.0700113802030102, 70 gd steps\n",
      "insert gradient: -0.0003238400114352173\n",
      "28-th iteration, new layer inserted. now 22 layers\n",
      "[  3.65480596  76.81649012 131.87775236  96.11375395 147.23195649\n",
      "  74.16183106 138.9735814   63.46194534 134.35739595  53.96337549\n",
      "  76.0976006   48.42478446 139.19004285  54.40520059  87.71993784\n",
      "  36.37254693  73.04120154  43.07084924  73.54490975  40.64283273\n",
      "  90.12693999  59.89208009]\n",
      "29-th iteration, loss: 0.06996412884721577, 46 gd steps\n",
      "insert gradient: -0.00014004222105924008\n",
      "29-th iteration, new layer inserted. now 22 layers\n",
      "[  2.93906203  78.23695751 129.38895388  96.44661622 148.85916359\n",
      "  73.43730899 139.40020434  63.23742458 135.30390473  53.28537606\n",
      "  77.54782676  47.71107354 140.51359838  54.10569143  88.00121928\n",
      "  36.5174      71.9714604   43.25630696  74.58455753  40.1731769\n",
      "  91.92490703  58.98697877]\n",
      "30-th iteration, loss: 0.06996355200538418, 64 gd steps\n",
      "insert gradient: -1.161902556984113e-05\n",
      "30-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86962664  78.42061868 129.17994551  96.43794981 149.16602968\n",
      "  73.29636655 139.4887447   63.17862341 135.37949639  53.26967795\n",
      "  77.60093898  47.70036155 140.58669323  54.08298828  88.04274733\n",
      "  36.52447694  71.88168107  43.27794897  74.62872337  40.21087452\n",
      "  91.91365565  58.96131789]\n",
      "31-th iteration, loss: 0.06996354985119523, 16 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.350918790919022e-06\n",
      "31-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86956893  78.43315498 129.1762037   96.43741303 149.17460953\n",
      "  73.29045817 139.49157464  63.17352155 135.38276225  53.27243091\n",
      "  77.60492084  47.69846903 140.59080799  54.08136712  88.0428921\n",
      "  36.52406606  71.8783677   43.2799512   74.63250348  40.21170992\n",
      "  91.9157793   58.95661645]\n",
      "32-th iteration, loss: 0.06996354876040609, 14 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.366890137223086e-07\n",
      "32-th iteration, new layer inserted. now 24 layers\n",
      "[2.86825290e+00 7.84315059e+01 1.29173318e+02 9.64385644e+01\n",
      " 1.49177931e+02 7.32891550e+01 1.39493140e+02 6.31738868e+01\n",
      " 1.35384022e+02 5.32708480e+01 7.76049447e+01 4.76977094e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.40593154e+02 5.40816834e+01\n",
      " 8.80430920e+01 3.65242835e+01 7.18777106e+01 4.32796457e+01\n",
      " 7.46333636e+01 4.02121101e+01 9.19168099e+01 5.89570606e+01]\n",
      "33-th iteration, loss: 0.06996354875989311, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.480786960633927e-07\n",
      "33-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86825235  78.43150463 129.17331649  96.43856337 149.17793171\n",
      "  73.28915368 139.49314064  63.17388687 135.38402202  53.27084661\n",
      "  77.60494414  47.69770816 140.59315551  54.08168246  88.04309137\n",
      "  36.52428257  71.87771031  43.27964594  74.63336419  40.21211065\n",
      "  91.91681067  58.95706152]\n",
      "34-th iteration, loss: 0.0699635487594211, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.60274687873903e-07\n",
      "34-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86825179  78.43150341 129.17331506  96.43856236 149.17793219\n",
      "  73.28915237 139.49314136  63.17388693 135.38402254  53.27084521\n",
      "  77.60494358  47.69770698 140.59315646  54.08168153  88.04309073\n",
      "  36.52428161  71.87771004  43.27964617  74.63336478  40.21211124\n",
      "  91.91681142  58.95706244]\n",
      "35-th iteration, loss: 0.06996354875896417, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.719228970943166e-07\n",
      "35-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86825124  78.43150221 129.17331365  96.43856137 149.17793269\n",
      "  73.2891511  139.4931421   63.17388702 135.38402308  53.27084385\n",
      "  77.60494305  47.69770584 140.59315743  54.08168064  88.0430901\n",
      "  36.52428068  71.87770979  43.27964641  74.63336538  40.21211182\n",
      "  91.91681216  58.95706336]\n",
      "36-th iteration, loss: 0.06996354875852125, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.830464920584803e-07\n",
      "36-th iteration, new layer inserted. now 24 layers\n",
      "[2.86825070e+00 7.84315010e+01 1.29173312e+02 9.64385604e+01\n",
      " 1.49177933e+02 7.32891499e+01 1.39493143e+02 6.31738872e+01\n",
      " 1.35384024e+02 5.32708425e+01 7.76049425e+01 4.76977047e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.40593158e+02 5.40816798e+01\n",
      " 8.80430895e+01 3.65242798e+01 7.18777095e+01 4.32796467e+01\n",
      " 7.46333660e+01 4.02121124e+01 9.19168129e+01 5.89570643e+01]\n",
      "37-th iteration, loss: 0.06996354875806343, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.92265467719755e-07\n",
      "37-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86825016  78.43149987 129.17331086  96.43855947 149.17793372\n",
      "  73.28914866 139.4931436   63.17388731 135.38402417  53.27084124\n",
      "  77.60494204  47.69770367 140.59316038  54.08167897  88.0430889\n",
      "  36.52427888  71.8777093   43.27964694  74.63336657  40.21211299\n",
      "  91.91681363  58.95706515]\n",
      "38-th iteration, loss: 0.06996354875764493, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0024384785384048e-06\n",
      "38-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824962  78.43149873 129.17330947  96.43855854 149.17793426\n",
      "  73.28914749 139.49314437  63.1738875  135.38402473  53.27083998\n",
      "  77.60494156  47.69770263 140.59316138  54.08167819  88.04308832\n",
      "  36.52427801  71.87770906  43.27964721  74.63336716  40.21211357\n",
      "  91.91681436  58.95706604]\n",
      "39-th iteration, loss: 0.06996354875723763, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0121493588093187e-06\n",
      "39-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824909  78.4314976  129.1733081   96.43855764 149.17793481\n",
      "  73.28914635 139.49314514  63.17388772 135.3840253   53.27083875\n",
      "  77.60494109  47.69770163 140.59316239  54.08167744  88.04308776\n",
      "  36.52427716  71.87770883  43.2796475   74.63336776  40.21211415\n",
      "  91.91681509  58.95706692]\n",
      "40-th iteration, loss: 0.06996354875684072, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0214176485747675e-06\n",
      "40-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824856  78.43149649 129.17330675  96.43855675 149.17793537\n",
      "  73.28914525 139.49314592  63.17388797 135.38402588  53.27083755\n",
      "  77.60494065  47.69770066 140.5931634   54.08167672  88.04308722\n",
      "  36.52427633  71.87770861  43.27964779  74.63336835  40.21211473\n",
      "  91.91681582  58.95706778]\n",
      "41-th iteration, loss: 0.06996354875645346, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0302620602372718e-06\n",
      "41-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824803  78.43149539 129.1733054   96.43855589 149.17793595\n",
      "  73.28914417 139.49314671  63.17388824 135.38402646  53.27083638\n",
      "  77.60494022  47.69769972 140.59316443  54.08167604  88.04308669\n",
      "  36.52427551  71.87770839  43.27964809  74.63336895  40.2121153\n",
      "  91.91681654  58.95706864]\n",
      "42-th iteration, loss: 0.06996354875607516, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0387005145291608e-06\n",
      "42-th iteration, new layer inserted. now 22 layers\n",
      "[  2.8682475   78.43149431 129.17330406  96.43855504 149.17793654\n",
      "  73.28914311 139.49314751  63.17388854 135.38402706  53.27083524\n",
      "  77.6049398   47.6976988  140.59316546  54.08167539  88.04308618\n",
      "  36.52427471  71.87770818  43.2796484   74.63336955  40.21211587\n",
      "  91.91681726  58.95706949]\n",
      "43-th iteration, loss: 0.06996354875570522, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.046750172748021e-06\n",
      "43-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824698  78.43149324 129.17330273  96.4385542  149.17793714\n",
      "  73.28914209 139.49314831  63.17388886 135.38402765  53.27083412\n",
      "  77.6049394   47.69769792 140.59316651  54.08167476  88.04308568\n",
      "  36.52427393  71.87770797  43.27964872  74.63337014  40.21211644\n",
      "  91.91681797  58.95707034]\n",
      "44-th iteration, loss: 0.06996354875534304, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0544274702073782e-06\n",
      "44-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824646  78.43149219 129.17330141  96.43855339 149.17793775\n",
      "  73.28914109 139.49314912  63.17388921 135.38402826  53.27083302\n",
      "  77.60493901  47.69769706 140.59316756  54.08167416  88.04308519\n",
      "  36.52427316  71.87770777  43.27964904  74.63337074  40.212117\n",
      "  91.91681868  58.95707117]\n",
      "45-th iteration, loss: 0.0699635487549881, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0617481466414001e-06\n",
      "45-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824595  78.43149115 129.17330009  96.43855259 149.17793838\n",
      "  73.28914012 139.49314994  63.17388957 135.38402887  53.27083195\n",
      "  77.60493863  47.69769623 140.59316861  54.08167359  88.04308472\n",
      "  36.52427241  71.87770757  43.27964937  74.63337134  40.21211757\n",
      "  91.91681939  58.957072  ]\n",
      "46-th iteration, loss: 0.0699635487546399, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.068727275662376e-06\n",
      "46-th iteration, new layer inserted. now 24 layers\n",
      "[2.86824543e+00 7.84314901e+01 1.29173299e+02 9.64385518e+01\n",
      " 1.49177939e+02 7.32891392e+01 1.39493151e+02 6.31738900e+01\n",
      " 1.35384029e+02 5.32708309e+01 7.76049383e+01 4.76976954e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.40593170e+02 5.40816730e+01\n",
      " 8.80430843e+01 3.65242717e+01 7.18777074e+01 4.32796497e+01\n",
      " 7.46333719e+01 4.02121181e+01 9.19168201e+01 5.89570728e+01]\n",
      "47-th iteration, loss: 0.06996354875426515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0738720169368501e-06\n",
      "47-th iteration, new layer inserted. now 24 layers\n",
      "[2.86824492e+00 7.84314891e+01 1.29173297e+02 9.64385510e+01\n",
      " 1.49177940e+02 7.32891382e+01 1.39493152e+02 6.31738904e+01\n",
      " 1.35384030e+02 5.32708299e+01 7.76049379e+01 4.76976946e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.40593172e+02 5.40816725e+01\n",
      " 8.80430838e+01 3.65242709e+01 7.18777072e+01 4.32796500e+01\n",
      " 7.46333725e+01 4.02121187e+01 9.19168208e+01 5.89570736e+01]\n",
      "48-th iteration, loss: 0.06996354875389559, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.0787325671829674e-06\n",
      "48-th iteration, new layer inserted. now 24 layers\n",
      "[2.86824441e+00 7.84314881e+01 1.29173296e+02 9.64385503e+01\n",
      " 1.49177940e+02 7.32891373e+01 1.39493152e+02 6.31738908e+01\n",
      " 1.35384031e+02 5.32708289e+01 7.76049376e+01 4.76976939e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.40593174e+02 5.40816720e+01\n",
      " 8.80430834e+01 3.65242702e+01 7.18777070e+01 4.32796504e+01\n",
      " 7.46333731e+01 4.02121192e+01 9.19168215e+01 5.89570744e+01]\n",
      "49-th iteration, loss: 0.06996354875353089, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.083321265995215e-06\n",
      "49-th iteration, new layer inserted. now 22 layers\n",
      "[  2.86824391  78.43148712 129.17329493  96.43854954 149.17794097\n",
      "  73.28913644 139.49315326  63.17389124 135.38403136  53.27082789\n",
      "  77.60493725  47.69769313 140.59317614  54.08167154  88.04308294\n",
      "  36.52426952  71.87770682  43.27965074  74.63337372  40.21211979\n",
      "  91.91682219  58.95707524]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535590565167535\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.20594943    0.         1562.83425158]\n",
      "1-th iteration, loss: 0.750575357082339, 11 gd steps\n",
      "insert gradient: -0.6195834240886907\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  43.47534951   62.25781836  228.70745145    0.         1334.12680013]\n",
      "2-th iteration, loss: 0.6023296024976048, 13 gd steps\n",
      "insert gradient: -0.6191114555693745\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.42897042   78.05459687  215.0218559    43.12722037  245.04369798\n",
      "    0.         1089.08310215]\n",
      "3-th iteration, loss: 0.45826475462637467, 22 gd steps\n",
      "insert gradient: -0.6276439647953657\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          50.49349843 224.98530657  53.50592004 157.91739519\n",
      "  53.62343706 591.21654116   0.         497.86656098]\n",
      "4-th iteration, loss: 0.36464408401522125, 13 gd steps\n",
      "insert gradient: -0.1946887410485706\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.27480115  51.08070608 221.20736561  65.34327554 135.8514602\n",
      "  49.77143906 207.45589649   0.         352.67502404  55.04073654\n",
      " 497.86656098]\n",
      "5-th iteration, loss: 0.28891078493116723, 48 gd steps\n",
      "insert gradient: -0.3944504986583091\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.94285506  55.75484756 224.3084831   55.51508599 123.63175231\n",
      "  62.67261881 153.29170177  45.89776394 104.64945331   0.\n",
      " 224.24882852  47.2573814  497.86656098]\n",
      "6-th iteration, loss: 0.23907682587139678, 20 gd steps\n",
      "insert gradient: -0.13721383154101507\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[4.28218157e-01 6.49329646e+01 1.23220571e+02 0.00000000e+00\n",
      " 9.85764566e+01 5.84007687e+01 1.12719437e+02 6.58856324e+01\n",
      " 1.59482358e+02 5.19384361e+01 7.13280329e+01 3.54700536e+01\n",
      " 1.94595150e+02 4.51718301e+01 4.97866561e+02]\n",
      "7-th iteration, loss: 0.2340515359655844, 22 gd steps\n",
      "insert gradient: -0.12304684366193344\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[1.57539168e-01 6.86231957e+01 1.10788976e+02 9.96985969e+00\n",
      " 8.37746885e+01 6.39396257e+01 1.10745306e+02 6.55778233e+01\n",
      " 1.61633651e+02 5.56858850e+01 6.90522493e+01 3.51617120e+01\n",
      " 1.94010556e+02 5.03656899e+01 4.04516581e+02 0.00000000e+00\n",
      " 9.33499802e+01]\n",
      "8-th iteration, loss: 0.21626658203330826, 28 gd steps\n",
      "insert gradient: -0.055297632052903806\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          67.59471642 117.67063633  32.92131546  46.65832197\n",
      "  58.50137592 124.74202478  68.99041916 162.78161979  61.63193095\n",
      "  75.82983844  26.65349046 202.93720667  55.7407629  192.36276037\n",
      "   0.         192.36276037  28.93555971  93.34998018]\n",
      "9-th iteration, loss: 0.20826693260295842, 43 gd steps\n",
      "insert gradient: -0.0989712273677046\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[  1.76003358  67.26017583 116.3618265   52.74744112  28.82372286\n",
      "  57.07668071 120.67578359  71.90078069 171.4905893   57.34580988\n",
      "  88.81178872  28.1110343  104.79147459   0.         104.79147459\n",
      "  44.06479724 185.17000442   9.27266782 186.7070037   39.81978163\n",
      "  93.34998018]\n",
      "10-th iteration, loss: 0.16247059549439327, 20 gd steps\n",
      "insert gradient: -0.05152452346682312\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          65.03939218 115.79077784  55.7588885   43.27421385\n",
      "  50.40533321 111.47344791  70.06673317 152.95012654  87.2929734\n",
      "  86.87355838  45.31582097  34.32773455  43.58936652  72.46250465\n",
      "  37.68135653 165.85278382  16.19039792  90.37354895   0.\n",
      " 108.44825874  56.01555087  93.34998018]\n",
      "11-th iteration, loss: 0.14282586430407224, 21 gd steps\n",
      "insert gradient: -0.042367834070450694\n",
      "11-th iteration, new layer inserted. now 25 layers\n",
      "[  0.49396663  67.26677951 114.38617966  58.9973561   39.56524858\n",
      "  55.84189961 108.84367147  73.57896725 125.42744651  97.83903819\n",
      " 103.39414764  54.98843268  32.65576062  35.94165615  81.01367562\n",
      "  32.07665699  90.77469782   0.          60.51646521  33.84737795\n",
      "  89.95940135   2.80580105 105.86965143  48.37373845  93.34998018]\n",
      "12-th iteration, loss: 0.1343537369826753, 23 gd steps\n",
      "insert gradient: -0.01553071268395392\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  3.57632679  69.84617719 112.79017079  61.77978501  41.31527366\n",
      "  54.55934228 108.33159216  74.45853389 123.667772    98.4628998\n",
      " 107.20953974  56.71506314  49.34481573  20.20138519  99.93388735\n",
      "  46.30205633  50.31798404  24.92989165  15.32564491  50.15220308\n",
      "  82.73341641   7.72182128  85.8172161   51.82316478  93.34998018]\n",
      "13-th iteration, loss: 0.13242050621484341, 24 gd steps\n",
      "insert gradient: -0.01050740251958524\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  0.97214259  67.85952381 116.71874749  59.63266545  38.96191488\n",
      "  57.13855249 110.56479995  73.28580806 126.49279384 100.2422689\n",
      " 107.64082199  55.97610594  54.89322233  20.29768515  93.18220617\n",
      "  46.75907078  56.59130651  36.84907195   7.06966786  46.10453157\n",
      "  78.91359701  10.55583757  74.48530636  52.52574894  93.34998018]\n",
      "14-th iteration, loss: 0.13149068270456968, 33 gd steps\n",
      "insert gradient: -0.0028327937804384397\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[7.99521081e-01 6.97371795e+01 1.16542916e+02 6.02157657e+01\n",
      " 3.80835660e+01 0.00000000e+00 1.42108547e-14 5.75901957e+01\n",
      " 1.11253270e+02 7.24413527e+01 1.25487446e+02 1.00127573e+02\n",
      " 1.08982940e+02 5.62212355e+01 6.36697020e+01 1.78121215e+01\n",
      " 9.22375651e+01 4.65166843e+01 5.72411155e+01 4.37712440e+01\n",
      " 4.28291400e+00 4.21868789e+01 7.81006919e+01 1.49500109e+01\n",
      " 6.10360706e+01 5.41850214e+01 9.33499802e+01]\n",
      "15-th iteration, loss: 0.13132651196150924, 20 gd steps\n",
      "insert gradient: -0.003783580837485578\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[1.15698045e+00 7.02142628e+01 1.16317140e+02 6.06139522e+01\n",
      " 3.82634207e+01 1.11389200e-01 2.51502836e-01 5.73397913e+01\n",
      " 1.11506468e+02 7.26282190e+01 1.25426672e+02 1.01144931e+02\n",
      " 1.08805909e+02 5.63220516e+01 6.75788015e+01 1.70139732e+01\n",
      " 9.03205068e+01 4.72508058e+01 5.83152073e+01 4.41787773e+01\n",
      " 5.48613109e+00 4.00442160e+01 8.06452547e+01 1.66485304e+01\n",
      " 5.60009828e+01 5.42644381e+01 9.33499802e+01]\n",
      "16-th iteration, loss: 0.13119500384184726, 25 gd steps\n",
      "insert gradient: -0.003167728607918269\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[6.07306828e-01 7.07823115e+01 1.16902245e+02 6.03836868e+01\n",
      " 3.82524627e+01 5.84865065e+01 1.11298554e+02 7.31153035e+01\n",
      " 1.25163653e+02 1.01239887e+02 1.09608999e+02 0.00000000e+00\n",
      " 2.48689958e-14 5.63224282e+01 6.85627540e+01 1.70375659e+01\n",
      " 9.03242142e+01 4.74130905e+01 5.90765976e+01 4.40922359e+01\n",
      " 6.29746060e+00 3.93951955e+01 8.05663784e+01 1.72708313e+01\n",
      " 5.50361247e+01 5.48533675e+01 9.33499802e+01]\n",
      "17-th iteration, loss: 0.13104763808557732, 30 gd steps\n",
      "insert gradient: -0.0018705651834587274\n",
      "17-th iteration, new layer inserted. now 25 layers\n",
      "[  0.36152312  71.26432108 117.08082516  59.97665789  38.760221\n",
      "  59.3046099  111.65846043  73.38015745 125.22350159 101.66257788\n",
      " 110.5108259   56.57244543  70.1902372   16.80668834  90.0511547\n",
      "  48.09204787  60.30783048  43.59803698   8.38346822  38.10395187\n",
      "  80.04891331  18.42692454  53.65906801  55.03325863  93.34998018]\n",
      "18-th iteration, loss: 0.13099689672824708, 21 gd steps\n",
      "insert gradient: -0.004673383016790428\n",
      "18-th iteration, new layer inserted. now 25 layers\n",
      "[  0.64207863  71.55836072 117.12548965  59.71886586  39.1367065\n",
      "  59.57861171 111.85345768  73.54151154 124.9317906  101.85090498\n",
      " 110.38648515  56.64368144  71.1120991   16.28941582  90.53812555\n",
      "  48.40427594  62.90900664  42.38568901  11.57772629  35.67353184\n",
      "  79.76373736  19.48048789  51.73378212  54.90971217  93.34998018]\n",
      "19-th iteration, loss: 0.13093095810377064, 21 gd steps\n",
      "insert gradient: -0.003052442821472081\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[  0.3149574   71.67071505 117.57335873  59.84460005  39.42710998\n",
      "  59.51309166 111.93438798  73.80582466 125.24542003 102.12747698\n",
      " 110.55995778  56.76958884  71.47488616  16.19579443  90.59168686\n",
      "  48.61474236  63.09763739  42.33136561  12.33622868  35.26194344\n",
      "  79.78639982  19.57315547  51.84339635  55.43282313  93.34998018]\n",
      "20-th iteration, loss: 0.13087533607710283, 23 gd steps\n",
      "insert gradient: -0.0039665778343559955\n",
      "20-th iteration, new layer inserted. now 25 layers\n",
      "[  0.24892427  71.49154672 117.50870134  59.77829635  39.88459429\n",
      "  59.94344053 112.12112811  73.71108335 125.40201862 102.5910375\n",
      " 110.73228145  56.88521068  71.7860332   16.10006228  90.87200333\n",
      "  49.00926857  63.62148365  41.8764796   13.23542853  34.97336493\n",
      "  79.39890117  19.90275816  51.7277778   55.53634748  93.34998018]\n",
      "21-th iteration, loss: 0.13081844437761034, 25 gd steps\n",
      "insert gradient: -0.002544321047421921\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[  0.30743779  72.05379019 117.7782917   59.54717241  40.26420365\n",
      "  59.94009491 112.26301194  74.02431415 125.50115516 102.75006522\n",
      " 110.8186989   56.90497583  72.24847161  15.87549977  91.16629728\n",
      "  49.30483405  64.30582246  41.24347403  14.54411572  34.50580559\n",
      "  78.76112059  20.36638299  51.54402533  55.55102699  93.34998018]\n",
      "22-th iteration, loss: 0.13077968448139207, 24 gd steps\n",
      "insert gradient: -0.0028679347935537144\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[3.31698784e-01 7.22976245e+01 1.17944228e+02 5.94649726e+01\n",
      " 4.05367782e+01 6.00969748e+01 1.12414903e+02 7.41505126e+01\n",
      " 1.25611650e+02 1.02965538e+02 1.10898096e+02 0.00000000e+00\n",
      " 1.77635684e-14 5.69062102e+01 7.25513462e+01 1.57204026e+01\n",
      " 9.14094902e+01 4.95739392e+01 6.50372552e+01 4.07929581e+01\n",
      " 1.55462708e+01 3.41128223e+01 7.82894434e+01 2.06927790e+01\n",
      " 5.14145238e+01 5.55678750e+01 9.33499802e+01]\n",
      "23-th iteration, loss: 0.13073996431892781, 18 gd steps\n",
      "insert gradient: -0.0038022696442717966\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[2.04945431e-01 7.20469650e+01 1.17840941e+02 5.93536570e+01\n",
      " 4.09433559e+01 6.04220300e+01 1.12555409e+02 7.40582523e+01\n",
      " 1.25635990e+02 1.03135190e+02 1.11087186e+02 1.30648094e-01\n",
      " 2.97370324e-02 5.70786022e+01 7.28996622e+01 1.55745023e+01\n",
      " 9.15858789e+01 4.97076314e+01 6.55548679e+01 4.03690741e+01\n",
      " 1.64782801e+01 3.37938359e+01 7.78440498e+01 2.10502043e+01\n",
      " 5.13130960e+01 5.55112335e+01 9.33499802e+01]\n",
      "24-th iteration, loss: 0.13070310857445352, 17 gd steps\n",
      "insert gradient: -0.0016417829121701682\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[  0.25316213  72.37524078 118.00230101  59.36186949  41.04774459\n",
      "  60.45857832 112.61554892  74.23868706 125.71346037 103.30863004\n",
      " 111.11096327  57.15289598  73.00329271  15.53754204  91.67257306\n",
      "  49.96720487  65.82741546  40.28311692  16.82453742  33.65036032\n",
      "  77.64818249  21.17814084  51.32225375  55.4793876   93.34998018]\n",
      "25-th iteration, loss: 0.13066671853718737, 21 gd steps\n",
      "insert gradient: -0.003927039878612333\n",
      "25-th iteration, new layer inserted. now 25 layers\n",
      "[  0.39023437  72.52824245 118.01355474  59.1445877   41.4507043\n",
      "  60.78954114 112.83166269  74.20063197 125.72753404 103.5337505\n",
      " 111.41654906  56.82719244  73.46312073  15.18356251  92.25968202\n",
      "  50.63356529  67.67202188  39.19681554  19.01112951  32.63638231\n",
      "  76.47858263  21.96188995  50.69274845  54.93002272  93.34998018]\n",
      "26-th iteration, loss: 0.13060527814939168, 20 gd steps\n",
      "insert gradient: -0.0028538287822337015\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[  0.15044093  72.44762924 118.41944754  59.42423116  41.87732737\n",
      "  60.49014005 112.77250539  74.72822911 125.80779956 103.65988085\n",
      " 111.1872945   57.7772373   73.78543887  15.08498202  92.42101134\n",
      "  50.20187956  67.89470281  38.99445842  19.7204027   32.62295272\n",
      "  76.13345298  22.07921365  50.99833629  55.59745514  93.34998018]\n",
      "27-th iteration, loss: 0.13054088770640673, 21 gd steps\n",
      "insert gradient: -0.0031037432572342213\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[  0.19374158  72.79076585 118.42206138  59.19326393  42.22776925\n",
      "  60.91738074 112.95399566  74.51401758 126.02904597 104.13900068\n",
      " 111.3227915   57.33170032  74.08083987  14.92971287  92.73528785\n",
      "  50.89735961  68.5796448   38.45272853  20.34471761  32.56658038\n",
      "  75.52152496  22.35494295  51.18651045  55.40393491  93.34998018]\n",
      "28-th iteration, loss: 0.13046990945902245, 23 gd steps\n",
      "insert gradient: -0.0025214030123512504\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[2.27038156e-01 7.29660640e+01 1.18589145e+02 5.90854758e+01\n",
      " 4.28554620e+01 0.00000000e+00 1.77635684e-15 6.08936411e+01\n",
      " 1.13045250e+02 7.48381442e+01 1.26184974e+02 1.04220610e+02\n",
      " 1.11469042e+02 5.75396221e+01 7.45830958e+01 1.46729363e+01\n",
      " 9.31441979e+01 5.10526666e+01 6.93475082e+01 3.75271680e+01\n",
      " 2.16733495e+01 3.24767136e+01 7.45106718e+01 2.28896537e+01\n",
      " 5.11914823e+01 5.55743805e+01 9.33499802e+01]\n",
      "29-th iteration, loss: 0.13042270574074913, 16 gd steps\n",
      "insert gradient: -0.004091947241100555\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 7.31633563e+01 1.19184195e+02 5.89260503e+01\n",
      " 4.33025605e+01 6.09243088e+01 1.13311352e+02 7.50892904e+01\n",
      " 1.26289482e+02 1.04398176e+02 1.11048207e+02 5.79741422e+01\n",
      " 7.48983030e+01 1.44082346e+01 9.37493906e+01 0.00000000e+00\n",
      " 3.55271368e-15 5.05963407e+01 7.10049070e+01 3.63905492e+01\n",
      " 2.27916472e+01 3.25424094e+01 7.32244840e+01 2.34571262e+01\n",
      " 5.12987605e+01 5.55057053e+01 9.33499802e+01]\n",
      "30-th iteration, loss: 0.13032125758102403, 18 gd steps\n",
      "insert gradient: -0.0050349651904289675\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[1.79230604e-01 7.33394234e+01 1.19083874e+02 5.91028698e+01\n",
      " 4.36981031e+01 0.00000000e+00 5.32907052e-15 6.08611447e+01\n",
      " 1.13165447e+02 7.53987242e+01 1.26247560e+02 1.04794493e+02\n",
      " 1.11657672e+02 5.77983768e+01 7.51858294e+01 1.43529904e+01\n",
      " 9.39163306e+01 5.16942047e+01 7.08727151e+01 3.61172794e+01\n",
      " 2.35104120e+01 3.24748277e+01 7.27650427e+01 2.37933794e+01\n",
      " 5.16456847e+01 5.51398067e+01 9.33499802e+01]\n",
      "31-th iteration, loss: 0.13017221241353705, 23 gd steps\n",
      "insert gradient: -0.006826072127685806\n",
      "31-th iteration, new layer inserted. now 27 layers\n",
      "[2.78627821e-03 7.34135272e+01 1.19427073e+02 5.87447938e+01\n",
      " 4.46295032e+01 6.64848656e-02 2.52737016e-01 6.12107852e+01\n",
      " 1.13477480e+02 7.54577440e+01 1.26275234e+02 1.05059200e+02\n",
      " 1.11875979e+02 5.82085445e+01 7.58753165e+01 1.37769553e+01\n",
      " 9.49353661e+01 5.25140726e+01 7.27225501e+01 3.38319989e+01\n",
      " 2.55258755e+01 3.28567870e+01 7.03949210e+01 2.51219831e+01\n",
      " 5.20361061e+01 5.43070763e+01 9.33499802e+01]\n",
      "32-th iteration, loss: 0.12999264819217274, 15 gd steps\n",
      "insert gradient: -0.005425781643252335\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          73.810171   119.65101915  58.76770264  45.81622184\n",
      "  61.38003482 113.63546127  75.79245754 126.38931873 105.70721996\n",
      " 111.3945139   58.38286149  76.64170417  13.36837954  95.08603328\n",
      "  52.61174466  73.39915774  32.92358974  26.38028501  33.15792489\n",
      "  69.40047127  25.49978012  52.8366168   54.55925186  93.34998018]\n",
      "33-th iteration, loss: 0.1293767845926686, 24 gd steps\n",
      "insert gradient: -0.007559513592020186\n",
      "33-th iteration, new layer inserted. now 25 layers\n",
      "[  0.27186884  75.22234554 120.71995385  58.85127358  47.83006393\n",
      "  60.51129419 113.75540244  76.9387757  126.93884046 106.62588329\n",
      " 112.0537097   57.99233231  77.70303127  12.44684249  96.81390384\n",
      "  54.34892398  75.90617578  28.16880438  29.82103131  34.38328294\n",
      "  65.6918532   28.03120659  54.18481483  53.60266244  93.34998018]\n",
      "34-th iteration, loss: 0.12489064446679621, 27 gd steps\n",
      "insert gradient: -0.02072196326784417\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[3.12269637e-02 7.64040553e+01 1.23985622e+02 5.87181031e+01\n",
      " 5.51216532e+01 0.00000000e+00 1.24344979e-14 5.80801973e+01\n",
      " 1.13194017e+02 7.93482253e+01 1.26129725e+02 1.10288879e+02\n",
      " 1.11054652e+02 5.87739300e+01 8.02422187e+01 1.00411763e+01\n",
      " 9.97573783e+01 5.77228425e+01 8.53080879e+01 1.46641965e+01\n",
      " 3.97096061e+01 3.81257349e+01 5.63939866e+01 3.46707131e+01\n",
      " 5.95255909e+01 5.13361431e+01 9.33499802e+01]\n",
      "35-th iteration, loss: 0.11092656410028051, 18 gd steps\n",
      "insert gradient: -0.03389301202956528\n",
      "35-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          80.68872998 123.01654853  53.51361145  71.39176693\n",
      "  58.07562006 109.73794491  81.00296494 123.31341813 114.1684368\n",
      " 109.91704849  58.17659277  87.41304858   7.82494645  97.65498812\n",
      "  58.06221316 155.86634634  46.42150058  41.14170246  41.95056185\n",
      "  76.51229784  40.47110185  93.34998018]\n",
      "36-th iteration, loss: 0.1027619805556414, 19 gd steps\n",
      "insert gradient: -0.025966755617533522\n",
      "36-th iteration, new layer inserted. now 25 layers\n",
      "[1.33184662e-02 7.79624159e+01 1.25921588e+02 5.58524581e+01\n",
      " 7.83057831e+01 5.68031393e+01 1.09495917e+02 8.46794303e+01\n",
      " 1.17020576e+02 1.17910351e+02 1.09779515e+02 5.72263467e+01\n",
      " 9.00018368e+01 1.17840258e+01 9.43014597e+01 5.48181737e+01\n",
      " 1.56089900e+02 5.31257159e+01 2.91569225e+01 3.92883491e+01\n",
      " 9.45176648e+01 4.37709366e+01 8.40149822e+01 0.00000000e+00\n",
      " 9.33499802e+00]\n",
      "37-th iteration, loss: 0.08524245593876173, 24 gd steps\n",
      "insert gradient: -0.016341251476412078\n",
      "37-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          66.62663127 141.76082284  49.98718322  90.99989798\n",
      "  61.70753748 107.91884225  78.71982119 105.73792122 124.09433957\n",
      " 114.63671882  52.84699802  78.49556112  22.4035079   88.2902082\n",
      "  59.85116267  75.98102553   0.          94.97628191  52.22038631\n",
      "   0.38385458  46.84840144  90.95206929  54.11075517  19.16927315\n",
      "  52.61357672   9.33499802]\n",
      "38-th iteration, loss: 0.07921755925035405, 359 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.47344658e+01 1.36218442e+02 5.42524350e+01\n",
      " 9.44351705e+01 6.05355047e+01 1.06568583e+02 7.91775871e+01\n",
      " 1.11459984e+02 1.21614203e+02 1.09970496e+02 5.50597762e+01\n",
      " 7.59038849e+01 2.14911151e+01 9.03323535e+01 5.51998705e+01\n",
      " 8.36554263e+01 1.65876161e-02 1.13660427e+01 0.00000000e+00\n",
      " 9.09283412e+01 9.62693551e+01 9.46685414e+01 4.74710776e+01\n",
      " 1.93763059e+01 5.48738057e+01 9.33499802e+00]\n",
      "39-th iteration, loss: 0.07819412676523796, 20 gd steps\n",
      "insert gradient: -0.003263296285597371\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.46984847e+01 1.31965525e+02 5.52690745e+01\n",
      " 9.63297046e+01 6.06040133e+01 1.06441136e+02 7.81150300e+01\n",
      " 1.15998752e+02 1.19455138e+02 1.10582616e+02 5.57472803e+01\n",
      " 7.11355659e+01 2.21683197e+01 9.33808078e+01 0.00000000e+00\n",
      " 2.48689958e-14 5.25855596e+01 1.00400273e+02 1.04335260e+00\n",
      " 9.41176068e+01 9.13903598e+01 9.93017768e+01 4.18735817e+01\n",
      " 2.12296869e+01 5.73708175e+01 9.33499802e+00]\n",
      "40-th iteration, loss: 0.07797228462059147, 157 gd steps\n",
      "insert gradient: -0.0004195758637051617\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[4.56658774e-02 6.41451699e+01 1.29638818e+02 5.69940525e+01\n",
      " 9.67290498e+01 5.91886010e+01 1.05547297e+02 7.79602026e+01\n",
      " 1.18392389e+02 1.18000990e+02 1.10446518e+02 5.63104993e+01\n",
      " 6.71079899e+01 2.31025475e+01 9.44119019e+01 5.28080343e+01\n",
      " 8.90228862e+01 0.00000000e+00 1.11278608e+01 7.34496896e-02\n",
      " 9.69317026e+01 9.00153202e+01 1.01763331e+02 3.75218012e+01\n",
      " 2.61220946e+01 5.69289689e+01 9.33499802e+00]\n",
      "41-th iteration, loss: 0.07796968330705184, 49 gd steps\n",
      "insert gradient: -7.084963227486494e-05\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[4.56017985e-02 6.41423906e+01 1.29550501e+02 5.70271213e+01\n",
      " 9.67330532e+01 5.92068745e+01 1.05546351e+02 7.79789535e+01\n",
      " 1.18393274e+02 1.17990273e+02 1.10422803e+02 5.63515975e+01\n",
      " 6.71062517e+01 2.30662963e+01 9.44583822e+01 5.28179166e+01\n",
      " 8.90113386e+01 0.00000000e+00 3.55271368e-15 2.35399247e-01\n",
      " 1.08005074e+02 8.99019167e+01 1.01716921e+02 3.74873788e+01\n",
      " 2.61496707e+01 5.69162652e+01 9.33499802e+00]\n",
      "42-th iteration, loss: 0.07795498611295795, 75 gd steps\n",
      "insert gradient: -2.983623387725649e-05\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[9.94841934e-02 6.41927386e+01 1.28774449e+02 5.71883161e+01\n",
      " 9.67379406e+01 0.00000000e+00 2.48689958e-14 5.93818946e+01\n",
      " 1.05834816e+02 7.77655324e+01 1.19040128e+02 1.17549518e+02\n",
      " 1.10736957e+02 5.66374954e+01 6.65424409e+01 2.30468558e+01\n",
      " 9.50243318e+01 5.30068162e+01 8.91863640e+01 2.00594606e+00\n",
      " 1.04202369e+02 8.94326467e+01 1.02598995e+02 3.67143597e+01\n",
      " 2.61720903e+01 5.74847968e+01 9.33499802e+00]\n",
      "43-th iteration, loss: 0.07795248125822524, 235 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.746484516666685e-06\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[1.46915313e-01 6.41649974e+01 1.28695557e+02 5.71920830e+01\n",
      " 9.67198771e+01 5.94474083e+01 1.05919065e+02 7.77029583e+01\n",
      " 1.19245707e+02 1.17432322e+02 1.10861830e+02 5.66981224e+01\n",
      " 6.65021162e+01 2.30207090e+01 9.51217895e+01 5.30249380e+01\n",
      " 9.01407422e+01 2.45580454e+00 1.02155677e+02 8.94392842e+01\n",
      " 1.02866654e+02 0.00000000e+00 3.19744231e-14 3.65829834e+01\n",
      " 2.61169711e+01 5.76070352e+01 9.33499802e+00]\n",
      "44-th iteration, loss: 0.07795248125253942, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.720659992025247e-06\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[1.46913420e-01 6.41649999e+01 1.28695557e+02 5.71920803e+01\n",
      " 9.67198790e+01 5.94474081e+01 1.05919066e+02 7.77029580e+01\n",
      " 1.19245706e+02 1.17432321e+02 1.10861833e+02 5.66981212e+01\n",
      " 6.65021193e+01 2.30207095e+01 9.51217859e+01 5.30249385e+01\n",
      " 9.01407373e+01 2.45580678e+00 1.02155678e+02 8.94392834e+01\n",
      " 1.02866655e+02 4.73818921e-06 1.62999926e-06 3.65829881e+01\n",
      " 2.61169621e+01 5.76070376e+01 9.33499802e+00]\n",
      "45-th iteration, loss: 0.07795248124686316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.695668868805764e-06\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[1.46911528e-01 6.41650023e+01 1.28695557e+02 5.71920777e+01\n",
      " 9.67198809e+01 5.94474080e+01 1.05919067e+02 7.77029578e+01\n",
      " 1.19245706e+02 1.17432321e+02 1.10861836e+02 5.66981199e+01\n",
      " 6.65021225e+01 2.30207099e+01 9.51217823e+01 5.30249390e+01\n",
      " 9.01407325e+01 2.45580900e+00 1.02155678e+02 8.94392826e+01\n",
      " 1.02866657e+02 9.44946106e-06 3.24386894e-06 3.65829928e+01\n",
      " 2.61169532e+01 5.76070400e+01 9.33499802e+00]\n",
      "46-th iteration, loss: 0.07795248124119601, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.671475689970248e-06\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[1.46909637e-01 6.41650048e+01 1.28695557e+02 5.71920751e+01\n",
      " 9.67198829e+01 5.94474079e+01 1.05919068e+02 7.77029575e+01\n",
      " 1.19245705e+02 1.17432320e+02 1.10861839e+02 5.66981187e+01\n",
      " 6.65021257e+01 2.30207104e+01 9.51217787e+01 5.30249394e+01\n",
      " 9.01407276e+01 2.45581119e+00 1.02155678e+02 8.94392817e+01\n",
      " 1.02866659e+02 1.41346507e-05 4.84209659e-06 3.65829975e+01\n",
      " 2.61169442e+01 5.76070424e+01 9.33499802e+00]\n",
      "47-th iteration, loss: 0.07795248123553747, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.648046622252978e-06\n",
      "47-th iteration, new layer inserted. now 27 layers\n",
      "[1.46907747e-01 6.41650073e+01 1.28695557e+02 5.71920724e+01\n",
      " 9.67198848e+01 5.94474079e+01 1.05919069e+02 7.77029573e+01\n",
      " 1.19245705e+02 1.17432319e+02 1.10861842e+02 5.66981175e+01\n",
      " 6.65021289e+01 2.30207108e+01 9.51217751e+01 5.30249398e+01\n",
      " 9.01407227e+01 2.45581335e+00 1.02155678e+02 8.94392808e+01\n",
      " 1.02866660e+02 1.87945580e-05 6.42514773e-06 3.65830022e+01\n",
      " 2.61169352e+01 5.76070448e+01 9.33499802e+00]\n",
      "48-th iteration, loss: 0.0779524812298872, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.625349381671159e-06\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[1.46905857e-01 6.41650098e+01 1.28695557e+02 5.71920698e+01\n",
      " 9.67198868e+01 5.94474078e+01 1.05919069e+02 7.77029571e+01\n",
      " 1.19245704e+02 1.17432319e+02 1.10861845e+02 5.66981163e+01\n",
      " 6.65021320e+01 2.30207112e+01 9.51217715e+01 5.30249402e+01\n",
      " 9.01407179e+01 2.45581549e+00 1.02155678e+02 8.94392800e+01\n",
      " 1.02866662e+02 2.34299490e-05 7.99346696e-06 3.65830068e+01\n",
      " 2.61169262e+01 5.76070471e+01 9.33499802e+00]\n",
      "49-th iteration, loss: 0.0779524812242448, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.6033531592009245e-06\n",
      "49-th iteration, new layer inserted. now 29 layers\n",
      "[1.46903968e-01 6.41650123e+01 1.28695557e+02 5.71920672e+01\n",
      " 9.67198887e+01 5.94474077e+01 1.05919070e+02 7.77029569e+01\n",
      " 1.19245703e+02 1.17432318e+02 1.10861848e+02 5.66981151e+01\n",
      " 6.65021352e+01 2.30207116e+01 9.51217678e+01 5.30249406e+01\n",
      " 9.01407130e+01 2.45581761e+00 1.02155678e+02 8.94392791e+01\n",
      " 1.02866663e+02 2.80415577e-05 9.54747896e-06 0.00000000e+00\n",
      " 1.69406589e-21 3.65830114e+01 2.61169171e+01 5.76070494e+01\n",
      " 9.33499802e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.532886424033857\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.91229239    0.         1587.25353676]\n",
      "1-th iteration, loss: 0.7518055542540727, 11 gd steps\n",
      "insert gradient: -0.6351719582439863\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  43.98565692   62.18124359  232.28100538    0.         1354.97253138]\n",
      "2-th iteration, loss: 0.6025493148362673, 13 gd steps\n",
      "insert gradient: -0.6232965855625416\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.99829413   77.45539524  217.86751306   42.62775719  248.87250576\n",
      "    0.         1106.10002562]\n",
      "3-th iteration, loss: 0.42981540539527424, 72 gd steps\n",
      "insert gradient: -0.6859046570812227\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.51743002  48.47466812 207.65547457  58.08671507 127.49689305\n",
      "  59.31081525 347.63143662   0.         758.46858899]\n",
      "4-th iteration, loss: 0.3299509839618015, 72 gd steps\n",
      "insert gradient: -0.2440110381233845\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.76312441  48.74275808  96.55437384   0.         103.98163337\n",
      "  55.59961253 122.02578925  61.88827007 281.33427116  58.78029681\n",
      " 758.46858899]\n",
      "5-th iteration, loss: 0.3225202916957651, 27 gd steps\n",
      "insert gradient: -0.23289943871967378\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.10668873  52.65802904  81.96398382   9.68317725  90.79114625\n",
      "  58.8441324  121.54375773  63.71726635 277.03116957  62.87515763\n",
      " 551.61351927   0.         206.85506973]\n",
      "6-th iteration, loss: 0.2943307640868762, 25 gd steps\n",
      "insert gradient: -0.1292389793723459\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  2.61503165  49.35453708  86.03964355   8.87299998  86.41695475\n",
      "  59.9004796  128.493872    64.49086923 273.09395043  55.09857255\n",
      " 533.70995111  45.21072884 206.85506973]\n",
      "7-th iteration, loss: 0.29138248014625945, 26 gd steps\n",
      "insert gradient: -0.20191506773455722\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  1.51176983  53.71249686  84.52428786   7.93350815  85.31233284\n",
      "  59.64654501 132.19270961  62.14690686 135.18334435   0.\n",
      " 135.18334435  54.09633311 531.43277629  50.55619113 206.85506973]\n",
      "8-th iteration, loss: 0.22464167588017145, 166 gd steps\n",
      "insert gradient: -0.28457466331848297\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  4.55113058  60.028518   171.94563698  66.71775067 145.65361048\n",
      "  58.32069187 101.58746896  32.03855439  88.67273495  49.2468509\n",
      " 212.87616519   0.         334.51968816  45.7942608  206.85506973]\n",
      "9-th iteration, loss: 0.17825859911181371, 40 gd steps\n",
      "insert gradient: -0.046427578809150265\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[  0.9867578   65.95707128 143.1970528   89.31592324 136.84578534\n",
      "  62.06314577  94.70140431  46.86449936  72.00850315  44.26257275\n",
      " 189.19641929  48.24014251 294.94624626  47.5041335  116.35597672\n",
      "   0.          90.49909301]\n",
      "10-th iteration, loss: 0.17449147234750545, 18 gd steps\n",
      "insert gradient: -0.03472360098908727\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  1.20178088  62.92794384 142.2025151   89.41611574 136.48870671\n",
      "  65.65480031  97.20498414  45.47999212  70.29991879  45.81689315\n",
      " 121.2488687    0.          67.36048261  46.63881542 292.82470751\n",
      "  49.31608446 114.05693681  12.86613509  90.49909301]\n",
      "11-th iteration, loss: 0.16668218048750977, 55 gd steps\n",
      "insert gradient: -0.01593312549237066\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  3.5253491   60.18686751 133.88348592  93.2798355  124.65661971\n",
      "  75.87484287 100.64902787  42.95968115  75.67480193  44.85961501\n",
      "  96.64026229  22.94527118  29.82029948  54.02827887 235.306908\n",
      "   0.          47.0613816   57.43313645 105.49395048  16.71655286\n",
      "  90.49909301]\n",
      "12-th iteration, loss: 0.1547942949646902, 48 gd steps\n",
      "insert gradient: -0.07919237317137039\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  0.88268592  58.92853285 147.63725007  80.80781992 146.04958625\n",
      "  70.10586061 106.19509103  51.51107997  72.94853399  45.57362684\n",
      "  95.02457061  48.79261038   2.20393854  51.97443494 100.88809161\n",
      "   0.         121.06570994  20.54556483  26.33884861  53.32893596\n",
      "  97.317477    26.45404666  90.49909301]\n",
      "13-th iteration, loss: 0.1396474533759769, 36 gd steps\n",
      "insert gradient: -0.02320794376938237\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  3.47692081  72.06206951 129.56180477  92.41047869 142.17789566\n",
      "  70.49211708 108.55220758  57.80530348  73.95210337  45.42801785\n",
      "  93.4474594   57.26641921  11.69126988  45.25980924  99.42510798\n",
      "  21.45460935  97.03700458  27.25892358  19.83238018  39.89116834\n",
      "  95.35464925  37.30792153  90.49909301]\n",
      "14-th iteration, loss: 0.13513290824912436, 25 gd steps\n",
      "insert gradient: -0.014511081786833671\n",
      "14-th iteration, new layer inserted. now 24 layers\n",
      "[  1.72598197  66.92385863 128.94483836  96.65802379 139.07595658\n",
      "  74.02403791 110.28878555  59.05990155  75.47375372  44.92714561\n",
      "  93.48355412  57.20293229  18.05813384  40.30634174 101.18140491\n",
      "  27.18931417  86.52640329  30.29710952  21.92896516  34.58111211\n",
      "  96.61488673  40.36948604  90.49909301   0.        ]\n",
      "15-th iteration, loss: 0.12850622920474158, 19 gd steps\n",
      "insert gradient: -0.03777971489251498\n",
      "15-th iteration, new layer inserted. now 24 layers\n",
      "[  3.63496437  72.92964109 132.03623451  99.78155057 124.05522742\n",
      "  72.43401663 110.77691589  61.84574125  79.82989526  43.36031684\n",
      "  93.8594505   52.99705723  41.29454229  23.14862423 105.36799164\n",
      "  46.30513991  56.0797098   41.15112157  30.53416517  15.72791787\n",
      " 105.5975147   51.39415713  77.06443862  13.5810605 ]\n",
      "16-th iteration, loss: 0.10986040957904207, 57 gd steps\n",
      "insert gradient: -0.013635997789560322\n",
      "16-th iteration, new layer inserted. now 22 layers\n",
      "[  1.63664213  69.16231876 131.56493414  96.50872113 136.05668388\n",
      "  78.35320526 115.91151073  61.73651816  87.854176    42.85377808\n",
      "  94.71914677  57.90341582  90.06126491  10.16930997  90.04335885\n",
      "  49.72232905  50.95124632  36.84882278 180.66265859  53.22206074\n",
      "  75.5418891   27.05870172]\n",
      "17-th iteration, loss: 0.1072726345774952, 28 gd steps\n",
      "insert gradient: -0.0333253104017554\n",
      "17-th iteration, new layer inserted. now 24 layers\n",
      "[  2.84095239  73.24814683 129.15991177  95.73515865 139.14672827\n",
      "  78.67795544 118.79135358  62.07523375  89.0421677   43.9140861\n",
      "  93.94509992  59.47971866 100.94917964  15.91238486  65.14374029\n",
      "  51.37378647  55.68006901  32.52081522 110.26321343   0.\n",
      "  73.50880895  55.28229074  81.06864492  28.7953789 ]\n",
      "18-th iteration, loss: 0.10184569383238011, 25 gd steps\n",
      "insert gradient: -0.02306975973988649\n",
      "18-th iteration, new layer inserted. now 24 layers\n",
      "[  2.40098667  72.48971147 131.04340563  96.21603724 138.99717014\n",
      "  81.4539457  121.25165847  64.15275131  92.69091775  44.07170211\n",
      "  92.66902462  60.38168458 104.80942303  24.06918234  49.07672959\n",
      "  54.04802449  60.185682    30.1085032  101.49777734   8.81488083\n",
      "  59.79978349  56.34692496  89.35405638  31.2249983 ]\n",
      "19-th iteration, loss: 0.08100848902295234, 34 gd steps\n",
      "insert gradient: -0.012399191313752306\n",
      "19-th iteration, new layer inserted. now 24 layers\n",
      "[  0.91726756  68.60516102 126.53313735  91.58871774 155.84660416\n",
      "  87.25151815 127.13624921  68.04771807 107.12519239  53.87876214\n",
      "  80.88873614  57.18373173 101.73090428  62.24313658  30.14643163\n",
      "  38.72391535  92.41054145  42.45680257  66.04690531  49.84623769\n",
      "  36.24959606  23.6469734   88.00488566  49.94797703]\n",
      "20-th iteration, loss: 0.07427854974484949, 42 gd steps\n",
      "insert gradient: -0.008623496513854549\n",
      "20-th iteration, new layer inserted. now 24 layers\n",
      "[  2.31596748  79.17324941 133.31627624  98.09912036 150.96299202\n",
      "  76.41918682 132.46060069  67.25605929 113.4302339   54.71894428\n",
      "  85.38220826  55.48048195 105.3639532   57.53675514  69.00676007\n",
      "  25.97459708  86.63322826  49.2309384   68.64711545  43.08567919\n",
      "  67.03915183  16.04183383  70.55290176  51.68220219]\n",
      "21-th iteration, loss: 0.07276548480387512, 21 gd steps\n",
      "insert gradient: -0.004323910178517996\n",
      "21-th iteration, new layer inserted. now 24 layers\n",
      "[  2.09954181  78.57821623 135.59438269  98.31154393 148.77818327\n",
      "  76.82616219 133.65667222  69.23297028 114.85899485  56.86293302\n",
      "  85.64291917  53.90768167 106.19634388  58.77426279  77.25171216\n",
      "  25.84402921  83.04577376  48.05859267  71.18599049  41.41188471\n",
      "  74.77302151  16.97372243  59.61837768  51.02448213]\n",
      "22-th iteration, loss: 0.06699161008108116, 31 gd steps\n",
      "insert gradient: -0.005723571363698873\n",
      "22-th iteration, new layer inserted. now 24 layers\n",
      "[  2.73864962  80.54529957 141.62918346  90.76565709 156.66601232\n",
      "  80.64947871 128.4356503   73.38040443 115.80744981  65.67327653\n",
      "  85.32709728  52.79817598 106.84901157  59.99654642  91.47104593\n",
      "  34.1074497   68.04886797  45.14080312  82.62143787  39.7424643\n",
      "  81.93618014  44.90487455  10.24426121  47.66655456]\n",
      "23-th iteration, loss: 0.06618363848505342, 34 gd steps\n",
      "insert gradient: -0.001638525403800355\n",
      "23-th iteration, new layer inserted. now 26 layers\n",
      "[2.31528170e+00 8.27755630e+01 1.40583933e+02 8.97719508e+01\n",
      " 1.61455083e+02 8.16727851e+01 1.27293651e+02 7.42473898e+01\n",
      " 1.15392984e+02 0.00000000e+00 2.48689958e-14 6.73007813e+01\n",
      " 8.86274488e+01 5.27517864e+01 1.05210359e+02 6.05982522e+01\n",
      " 9.33600327e+01 3.86848969e+01 6.14607935e+01 4.72516945e+01\n",
      " 8.22165509e+01 4.12823658e+01 7.57052858e+01 5.21079653e+01\n",
      " 1.20644918e+01 4.13661571e+01]\n",
      "24-th iteration, loss: 0.06602349326029458, 38 gd steps\n",
      "insert gradient: -0.0011002026784198273\n",
      "24-th iteration, new layer inserted. now 26 layers\n",
      "[2.11275872e+00 8.38073599e+01 1.41073348e+02 9.01332602e+01\n",
      " 1.62599981e+02 8.19638423e+01 1.27486578e+02 7.46615885e+01\n",
      " 1.15624277e+02 6.78030752e+01 9.05779973e+01 5.27129474e+01\n",
      " 1.05165357e+02 6.11999301e+01 9.43145323e+01 0.00000000e+00\n",
      " 3.55271368e-15 3.92247650e+01 6.11538115e+01 4.67685283e+01\n",
      " 8.34912019e+01 4.15385234e+01 7.63104843e+01 5.13128880e+01\n",
      " 1.51770691e+01 3.92649370e+01]\n",
      "25-th iteration, loss: 0.06599337192939551, 218 gd steps\n",
      "insert gradient: -4.587454294683503e-05\n",
      "25-th iteration, new layer inserted. now 26 layers\n",
      "[2.02743694e+00 8.44618739e+01 1.40859074e+02 9.02795189e+01\n",
      " 1.63349980e+02 8.21398512e+01 1.27447668e+02 7.49055016e+01\n",
      " 1.15576898e+02 6.79220411e+01 9.19144843e+01 5.23939565e+01\n",
      " 1.05245305e+02 6.15878365e+01 9.46489923e+01 3.97731878e+01\n",
      " 6.10919882e+01 4.65590942e+01 8.39239452e+01 4.18415787e+01\n",
      " 7.61659646e+01 5.13423461e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.68898725e+01 3.78259762e+01]\n",
      "26-th iteration, loss: 0.06599234126470417, 27 gd steps\n",
      "insert gradient: -5.1756614904157374e-05\n",
      "26-th iteration, new layer inserted. now 26 layers\n",
      "[1.98528565e+00 8.45520625e+01 1.40809321e+02 9.02174743e+01\n",
      " 1.63493259e+02 8.21390666e+01 1.27436140e+02 7.49172124e+01\n",
      " 1.15558939e+02 6.78763419e+01 9.21853967e+01 5.22932622e+01\n",
      " 1.05284315e+02 0.00000000e+00 2.48689958e-14 6.16395379e+01\n",
      " 9.47399211e+01 3.97906119e+01 6.10902114e+01 4.65163416e+01\n",
      " 8.39826522e+01 4.18314729e+01 7.63622822e+01 5.13360730e+01\n",
      " 1.73806399e+01 3.73147822e+01]\n",
      "27-th iteration, loss: 0.06599227969769453, 14 gd steps\n",
      "insert gradient: -2.3268583537884075e-05\n",
      "27-th iteration, new layer inserted. now 24 layers\n",
      "[  1.98565535  84.57257179 140.76949121  90.22603005 163.50479508\n",
      "  82.1433572  127.42212066  74.93035004 115.52797638  67.89276038\n",
      "  92.19760269  52.28790147 105.29786702  61.65972464  94.7199023\n",
      "  39.79607929  61.10470779  46.49887855  83.9908809   41.85521371\n",
      "  76.33349407  51.3439641   17.42516875  37.25023689]\n",
      "28-th iteration, loss: 0.06599224885019563, 19 gd steps\n",
      "insert gradient: -1.4325818509621193e-05\n",
      "28-th iteration, new layer inserted. now 26 layers\n",
      "[1.98465543e+00 8.45861047e+01 1.40770473e+02 9.02261693e+01\n",
      " 1.63512460e+02 0.00000000e+00 2.13162821e-14 8.21453047e+01\n",
      " 1.27423501e+02 7.49271322e+01 1.15537919e+02 6.78913887e+01\n",
      " 9.22122638e+01 5.22815978e+01 1.05303597e+02 6.16607490e+01\n",
      " 9.47522771e+01 3.98101808e+01 6.10936168e+01 4.64952033e+01\n",
      " 8.39892642e+01 4.18478017e+01 7.63724917e+01 5.13515476e+01\n",
      " 1.74673472e+01 3.72044577e+01]\n",
      "29-th iteration, loss: 0.06599223700719413, 24 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.755386603621342e-06\n",
      "29-th iteration, new layer inserted. now 26 layers\n",
      "[1.98486631e+00 8.45953819e+01 1.40767472e+02 9.02294172e+01\n",
      " 1.63517465e+02 8.21500758e+01 1.27419554e+02 0.00000000e+00\n",
      " 3.19744231e-14 7.49297675e+01 1.15530734e+02 6.78992278e+01\n",
      " 9.22214485e+01 5.22767799e+01 1.05303950e+02 6.16733311e+01\n",
      " 9.47416142e+01 3.98141665e+01 6.11023099e+01 4.64893753e+01\n",
      " 8.39930145e+01 4.18552841e+01 7.63644731e+01 5.13575961e+01\n",
      " 1.74873286e+01 3.71822403e+01]\n",
      "30-th iteration, loss: 0.06599223699734316, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.661831381199247e-06\n",
      "30-th iteration, new layer inserted. now 28 layers\n",
      "[1.98486691e+00 8.45953854e+01 1.40767473e+02 9.02294206e+01\n",
      " 1.63517467e+02 8.21500800e+01 1.27419557e+02 6.70781852e-06\n",
      " 2.42670365e-06 0.00000000e+00 7.41153829e-22 7.49297742e+01\n",
      " 1.15530737e+02 6.78992342e+01 9.22214514e+01 5.22767817e+01\n",
      " 1.05303952e+02 6.16733335e+01 9.47416145e+01 3.98141641e+01\n",
      " 6.11023074e+01 4.64893689e+01 8.39930118e+01 4.18552802e+01\n",
      " 7.63644727e+01 5.13575978e+01 1.74873336e+01 3.71822419e+01]\n",
      "31-th iteration, loss: 0.06599223698630036, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.546706511204982e-06\n",
      "31-th iteration, new layer inserted. now 30 layers\n",
      "[1.98486750e+00 8.45953888e+01 1.40767473e+02 9.02294239e+01\n",
      " 1.63517469e+02 8.21500841e+01 1.27419559e+02 1.33105737e-05\n",
      " 4.79983832e-06 6.60489571e-06 2.37313467e-06 0.00000000e+00\n",
      " 2.11758237e-22 7.49297808e+01 1.15530740e+02 6.78992405e+01\n",
      " 9.22214544e+01 5.22767835e+01 1.05303953e+02 6.16733359e+01\n",
      " 9.47416149e+01 3.98141618e+01 6.11023050e+01 4.64893627e+01\n",
      " 8.39930091e+01 4.18552764e+01 7.63644723e+01 5.13575995e+01\n",
      " 1.74873385e+01 3.71822435e+01]\n",
      "32-th iteration, loss: 0.06599223697421329, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.4112416502453454e-06\n",
      "32-th iteration, new layer inserted. now 32 layers\n",
      "[1.98486808e+00 8.45953922e+01 1.40767474e+02 9.02294271e+01\n",
      " 1.63517471e+02 8.21500882e+01 1.27419562e+02 1.97851651e-05\n",
      " 7.10982600e-06 1.30837209e-05 4.67729633e-06 6.48091847e-06\n",
      " 2.30416166e-06 0.00000000e+00 1.05879118e-22 7.49297873e+01\n",
      " 1.15530743e+02 6.78992468e+01 9.22214573e+01 5.22767852e+01\n",
      " 1.05303955e+02 6.16733383e+01 9.47416154e+01 3.98141598e+01\n",
      " 6.11023028e+01 4.64893566e+01 8.39930065e+01 4.18552727e+01\n",
      " 7.63644720e+01 5.13576012e+01 1.74873434e+01 3.71822451e+01]\n",
      "33-th iteration, loss: 0.06599223696124887, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.2568852947006584e-06\n",
      "33-th iteration, new layer inserted. now 34 layers\n",
      "[1.98486865e+00 8.45953956e+01 1.40767474e+02 9.02294303e+01\n",
      " 1.63517474e+02 8.21500922e+01 1.27419564e+02 2.61099477e-05\n",
      " 9.34775365e-06 1.94147750e-05 6.90368313e-06 1.28160983e-05\n",
      " 4.52483179e-06 6.33721229e-06 2.22067013e-06 0.00000000e+00\n",
      " 4.23516474e-22 7.49297936e+01 1.15530746e+02 6.78992530e+01\n",
      " 9.22214602e+01 5.22767870e+01 1.05303957e+02 6.16733407e+01\n",
      " 9.47416159e+01 3.98141579e+01 6.11023006e+01 4.64893507e+01\n",
      " 8.39930040e+01 4.18552691e+01 7.63644716e+01 5.13576029e+01\n",
      " 1.74873483e+01 3.71822467e+01]\n",
      "34-th iteration, loss: 0.06599223694758749, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.111065349495783e-06\n",
      "34-th iteration, new layer inserted. now 36 layers\n",
      "[1.98486921e+00 8.45953989e+01 1.40767475e+02 9.02294333e+01\n",
      " 1.63517476e+02 8.21500961e+01 1.27419566e+02 3.22649724e-05\n",
      " 1.15054728e-05 2.55780451e-05 9.04427696e-06 1.89854580e-05\n",
      " 6.65412081e-06 1.25105633e-05 4.34436924e-06 6.17530979e-06\n",
      " 2.12369910e-06 7.49297998e+01 1.15530749e+02 0.00000000e+00\n",
      " 1.42108547e-14 6.78992591e+01 9.22214631e+01 5.22767888e+01\n",
      " 1.05303958e+02 6.16733432e+01 9.47416164e+01 3.98141561e+01\n",
      " 6.11022986e+01 4.64893450e+01 8.39930016e+01 4.18552656e+01\n",
      " 7.63644713e+01 5.13576046e+01 1.74873532e+01 3.71822483e+01]\n",
      "35-th iteration, loss: 0.06599223693325777, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.0229272308871275e-06\n",
      "35-th iteration, new layer inserted. now 36 layers\n",
      "[1.98486976e+00 8.45954022e+01 1.40767475e+02 9.02294364e+01\n",
      " 1.63517478e+02 8.21501000e+01 1.27419568e+02 3.82351869e-05\n",
      " 1.35773272e-05 3.15584084e-05 1.10935694e-05 2.49737991e-05\n",
      " 8.68666659e-06 1.85047738e-05 6.36587970e-06 1.21733524e-05\n",
      " 4.13976245e-06 7.49298058e+01 1.15530752e+02 6.04919216e-06\n",
      " 2.84303547e-06 6.78992652e+01 9.22214660e+01 5.22767906e+01\n",
      " 1.05303960e+02 6.16733457e+01 9.47416170e+01 3.98141546e+01\n",
      " 6.11022967e+01 4.64893395e+01 8.39929993e+01 4.18552622e+01\n",
      " 7.63644710e+01 5.13576062e+01 1.74873581e+01 3.71822500e+01]\n",
      "36-th iteration, loss: 0.0659922369196319, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.9371858232163504e-06\n",
      "36-th iteration, new layer inserted. now 36 layers\n",
      "[1.98487030e+00 8.45954054e+01 1.40767475e+02 9.02294393e+01\n",
      " 1.63517480e+02 8.21501037e+01 1.27419570e+02 4.40245268e-05\n",
      " 1.55673067e-05 3.73597246e-05 1.30557119e-05 3.07849007e-05\n",
      " 1.06267802e-05 2.43235377e-05 8.28967057e-06 1.79977315e-05\n",
      " 6.05281544e-06 7.49298116e+01 1.15530755e+02 1.20094129e-05\n",
      " 5.60629231e-06 6.78992712e+01 9.22214689e+01 5.22767924e+01\n",
      " 1.05303962e+02 6.16733483e+01 9.47416177e+01 3.98141533e+01\n",
      " 6.11022949e+01 4.64893341e+01 8.39929970e+01 4.18552589e+01\n",
      " 7.63644707e+01 5.13576079e+01 1.74873629e+01 3.71822516e+01]\n",
      "37-th iteration, loss: 0.0659922369066638, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.853763542581105e-06\n",
      "37-th iteration, new layer inserted. now 38 layers\n",
      "[1.98487083e+00 8.45954086e+01 1.40767476e+02 9.02294422e+01\n",
      " 1.63517482e+02 8.21501074e+01 1.27419572e+02 4.96391001e-05\n",
      " 1.74782521e-05 4.29880295e-05 1.49337030e-05 3.64247218e-05\n",
      " 1.24776164e-05 2.99727326e-05 1.01190514e-05 2.36542385e-05\n",
      " 7.86632082e-06 7.49298173e+01 1.15530758e+02 1.78830762e-05\n",
      " 8.29216170e-06 0.00000000e+00 1.27054942e-21 6.78992770e+01\n",
      " 9.22214717e+01 5.22767941e+01 1.05303964e+02 6.16733508e+01\n",
      " 9.47416185e+01 3.98141522e+01 6.11022933e+01 4.64893290e+01\n",
      " 8.39929949e+01 4.18552557e+01 7.63644705e+01 5.13576095e+01\n",
      " 1.74873677e+01 3.71822532e+01]\n",
      "38-th iteration, loss: 0.06599223689310935, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.75612239460469e-06\n",
      "38-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487136e+00 8.45954118e+01 1.40767476e+02 9.02294451e+01\n",
      " 1.63517484e+02 8.21501111e+01 1.27419574e+02 5.50791616e-05\n",
      " 1.93101844e-05 4.84435083e-05 1.67277162e-05 4.18933734e-05\n",
      " 1.42395000e-05 3.54523904e-05 1.18544966e-05 2.91428222e-05\n",
      " 9.58090096e-06 7.49298228e+01 1.15530760e+02 2.36644988e-05\n",
      " 1.08993138e-05 5.78810705e-06 2.60715214e-06 0.00000000e+00\n",
      " 1.05879118e-22 6.78992828e+01 9.22214746e+01 5.22767959e+01\n",
      " 1.05303965e+02 6.16733534e+01 9.47416193e+01 3.98141512e+01\n",
      " 6.11022918e+01 4.64893240e+01 8.39929928e+01 4.18552526e+01\n",
      " 7.63644703e+01 5.13576111e+01 1.74873725e+01 3.71822548e+01]\n",
      "39-th iteration, loss: 0.06599223687906515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.645085078817563e-06\n",
      "39-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487187e+00 8.45954149e+01 1.40767476e+02 9.02294479e+01\n",
      " 1.63517486e+02 8.21501146e+01 1.27419576e+02 6.03431195e-05\n",
      " 2.10642701e-05 5.37244994e-05 1.84390707e-05 4.71891197e-05\n",
      " 1.59139009e-05 4.07606972e-05 1.34976255e-05 3.44615857e-05\n",
      " 1.11983234e-05 7.49298281e+01 1.15530763e+02 2.93378127e-05\n",
      " 1.34226002e-05 1.14702070e-05 5.12577260e-06 5.68420158e-06\n",
      " 2.51862046e-06 6.78992885e+01 9.22214774e+01 5.22767976e+01\n",
      " 1.05303967e+02 6.16733560e+01 9.47416202e+01 3.98141505e+01\n",
      " 6.11022904e+01 4.64893192e+01 8.39929908e+01 4.18552497e+01\n",
      " 7.63644701e+01 5.13576127e+01 1.74873772e+01 3.71822564e+01]\n",
      "40-th iteration, loss: 0.06599223686573, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.537295221607628e-06\n",
      "40-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487238e+00 8.45954180e+01 1.40767476e+02 9.02294506e+01\n",
      " 1.63517488e+02 8.21501182e+01 1.27419577e+02 6.54352241e-05\n",
      " 2.27443853e-05 5.88351845e-05 2.00717962e-05 5.23160697e-05\n",
      " 1.75050016e-05 4.59016846e-05 1.50527717e-05 3.96144792e-05\n",
      " 1.27230716e-05 7.49298333e+01 1.15530765e+02 3.48958217e-05\n",
      " 1.58607917e-05 1.70390362e-05 7.55471780e-06 1.12571628e-05\n",
      " 4.94298354e-06 6.78992941e+01 9.22214802e+01 5.22767994e+01\n",
      " 1.05303969e+02 6.16733587e+01 9.47416211e+01 3.98141500e+01\n",
      " 6.11022891e+01 4.64893146e+01 8.39929889e+01 4.18552468e+01\n",
      " 7.63644699e+01 5.13576143e+01 1.74873819e+01 3.71822580e+01]\n",
      "41-th iteration, loss: 0.06599223685305589, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.432644580232234e-06\n",
      "41-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487288e+00 8.45954211e+01 1.40767476e+02 9.02294533e+01\n",
      " 1.63517490e+02 8.21501216e+01 1.27419579e+02 7.03616710e-05\n",
      " 2.43532836e-05 6.37816939e-05 2.16287967e-05 5.72802842e-05\n",
      " 1.90158548e-05 5.08813401e-05 1.65231356e-05 4.46074125e-05\n",
      " 1.41584920e-05 7.49298383e+01 1.15530768e+02 4.03419292e-05\n",
      " 1.82166344e-05 2.24979293e-05 9.89682514e-06 1.67221459e-05\n",
      " 7.27601632e-06 6.78992996e+01 9.22214829e+01 5.22768010e+01\n",
      " 1.05303970e+02 6.16733613e+01 9.47416221e+01 3.98141496e+01\n",
      " 6.11022880e+01 4.64893101e+01 8.39929871e+01 4.18552440e+01\n",
      " 7.63644697e+01 5.13576159e+01 1.74873866e+01 3.71822596e+01]\n",
      "42-th iteration, loss: 0.06599223684099824, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.331028716504168e-06\n",
      "42-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487338e+00 8.45954241e+01 1.40767476e+02 9.02294560e+01\n",
      " 1.63517492e+02 8.21501250e+01 1.27419581e+02 7.51284307e-05\n",
      " 2.58936167e-05 6.85699354e-05 2.31128686e-05 6.20876042e-05\n",
      " 2.04494004e-05 5.57054338e-05 1.79117994e-05 4.94460811e-05\n",
      " 1.55078081e-05 7.49298431e+01 1.15530770e+02 4.56794253e-05\n",
      " 2.04927788e-05 2.78501102e-05 1.21548336e-05 2.20823047e-05\n",
      " 9.52054476e-06 6.78993049e+01 9.22214857e+01 5.22768027e+01\n",
      " 1.05303972e+02 6.16733640e+01 9.47416232e+01 3.98141494e+01\n",
      " 6.11022870e+01 4.64893058e+01 8.39929853e+01 4.18552413e+01\n",
      " 7.63644695e+01 5.13576175e+01 1.74873913e+01 3.71822612e+01]\n",
      "43-th iteration, loss: 0.0659922368295156, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.232346859305266e-06\n",
      "43-th iteration, new layer inserted. now 40 layers\n",
      "[1.98487387e+00 8.45954271e+01 1.40767476e+02 9.02294586e+01\n",
      " 1.63517494e+02 8.21501284e+01 1.27419582e+02 7.97412571e-05\n",
      " 2.73679378e-05 7.32056022e-05 2.45267049e-05 6.67436587e-05\n",
      " 2.18084698e-05 6.03795264e-05 1.92217316e-05 5.41359735e-05\n",
      " 1.67741244e-05 7.49298478e+01 1.15530772e+02 5.09114913e-05\n",
      " 2.26917829e-05 3.30986958e-05 1.43313871e-05 2.73406885e-05\n",
      " 1.16792974e-05 6.78993102e+01 9.22214884e+01 5.22768044e+01\n",
      " 1.05303973e+02 6.16733667e+01 9.47416243e+01 3.98141494e+01\n",
      " 6.11022861e+01 4.64893017e+01 8.39929837e+01 4.18552388e+01\n",
      " 7.63644694e+01 5.13576190e+01 1.74873959e+01 3.71822628e+01]\n",
      "44-th iteration, loss: 0.0659922368185694, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.13650177313823e-06\n",
      "44-th iteration, new layer inserted. now 42 layers\n",
      "[1.98487435e+00 8.45954301e+01 1.40767476e+02 9.02294612e+01\n",
      " 1.63517496e+02 8.21501317e+01 1.27419584e+02 8.42056947e-05\n",
      " 2.87787056e-05 7.76941809e-05 2.58728987e-05 7.12538722e-05\n",
      " 2.30957896e-05 6.49089769e-05 2.04557911e-05 5.86823794e-05\n",
      " 1.79604311e-05 7.49298524e+01 1.15530774e+02 5.60412033e-05\n",
      " 2.48161156e-05 3.82467000e-05 1.64290383e-05 3.25002456e-05\n",
      " 1.37549087e-05 0.00000000e+00 8.47032947e-22 6.78993154e+01\n",
      " 9.22214910e+01 5.22768060e+01 1.05303975e+02 6.16733695e+01\n",
      " 9.47416254e+01 3.98141496e+01 6.11022853e+01 4.64892977e+01\n",
      " 8.39929821e+01 4.18552363e+01 7.63644693e+01 5.13576205e+01\n",
      " 1.74874006e+01 3.71822643e+01]\n",
      "45-th iteration, loss: 0.06599223680724431, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.029288339379959e-06\n",
      "45-th iteration, new layer inserted. now 44 layers\n",
      "[1.98487483e+00 8.45954331e+01 1.40767476e+02 9.02294637e+01\n",
      " 1.63517498e+02 8.21501350e+01 1.27419585e+02 8.85223154e-05\n",
      " 3.01259897e-05 8.20361871e-05 2.71516494e-05 7.56187006e-05\n",
      " 2.43116879e-05 6.92941780e-05 2.16144337e-05 6.30856243e-05\n",
      " 1.90673104e-05 7.49298568e+01 1.15530776e+02 6.10646758e-05\n",
      " 2.68650713e-05 4.32901774e-05 1.84471626e-05 3.75569668e-05\n",
      " 1.57468338e-05 5.06780929e-06 1.99192514e-06 0.00000000e+00\n",
      " 1.05879118e-22 6.78993204e+01 9.22214937e+01 5.22768076e+01\n",
      " 1.05303977e+02 6.16733722e+01 9.47416267e+01 3.98141499e+01\n",
      " 6.11022846e+01 4.64892939e+01 8.39929806e+01 4.18552338e+01\n",
      " 7.63644692e+01 5.13576220e+01 1.74874052e+01 3.71822659e+01]\n",
      "46-th iteration, loss: 0.06599223679563104, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.911606553781197e-06\n",
      "46-th iteration, new layer inserted. now 44 layers\n",
      "[1.98487530e+00 8.45954360e+01 1.40767476e+02 9.02294662e+01\n",
      " 1.63517500e+02 8.21501382e+01 1.27419587e+02 9.26902958e-05\n",
      " 3.14109345e-05 8.62307414e-05 2.83642307e-05 7.98372052e-05\n",
      " 2.54575664e-05 7.35341278e-05 2.26991883e-05 6.73446401e-05\n",
      " 2.00964175e-05 7.49298611e+01 1.15530778e+02 6.59695559e-05\n",
      " 2.88348139e-05 4.82167140e-05 2.03820080e-05 4.24983740e-05\n",
      " 1.76514038e-05 1.00219104e-05 3.89240982e-06 4.95570687e-06\n",
      " 1.90048467e-06 6.78993254e+01 9.22214963e+01 5.22768091e+01\n",
      " 1.05303978e+02 6.16733749e+01 9.47416279e+01 3.98141504e+01\n",
      " 6.11022841e+01 4.64892903e+01 8.39929791e+01 4.18552315e+01\n",
      " 7.63644691e+01 5.13576235e+01 1.74874097e+01 3.71822675e+01]\n",
      "47-th iteration, loss: 0.0659922367846083, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.797763286173402e-06\n",
      "47-th iteration, new layer inserted. now 46 layers\n",
      "[1.98487577e+00 8.45954389e+01 1.40767476e+02 9.02294687e+01\n",
      " 1.63517502e+02 8.21501413e+01 1.27419588e+02 9.67137843e-05\n",
      " 3.26369625e-05 9.02819373e-05 2.95141950e-05 8.39134210e-05\n",
      " 2.65371067e-05 7.76327996e-05 2.37138646e-05 7.14633347e-05\n",
      " 2.10516895e-05 7.49298652e+01 1.15530780e+02 7.07510601e-05\n",
      " 3.07248894e-05 5.30214627e-05 2.22332148e-05 4.73195531e-05\n",
      " 1.94683516e-05 1.48573187e-05 5.70127873e-06 9.79425292e-06\n",
      " 3.70535868e-06 0.00000000e+00 2.11758237e-22 6.78993302e+01\n",
      " 9.22214988e+01 5.22768106e+01 1.05303980e+02 6.16733777e+01\n",
      " 9.47416293e+01 3.98141511e+01 6.11022836e+01 4.64892867e+01\n",
      " 8.39929777e+01 4.18552293e+01 7.63644690e+01 5.13576250e+01\n",
      " 1.74874143e+01 3.71822690e+01]\n",
      "48-th iteration, loss: 0.06599223677338979, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.674640846175244e-06\n",
      "48-th iteration, new layer inserted. now 46 layers\n",
      "[1.98487623e+00 8.45954418e+01 1.40767476e+02 9.02294712e+01\n",
      " 1.63517504e+02 8.21501445e+01 1.27419589e+02 1.00594087e-04\n",
      " 3.38043603e-05 9.41910287e-05 3.06019555e-05 8.78485468e-05\n",
      " 2.75508476e-05 8.15913336e-05 2.46591259e-05 7.54427866e-05\n",
      " 2.19339134e-05 7.49298692e+01 1.15530782e+02 7.54070034e-05\n",
      " 3.25353115e-05 5.77021742e-05 2.40008949e-05 5.20181874e-05\n",
      " 2.11978862e-05 1.95716469e-05 7.41883656e-06 1.45131770e-05\n",
      " 5.41502112e-06 4.72191109e-06 1.70966245e-06 6.78993349e+01\n",
      " 9.22215014e+01 5.22768121e+01 1.05303981e+02 6.16733805e+01\n",
      " 9.47416306e+01 3.98141519e+01 6.11022833e+01 4.64892834e+01\n",
      " 8.39929764e+01 4.18552271e+01 7.63644689e+01 5.13576265e+01\n",
      " 1.74874188e+01 3.71822705e+01]\n",
      "49-th iteration, loss: 0.0659922367627618, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.555803791443222e-06\n",
      "49-th iteration, new layer inserted. now 48 layers\n",
      "[1.98487670e+00 8.45954447e+01 1.40767476e+02 9.02294736e+01\n",
      " 1.63517506e+02 8.21501476e+01 1.27419590e+02 1.04335531e-04\n",
      " 3.49164798e-05 9.79622921e-05 3.16309894e-05 9.16468041e-05\n",
      " 2.85023906e-05 8.54138934e-05 2.55386974e-05 7.92870981e-05\n",
      " 2.27469371e-05 7.49298731e+01 1.15530784e+02 7.99336580e-05\n",
      " 3.42660187e-05 6.22550563e-05 2.56850868e-05 5.65904173e-05\n",
      " 2.28401449e-05 2.41609650e-05 9.04531803e-06 1.91084756e-05\n",
      " 7.02980315e-06 9.32157488e-06 3.32063802e-06 0.00000000e+00\n",
      " 2.11758237e-22 6.78993395e+01 9.22215038e+01 5.22768135e+01\n",
      " 1.05303983e+02 6.16733833e+01 9.47416321e+01 3.98141529e+01\n",
      " 6.11022830e+01 4.64892802e+01 8.39929752e+01 4.18552250e+01\n",
      " 7.63644689e+01 5.13576279e+01 1.74874233e+01 3.71822721e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.52869947125054\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.61863535    0.         1611.67282194]\n",
      "1-th iteration, loss: 0.7529863819158966, 11 gd steps\n",
      "insert gradient: -0.6442270635205459\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.49687716   62.08426795  235.85455931    0.         1375.81826263]\n",
      "2-th iteration, loss: 0.6029245502615721, 13 gd steps\n",
      "insert gradient: -0.608710860623655\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.54822329   76.74823385  220.63789184   41.80686294  252.70131354\n",
      "    0.         1123.11694909]\n",
      "3-th iteration, loss: 0.44000070178208356, 29 gd steps\n",
      "insert gradient: -0.6344719020480623\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[5.92823581e-01 4.23367158e+01 1.10170741e+02 0.00000000e+00\n",
      " 1.16651372e+02 7.32876459e+01 1.11730747e+02 7.32180347e+01\n",
      " 1.12311695e+03]\n",
      "4-th iteration, loss: 0.36328076069282983, 62 gd steps\n",
      "insert gradient: -0.42948561453371903\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  0.          44.35555533  55.34268203  48.90080444 106.83695294\n",
      "  76.81961079 123.18013232  80.14857522 207.9846202    0.\n",
      " 915.13232889]\n",
      "5-th iteration, loss: 0.24322541346869353, 40 gd steps\n",
      "insert gradient: -0.207056439762869\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.59280467  44.80199759  76.00500607  44.2983983   94.57683468\n",
      "  80.83776388 118.04880692  82.87613668 135.91254659  77.62485041\n",
      " 207.9846202    0.         707.14770868]\n",
      "6-th iteration, loss: 0.2088069488786573, 14 gd steps\n",
      "insert gradient: -0.0905919832547291\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          43.26841113  87.22525368  47.89632124  85.23444191\n",
      "  86.38802937 115.54985506  85.31021729 123.46929291  81.6796617\n",
      " 179.79951255  36.98827892 235.71590289   0.         471.43180579]\n",
      "7-th iteration, loss: 0.17523488157915063, 62 gd steps\n",
      "insert gradient: -0.05187834862696288\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  1.41737043  54.20854703  89.71810696  48.32393543  85.02035698\n",
      "  88.47074911  99.84591749  97.3126366  105.16998411  91.3238757\n",
      " 155.40335527  63.89054746 184.95494867  50.56863733 441.96731793\n",
      "   0.          29.46448786]\n",
      "8-th iteration, loss: 0.1541543505485729, 24 gd steps\n",
      "insert gradient: -0.054837621859614154\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  3.43577552  54.38255407  92.59236028  49.00952101  90.49031649\n",
      "  81.96637209  94.65655867 112.85273888  95.0876526   91.99734958\n",
      " 154.56921726  60.65871541 200.9455791   51.69910409 304.26598285\n",
      "   0.         121.70639314  22.78432054  29.46448786]\n",
      "9-th iteration, loss: 0.1459439124073753, 23 gd steps\n",
      "insert gradient: -0.01732311673399795\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[1.69832434e+00 5.22018540e+01 8.73834010e+01 4.91952503e+01\n",
      " 1.03440996e+02 0.00000000e+00 2.84217094e-14 7.84730137e+01\n",
      " 9.49006821e+01 1.11343826e+02 9.65891761e+01 9.06877195e+01\n",
      " 1.59967397e+02 5.69480352e+01 1.99229552e+02 5.97531771e+01\n",
      " 2.76209209e+02 1.90674770e+01 1.06583938e+02 1.73579935e+01\n",
      " 2.94644879e+01]\n",
      "10-th iteration, loss: 0.14296298762451332, 84 gd steps\n",
      "insert gradient: -0.02589273229087802\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  2.81087128  50.33263544  89.97898047  49.53302724 102.79636848\n",
      "  80.76916356  91.48657902 112.67522962  92.71669445  84.52713269\n",
      " 175.99122151  57.73395772 196.0790028   68.02478117 186.98625458\n",
      "   0.          62.32875153  28.17481503 111.70594451  13.0935074\n",
      "  29.46448786]\n",
      "11-th iteration, loss: 0.13581272427130692, 75 gd steps\n",
      "insert gradient: -0.028972322911079557\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  2.58366989  49.74911113  90.44031797  50.76360547 109.10839302\n",
      "  78.14394836  88.62266349  62.80419597   0.          52.33682998\n",
      "  88.48980669  83.11615193 178.61260086  58.86604573 202.39486719\n",
      "  55.824538   164.61873087  17.79153651  28.10883705  60.15787613\n",
      "  98.66999234  10.54818587  29.46448786]\n",
      "12-th iteration, loss: 0.11593877774569324, 55 gd steps\n",
      "insert gradient: -0.002785734445369024\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  0.65602482  55.37261531 100.46324699  73.55054938  97.0927855\n",
      "  53.54598119  92.12786288  48.55356762  33.16527637  40.14317479\n",
      "  93.599414    86.36303214 173.34873198  59.51142656 187.65119703\n",
      "  66.37228179 152.31741867  24.64351005   6.272314    68.69042983\n",
      " 128.80184861]\n",
      "13-th iteration, loss: 0.11559389178871801, 41 gd steps\n",
      "insert gradient: -0.0014387513644371166\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  0.72130701  55.31765782 101.50898803  72.5896766   95.71269625\n",
      "  52.48950773  91.74824263  49.0048352   31.99453973  39.92595155\n",
      "  93.39051954  85.62396534 171.30916501  61.17520707 185.7402009\n",
      "  64.20842277 152.35708532  26.50144114   2.02083523  70.22428525\n",
      " 117.09258964   0.          11.70925896]\n",
      "14-th iteration, loss: 0.11557121651652742, 31 gd steps\n",
      "insert gradient: -0.0001452631589684807\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[6.85017105e-01 0.00000000e+00 1.11022302e-16 5.51242361e+01\n",
      " 1.01758033e+02 7.25131932e+01 9.52339733e+01 5.24554707e+01\n",
      " 9.17101625e+01 4.91055195e+01 3.17594153e+01 3.98594445e+01\n",
      " 9.34068840e+01 8.55774054e+01 1.70663824e+02 6.15359701e+01\n",
      " 1.85377738e+02 6.37994906e+01 1.53273934e+02 2.62986893e+01\n",
      " 1.35862262e+00 7.05357811e+01 1.17180833e+02 8.89485028e-01\n",
      " 1.17092590e+01]\n",
      "15-th iteration, loss: 0.11556613107735464, 557 gd steps\n",
      "insert gradient: -4.996562908115255e-05\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69860284  55.17826513 102.00215632  72.29760252  95.05415684\n",
      "  52.39596723  91.671674    49.09642772  31.72210005  39.90132751\n",
      "  93.32947306  85.75769935 169.89295665  61.76882316 185.29562717\n",
      "  63.43035437 154.61925428  25.44162968   1.1818331   70.85148522\n",
      " 117.23776096   0.985106    11.70925896]\n",
      "16-th iteration, loss: 0.11556419343804325, 184 gd steps\n",
      "insert gradient: -6.998698943120867e-05\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69354278  55.1733945  102.09783089  72.17910747  95.02942978\n",
      "  52.41914458  91.6674184   49.06664503  31.72275755  39.94454306\n",
      "  93.24745165  85.93218187 169.4037025   61.89533328 185.33658746\n",
      "  63.19936009 155.51671257  23.15939041   1.16358407  72.79903616\n",
      " 116.41252957   1.0676813   11.70925896]\n",
      "17-th iteration, loss: 0.11556416533759595, 30 gd steps\n",
      "insert gradient: -2.8696378953557673e-05\n",
      "17-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69856795  55.17958003 102.1005112   72.17336541  95.02898099\n",
      "  52.41159213  91.67130202  49.06804907  31.71946879  39.94566163\n",
      "  93.24818769  85.93759151 169.39105658  61.88887755 185.33615569\n",
      "  63.20031996 155.53692938  23.06489105   1.16017237  72.8842817\n",
      " 116.37309062   1.06906878  11.70925896]\n",
      "18-th iteration, loss: 0.11556416125537894, 14 gd steps\n",
      "insert gradient: -1.1733302571863393e-05\n",
      "18-th iteration, new layer inserted. now 25 layers\n",
      "[6.90618832e-01 5.51883378e+01 1.02111240e+02 7.21683402e+01\n",
      " 9.50238234e+01 5.24123569e+01 9.16643046e+01 4.90701528e+01\n",
      " 3.17219115e+01 3.99443739e+01 9.32536155e+01 0.00000000e+00\n",
      " 1.06581410e-14 8.59352876e+01 1.69388743e+02 6.18918200e+01\n",
      " 1.85336061e+02 6.31980004e+01 1.55543321e+02 2.30469195e+01\n",
      " 1.16260483e+00 7.28971285e+01 1.16366616e+02 1.07014223e+00\n",
      " 1.17092590e+01]\n",
      "19-th iteration, loss: 0.11556413868243863, 255 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5094743359472164e-06\n",
      "19-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203969  55.18408159 102.11392818  72.16329322  95.02308372\n",
      "  52.41255124  91.66506339  49.06696269  31.72171653  39.9461023\n",
      "  93.24866757  85.94317798 169.36549475  61.89669588 185.33501847\n",
      "  63.18983969 155.58028182  22.92573921   1.15948677  73.0041369\n",
      " 116.31950072   1.07274016  11.70925896]\n",
      "20-th iteration, loss: 0.11556413868185714, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5093844642679324e-06\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203969  55.18408163 102.1139284   72.16329289  95.02308345\n",
      "  52.41255103  91.66506324  49.06696262  31.72171645  39.9461022\n",
      "  93.24866746  85.94317793 169.36549393  61.89669605 185.33501814\n",
      "  63.18983939 155.58028293  22.92573517   1.15948653  73.00414041\n",
      " 116.31949916   1.07274023  11.70925896]\n",
      "21-th iteration, loss: 0.11556413868127569, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.5092971734876337e-06\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039681e-01 5.51840817e+01 1.02113929e+02 7.21632926e+01\n",
      " 9.50230832e+01 5.24125508e+01 9.16650631e+01 4.90669625e+01\n",
      " 3.17217164e+01 3.99461021e+01 9.32486674e+01 8.59431779e+01\n",
      " 1.69365493e+02 6.18966962e+01 1.85335018e+02 6.31898391e+01\n",
      " 1.55580284e+02 2.29257311e+01 1.15948629e+00 0.00000000e+00\n",
      " 1.66533454e-16 7.30041439e+01 1.16319498e+02 1.07274029e+00\n",
      " 1.17092590e+01]\n",
      "22-th iteration, loss: 0.11556413868048176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.496085113903873e-06\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203968  55.18408171 102.11392885  72.16329224  95.02308292\n",
      "  52.41255059  91.66506292  49.06696247  31.72171628  39.94610201\n",
      "  93.24866725  85.94317784 169.36549229  61.89669639 185.33501749\n",
      "  63.18983878 155.58028515  22.92572709   1.15948604  73.00415093\n",
      " 116.31949603   1.07274035  11.70925896]\n",
      "23-th iteration, loss: 0.11556413867990042, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4962204023557176e-06\n",
      "23-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203967  55.18408176 102.11392908  72.16329193  95.02308266\n",
      "  52.41255038  91.66506277  49.06696241  31.7217162   39.94610192\n",
      "  93.24866715  85.94317779 169.36549147  61.89669656 185.33501716\n",
      "  63.18983847 155.58028625  22.92572304   1.15948579  73.00415443\n",
      " 116.31949447   1.07274042  11.70925896]\n",
      "24-th iteration, loss: 0.1155641386793191, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.49635324267711e-06\n",
      "24-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203967  55.1840818  102.1139293   72.16329161  95.02308239\n",
      "  52.41255017  91.66506261  49.06696234  31.72171612  39.94610183\n",
      "  93.24866705  85.94317775 169.36549066  61.89669673 185.33501683\n",
      "  63.18983816 155.58028735  22.92571899   1.15948554  73.00415792\n",
      " 116.3194929    1.07274048  11.70925896]\n",
      "25-th iteration, loss: 0.11556413867873781, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.496483894680288e-06\n",
      "25-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039664e-01 5.51840818e+01 1.02113930e+02 7.21632913e+01\n",
      " 9.50230821e+01 5.24125500e+01 9.16650625e+01 4.90669623e+01\n",
      " 3.17217160e+01 3.99461017e+01 9.32486669e+01 8.59431777e+01\n",
      " 1.69365490e+02 6.18966969e+01 1.85335016e+02 6.31898378e+01\n",
      " 1.55580288e+02 2.29257149e+01 1.15948529e+00 0.00000000e+00\n",
      " 2.77555756e-17 7.30041614e+01 1.16319491e+02 1.07274055e+00\n",
      " 1.17092590e+01]\n",
      "26-th iteration, loss: 0.1155641386779456, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.483532696422051e-06\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039660e-01 5.51840819e+01 1.02113930e+02 7.21632910e+01\n",
      " 9.50230819e+01 5.24125498e+01 9.16650623e+01 4.90669622e+01\n",
      " 3.17217160e+01 3.99461017e+01 9.32486668e+01 8.59431777e+01\n",
      " 1.69365489e+02 6.18966971e+01 1.85335016e+02 6.31898375e+01\n",
      " 1.55580290e+02 2.29257109e+01 1.15948504e+00 0.00000000e+00\n",
      " 2.77555756e-17 7.30041684e+01 1.16319490e+02 1.07274061e+00\n",
      " 1.17092590e+01]\n",
      "27-th iteration, loss: 0.11556413867715493, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.470844156756956e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039656e-01 5.51840819e+01 1.02113930e+02 7.21632907e+01\n",
      " 9.50230816e+01 5.24125496e+01 9.16650622e+01 4.90669622e+01\n",
      " 3.17217159e+01 3.99461016e+01 9.32486667e+01 8.59431776e+01\n",
      " 1.69365488e+02 6.18966972e+01 1.85335016e+02 6.31898372e+01\n",
      " 1.55580291e+02 2.29257068e+01 1.15948478e+00 0.00000000e+00\n",
      " 2.77555756e-17 7.30041754e+01 1.16319488e+02 1.07274068e+00\n",
      " 1.17092590e+01]\n",
      "28-th iteration, loss: 0.11556413867636577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4584114088976987e-06\n",
      "28-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203965  55.18408198 102.11393022  72.16329037  95.02308135\n",
      "  52.41254936  91.66506201  49.06696211  31.72171583  39.9461015\n",
      "  93.24866665  85.94317758 169.36548739  61.89669742 185.3350155\n",
      "  63.18983689 155.58029174  22.92570274   1.15948451  73.0041823\n",
      " 116.31948665   1.07274074  11.70925896]\n",
      "29-th iteration, loss: 0.1155641386757845, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.459166405434826e-06\n",
      "29-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203965  55.18408203 102.11393045  72.16329008  95.0230811\n",
      "  52.41254916  91.66506187  49.06696206  31.72171576  39.94610143\n",
      "  93.24866655  85.94317754 169.36548658  61.89669759 185.33501517\n",
      "  63.18983656 155.58029282  22.92569865   1.15948424  73.00418576\n",
      " 116.31948508   1.07274081  11.70925896]\n",
      "30-th iteration, loss: 0.11556413867520328, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4599049379626535e-06\n",
      "30-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203965  55.18408208 102.11393069  72.16328979  95.02308085\n",
      "  52.41254897  91.66506172  49.06696201  31.7217157   39.94610136\n",
      "  93.24866645  85.9431775  169.36548577  61.89669776 185.33501483\n",
      "  63.18983623 155.5802939   22.92569456   1.15948397  73.00418922\n",
      " 116.31948352   1.07274088  11.70925896]\n",
      "31-th iteration, loss: 0.1155641386746221, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4606276944957614e-06\n",
      "31-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203964  55.18408213 102.11393092  72.16328949  95.0230806\n",
      "  52.41254878  91.66506158  49.06696196  31.72171564  39.94610129\n",
      "  93.24866636  85.94317747 169.36548495  61.89669793 185.3350145\n",
      "  63.1898359  155.58029499  22.92569047   1.15948369  73.00419268\n",
      " 116.31948196   1.07274094  11.70925896]\n",
      "32-th iteration, loss: 0.11556413867404097, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.461335328761198e-06\n",
      "32-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203964  55.18408218 102.11393115  72.16328919  95.02308034\n",
      "  52.41254858  91.66506143  49.06696192  31.72171557  39.94610122\n",
      "  93.24866626  85.94317743 169.36548414  61.8966981  185.33501416\n",
      "  63.18983557 155.58029607  22.92568638   1.15948342  73.00419614\n",
      " 116.31948039   1.07274101  11.70925896]\n",
      "33-th iteration, loss: 0.1155641386734599, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4620284615579294e-06\n",
      "33-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203963  55.18408222 102.11393139  72.1632889   95.02308009\n",
      "  52.41254839  91.66506129  49.06696187  31.72171551  39.94610115\n",
      "  93.24866617  85.9431774  169.36548333  61.89669828 185.33501383\n",
      "  63.18983524 155.58029715  22.9256823    1.15948315  73.0041996\n",
      " 116.31947883   1.07274107  11.70925896]\n",
      "34-th iteration, loss: 0.11556413867287885, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.462707682346106e-06\n",
      "34-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039630e-01 5.51840823e+01 1.02113932e+02 7.21632886e+01\n",
      " 9.50230798e+01 5.24125482e+01 9.16650611e+01 4.90669618e+01\n",
      " 3.17217154e+01 3.99461011e+01 9.32486661e+01 8.59431774e+01\n",
      " 1.69365483e+02 6.18966985e+01 1.85335013e+02 6.31898349e+01\n",
      " 1.55580298e+02 2.29256782e+01 1.15948288e+00 0.00000000e+00\n",
      " 1.66533454e-16 7.30042031e+01 1.16319477e+02 1.07274114e+00\n",
      " 1.17092590e+01]\n",
      "35-th iteration, loss: 0.11556413867209087, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.450419032689211e-06\n",
      "35-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039626e-01 5.51840823e+01 1.02113932e+02 7.21632883e+01\n",
      " 9.50230796e+01 5.24125480e+01 9.16650610e+01 4.90669618e+01\n",
      " 3.17217154e+01 3.99461010e+01 9.32486660e+01 8.59431773e+01\n",
      " 1.69365482e+02 6.18966986e+01 1.85335013e+02 6.31898346e+01\n",
      " 1.55580299e+02 2.29256741e+01 1.15948261e+00 0.00000000e+00\n",
      " 1.66533454e-16 7.30042100e+01 1.16319476e+02 1.07274121e+00\n",
      " 1.17092590e+01]\n",
      "36-th iteration, loss: 0.11556413867130429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4383774136211052e-06\n",
      "36-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203962  55.18408237 102.11393209  72.16328801  95.02307933\n",
      "  52.41254781  91.66506086  49.06696174  31.72171533  39.94610095\n",
      "  93.24866589  85.94317731 169.3654809   61.89669881 185.33501283\n",
      "  63.18983425 155.5803004   22.92567002   1.15948233  73.00421688\n",
      " 116.31947413   1.07274127  11.70925896]\n",
      "37-th iteration, loss: 0.11556413867072318, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4394406132532924e-06\n",
      "37-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039619e-01 5.51840824e+01 1.02113932e+02 7.21632877e+01\n",
      " 9.50230791e+01 5.24125476e+01 9.16650607e+01 4.90669617e+01\n",
      " 3.17217153e+01 3.99461009e+01 9.32486658e+01 8.59431773e+01\n",
      " 1.69365480e+02 6.18966990e+01 1.85335012e+02 6.31898339e+01\n",
      " 1.55580301e+02 2.29256659e+01 1.15948204e+00 0.00000000e+00\n",
      " 1.11022302e-16 7.30042203e+01 1.16319473e+02 1.07274134e+00\n",
      " 1.17092590e+01]\n",
      "38-th iteration, loss: 0.11556413866993785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4276135126706675e-06\n",
      "38-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203962  55.18408246 102.11393257  72.16328745  95.02307883\n",
      "  52.41254745  91.66506058  49.06696167  31.72171522  39.94610083\n",
      "  93.24866571  85.94317725 169.36547928  61.89669916 185.33501216\n",
      "  63.18983357 155.58030254  22.9256618    1.15948176  73.00422719\n",
      " 116.319471     1.07274141  11.70925896]\n",
      "39-th iteration, loss: 0.11556413866935673, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4288460329192063e-06\n",
      "39-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203961  55.18408251 102.11393281  72.16328717  95.02307859\n",
      "  52.41254727  91.66506045  49.06696164  31.72171517  39.94610078\n",
      "  93.24866562  85.94317722 169.36547847  61.89669934 185.33501182\n",
      "  63.18983323 155.58030361  22.92565768   1.15948147  73.00423061\n",
      " 116.31946944   1.07274148  11.70925896]\n",
      "40-th iteration, loss: 0.11556413866877562, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4300527613190935e-06\n",
      "40-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203961  55.18408256 102.11393305  72.16328689  95.02307834\n",
      "  52.41254709  91.66506031  49.06696161  31.72171512  39.94610072\n",
      "  93.24866553  85.9431772  169.36547767  61.89669951 185.33501149\n",
      "  63.18983289 155.58030467  22.92565357   1.15948118  73.00423404\n",
      " 116.31946788   1.07274154  11.70925896]\n",
      "41-th iteration, loss: 0.11556413866819457, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.431234612548833e-06\n",
      "41-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039606e-01 5.51840826e+01 1.02113933e+02 7.21632866e+01\n",
      " 9.50230781e+01 5.24125469e+01 9.16650602e+01 4.90669616e+01\n",
      " 3.17217151e+01 3.99461007e+01 9.32486654e+01 8.59431772e+01\n",
      " 1.69365477e+02 6.18966997e+01 1.85335011e+02 6.31898325e+01\n",
      " 1.55580306e+02 2.29256494e+01 1.15948089e+00 0.00000000e+00\n",
      " 1.38777878e-16 7.30042375e+01 1.16319466e+02 1.07274161e+00\n",
      " 1.17092590e+01]\n",
      "42-th iteration, loss: 0.11556413866741028, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4195547852261163e-06\n",
      "42-th iteration, new layer inserted. now 23 layers\n",
      "[  0.6920396   55.18408266 102.11393353  72.16328633  95.02307785\n",
      "  52.41254673  91.66506004  49.06696155  31.72171501  39.94610062\n",
      "  93.24866536  85.94317715 169.36547605  61.89669987 185.33501082\n",
      "  63.1898322  155.58030681  22.92564533   1.15948059  73.00424433\n",
      " 116.31946475   1.07274168  11.70925896]\n",
      "43-th iteration, loss: 0.1155641386668292, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.42090176729269e-06\n",
      "43-th iteration, new layer inserted. now 23 layers\n",
      "[  0.6920396   55.18408271 102.11393377  72.16328606  95.0230776\n",
      "  52.41254655  91.66505991  49.06696153  31.72171497  39.94610057\n",
      "  93.24866527  85.94317713 169.36547525  61.89670005 185.33501048\n",
      "  63.18983186 155.58030787  22.9256412    1.1594803   73.00424775\n",
      " 116.31946318   1.07274175  11.70925896]\n",
      "44-th iteration, loss: 0.11556413866624814, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.422221163502609e-06\n",
      "44-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039595e-01 5.51840828e+01 1.02113934e+02 7.21632858e+01\n",
      " 9.50230774e+01 5.24125464e+01 9.16650598e+01 4.90669615e+01\n",
      " 3.17217149e+01 3.99461005e+01 9.32486652e+01 8.59431771e+01\n",
      " 1.69365474e+02 6.18967002e+01 1.85335010e+02 6.31898315e+01\n",
      " 1.55580309e+02 2.29256371e+01 1.15948000e+00 0.00000000e+00\n",
      " 1.38777878e-16 7.30042512e+01 1.16319462e+02 1.07274182e+00\n",
      " 1.17092590e+01]\n",
      "45-th iteration, loss: 0.11556413866546493, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4107097069287255e-06\n",
      "45-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203959  55.18408281 102.11393425  72.16328551  95.02307711\n",
      "  52.4125462   91.66505964  49.06696148  31.72171487  39.94610047\n",
      "  93.2486651   85.94317709 169.36547364  61.89670042 185.33500981\n",
      "  63.18983116 155.58031     22.92563295   1.15947971  73.00425801\n",
      " 116.31946005   1.07274188  11.70925896]\n",
      "46-th iteration, loss: 0.11556413866488383, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.412188457955958e-06\n",
      "46-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203959  55.18408286 102.11393449  72.16328524  95.02307687\n",
      "  52.41254603  91.66505951  49.06696145  31.72171483  39.94610043\n",
      "  93.24866502  85.94317708 169.36547284  61.8967006  185.33500948\n",
      "  63.18983081 155.58031105  22.92562881   1.15947941  73.00426142\n",
      " 116.31945849   1.07274195  11.70925896]\n",
      "47-th iteration, loss: 0.11556413866430278, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4136371188680813e-06\n",
      "47-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203958  55.18408291 102.11393474  72.16328497  95.02307663\n",
      "  52.41254586  91.66505938  49.06696143  31.72171478  39.94610038\n",
      "  93.24866494  85.94317706 169.36547203  61.89670078 185.33500914\n",
      "  63.18983046 155.58031211  22.92562468   1.15947911  73.00426483\n",
      " 116.31945692   1.07274202  11.70925896]\n",
      "48-th iteration, loss: 0.11556413866372178, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4150566873640093e-06\n",
      "48-th iteration, new layer inserted. now 23 layers\n",
      "[  0.69203958  55.18408296 102.11393498  72.1632847   95.02307638\n",
      "  52.41254568  91.66505924  49.06696141  31.72171474  39.94610034\n",
      "  93.24866485  85.94317705 169.36547123  61.89670097 185.33500881\n",
      "  63.18983012 155.58031317  22.92562054   1.15947881  73.00426825\n",
      " 116.31945536   1.07274209  11.70925896]\n",
      "49-th iteration, loss: 0.11556413866314083, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4164481177100707e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[6.92039577e-01 5.51840830e+01 1.02113935e+02 7.21632844e+01\n",
      " 9.50230761e+01 5.24125455e+01 9.16650591e+01 4.90669614e+01\n",
      " 3.17217147e+01 3.99461003e+01 9.32486648e+01 8.59431770e+01\n",
      " 1.69365470e+02 6.18967012e+01 1.85335008e+02 6.31898298e+01\n",
      " 1.55580314e+02 2.29256164e+01 1.15947851e+00 0.00000000e+00\n",
      " 1.38777878e-16 7.30042717e+01 1.16319454e+02 1.07274216e+00\n",
      " 1.17092590e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.523039804921394\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  47.32497831    0.         1636.09210712]\n",
      "1-th iteration, loss: 0.7541187154625382, 11 gd steps\n",
      "insert gradient: -0.6651248840964153\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  45.00936734   61.96666089  119.71405662    0.         1516.3780505 ]\n",
      "2-th iteration, loss: 0.527017622710657, 23 gd steps\n",
      "insert gradient: -0.5739212793597152\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   0.           55.26690708  109.74096506  116.3861799   247.57192661\n",
      "    0.         1268.80612389]\n",
      "3-th iteration, loss: 0.4343233805692907, 27 gd steps\n",
      "insert gradient: -0.40944808836009416\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[2.68724896e-01 6.44189016e+01 9.95428223e+01 9.83442357e+01\n",
      " 2.15304839e+02 4.45301923e+01 6.16277260e+02 0.00000000e+00\n",
      " 6.52528864e+02]\n",
      "4-th iteration, loss: 0.37404333165948933, 13 gd steps\n",
      "insert gradient: -0.5774421075423294\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.16664682  65.50990027 113.20483103  86.37368167 112.80292543\n",
      "   0.         104.74557362  45.14598367 597.74821371  40.95609214\n",
      " 652.52886372]\n",
      "5-th iteration, loss: 0.32497558658795417, 64 gd steps\n",
      "insert gradient: -0.17462486865534385\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.35879178  68.84557876 125.78063484  69.97710552  98.03937097\n",
      "  30.59498891  61.6679979   53.92589245 265.13655059   0.\n",
      " 318.16386071  42.48376292 652.52886372]\n",
      "6-th iteration, loss: 0.29137710213678375, 39 gd steps\n",
      "insert gradient: -0.11760841364523601\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.17725806  68.04138556 119.14363772  71.33308168 106.30321094\n",
      "  37.69194944  41.67320856  52.40499511 124.61874888   0.\n",
      "  99.69499911  48.28496176 290.35708962  35.95474668 652.52886372]\n",
      "7-th iteration, loss: 0.27931787318197915, 15 gd steps\n",
      "insert gradient: -0.16795267330515817\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[4.19387462e+00 6.84295174e+01 1.14707518e+02 0.00000000e+00\n",
      " 2.30926389e-14 6.65584186e+01 1.15429279e+02 4.46328143e+01\n",
      " 2.27493916e+01 6.43135316e+01 8.35743573e+01 2.60925429e+01\n",
      " 5.23191389e+01 6.52852660e+01 2.77824955e+02 4.11301370e+01\n",
      " 6.52528864e+02]\n",
      "8-th iteration, loss: 0.27277035907446107, 16 gd steps\n",
      "insert gradient: -0.16823591086193165\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  2.54116773  66.53560438 116.85718597  73.49296247 113.84930181\n",
      "  45.53819595  23.64923111  60.89807965  88.21933888  28.7363895\n",
      "  50.67353257  62.19081906 207.39007003   0.          69.13002334\n",
      "  41.23409315 652.52886372]\n",
      "9-th iteration, loss: 0.23419929907759005, 24 gd steps\n",
      "insert gradient: -0.08269916566129959\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.89374277  62.9544327  123.56573857  75.98889418 113.86704459\n",
      "  57.8950245   37.01095689  45.62898212  99.3022057   52.48425727\n",
      "  26.25264071  46.08655579 173.54072751  44.03487681  31.90090746\n",
      "  45.4972184  512.70125006   0.         139.82761365]\n",
      "10-th iteration, loss: 0.22566347717720223, 14 gd steps\n",
      "insert gradient: -0.07101226415908479\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[4.30264449e-01 6.24088910e+01 1.24904220e+02 7.62854873e+01\n",
      " 1.14952493e+02 6.20134645e+01 3.91829165e+01 4.35243078e+01\n",
      " 9.56386028e+01 5.54125131e+01 2.90548738e+01 3.92003349e+01\n",
      " 1.74740677e+02 5.56750941e+01 3.23815515e+01 3.71930295e+01\n",
      " 5.08063494e+02 1.28063152e+01 1.39827614e+02]\n",
      "11-th iteration, loss: 0.2182487772069317, 60 gd steps\n",
      "insert gradient: -0.06880247447116718\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  2.08356025  68.68946607 120.24095408  81.10184923 113.61215893\n",
      "  63.09540012  53.6304133   35.85400287  99.38042523  60.1303285\n",
      "  38.62058354  25.03497419 187.624436    58.3702496   55.96045082\n",
      "  26.03545295 125.10355613   0.         375.3106684   20.64329998\n",
      " 139.82761365]\n",
      "12-th iteration, loss: 0.20831818487952272, 19 gd steps\n",
      "insert gradient: -0.04959480271745532\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  2.20437226  67.50130578 120.13027009  81.02257144 112.27077662\n",
      "  62.31467999  59.57326314  32.77254487  99.24057401  60.09739833\n",
      "  46.53449239  19.78639606 134.7388975    0.          50.52708656\n",
      "  61.93910317  68.70739402  22.97003363 111.23224513  22.41103058\n",
      " 341.73913528  22.15142005 139.82761365]\n",
      "13-th iteration, loss: 0.1795743410358091, 64 gd steps\n",
      "insert gradient: -0.047911049494734896\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  1.90681605  66.84064423 118.81083962  78.83866879 123.73261468\n",
      "  63.14472175  85.45276869  23.17791115  95.75112445  57.57614567\n",
      "  99.9505564    8.06538565 104.90796929  42.94670186   0.34712937\n",
      "  56.00828275  96.19277798  18.42868114  85.35473644  42.67415535\n",
      " 218.67662582   0.          93.71855392  16.43577729 139.82761365]\n",
      "14-th iteration, loss: 0.15981723781481802, 59 gd steps\n",
      "insert gradient: -0.019963665344196205\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  1.94750003  70.05023991 123.21258593  84.5528046  121.38319088\n",
      "  64.94354358  99.57866273  28.29813649  78.5876753   62.25484842\n",
      " 109.89875408  24.59801781  36.42655372  83.26668446   0.\n",
      "  35.68572191 107.42587477  43.04022239  48.74326173  40.50582441\n",
      " 212.30816106  30.37877976  66.49721123  25.17689261 139.82761365]\n",
      "15-th iteration, loss: 0.15578041323474442, 22 gd steps\n",
      "insert gradient: -0.017110703278059646\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[  1.6742958   68.61504255 122.96166388  87.19789314 122.47656556\n",
      "  65.01352743 101.59727768  31.94176479  74.23384596  61.78795253\n",
      " 109.72498738  29.10843341  26.60507645  80.23964577  12.22618289\n",
      "  32.979133   106.31001575  50.70976311  43.80241851  37.2554061\n",
      " 140.28051802   0.          70.14025901  35.65147943  66.03810359\n",
      "  23.13473611 139.82761365]\n",
      "16-th iteration, loss: 0.13070201964852948, 63 gd steps\n",
      "insert gradient: -0.012212867971194374\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  2.29023764  73.61137144 127.11193934  85.92630835 128.21111189\n",
      "  67.82002922 109.94363388  49.49319002  54.59963701  56.64973107\n",
      " 110.60750413  68.69983248  24.32994673  45.87334204  77.44852572\n",
      "  13.13937788  85.7292456   52.59002977  92.00299535  23.79137169\n",
      "  87.48522779  48.05974337  19.0521249   45.56870519  85.15575066\n",
      "  16.08945265 139.82761365]\n",
      "17-th iteration, loss: 0.06688135730830944, 139 gd steps\n",
      "insert gradient: -0.011797757050360352\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  1.64356035  71.37830109 133.98434839  96.60382803 137.58866694\n",
      "  86.20575017 137.85721744  74.12691905 114.09265324  56.11259956\n",
      "  81.44494596  55.07681907 110.29999201  64.61342268  92.5279782\n",
      "  25.00812528  77.86930876  48.02284791  49.10118412  44.68211832\n",
      " 101.0774331   19.2997909   50.33196969  51.66497851  85.13847445\n",
      "  35.76039332 139.82761365]\n",
      "18-th iteration, loss: 0.05112684156674076, 147 gd steps\n",
      "insert gradient: -0.0024254452651178066\n",
      "18-th iteration, new layer inserted. now 28 layers\n",
      "[  1.46966979  83.85555299 126.26770368  88.96444146 169.8114489\n",
      "  78.60727728 131.5764829   78.42237423 120.54973991  68.97783808\n",
      " 105.23553677  45.9869309   94.35000425  64.87064762 105.42939567\n",
      "  46.54796797  68.20461463  37.60627602  86.25694342  43.35382862\n",
      "  75.58687415  43.60478147  78.61889962   8.85322876  81.78997023\n",
      "  52.38232761 139.82761365   0.        ]\n",
      "19-th iteration, loss: 0.049110323598773375, 35 gd steps\n",
      "insert gradient: -0.00246578005332778\n",
      "19-th iteration, new layer inserted. now 28 layers\n",
      "[  0.66705787  78.04857663 126.05025233  89.18860198 168.16544121\n",
      "  78.86741548 139.2124744   80.5967411  116.82846059  69.51311615\n",
      " 105.68910254  45.02272151  92.77025622  69.13982635 106.61835245\n",
      "  47.65208587  68.30645766  35.81399194  87.34855579  42.23483774\n",
      "  72.51215884  43.91415789  85.34930372  10.19117173  82.52116779\n",
      "  55.35958313 132.68859458  20.00428189]\n",
      "20-th iteration, loss: 0.04800095338464709, 25 gd steps\n",
      "insert gradient: -0.0014645773035719383\n",
      "20-th iteration, new layer inserted. now 30 layers\n",
      "[2.47501688e+00 7.46438450e+01 1.30614940e+02 9.38107838e+01\n",
      " 1.55789596e+02 8.02293726e+01 1.47082956e+02 7.81843922e+01\n",
      " 1.18583217e+02 6.73228839e+01 1.08174986e+02 4.68099810e+01\n",
      " 8.94689820e+01 6.83395557e+01 1.09619677e+02 4.90731679e+01\n",
      " 7.07995734e+01 3.26309050e+01 8.71076086e+01 4.23908628e+01\n",
      " 6.75327400e+01 4.22102138e+01 8.56422832e+01 0.00000000e+00\n",
      " 3.55271368e-15 1.26737146e+01 8.38783810e+01 6.40604969e+01\n",
      " 1.15809388e+02 2.77055828e+01]\n",
      "21-th iteration, loss: 0.04628015249427054, 23 gd steps\n",
      "insert gradient: -0.00511713580259556\n",
      "21-th iteration, new layer inserted. now 28 layers\n",
      "[  1.95880842  71.86568217 134.46436132  98.69052362 145.44511646\n",
      "  82.87830341 147.65189003  77.20751094 122.63071229  66.69524302\n",
      " 108.70970081  48.6929577   89.3898925   64.57341528 115.09294819\n",
      "  56.38931377  73.25303747  30.7010136   83.92429991  41.2302675\n",
      "  63.30085058  45.72073623  84.02239553  16.33045814  74.26870335\n",
      "  73.41678894  91.70444654  35.80971922]\n",
      "22-th iteration, loss: 0.043243649945086116, 24 gd steps\n",
      "insert gradient: -0.0036261388950357687\n",
      "22-th iteration, new layer inserted. now 30 layers\n",
      "[1.15574917e+00 7.21648296e+01 1.33255373e+02 1.01673317e+02\n",
      " 1.44378935e+02 8.54509757e+01 1.45767548e+02 8.08977991e+01\n",
      " 1.22840791e+02 6.99196703e+01 1.10482532e+02 5.16515568e+01\n",
      " 8.94904229e+01 6.12270236e+01 1.21586295e+02 5.81951412e+01\n",
      " 7.95515458e+01 0.00000000e+00 1.42108547e-14 3.10681785e+01\n",
      " 7.82839731e+01 4.56122756e+01 6.64891561e+01 4.05068804e+01\n",
      " 8.42111518e+01 2.62126187e+01 5.04354235e+01 7.99186240e+01\n",
      " 9.07201158e+01 4.30946816e+01]\n",
      "23-th iteration, loss: 0.041064081387078885, 24 gd steps\n",
      "insert gradient: -0.0019927882495971044\n",
      "23-th iteration, new layer inserted. now 30 layers\n",
      "[1.39434972e+00 7.24899522e+01 1.36012711e+02 1.09960270e+02\n",
      " 1.43230722e+02 8.39190323e+01 1.41345680e+02 8.41474824e+01\n",
      " 1.23231838e+02 7.20053595e+01 1.14518841e+02 5.64266320e+01\n",
      " 8.94641514e+01 5.87667397e+01 1.23802916e+02 6.04998613e+01\n",
      " 8.76358613e+01 3.45278937e+01 7.09338802e+01 4.57966395e+01\n",
      " 7.43619219e+01 0.00000000e+00 7.10542736e-15 3.69077528e+01\n",
      " 7.63088441e+01 3.94996894e+01 3.91663498e+01 8.09106808e+01\n",
      " 9.26081669e+01 4.96049474e+01]\n",
      "24-th iteration, loss: 0.040342609710383934, 289 gd steps\n",
      "insert gradient: -0.0008178686949468344\n",
      "24-th iteration, new layer inserted. now 30 layers\n",
      "[  1.78677353  75.11692701 135.85294077 114.20367897 141.61123238\n",
      "  82.27225106 141.78787689  87.09827503 120.94955062  71.40898987\n",
      " 115.19499804  58.62261271  91.73587047  56.58174729 125.28527201\n",
      "  60.32315872  88.53372961  35.71786528  67.40368639  46.8008712\n",
      "  79.75834102  36.57035258  70.14805188  42.88133892  42.11054636\n",
      "  50.67278753   0.          30.40367252  88.45289456  50.05843532]\n",
      "25-th iteration, loss: 0.03672361708589781, 168 gd steps\n",
      "insert gradient: -0.00025597155317352307\n",
      "25-th iteration, new layer inserted. now 28 layers\n",
      "[  2.55925866  73.89607715 140.21234633 110.99901208 144.04305561\n",
      "  78.96407662 144.6693344   86.38339297 121.68269187  72.38360982\n",
      " 110.56734678  64.47676515  98.51069971  49.19644271 113.3764258\n",
      "  68.52387844  95.23938107  38.58626303  65.29329046  44.18739107\n",
      "  83.39447906  39.08097649  68.12100123  43.30326946  64.95407409\n",
      "  38.70131906 106.86911596  62.72002005]\n",
      "26-th iteration, loss: 0.03622832380842136, 37 gd steps\n",
      "insert gradient: -0.0002865173531216646\n",
      "26-th iteration, new layer inserted. now 28 layers\n",
      "[  3.07296186  75.56631859 140.8341471  107.35474976 148.32336801\n",
      "  77.10884419 149.86023566  85.29510167 121.51218155  72.820638\n",
      " 110.37678873  61.38419181 104.26549273  51.45031373 103.30852442\n",
      "  71.26156161  99.18895371  40.54482911  62.34998936  43.53254617\n",
      "  84.79223806  38.49865476  71.83466366  42.79018667  65.65736635\n",
      "  37.39144898  96.25779037  68.26629825]\n",
      "27-th iteration, loss: 0.03557286990376858, 44 gd steps\n",
      "insert gradient: -0.00044187462025355043\n",
      "27-th iteration, new layer inserted. now 28 layers\n",
      "[  0.58770495  82.09296929 143.365732   107.96200404 139.84878527\n",
      "  81.90503184 151.05466678  83.18828344 125.29832275  70.71722313\n",
      " 115.258151    59.01511753 105.48297272  56.17283924 100.86551996\n",
      "  70.48913872 102.56167806  43.33586579  64.2868975   42.13156686\n",
      "  84.25365411  40.15889266  73.5290762   42.29720952  69.47828946\n",
      "  35.527664    87.39597609  72.54245573]\n",
      "28-th iteration, loss: 0.03518956000702174, 59 gd steps\n",
      "insert gradient: -0.00022979452199988488\n",
      "28-th iteration, new layer inserted. now 28 layers\n",
      "[  0.4568954   82.90418949 141.43556524 111.15300763 137.54594156\n",
      "  82.8250134  146.14743628  84.43710514 130.99866736  68.43220042\n",
      " 119.43223463  60.09520965  98.98393432  61.74104905  98.87857385\n",
      "  67.403012   111.10471384  46.26571973  65.14095264  40.81592385\n",
      "  83.24901542  41.30918658  73.15169538  41.40338792  73.56666871\n",
      "  36.51923064  76.61614237  75.34074432]\n",
      "29-th iteration, loss: 0.03517002733467423, 56 gd steps\n",
      "insert gradient: -8.833323794381135e-05\n",
      "29-th iteration, new layer inserted. now 30 layers\n",
      "[8.98146441e-01 8.27080279e+01 1.41378696e+02 1.11271308e+02\n",
      " 1.38795358e+02 8.24245464e+01 1.44781476e+02 8.54558374e+01\n",
      " 1.31609121e+02 6.83299795e+01 1.19551863e+02 6.05295519e+01\n",
      " 9.83033449e+01 0.00000000e+00 1.06581410e-14 6.24460080e+01\n",
      " 9.87590155e+01 6.63075199e+01 1.13738012e+02 4.70205731e+01\n",
      " 6.55132695e+01 4.05189674e+01 8.29100369e+01 4.14470256e+01\n",
      " 7.31407686e+01 4.14473312e+01 7.39288328e+01 3.66825299e+01\n",
      " 7.55387586e+01 7.52384467e+01]\n",
      "30-th iteration, loss: 0.035166809097639695, 34 gd steps\n",
      "insert gradient: -7.034312746997273e-05\n",
      "30-th iteration, new layer inserted. now 30 layers\n",
      "[1.07952424e+00 8.26233874e+01 1.41551673e+02 1.11164563e+02\n",
      " 1.39209339e+02 8.23369844e+01 1.44348420e+02 8.57506313e+01\n",
      " 1.31826869e+02 6.83199031e+01 1.19527364e+02 6.06551041e+01\n",
      " 9.81647441e+01 6.27704558e+01 9.87386479e+01 6.57708893e+01\n",
      " 1.14878223e+02 4.73932536e+01 6.56384479e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.02113367e+01 8.27201293e+01 4.14854764e+01\n",
      " 7.32348379e+01 4.15860051e+01 7.40551131e+01 3.67199386e+01\n",
      " 7.52008462e+01 7.50237876e+01]\n",
      "31-th iteration, loss: 0.035165918588603025, 19 gd steps\n",
      "insert gradient: -2.2963171266873954e-05\n",
      "31-th iteration, new layer inserted. now 32 layers\n",
      "[1.09796508e+00 0.00000000e+00 2.77555756e-16 8.26130116e+01\n",
      " 1.41559933e+02 1.11202242e+02 1.39277850e+02 8.22539280e+01\n",
      " 1.44277295e+02 8.58357948e+01 1.31856254e+02 6.82919897e+01\n",
      " 1.19627040e+02 6.07543976e+01 9.80589536e+01 6.27894414e+01\n",
      " 9.87165209e+01 6.56868973e+01 1.15121599e+02 4.73832443e+01\n",
      " 6.57074295e+01 4.21997398e-02 7.77660919e-04 4.02754626e+01\n",
      " 8.27117100e+01 4.14873385e+01 7.32278409e+01 4.15426477e+01\n",
      " 7.41051189e+01 3.66453524e+01 7.50672205e+01 7.49880392e+01]\n",
      "32-th iteration, loss: 0.03516499438414585, 22 gd steps\n",
      "insert gradient: -3.374647532293821e-05\n",
      "32-th iteration, new layer inserted. now 28 layers\n",
      "[  1.22563655  82.54465072 141.7536737  111.13085968 139.37127852\n",
      "  82.21039452 144.1110491   85.9503818  131.88326429  68.29454969\n",
      " 119.61586     60.80030423  97.96595984  62.88601108  98.75437082\n",
      "  65.39015376 115.75545601  47.4578297   65.81977146  40.24190586\n",
      "  82.60711992  41.5082834   73.27751657  41.58452326  74.17103251\n",
      "  36.60769804  74.89967609  74.80656606]\n",
      "33-th iteration, loss: 0.035162380979517883, 70 gd steps\n",
      "insert gradient: -5.306427001235218e-05\n",
      "33-th iteration, new layer inserted. now 28 layers\n",
      "[  1.35137561  82.49904997 142.02921362 111.05470981 139.67743712\n",
      "  82.04076594 143.68158069  86.23212728 132.10708432  68.27858498\n",
      " 119.69092924  60.92156526  97.70367234  63.19931998  98.71274773\n",
      "  64.75441275 117.30715407  47.75335511  66.02086246  40.09196487\n",
      "  82.45065546  41.48278005  73.33181873  41.65658631  74.38062742\n",
      "  36.58177183  74.53441703  74.31197202]\n",
      "34-th iteration, loss: 0.0351332100992866, 92 gd steps\n",
      "insert gradient: -0.0003325299063727651\n",
      "34-th iteration, new layer inserted. now 30 layers\n",
      "[2.34528407e+00 8.19393208e+01 1.43473862e+02 1.10509425e+02\n",
      " 1.39658991e+02 8.19508124e+01 1.42479004e+02 8.68403605e+01\n",
      " 1.32757459e+02 6.79620786e+01 1.20705599e+02 6.08829683e+01\n",
      " 9.58352180e+01 6.47429585e+01 9.99171009e+01 6.08995299e+01\n",
      " 1.24970882e+02 0.00000000e+00 2.13162821e-14 4.80943544e+01\n",
      " 6.74339713e+01 3.98829568e+01 8.15245586e+01 4.15660685e+01\n",
      " 7.38799926e+01 4.18616636e+01 7.52237057e+01 3.57326919e+01\n",
      " 7.50079867e+01 6.92492394e+01]\n",
      "35-th iteration, loss: 0.03470699642799272, 29 gd steps\n",
      "insert gradient: -0.0007908949828248713\n",
      "35-th iteration, new layer inserted. now 30 layers\n",
      "[3.43388514e+00 8.27689712e+01 1.39247776e+02 1.08152160e+02\n",
      " 1.42765167e+02 8.23522848e+01 1.39349909e+02 8.85969781e+01\n",
      " 1.34480696e+02 6.73098863e+01 1.20169417e+02 6.19398539e+01\n",
      " 9.30868060e+01 6.54629152e+01 1.03071110e+02 5.79811678e+01\n",
      " 1.26006843e+02 5.36942883e+01 6.96522757e+01 0.00000000e+00\n",
      " 1.06581410e-14 3.87615777e+01 8.10799068e+01 3.99602801e+01\n",
      " 7.47068004e+01 4.43145154e+01 7.63650056e+01 3.42174168e+01\n",
      " 7.50284018e+01 5.66004393e+01]\n",
      "36-th iteration, loss: 0.03172714774038953, 330 gd steps\n",
      "insert gradient: -0.0003128532728296705\n",
      "36-th iteration, new layer inserted. now 30 layers\n",
      "[9.34904453e+00 9.04182554e+01 0.00000000e+00 3.55271368e-15\n",
      " 1.57714002e+02 9.57869206e+01 1.36558157e+02 7.90087044e+01\n",
      " 1.46630912e+02 8.95554667e+01 1.31059679e+02 7.72174822e+01\n",
      " 1.12547436e+02 7.09450532e+01 9.45630502e+01 5.29130679e+01\n",
      " 1.18894213e+02 6.17608045e+01 1.03694544e+02 6.84497909e+01\n",
      " 8.67265438e+01 4.36718963e+01 8.40762922e+01 3.88573838e+01\n",
      " 6.84449132e+01 4.21129397e+01 8.82655301e+01 4.48180911e+01\n",
      " 6.46770935e+01 3.94893908e+01]\n",
      "37-th iteration, loss: 0.03134339536930435, 636 gd steps\n",
      "insert gradient: -6.752715835355558e-05\n",
      "37-th iteration, new layer inserted. now 30 layers\n",
      "[5.52168851e+00 9.21822176e+01 1.63678088e+02 9.20778506e+01\n",
      " 1.30028029e+02 8.09929228e+01 1.49365059e+02 8.79784846e+01\n",
      " 0.00000000e+00 1.06581410e-14 1.33867799e+02 7.93758732e+01\n",
      " 1.09963355e+02 7.23358056e+01 9.73479162e+01 4.97491932e+01\n",
      " 1.21039279e+02 6.25434449e+01 1.01370694e+02 7.10808802e+01\n",
      " 8.79993127e+01 4.34232564e+01 8.49771050e+01 3.99169572e+01\n",
      " 6.86747730e+01 4.09956665e+01 8.82723107e+01 4.51528109e+01\n",
      " 7.33011810e+01 3.68403271e+01]\n",
      "38-th iteration, loss: 0.03126385916816785, 160 gd steps\n",
      "insert gradient: -9.632832399735709e-05\n",
      "38-th iteration, new layer inserted. now 28 layers\n",
      "[  0.          92.56444319 164.03692459  91.66277278 128.83726521\n",
      "  80.44538762 147.95691811  84.16807364 143.08677008  81.1562663\n",
      " 109.19085542  72.70027128  98.01189534  48.58967722 119.60266076\n",
      "  63.52883384 100.07966547  71.8948683   89.20375379  43.61806809\n",
      "  85.77259609  40.54039048  68.01222305  39.97977206  87.97566697\n",
      "  45.06030963  76.30934371  37.39739776]\n",
      "39-th iteration, loss: 0.03105370768540415, 30 gd steps\n",
      "insert gradient: -0.0005647845070888763\n",
      "39-th iteration, new layer inserted. now 28 layers\n",
      "[  0.          91.18124704 162.61205954  91.65950009 129.74024583\n",
      "  79.67838064 143.23467159  79.07951054 155.60501474  83.56742089\n",
      " 110.32390347  73.23473902  97.94735566  47.56268174 116.65654427\n",
      "  64.5562838   99.79909771  72.43458045  90.27287251  44.12935173\n",
      "  86.50669081  41.04661914  66.16378426  38.4921455   88.15807852\n",
      "  45.96897725  78.40842317  39.29050395]\n",
      "40-th iteration, loss: 0.03098434739774447, 39 gd steps\n",
      "insert gradient: -0.00033891289349464494\n",
      "40-th iteration, new layer inserted. now 30 layers\n",
      "[0.00000000e+00 9.14326977e+01 1.62545696e+02 9.17435652e+01\n",
      " 1.29145272e+02 7.98694398e+01 1.42464845e+02 7.80377681e+01\n",
      " 1.58457138e+02 8.38497114e+01 1.11032903e+02 7.38475692e+01\n",
      " 9.78571371e+01 4.73349289e+01 1.16653771e+02 6.47009360e+01\n",
      " 9.96363312e+01 7.30954685e+01 9.04665787e+01 4.36893384e+01\n",
      " 8.64300137e+01 4.16359038e+01 6.60632456e+01 3.86215433e+01\n",
      " 8.80073513e+01 0.00000000e+00 1.77635684e-14 4.58068466e+01\n",
      " 7.92843545e+01 3.97290059e+01]\n",
      "41-th iteration, loss: 0.0309570334689184, 18 gd steps\n",
      "insert gradient: -0.0004065966611482175\n",
      "41-th iteration, new layer inserted. now 28 layers\n",
      "[  0.          91.28083232 162.2391444   92.30033905 128.43377175\n",
      "  80.12156453 141.80674923  76.65333202 161.06475023  84.09863671\n",
      " 111.67909666  74.32197857  97.38787954  46.8962151  117.02416028\n",
      "  64.59283336  99.61798238  73.69858623  89.98540486  43.68835273\n",
      "  86.4917664   41.80093188  65.22724385  38.445825    88.57246245\n",
      "  46.23896565  80.60026557  40.34689493]\n",
      "42-th iteration, loss: 0.030824934210415253, 27 gd steps\n",
      "insert gradient: -0.0006732699493629144\n",
      "42-th iteration, new layer inserted. now 28 layers\n",
      "[  0.          91.31262004 163.17011177  92.59139281 127.33065044\n",
      "  80.43913096 142.42525269  76.3607341  162.73695053  85.11210348\n",
      " 110.98581251  75.74425631  96.98456906  46.89258606 117.97057836\n",
      "  64.10430647  99.15180044  75.23334093  90.092714    43.75054752\n",
      "  86.08129337  41.9756991   65.57017412  38.73733132  88.53493858\n",
      "  47.25778952  81.17682397  40.81854005]\n",
      "43-th iteration, loss: 0.029902734898459373, 62 gd steps\n",
      "insert gradient: -0.0008231345779901945\n",
      "43-th iteration, new layer inserted. now 28 layers\n",
      "[  0.          91.3658668  167.37619947  94.98726011 124.49909709\n",
      "  81.74881949 147.00505586  73.7219862  168.52853306  88.84019815\n",
      " 109.45295591  80.25618859  96.53868221  47.88286821 128.56810401\n",
      "  59.58187299  98.40678315  79.01781196  92.84320388  44.36443162\n",
      "  85.04046023  43.4224296   68.75386836  38.54606128  88.39480796\n",
      "  49.63457784  86.82597721  42.20000797]\n",
      "44-th iteration, loss: 0.028394222434387523, 144 gd steps\n",
      "insert gradient: -0.0008816895563889622\n",
      "44-th iteration, new layer inserted. now 30 layers\n",
      "[  0.          94.49376157 174.65866559  96.20570216 116.88293159\n",
      "  85.40046483 116.30400206   0.          38.76800069  69.71300011\n",
      " 176.5795426   92.77913461 106.97851292  84.61381331 100.40653505\n",
      "  53.90400974 138.65760843  51.30543248  97.98427297  81.50266789\n",
      " 100.79182419  49.42703732  84.85640553  42.42313137  77.38939155\n",
      "  40.73556482  82.8534858   48.18183852  93.38118096  51.81509122]\n",
      "45-th iteration, loss: 0.027645564242437394, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "45-th iteration, new layer inserted. now 30 layers\n",
      "[  0.          93.92424371 176.73701572  96.07290932 119.17513334\n",
      "  86.1857005  111.86204876   6.81320827  30.46533398  68.24384159\n",
      " 177.90610966  93.59011153 108.80895402  84.9513382  102.81039551\n",
      "  56.25995493 139.88037372  51.39870742  97.03331244  82.77825593\n",
      " 101.61889161  50.16103456  85.09310585  42.82902656  77.47073713\n",
      "  41.77628604  83.43841562  48.82176556  93.73078723  52.441705  ]\n",
      "46-th iteration, loss: 0.027364651122269556, 35 gd steps\n",
      "insert gradient: -0.0008031583118582705\n",
      "46-th iteration, new layer inserted. now 32 layers\n",
      "[  0.          95.50758025 178.04978915  96.22611125 118.85829101\n",
      "  86.92428874 113.41355818  11.05671815  22.43201911  67.06209857\n",
      "  76.95125161   0.         102.60166881  94.24857644 107.91764644\n",
      "  85.66797208 103.71734734  57.55281076 141.88370352  51.09761651\n",
      "  96.22613046  83.03929487 102.8981303   51.03719253  85.80310386\n",
      "  41.95964818  76.758138    42.51585831  84.54658401  49.26406612\n",
      "  93.87585383  53.25213914]\n",
      "47-th iteration, loss: 0.027292761380606028, 75 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "47-th iteration, new layer inserted. now 34 layers\n",
      "[  0.          27.54949002   0.          68.87372505 178.26225491\n",
      "  96.07455592 119.23834924  87.40134778 114.25578515  12.63599461\n",
      "  19.81076462  67.37120296  74.4381688    1.81721089 100.24307733\n",
      "  94.41182639 107.69558972  86.02794286 103.83206421  58.08789822\n",
      " 142.7852952   50.93637042  96.49141434  82.7369461  104.34522911\n",
      "  50.98827297  86.05451406  42.2021612   76.83049196  43.07403318\n",
      "  84.98844479  49.21172677  93.41495744  52.64902562]\n",
      "48-th iteration, loss: 0.0272390235065527, 999 gd steps\n",
      "insert gradient: -1.2949565994141365e-05\n",
      "48-th iteration, new layer inserted. now 34 layers\n",
      "[  0.          21.9875373    6.04550223  70.14698822 178.86815212\n",
      "  96.377849   119.43506397  87.32915151 114.80105579  12.7452379\n",
      "  19.10242154  67.2504097   74.82447312   1.57681532 100.70143064\n",
      "  94.72212695 107.40826177  86.15392273 103.7127022   57.56662418\n",
      " 143.7087018   50.54548245  96.84887421  81.93028623 105.74269062\n",
      "  51.32001556  86.24703234  42.21868544  76.81023062  42.94147689\n",
      "  84.65418748  49.25128541  93.53292474  52.35763517]\n",
      "49-th iteration, loss: 0.027239022013361366, 102 gd steps\n",
      "insert gradient: -1.2947763368425587e-05\n",
      "49-th iteration, new layer inserted. now 34 layers\n",
      "[  0.          21.98680599   6.04570518  70.14771236 178.86811486\n",
      "  96.37784595 119.43506001  87.32914021 114.80106216  12.74526818\n",
      "  19.1023667   67.25039235  74.82454492   1.57678399 100.70146883\n",
      "  94.72212992 107.40823363  86.15393533 103.71268947  57.56655581\n",
      " 143.70882373  50.54545274  96.84886537  81.93028269 105.7427067\n",
      "  51.32002907  86.24704455  42.21869468  76.81021915  42.94146888\n",
      "  84.65416637  49.251294    93.53294093  52.3576443 ]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5222158005119684\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.16970394    0.         1667.37300963]\n",
      "1-th iteration, loss: 0.7425587563307535, 11 gd steps\n",
      "insert gradient: -0.6003432631264678\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  40.5599553    62.3140177   223.6719891     0.         1443.70102053]\n",
      "2-th iteration, loss: 0.5318381017220657, 25 gd steps\n",
      "insert gradient: -0.4048553761826215\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   0.           50.29534573  181.88095472   56.68700413  324.09614747\n",
      "    0.         1119.60487306]\n",
      "3-th iteration, loss: 0.4678986064215034, 13 gd steps\n",
      "insert gradient: -0.46993017145424304\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  0.          55.22983266 176.32147053  58.7502048  296.8322853\n",
      "  37.61665523 159.94355329   0.         959.66131977]\n",
      "4-th iteration, loss: 0.3790714652200406, 29 gd steps\n",
      "insert gradient: -0.36567803328284587\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.65032223  50.9285949  188.92933579  52.69393564 270.85747222\n",
      "  50.2816582  120.04112399  63.11454433 675.31722502   0.\n",
      " 284.34409475]\n",
      "5-th iteration, loss: 0.2841861193709912, 33 gd steps\n",
      "insert gradient: -0.24492614093343662\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.53896912  45.65417685 203.3768725   46.41304023 247.80021646\n",
      "  65.82948676 104.71194346  70.35208984 602.14075242  70.68767299\n",
      " 168.02151053   0.         116.32258421]\n",
      "6-th iteration, loss: 0.24398371396847082, 19 gd steps\n",
      "insert gradient: -0.17236759768978402\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.94929231  47.96935944 201.95574621  49.0838173  103.88109865\n",
      "   0.         129.85137331  68.8721158   99.68555036  69.75518679\n",
      " 607.91701796  53.52458926 149.34277924  41.05819203 116.32258421]\n",
      "7-th iteration, loss: 0.22299806404189, 16 gd steps\n",
      "insert gradient: -0.19781681513849106\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.92526394  48.99125786  99.47171999   0.          99.47171999\n",
      "  51.84627478  82.01962672  17.98945251 107.37669051  69.84547984\n",
      " 104.27963245  71.56492827 605.29282678  51.78016377 142.24813272\n",
      "  55.60681025 116.32258421]\n",
      "8-th iteration, loss: 0.2149279584950924, 28 gd steps\n",
      "insert gradient: -0.10579817883930384\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  2.19584362  55.45956124  90.43109598   5.44126672  91.47117865\n",
      "  52.61254381  76.41027962  22.74850438  96.30754093  72.19668374\n",
      " 106.66626146  68.93619905 172.92846711   0.         432.32116777\n",
      "  56.6280565  131.30350596  64.63100934 116.32258421]\n",
      "9-th iteration, loss: 0.19954469901827665, 21 gd steps\n",
      "insert gradient: -0.08541057207103768\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[  2.69195486  60.65808357  92.56179851   5.00589614  91.61563827\n",
      "  51.55052298  78.40511345  23.18775542  92.06479641  70.41639841\n",
      " 115.59319108  62.24698993 150.61284989  21.56489595 107.53248688\n",
      "   0.         322.59746064  48.50047638 131.12526972  70.43079125\n",
      " 116.32258421]\n",
      "10-th iteration, loss: 0.18156521420226607, 25 gd steps\n",
      "insert gradient: -0.08741155260670341\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "[  1.9135896   60.50378815  97.20621146   4.43667097  92.24523279\n",
      "  53.47065305  86.11653379  23.55142191  84.63714504  71.36021689\n",
      " 120.01849852  62.38893743 134.2141138   53.75819559  72.97807936\n",
      "  14.17810081 251.87869221   0.          55.97304271  38.37706121\n",
      " 137.27095804  76.04564124 116.32258421]\n",
      "11-th iteration, loss: 0.17386207078993335, 25 gd steps\n",
      "insert gradient: -0.022246426554175556\n",
      "11-th iteration, new layer inserted. now 25 layers\n",
      "[  1.2459089   58.46309478  99.32003579   4.28520356  92.36912878\n",
      "  56.47043312  87.00671602  24.13018425  83.10008791  71.61315987\n",
      " 117.12217798  62.61163922 131.90958981  63.41036323  60.63574683\n",
      "  13.07911111 121.97640072   0.         121.97640072  13.05550545\n",
      "  48.96863554  41.10701751 140.21893802  69.86130713 116.32258421]\n",
      "12-th iteration, loss: 0.1622951744560694, 55 gd steps\n",
      "insert gradient: -0.04177475707769987\n",
      "12-th iteration, new layer inserted. now 27 layers\n",
      "[2.28736464e+00 5.58296781e+01 9.73728638e+01 2.47038658e+00\n",
      " 9.61208972e+01 5.98912108e+01 9.94339703e+01 2.43137216e+01\n",
      " 8.54239065e+01 6.01428020e+01 1.30399546e+02 6.51895125e+01\n",
      " 1.12543474e+02 8.57717858e+01 7.24056573e+01 1.31637619e+01\n",
      " 9.70011474e+01 0.00000000e+00 1.77635684e-14 2.92215789e+01\n",
      " 5.02280702e+01 2.87917830e+01 3.38645019e+01 4.46382559e+01\n",
      " 1.41654759e+02 7.13536445e+01 1.16322584e+02]\n",
      "13-th iteration, loss: 0.14649207244337872, 33 gd steps\n",
      "insert gradient: -0.03161029684400275\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[1.79538710e+00 6.36164006e+01 1.98150362e+02 5.69451522e+01\n",
      " 9.08959994e+01 0.00000000e+00 1.06581410e-14 3.49117984e+01\n",
      " 8.41381968e+01 6.22076477e+01 1.23145413e+02 7.07993996e+01\n",
      " 1.11566332e+02 9.86126534e+01 8.44153605e+01 1.02880606e+00\n",
      " 9.40823493e+01 5.96461211e+01 4.70517661e+01 3.01425119e+01\n",
      " 3.07548246e+01 3.63443175e+01 1.10214895e+02 8.91310172e+01\n",
      " 1.16322584e+02]\n",
      "14-th iteration, loss: 0.14404326203759818, 16 gd steps\n",
      "insert gradient: -0.03998865808972064\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[1.34845699e+00 6.42652404e+01 1.98834359e+02 5.96387623e+01\n",
      " 9.01089890e+01 0.00000000e+00 2.84217094e-14 3.61470891e+01\n",
      " 8.52451777e+01 6.18881126e+01 1.24199709e+02 7.11876989e+01\n",
      " 1.12434363e+02 9.95328565e+01 1.86153202e+02 6.04154647e+01\n",
      " 5.76654651e+01 2.90541972e+01 3.23546787e+01 3.61900972e+01\n",
      " 9.79662837e+01 9.32022403e+01 1.16322584e+02]\n",
      "15-th iteration, loss: 0.1404940276248858, 21 gd steps\n",
      "insert gradient: -0.02627446942128225\n",
      "15-th iteration, new layer inserted. now 21 layers\n",
      "[  0.56311807  58.62096601 201.41709593  62.63043058  92.18640983\n",
      "  38.4078711   85.61942156  62.6168187  125.80446027  71.56177408\n",
      " 109.68392742 100.74003011 176.62051682  63.27031017  84.37186547\n",
      "  18.07324247  40.8724733   38.49795482  78.25378857  99.28113018\n",
      " 116.32258421]\n",
      "16-th iteration, loss: 0.13724302877145195, 23 gd steps\n",
      "insert gradient: -0.025959765314628156\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[  0.96069231  62.00665026 205.00649637  58.97174253  91.79561618\n",
      "  40.95454228  85.93688473  62.15569168 127.5311317   71.7581716\n",
      " 114.67128021  99.83202785 113.03450803   0.          64.59114745\n",
      "  62.7142929   89.14728367  13.93933987  45.52062335  42.46600063\n",
      "  72.96066012 101.4710952  116.32258421]\n",
      "17-th iteration, loss: 0.115375033226095, 36 gd steps\n",
      "insert gradient: -0.07000414021073387\n",
      "17-th iteration, new layer inserted. now 21 layers\n",
      "[  1.24238734  50.32039466 229.96243958  59.28767875  91.93886687\n",
      "  47.96153388  87.04949341  68.44520666 122.11399434  72.58876338\n",
      " 110.51494346  98.55288938 108.20239515  13.08900073  58.68797767\n",
      "  55.38418623 186.54792554  57.23858008  96.24243341 109.81282463\n",
      " 116.32258421]\n",
      "18-th iteration, loss: 0.10065589585682377, 35 gd steps\n",
      "insert gradient: -0.033907737382623405\n",
      "18-th iteration, new layer inserted. now 23 layers\n",
      "[  0.5676111   56.64019152 109.11090738   0.         130.93308885\n",
      "  54.49058143  90.71991764  48.37304921  95.30109953  66.53408957\n",
      " 117.47951718  82.09704645 101.86060801 100.65991488 103.32223479\n",
      "  35.89790126  30.56704127  53.32038367 178.40067136  60.46785201\n",
      " 114.77943411 101.02392275 116.32258421]\n",
      "19-th iteration, loss: 0.09623887845965903, 26 gd steps\n",
      "insert gradient: -0.019128404222734666\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[  0.32860766  59.39122762  97.65635152  11.09271662 112.15430672\n",
      "  59.34612233  90.7075767   48.54637528  93.5813393   70.12156005\n",
      " 120.57659995  80.29437613 103.5853519  100.47638698  97.17939933\n",
      "  44.19572333  23.62602132  51.92597633 114.83999001   0.\n",
      "  76.55999334  56.21952744 112.77686249 103.19379627 116.32258421]\n",
      "20-th iteration, loss: 0.08425292314530704, 66 gd steps\n",
      "insert gradient: -0.00494453368455781\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[1.27908398e-02 6.67003355e+01 9.09849628e+01 3.38397265e+01\n",
      " 6.85246549e+01 6.52460024e+01 8.58129635e+01 4.97842830e+01\n",
      " 9.64370898e+01 6.79904716e+01 1.33656849e+02 0.00000000e+00\n",
      " 2.48689958e-14 7.37174609e+01 1.10300233e+02 9.67643838e+01\n",
      " 1.01371292e+02 4.89392144e+01 1.56045521e+01 4.79681981e+01\n",
      " 9.59306598e+01 1.83183891e+01 5.86339985e+01 6.47493817e+01\n",
      " 1.70387705e+02 7.07189405e+01 1.16322584e+02]\n",
      "21-th iteration, loss: 0.0830119319014845, 45 gd steps\n",
      "insert gradient: -0.00029375928806967883\n",
      "21-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.26155846  86.99806759  35.593791    70.44315705\n",
      "  63.1505199   88.41663095  49.90856431  94.26315828  67.36638711\n",
      " 135.8350961   74.38800511 111.84301845  95.31787034 101.7687038\n",
      "  50.73595204   7.55872445  50.79667545  96.34260524  18.95437023\n",
      "  65.65381747  56.56474037 177.89213206  69.25438328 116.32258421]\n",
      "22-th iteration, loss: 0.08286947568917147, 47 gd steps\n",
      "insert gradient: -0.00010315809990238108\n",
      "22-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.74275477  84.0125971   36.12726124  72.73101713\n",
      "  61.2805904   90.790762    49.52703629  93.88762219  66.82271887\n",
      " 136.88303453  74.02973915 112.50797178  94.7184091  101.91082948\n",
      "  51.31354333   4.46388584  51.90054336  95.43468425  19.28094794\n",
      "  66.95051991  55.09605408 180.87820772  67.85536755 116.32258421]\n",
      "23-th iteration, loss: 0.08285575805159323, 18 gd steps\n",
      "insert gradient: -0.00017902782918995138\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.86851873e+01 8.40783424e+01 0.00000000e+00\n",
      " 2.48689958e-14 3.61403886e+01 7.28593430e+01 6.11852340e+01\n",
      " 9.09748312e+01 4.94084880e+01 9.36801397e+01 6.67770995e+01\n",
      " 1.37024209e+02 7.40225695e+01 1.12526201e+02 9.46590841e+01\n",
      " 1.01906648e+02 5.15746793e+01 3.68266064e+00 5.20253570e+01\n",
      " 9.55280991e+01 1.96017247e+01 6.64728442e+01 5.49089862e+01\n",
      " 1.81237798e+02 6.77336227e+01 1.16322584e+02]\n",
      "24-th iteration, loss: 0.08284719446910249, 18 gd steps\n",
      "insert gradient: -1.6186223421246328e-05\n",
      "24-th iteration, new layer inserted. now 29 layers\n",
      "[0.00000000e+00 6.86228689e+01 8.41065104e+01 5.79342880e-03\n",
      " 1.41524819e-02 3.61644397e+01 0.00000000e+00 7.10542736e-15\n",
      " 7.29608109e+01 6.11493227e+01 9.10950159e+01 4.93114755e+01\n",
      " 9.35601478e+01 6.66885696e+01 1.37112243e+02 7.39914376e+01\n",
      " 1.12536769e+02 9.46113313e+01 1.01914786e+02 5.17203660e+01\n",
      " 3.16525604e+00 5.20705038e+01 9.55093163e+01 1.98687530e+01\n",
      " 6.61733052e+01 5.47946118e+01 1.81449804e+02 6.76730762e+01\n",
      " 1.16322584e+02]\n",
      "25-th iteration, loss: 0.08284319470017848, 17 gd steps\n",
      "insert gradient: -2.905486827671122e-05\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85811437e+01 8.41469649e+01 3.61716333e+01\n",
      " 7.30143836e+01 6.11462784e+01 9.11266644e+01 4.92560560e+01\n",
      " 9.35017167e+01 6.66266384e+01 1.37138025e+02 7.39798696e+01\n",
      " 1.12541296e+02 9.45725123e+01 1.01947338e+02 5.17947815e+01\n",
      " 2.88243472e+00 5.21066824e+01 9.55100083e+01 1.99655504e+01\n",
      " 6.60553002e+01 5.47363962e+01 0.00000000e+00 7.10542736e-15\n",
      " 1.81533786e+02 6.76081786e+01 1.16322584e+02]\n",
      "26-th iteration, loss: 0.08284009731231819, 24 gd steps\n",
      "insert gradient: -1.0961045064738884e-05\n",
      "26-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.54769096  84.16651061  36.17741512  73.02511979\n",
      "  61.14740842  91.14991541  49.19534148  93.43844256  66.57924829\n",
      " 137.16803497  73.96893491 112.54821115  94.53571229 101.97201456\n",
      "  51.86162626   2.61539585  52.14027074  95.51020851  20.05001012\n",
      "  65.9623477   54.67625586 181.65174872  67.5366041  116.32258421]\n",
      "27-th iteration, loss: 0.08283706720178527, 268 gd steps\n",
      "insert gradient: -4.532346355049533e-05\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85033377e+01 8.42113844e+01 3.61846493e+01\n",
      " 7.30176738e+01 6.11553130e+01 9.11856026e+01 4.91140763e+01\n",
      " 9.33389696e+01 6.65109552e+01 1.37209021e+02 7.39470271e+01\n",
      " 1.12562849e+02 9.44820155e+01 1.02004860e+02 5.19625052e+01\n",
      " 2.22792717e+00 5.21956730e+01 9.55093962e+01 0.00000000e+00\n",
      " 7.10542736e-15 2.01620356e+01 6.58470553e+01 5.45952028e+01\n",
      " 1.81736570e+02 6.74319244e+01 1.16322584e+02]\n",
      "28-th iteration, loss: 0.08283703920547822, 119 gd steps\n",
      "insert gradient: -3.880044828359517e-05\n",
      "28-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.50282061  84.21212276  36.18500306  73.01752202\n",
      "  61.15531646  91.18614114  49.11322324  93.3377449   66.51027027\n",
      " 137.20962605  73.94682988 112.56305748  94.48149266 102.00502342\n",
      "  51.96369456   2.22332156  52.19629985  95.50921226  20.16461415\n",
      "  65.84551886  54.59410224 181.737837    67.43089989 116.32258421]\n",
      "29-th iteration, loss: 0.08283701912703145, 91 gd steps\n",
      "insert gradient: -4.111701576009351e-05\n",
      "29-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.50242553  84.21268652  36.18528275  73.01740958\n",
      "  61.15531753  91.18655268  49.11259015  93.33682937  66.50978102\n",
      " 137.21010251  73.9467091  112.56323554  94.48113616 102.00516282\n",
      "  51.96462599   2.21985455  52.19680494  95.50907646  20.1655793\n",
      "  65.84434714  54.59327336 181.73880204  67.4301383  116.32258421]\n",
      "30-th iteration, loss: 0.08283700031402112, 87 gd steps\n",
      "insert gradient: -4.34285425223591e-05\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85020452e+01 8.42132087e+01 3.61855110e+01\n",
      " 7.30172830e+01 6.11553050e+01 9.11869435e+01 4.91119996e+01\n",
      " 9.33359649e+01 6.65093144e+01 1.37210550e+02 7.39465865e+01\n",
      " 1.12563402e+02 9.44808070e+01 1.02005301e+02 5.19655476e+01\n",
      " 2.21661429e+00 5.21973226e+01 9.55089749e+01 0.00000000e+00\n",
      " 7.10542736e-15 2.01665760e+01 6.58432726e+01 5.45925184e+01\n",
      " 1.81739731e+02 6.74294264e+01 1.16322584e+02]\n",
      "31-th iteration, loss: 0.08283697682842196, 103 gd steps\n",
      "insert gradient: -3.920947372062716e-05\n",
      "31-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85016146e+01 8.42138606e+01 3.61858490e+01\n",
      " 7.30171599e+01 6.11553116e+01 9.11874267e+01 4.91113154e+01\n",
      " 9.33349410e+01 6.65087757e+01 1.37211096e+02 7.39464558e+01\n",
      " 1.12563602e+02 9.44804061e+01 1.02005442e+02 5.19666031e+01\n",
      " 2.21270514e+00 5.21978950e+01 9.55088130e+01 2.01687615e+01\n",
      " 6.58419367e+01 5.45915748e+01 0.00000000e+00 8.88178420e-15\n",
      " 1.81740835e+02 6.74285850e+01 1.16322584e+02]\n",
      "32-th iteration, loss: 0.08283695516253566, 97 gd steps\n",
      "insert gradient: -3.991270632041935e-05\n",
      "32-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85012124e+01 8.42144689e+01 3.61861737e+01\n",
      " 7.30170474e+01 6.11553106e+01 9.11878776e+01 4.91106915e+01\n",
      " 9.33340002e+01 6.65082980e+01 1.37211617e+02 7.39463523e+01\n",
      " 1.12563796e+02 9.44800548e+01 1.02005574e+02 5.19675939e+01\n",
      " 2.20909041e+00 5.21984372e+01 9.55086591e+01 0.00000000e+00\n",
      " 1.06581410e-14 2.01697640e+01 6.58406860e+01 5.45906918e+01\n",
      " 1.81742880e+02 6.74278160e+01 1.16322584e+02]\n",
      "33-th iteration, loss: 0.0828369342432879, 94 gd steps\n",
      "insert gradient: -3.885793568581123e-05\n",
      "33-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.50082497  84.2150686   36.18650924  73.01694396\n",
      "  61.15530795  91.18832134  49.11009425  93.33308932  66.50784241\n",
      " 137.21212943  73.94625694 112.56398501  94.47971139 102.00569107\n",
      "  51.96854512   2.20556422  52.19895137  95.50849461  20.17165529\n",
      "  65.83944908  54.58981425 181.74387024  67.42707497 116.32258421]\n",
      "34-th iteration, loss: 0.08283691827800947, 76 gd steps\n",
      "insert gradient: -3.974512254511065e-05\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85005120e+01 8.42155464e+01 3.61867694e+01\n",
      " 7.30168537e+01 6.11552927e+01 9.11886753e+01 4.91096169e+01\n",
      " 9.33323604e+01 6.65074801e+01 1.37212543e+02 7.39461831e+01\n",
      " 1.12564140e+02 9.44794493e+01 1.02005787e+02 5.19693255e+01\n",
      " 2.20275792e+00 5.21993814e+01 9.55083695e+01 2.01724299e+01\n",
      " 6.58384631e+01 5.45891216e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.81744672e+02 6.74264937e+01 1.16322584e+02]\n",
      "35-th iteration, loss: 0.08283689943722508, 86 gd steps\n",
      "insert gradient: -4.1378613905738266e-05\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.85001498e+01 8.42160775e+01 3.61870327e+01\n",
      " 7.30167340e+01 6.11552608e+01 9.11890773e+01 4.91090812e+01\n",
      " 9.33315335e+01 6.65070621e+01 1.37213008e+02 7.39460866e+01\n",
      " 1.12564308e+02 9.44791506e+01 1.02005895e+02 5.19702374e+01\n",
      " 2.19960511e+00 5.21998953e+01 9.55082477e+01 0.00000000e+00\n",
      " 7.10542736e-15 2.01733816e+01 6.58373726e+01 5.45883546e+01\n",
      " 1.81746509e+02 6.74258395e+01 1.16322584e+02]\n",
      "36-th iteration, loss: 0.08283688080289894, 86 gd steps\n",
      "insert gradient: -3.886069748175734e-05\n",
      "36-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84998032e+01 8.42166207e+01 3.61873343e+01\n",
      " 7.30166325e+01 6.11552401e+01 9.11894857e+01 4.91085621e+01\n",
      " 9.33307176e+01 6.65066568e+01 1.37213476e+02 7.39459969e+01\n",
      " 1.12564472e+02 9.44788442e+01 1.02005982e+02 5.19711092e+01\n",
      " 2.19645159e+00 5.22003717e+01 9.55080997e+01 2.01751530e+01\n",
      " 6.58362573e+01 5.45875622e+01 0.00000000e+00 1.24344979e-14\n",
      " 1.81747412e+02 6.74251946e+01 1.16322584e+02]\n",
      "37-th iteration, loss: 0.0828368632680361, 81 gd steps\n",
      "insert gradient: -3.935206724128607e-05\n",
      "37-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49947431  84.21713462  36.18762343  73.01653641\n",
      "  61.15521344  91.18987215  49.10807659  93.32995101  66.50628371\n",
      " 137.21392371  73.94591997 112.56463092  94.47856691 102.00606406\n",
      "  51.97194003   2.19348427  52.20082905  95.50795857  20.17598748\n",
      "  65.83519946  54.58681183 181.74911703  67.42459471 116.32258421]\n",
      "38-th iteration, loss: 0.08283684972225619, 66 gd steps\n",
      "insert gradient: -4.197545546323518e-05\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84991986e+01 8.42175442e+01 3.61878326e+01\n",
      " 7.30164437e+01 6.11551784e+01 9.11901859e+01 4.91076817e+01\n",
      " 9.33293245e+01 6.65059736e+01 1.37214286e+02 7.39458488e+01\n",
      " 1.12564757e+02 9.44783427e+01 1.02006132e+02 5.19726402e+01\n",
      " 2.19108719e+00 5.22012241e+01 9.55078598e+01 0.00000000e+00\n",
      " 2.48689958e-14 2.01767238e+01 6.58343583e+01 5.45862154e+01\n",
      " 1.81749819e+02 6.74241094e+01 1.16322584e+02]\n",
      "39-th iteration, loss: 0.08283683289264478, 79 gd steps\n",
      "insert gradient: -3.884704068129217e-05\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84988837e+01 8.42180423e+01 3.61881094e+01\n",
      " 7.30163455e+01 6.11551433e+01 9.11905642e+01 4.91072197e+01\n",
      " 9.33285823e+01 6.65056073e+01 1.37214718e+02 7.39457662e+01\n",
      " 1.12564904e+02 9.44780684e+01 1.02006197e+02 5.19734444e+01\n",
      " 2.18822580e+00 5.22016662e+01 9.55077222e+01 2.01783780e+01\n",
      " 6.58333364e+01 5.45854899e+01 0.00000000e+00 1.59872116e-14\n",
      " 1.81750651e+02 6.74235406e+01 1.16322584e+02]\n",
      "40-th iteration, loss: 0.0828368170269137, 75 gd steps\n",
      "insert gradient: -3.9400352052770056e-05\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84985857e+01 8.42185164e+01 3.61883798e+01\n",
      " 7.30162550e+01 6.11551067e+01 9.11909238e+01 4.91067871e+01\n",
      " 9.33278835e+01 6.65052698e+01 1.37215133e+02 7.39456959e+01\n",
      " 1.12565046e+02 9.44778187e+01 1.02006258e+02 5.19742083e+01\n",
      " 2.18552392e+00 5.22020874e+01 9.55075881e+01 0.00000000e+00\n",
      " 3.55271368e-15 2.01791523e+01 6.58323621e+01 5.45847986e+01\n",
      " 1.81752223e+02 6.74230097e+01 1.16322584e+02]\n",
      "41-th iteration, loss: 0.08283680143684262, 74 gd steps\n",
      "insert gradient: -3.8568027760911544e-05\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84982930e+01 8.42189889e+01 3.61886579e+01\n",
      " 7.30161691e+01 6.11550686e+01 9.11912812e+01 4.91063628e+01\n",
      " 9.33271939e+01 6.65049385e+01 1.37215545e+02 7.39456279e+01\n",
      " 1.12565186e+02 9.44775696e+01 1.02006310e+02 5.19749531e+01\n",
      " 2.18284683e+00 5.22024932e+01 9.55074461e+01 2.01806450e+01\n",
      " 6.58313872e+01 5.45841051e+01 0.00000000e+00 5.32907052e-15\n",
      " 1.81752997e+02 6.74224895e+01 1.16322584e+02]\n",
      "42-th iteration, loss: 0.08283678651703785, 71 gd steps\n",
      "insert gradient: -3.877408580472801e-05\n",
      "42-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49800985  84.21944072  36.18892329  73.01608467\n",
      "  61.15502524  91.19162351  49.10595826  93.32653478  66.5046258\n",
      " 137.21594454  73.94556667 112.56532319  94.47734033 102.00635998\n",
      "  51.9756764    2.18029247  52.2028914   95.50731191  20.18136654\n",
      "  65.83045316  54.58344255 181.75448312  67.42199809 116.32258421]\n",
      "43-th iteration, loss: 0.08283677482076733, 59 gd steps\n",
      "insert gradient: -4.126165320845784e-05\n",
      "43-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.4977687   84.21980627  36.18911987  73.01600255\n",
      "  61.15497816  91.19190531  49.10562321  93.32598744  66.5043609\n",
      " 137.21627193  73.9455083  112.56543316  94.47715218 102.00640311\n",
      "  51.97629455   2.17819703  52.20324003  95.50721511  20.18201144\n",
      "  65.82969869  54.58290833 181.7551047   67.42159435 116.32258421]\n",
      "44-th iteration, loss: 0.08283676346748399, 58 gd steps\n",
      "insert gradient: -4.280889040643651e-05\n",
      "44-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49753283  84.22015888  36.18929815  73.01591503\n",
      "  61.15492648  91.19218072  49.10529768  93.32545181  66.50409574\n",
      " 137.21658742  73.94544383 112.56553665  94.47696694 102.00644358\n",
      "  51.97690762   2.17616628  52.20359     95.50712911  20.18267258\n",
      "  65.82897547  54.58239798 181.75571796  67.42120465 116.32258421]\n",
      "45-th iteration, loss: 0.08283675239493152, 57 gd steps\n",
      "insert gradient: -4.3814761641272706e-05\n",
      "45-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49730356  84.220503    36.18946674  73.01582577\n",
      "  61.15487394  91.19245235  49.10498193  93.32492674  66.50383193\n",
      " 137.21689374  73.94537561 112.56563474  94.47678299 102.00647975\n",
      "  51.97751174   2.17418633  52.20393639  95.5070488   20.18334008\n",
      "  65.82827463  54.58190385 181.75632315  67.42082655 116.32258421]\n",
      "46-th iteration, loss: 0.08283674157342574, 56 gd steps\n",
      "insert gradient: -4.4511943237152204e-05\n",
      "46-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49708098  84.22084052  36.18962951  73.01573642\n",
      "  61.15482197  91.19272113  49.10467564  93.3244117   66.50357078\n",
      " 137.21719261  73.94530552 112.56572844  94.47660041 102.00651136\n",
      "  51.97810535   2.17225034  52.20427713  95.50697172  20.18400844\n",
      "  65.82759164  54.58142219 181.75692027  67.42045878 116.32258421]\n",
      "47-th iteration, loss: 0.08283673098456881, 55 gd steps\n",
      "insert gradient: -4.502159522100878e-05\n",
      "47-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84968648e+01 8.42211724e+01 3.61897883e+01\n",
      " 7.30156477e+01 6.11547710e+01 9.11929873e+01 4.91043783e+01\n",
      " 9.33239063e+01 6.65033132e+01 1.37217485e+02 7.39452349e+01\n",
      " 1.12565819e+02 9.44764197e+01 1.02006539e+02 5.19786880e+01\n",
      " 2.17035423e+00 5.22046114e+01 9.55068966e+01 0.00000000e+00\n",
      " 7.10542736e-15 2.01846745e+01 6.58269239e+01 5.45809510e+01\n",
      " 1.81757509e+02 6.74201005e+01 1.16322584e+02]\n",
      "48-th iteration, loss: 0.08283671743048902, 66 gd steps\n",
      "insert gradient: -3.911239491567888e-05\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          68.49661548  84.22158827  36.19001719  73.01555727\n",
      "  61.15472141  91.19331601  49.10402519  93.32329783  66.50300886\n",
      " 137.21784401  73.94515784 112.5659282   94.47619491 102.006558\n",
      "  51.97935952   2.16803539  52.2049826   95.50678026  20.18615988\n",
      "  65.82608541  54.58035613 181.75821534  67.41967059 116.32258421]\n",
      "49-th iteration, loss: 0.08283670715581053, 53 gd steps\n",
      "insert gradient: -4.1103288937319724e-05\n",
      "49-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 6.84964165e+01 8.42219264e+01 3.61902116e+01\n",
      " 7.30154886e+01 6.11546813e+01 9.11935813e+01 4.91037428e+01\n",
      " 9.33228105e+01 6.65027715e+01 1.37218138e+02 7.39451045e+01\n",
      " 1.12566021e+02 9.44760230e+01 1.02006573e+02 5.19798979e+01\n",
      " 2.16617025e+00 5.22052793e+01 9.55066810e+01 0.00000000e+00\n",
      " 1.06581410e-14 2.01867399e+01 6.58254018e+01 5.45798733e+01\n",
      " 1.81758783e+02 6.74193302e+01 1.16322584e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.527373772247161\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.77514076    0.         1691.89320094]\n",
      "1-th iteration, loss: 0.74387146324293, 11 gd steps\n",
      "insert gradient: -0.6133567363831649\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  40.99852774   62.34624526  226.96128305    0.         1464.93191789]\n",
      "2-th iteration, loss: 0.52910136033178, 30 gd steps\n",
      "insert gradient: -0.4258102123127809\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.34359316   57.23789236  176.43915965   54.01499956  328.86226728\n",
      "    0.         1136.06965061]\n",
      "3-th iteration, loss: 0.43983873620372205, 70 gd steps\n",
      "insert gradient: -0.47888239082811646\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[6.40487376e-01 5.35514346e+01 1.64425543e+02 5.29134996e+01\n",
      " 2.50909473e+02 5.43227431e+01 1.29836531e+02 0.00000000e+00\n",
      " 1.00623312e+03]\n",
      "4-th iteration, loss: 0.3742147489302513, 20 gd steps\n",
      "insert gradient: -0.3747551317097983\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.43194339  56.69461333 168.58954309  50.22081756 233.90784791\n",
      "  61.0224746  103.61096492  49.6435091  633.55418611   0.\n",
      " 372.678933  ]\n",
      "5-th iteration, loss: 0.2786932668269982, 14 gd steps\n",
      "insert gradient: -0.4617004617167589\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  3.74749884  52.96732222 195.59485761  41.1638029  100.60420873\n",
      "   0.         120.72505048  68.63472911 101.33227661  62.83575889\n",
      " 579.91509381  74.61275271 372.678933  ]\n",
      "6-th iteration, loss: 0.25051294530631085, 22 gd steps\n",
      "insert gradient: -0.1610728348152303\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  3.62755044  55.16116485 183.58158314  52.38937755  75.4675391\n",
      "  19.69775913  97.03855743  65.09234637 113.61868645  60.70236166\n",
      " 414.36307224   0.         159.3704124   73.10585943 372.678933  ]\n",
      "7-th iteration, loss: 0.22654820875865492, 20 gd steps\n",
      "insert gradient: -0.07671379021527887\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.62832769  57.94572427 181.28205758  53.6496729   66.92054316\n",
      "  24.04645846  99.72198045  69.10228628 110.37048689  63.64823957\n",
      " 399.63105532  27.90109609 136.21041054  66.49723368 186.3394665\n",
      "   0.         186.3394665 ]\n",
      "8-th iteration, loss: 0.21964880916298285, 16 gd steps\n",
      "insert gradient: -0.08208928895359782\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  3.77049355  59.94554211 179.94921354  52.36452771  73.93782692\n",
      "  24.83314554  93.82442735  70.00970809 108.29509018  67.32019425\n",
      " 284.09697082   0.         113.63878833  32.82753121 125.36357951\n",
      "  66.39918817 180.51817914  15.50929946 186.3394665 ]\n",
      "9-th iteration, loss: 0.21571021874862595, 21 gd steps\n",
      "insert gradient: -0.07249034936278115\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[  1.63224134  54.51267554 182.44274344  56.87524243  75.33186899\n",
      "  24.97120093  93.3877843   68.1434507  111.6157941   67.22241982\n",
      " 186.9696041    0.          93.48480205   4.82600534 110.43911692\n",
      "  33.77520944 122.51495347  66.92590252 174.94280995  21.68315498\n",
      " 186.3394665 ]\n",
      "10-th iteration, loss: 0.20031732930678486, 27 gd steps\n",
      "insert gradient: -0.06423163013382292\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.14088232  56.78236731 183.8475936   56.30153148  84.02463076\n",
      "  25.89478271  87.0888877   65.87542878 118.31829277  64.42425844\n",
      " 177.86775164  15.31337956  65.03488524  16.75395055 104.36017382\n",
      "  29.93110254 131.20067837  65.25242877 155.42215379  36.32157375\n",
      " 186.3394665 ]\n",
      "11-th iteration, loss: 0.13963014258132367, 34 gd steps\n",
      "insert gradient: -0.09254007160053673\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  2.12609095  60.61173164 102.36501654   0.         102.36501654\n",
      "  57.24635415  83.93192464  26.81246172  78.45198836  55.90171653\n",
      " 140.00326508  64.10139442 188.32602747  85.47945699 102.09400424\n",
      "  51.98508535 134.21653731  49.62556869 111.86859892  71.84866241\n",
      " 186.3394665 ]\n",
      "12-th iteration, loss: 0.12511878888743194, 24 gd steps\n",
      "insert gradient: -0.07773546892664344\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  4.15049917  61.95330846  97.27257331  10.93439303  98.17806177\n",
      "  53.90541858  98.16413403  22.30988928  67.64124444  53.50357491\n",
      " 161.15054554  58.31877519 188.47973731  80.21793744 104.54122003\n",
      "  57.19581373 119.00272165  56.36649566  99.97403008  74.11069636\n",
      " 152.4595635    0.          33.879903  ]\n",
      "13-th iteration, loss: 0.08253178493586935, 31 gd steps\n",
      "insert gradient: -0.013515651904869924\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  2.46436852  54.70654225  95.29231455  13.31918833 100.29437728\n",
      "  61.33261481  96.04556917  29.77440949  58.35396307  52.00811298\n",
      " 164.5520269   59.17641606  77.51453795   0.         116.27180693\n",
      "  83.73902489 112.09948579  61.67465074 131.02236676  51.83971538\n",
      "  95.98916619  68.60686467 126.65839442  58.52023416  33.879903  ]\n",
      "14-th iteration, loss: 0.07886546719560546, 37 gd steps\n",
      "insert gradient: -0.005748877232592862\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  1.94594408  57.72324767  77.84709272  21.16368094 101.50231852\n",
      "  63.24885639  97.38229286  28.89442845  57.76580869  54.90314185\n",
      " 162.60205062  60.85708964  73.58096814   6.03639908 106.50148189\n",
      "  86.68891313 114.23297808  64.81073719 132.19368314  56.67416883\n",
      "  88.63546889  72.99304995 121.23577773  59.28408376  33.879903  ]\n",
      "15-th iteration, loss: 0.07819351102046929, 25 gd steps\n",
      "insert gradient: -0.004123440676803318\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  1.36597697  58.08962025  74.32233074  22.76306616 103.62535935\n",
      "  63.48244104  97.11434732  28.69469507  59.7753677   55.58946552\n",
      " 163.00963279  60.94856019  73.507048     6.47959629 106.98811264\n",
      "  86.92658208 115.5154896   65.29381232 133.68985454  57.67473429\n",
      "  87.63554587  74.13315515 119.66825799  59.6270491   33.879903  ]\n",
      "16-th iteration, loss: 0.07784888815235068, 22 gd steps\n",
      "insert gradient: -0.003260494749085871\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[1.47175231e+00 5.83906103e+01 7.16594365e+01 2.41183353e+01\n",
      " 1.04162380e+02 6.37073809e+01 9.70134572e+01 2.84042754e+01\n",
      " 6.13056437e+01 5.59044910e+01 1.63352581e+02 6.14081975e+01\n",
      " 7.25495066e+01 7.16554140e+00 1.07106882e+02 8.71658450e+01\n",
      " 1.15855274e+02 0.00000000e+00 2.48689958e-14 6.59025776e+01\n",
      " 1.34229352e+02 5.83046628e+01 8.67530312e+01 7.58516927e+01\n",
      " 1.17182033e+02 6.02228236e+01 3.38799030e+01]\n",
      "17-th iteration, loss: 0.07758036810683784, 23 gd steps\n",
      "insert gradient: -0.0029690662248117966\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[1.19095146e+00 5.87220457e+01 7.01754800e+01 2.50084901e+01\n",
      " 1.04776446e+02 6.37976529e+01 9.69915936e+01 2.83257081e+01\n",
      " 6.22000636e+01 5.61232850e+01 1.63458380e+02 6.14086852e+01\n",
      " 7.19005850e+01 7.70479006e+00 1.07771394e+02 8.74802935e+01\n",
      " 1.16366450e+02 7.54553161e-02 2.98028985e-02 6.64111346e+01\n",
      " 1.34709045e+02 0.00000000e+00 2.13162821e-14 5.87085985e+01\n",
      " 8.64264838e+01 7.73059122e+01 1.14346327e+02 6.12309777e+01\n",
      " 3.38799030e+01]\n",
      "18-th iteration, loss: 0.07735411548953489, 23 gd steps\n",
      "insert gradient: -0.00374758041230633\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[1.35010773e+00 5.90030834e+01 6.89981394e+01 2.62380655e+01\n",
      " 1.05120118e+02 6.38843311e+01 9.69666378e+01 2.80359822e+01\n",
      " 6.32450437e+01 5.64253629e+01 1.63511481e+02 6.15012019e+01\n",
      " 7.10988674e+01 8.12993697e+00 1.08101604e+02 8.76580105e+01\n",
      " 1.16692115e+02 6.71977899e+01 1.35166307e+02 1.59102664e-01\n",
      " 1.17833088e-01 5.92172463e+01 8.64502722e+01 7.88751846e+01\n",
      " 1.11468314e+02 6.20445389e+01 3.38799030e+01]\n",
      "19-th iteration, loss: 0.07723840671297567, 20 gd steps\n",
      "insert gradient: -0.004216915349392321\n",
      "19-th iteration, new layer inserted. now 27 layers\n",
      "[  1.01761395  58.92655524  68.17141483  26.98553732 105.39974123\n",
      "  64.01392512  97.15745574  28.01297779  63.57049839  56.76350694\n",
      " 163.13654308  61.93361497  70.65145574   8.40610442 108.64943551\n",
      "  87.66474066 117.25200454  67.27769617 135.53550148   0.17894567\n",
      "   0.23094486  59.68722565  86.77124122  79.93279645 109.50956348\n",
      "  62.38059697  33.879903  ]\n",
      "20-th iteration, loss: 0.07710725274688576, 24 gd steps\n",
      "insert gradient: -0.0024005818643012284\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[1.09298933e+00 5.89621028e+01 6.79122676e+01 2.70708676e+01\n",
      " 1.05533535e+02 6.43425005e+01 9.71857586e+01 2.82640535e+01\n",
      " 6.40475995e+01 5.73036168e+01 1.63843089e+02 6.19137593e+01\n",
      " 7.06941415e+01 8.81020782e+00 1.08798359e+02 8.80410923e+01\n",
      " 1.17482837e+02 6.76834278e+01 1.35725037e+02 3.99229130e-02\n",
      " 3.11262723e-01 5.98436921e+01 8.67760787e+01 8.01700031e+01\n",
      " 1.09077473e+02 6.24600314e+01 3.38799030e+01]\n",
      "21-th iteration, loss: 0.07699660244332793, 30 gd steps\n",
      "insert gradient: -0.0018746542909274702\n",
      "21-th iteration, new layer inserted. now 27 layers\n",
      "[1.12636353e+00 5.94092650e+01 6.70054406e+01 2.78650215e+01\n",
      " 1.05903779e+02 0.00000000e+00 1.42108547e-14 6.43827243e+01\n",
      " 9.71337123e+01 2.82312771e+01 6.48266178e+01 5.71995451e+01\n",
      " 1.64215459e+02 6.18984708e+01 7.04929128e+01 8.92714221e+00\n",
      " 1.09155516e+02 8.80963841e+01 1.18186326e+02 6.79713284e+01\n",
      " 1.36746162e+02 6.03687280e+01 8.70866445e+01 8.10802350e+01\n",
      " 1.07281171e+02 6.30686398e+01 3.38799030e+01]\n",
      "22-th iteration, loss: 0.07692550961103015, 40 gd steps\n",
      "insert gradient: -0.0010517953363505849\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[1.03514677e+00 5.95149763e+01 6.53717282e+01 2.88780932e+01\n",
      " 1.06239089e+02 8.14856276e-02 6.95304416e-03 6.45059179e+01\n",
      " 9.71019267e+01 2.80793124e+01 6.59071176e+01 5.73891594e+01\n",
      " 1.64605546e+02 6.18864841e+01 7.00899194e+01 9.25637123e+00\n",
      " 1.09726176e+02 8.82059126e+01 1.18505348e+02 6.82451805e+01\n",
      " 1.36808728e+02 6.07221355e+01 8.71781586e+01 8.19819526e+01\n",
      " 1.05143145e+02 6.38801919e+01 3.38799030e+01]\n",
      "23-th iteration, loss: 0.07690573870429189, 23 gd steps\n",
      "insert gradient: -0.0007475532358064268\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[9.66980021e-01 5.94470897e+01 6.52676422e+01 2.90527784e+01\n",
      " 1.06337020e+02 6.46058147e+01 9.71872530e+01 2.81372642e+01\n",
      " 6.61082323e+01 5.72987828e+01 1.64758161e+02 6.19682135e+01\n",
      " 7.00637800e+01 9.32624801e+00 1.09880070e+02 8.81687739e+01\n",
      " 1.18763885e+02 0.00000000e+00 2.48689958e-14 6.82604983e+01\n",
      " 1.37073833e+02 6.08892558e+01 8.73244973e+01 8.21382076e+01\n",
      " 1.04784001e+02 6.39905663e+01 3.38799030e+01]\n",
      "24-th iteration, loss: 0.0768943899236785, 20 gd steps\n",
      "insert gradient: -0.001026468883708724\n",
      "24-th iteration, new layer inserted. now 25 layers\n",
      "[  0.96356738  59.47067972  65.07330752  29.18732795 106.37177155\n",
      "  64.61299421  97.1983426   28.13598436  66.33269727  57.45716143\n",
      " 164.84896957  61.86915569  70.02723672   9.44753686 110.10999327\n",
      "  88.12306256 118.85336367  68.37641833 137.29499448  60.93112623\n",
      "  87.46533661  82.3868395  104.12782023  64.12754714  33.879903  ]\n",
      "25-th iteration, loss: 0.07688287252001867, 21 gd steps\n",
      "insert gradient: -0.0006770327128297123\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[9.84810345e-01 5.95568496e+01 6.50661205e+01 2.93076008e+01\n",
      " 1.06412522e+02 6.46615731e+01 9.72203328e+01 2.82077092e+01\n",
      " 6.64048656e+01 5.72459743e+01 1.65024784e+02 6.19794994e+01\n",
      " 7.00059093e+01 9.44945516e+00 1.10147812e+02 8.82294917e+01\n",
      " 1.19006642e+02 6.84270639e+01 1.37405420e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.10023481e+01 8.75071704e+01 8.24406840e+01\n",
      " 1.04022519e+02 6.42608076e+01 3.38799030e+01]\n",
      "26-th iteration, loss: 0.07687600644016027, 18 gd steps\n",
      "insert gradient: -0.0005538054483562765\n",
      "26-th iteration, new layer inserted. now 29 layers\n",
      "[9.71827144e-01 5.95503728e+01 6.50525895e+01 2.94088520e+01\n",
      " 1.06446565e+02 6.47001501e+01 9.72461747e+01 2.82270087e+01\n",
      " 6.64437306e+01 5.72382332e+01 1.65067761e+02 6.19423244e+01\n",
      " 7.00198401e+01 0.00000000e+00 1.42108547e-14 9.47210973e+00\n",
      " 1.10197523e+02 8.82265480e+01 1.19135739e+02 6.84500093e+01\n",
      " 1.37477635e+02 2.79977342e-02 1.81159423e-02 6.10639578e+01\n",
      " 8.76126239e+01 8.25363123e+01 1.03888139e+02 6.43125044e+01\n",
      " 3.38799030e+01]\n",
      "27-th iteration, loss: 0.07686819036819305, 20 gd steps\n",
      "insert gradient: -0.0006299387110398809\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[9.44475196e-01 5.95823646e+01 6.48168655e+01 0.00000000e+00\n",
      " 1.24344979e-14 2.95660885e+01 1.06509212e+02 6.47262803e+01\n",
      " 9.72531383e+01 2.82614547e+01 6.66755509e+01 5.71364890e+01\n",
      " 1.65264125e+02 6.19492981e+01 7.00274002e+01 9.49952402e+00\n",
      " 1.10422878e+02 8.83119414e+01 1.19164591e+02 6.86813387e+01\n",
      " 1.37588434e+02 6.10042065e+01 8.78610053e+01 8.28501056e+01\n",
      " 1.03005650e+02 6.45534483e+01 3.38799030e+01]\n",
      "28-th iteration, loss: 0.0768632641030886, 18 gd steps\n",
      "insert gradient: -0.00036387140358083806\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[9.68123570e-01 5.96312136e+01 6.48311254e+01 2.26643780e-02\n",
      " 2.92042975e-03 2.96036329e+01 1.06506026e+02 6.47627996e+01\n",
      " 9.73065780e+01 2.82484676e+01 6.67051139e+01 5.72496208e+01\n",
      " 1.65316776e+02 6.18553541e+01 7.00522037e+01 9.53324952e+00\n",
      " 1.10455787e+02 8.82819577e+01 1.19338968e+02 6.85602880e+01\n",
      " 1.37658661e+02 6.12159840e+01 8.78885714e+01 8.28296158e+01\n",
      " 1.02990237e+02 6.46365744e+01 3.38799030e+01]\n",
      "29-th iteration, loss: 0.07685867950320506, 28 gd steps\n",
      "insert gradient: -0.00041384849379352937\n",
      "29-th iteration, new layer inserted. now 25 layers\n",
      "[  0.96308162  59.64975267  64.78986331  29.70098283 106.54446996\n",
      "  64.80499706  97.32561917  28.31330309  66.78249988  57.15363714\n",
      " 165.45034781  61.89025908  70.15498662   9.542047   110.5069455\n",
      "  88.26896667 119.49856203  68.59343285 137.83351182  61.23525656\n",
      "  87.94648287  82.90326958 102.87405148  64.67323439  33.879903  ]\n",
      "30-th iteration, loss: 0.07685628188020835, 22 gd steps\n",
      "insert gradient: -0.000260495606472836\n",
      "30-th iteration, new layer inserted. now 25 layers\n",
      "[  0.96091637  59.65239737  64.77002995  29.73747184 106.55295961\n",
      "  64.82433687  97.35236023  28.34612863  66.81843894  57.13758223\n",
      " 165.52901866  61.86253181  70.18896049   9.55766576 110.54628108\n",
      "  88.2628125  119.58506598  68.59552197 137.90185346  61.29837188\n",
      "  88.01531541  82.9440116  102.77082983  64.7368795   33.879903  ]\n",
      "31-th iteration, loss: 0.07685439406322267, 22 gd steps\n",
      "insert gradient: -0.0002597523166581418\n",
      "31-th iteration, new layer inserted. now 27 layers\n",
      "[9.55986236e-01 5.96697553e+01 6.47381012e+01 2.97822498e+01\n",
      " 1.06566146e+02 6.48392334e+01 9.73767210e+01 2.83621888e+01\n",
      " 6.68688231e+01 5.71384532e+01 1.65597591e+02 6.18284841e+01\n",
      " 7.02437353e+01 9.55896725e+00 1.10612106e+02 8.82580665e+01\n",
      " 1.19681542e+02 0.00000000e+00 1.42108547e-14 6.85955225e+01\n",
      " 1.37978027e+02 6.13211805e+01 8.81069426e+01 8.30043897e+01\n",
      " 1.02565531e+02 6.48136311e+01 3.38799030e+01]\n",
      "32-th iteration, loss: 0.07685325660043085, 17 gd steps\n",
      "insert gradient: -0.0002835720980531245\n",
      "32-th iteration, new layer inserted. now 25 layers\n",
      "[  0.97861763  59.73526169  64.69741125  29.82261204 106.57605878\n",
      "  64.84676909  97.38326455  28.3798907   66.93866748  57.01873945\n",
      " 165.6882366   61.84469924  70.27635728   9.55212574 110.64071677\n",
      "  88.27695358 119.73804303  68.64022516 138.02627241  61.34337415\n",
      "  88.1618699   83.06036915 102.37949355  64.9106136   33.879903  ]\n",
      "33-th iteration, loss: 0.07685189297404856, 22 gd steps\n",
      "insert gradient: -0.0002066723454039689\n",
      "33-th iteration, new layer inserted. now 27 layers\n",
      "[9.56049005e-01 5.96916725e+01 6.47196759e+01 2.98574656e+01\n",
      " 1.06585251e+02 6.48692074e+01 9.74098087e+01 2.84029020e+01\n",
      " 6.69276692e+01 5.70669438e+01 1.65693783e+02 6.18382973e+01\n",
      " 7.03005730e+01 0.00000000e+00 1.24344979e-14 9.56616204e+00\n",
      " 1.10676644e+02 8.82692131e+01 1.19777457e+02 6.86475217e+01\n",
      " 1.38061297e+02 6.13812552e+01 8.82123122e+01 8.31011558e+01\n",
      " 1.02332640e+02 6.48973414e+01 3.38799030e+01]\n",
      "34-th iteration, loss: 0.07685093409092741, 23 gd steps\n",
      "insert gradient: -0.00016325630711734018\n",
      "34-th iteration, new layer inserted. now 27 layers\n",
      "[9.58088777e-01 5.96895122e+01 6.47276799e+01 2.98782117e+01\n",
      " 1.06592587e+02 6.48884640e+01 9.74125722e+01 2.84008620e+01\n",
      " 6.69434321e+01 5.71299418e+01 1.65738217e+02 6.17653777e+01\n",
      " 7.03210421e+01 1.30864430e-02 8.70965189e-03 9.58017332e+00\n",
      " 1.10695428e+02 8.82839946e+01 1.19819800e+02 6.86585758e+01\n",
      " 1.38104234e+02 6.14041226e+01 8.82456224e+01 8.31209222e+01\n",
      " 1.02268758e+02 6.49162185e+01 3.38799030e+01]\n",
      "35-th iteration, loss: 0.07685037200535856, 16 gd steps\n",
      "insert gradient: -0.0001916935885082527\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[9.55115977e-01 5.97204209e+01 6.46832952e+01 2.99075638e+01\n",
      " 1.06606371e+02 6.48901929e+01 9.74238388e+01 2.83956331e+01\n",
      " 6.70086484e+01 5.71149317e+01 1.65786867e+02 6.17363369e+01\n",
      " 7.03733939e+01 9.59859514e+00 1.10732459e+02 8.82856133e+01\n",
      " 1.19868406e+02 6.86605390e+01 1.38146779e+02 0.00000000e+00\n",
      " 2.48689958e-14 6.13988756e+01 8.83300968e+01 8.31659473e+01\n",
      " 1.02062498e+02 6.50075167e+01 3.38799030e+01]\n",
      "36-th iteration, loss: 0.07684986452103201, 17 gd steps\n",
      "insert gradient: -0.00013613822282312934\n",
      "36-th iteration, new layer inserted. now 27 layers\n",
      "[9.57236272e-01 5.97099789e+01 6.47031579e+01 2.99253333e+01\n",
      " 1.06608749e+02 6.49000462e+01 9.74305420e+01 2.84153399e+01\n",
      " 6.70031101e+01 5.71142864e+01 1.65815342e+02 6.17429936e+01\n",
      " 7.03680929e+01 9.59688092e+00 1.10731021e+02 8.82944436e+01\n",
      " 1.19874397e+02 6.86901933e+01 1.38166024e+02 7.11602448e-03\n",
      " 1.46918502e-03 6.14213965e+01 8.83281652e+01 8.31855077e+01\n",
      " 1.02070877e+02 6.49855006e+01 3.38799030e+01]\n",
      "37-th iteration, loss: 0.07684946597128355, 16 gd steps\n",
      "insert gradient: -0.0002131463725604327\n",
      "37-th iteration, new layer inserted. now 25 layers\n",
      "[  0.95068156  59.70181635  64.70622409  29.92988225 106.60320313\n",
      "  64.91156203  97.45044407  28.43845589  67.0177234   57.07467763\n",
      " 165.8584419   61.72943805  70.40742786   9.59245601 110.78734594\n",
      "  88.28303221 119.93933039  68.66965914 138.26087465  61.43647304\n",
      "  88.37555879  83.20959015 101.98668158  65.01782992  33.879903  ]\n",
      "38-th iteration, loss: 0.07684913067891659, 20 gd steps\n",
      "insert gradient: -0.00013495884460962556\n",
      "38-th iteration, new layer inserted. now 25 layers\n",
      "[  0.9576615   59.7092102   64.71092755  29.93970372 106.60881846\n",
      "  64.92680065  97.45657004  28.4615958   67.02333919  57.0644536\n",
      " 165.89164312  61.73736132  70.42239024   9.60658255 110.77945061\n",
      "  88.29811025 119.95218763  68.69819202 138.22663116  61.45866633\n",
      "  88.39755764  83.22761411 101.95978149  65.02713439  33.879903  ]\n",
      "39-th iteration, loss: 0.07684877743273037, 28 gd steps\n",
      "insert gradient: -8.927524112261377e-05\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[9.59751112e-01 5.97236916e+01 6.47184712e+01 2.99633878e+01\n",
      " 1.06619239e+02 6.49364077e+01 9.74588509e+01 2.84564155e+01\n",
      " 6.70333747e+01 5.70919007e+01 1.65919340e+02 6.17089636e+01\n",
      " 7.04338238e+01 9.61552221e+00 1.10786465e+02 8.83067766e+01\n",
      " 1.19975154e+02 6.87145083e+01 1.38258252e+02 0.00000000e+00\n",
      " 3.19744231e-14 6.14791606e+01 8.84127572e+01 8.32383182e+01\n",
      " 1.01936184e+02 6.50411989e+01 3.38799030e+01]\n",
      "40-th iteration, loss: 0.07684864320533367, 18 gd steps\n",
      "insert gradient: -6.501184331972307e-05\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[9.59299084e-01 5.97253126e+01 6.47178595e+01 2.99721988e+01\n",
      " 1.06620495e+02 6.49410890e+01 9.74654599e+01 2.84607625e+01\n",
      " 6.70403451e+01 5.70873243e+01 1.65936066e+02 6.16951415e+01\n",
      " 7.04407475e+01 9.61926759e+00 1.10793079e+02 8.83055136e+01\n",
      " 1.19996834e+02 6.87134507e+01 1.38273324e+02 7.29902787e-04\n",
      " 7.65190960e-03 6.14870811e+01 8.84319613e+01 8.32484665e+01\n",
      " 1.01911722e+02 6.50516302e+01 3.38799030e+01]\n",
      "41-th iteration, loss: 0.07684847018774642, 19 gd steps\n",
      "insert gradient: -0.00015383703661134556\n",
      "41-th iteration, new layer inserted. now 25 layers\n",
      "[  0.95940394  59.73325809  64.71654293  29.98321576 106.62032251\n",
      "  64.94192728  97.4757212   28.47106748  67.06024342  57.08078823\n",
      " 165.96102456  61.67772059  70.46147733   9.61930324 110.81621844\n",
      "  88.3033067  120.03405684  68.72833042 138.32040441  61.49221006\n",
      "  88.47779741  83.27939587 101.82628975  65.07239341  33.879903  ]\n",
      "42-th iteration, loss: 0.07684837260378467, 17 gd steps\n",
      "insert gradient: -4.839218266887196e-05\n",
      "42-th iteration, new layer inserted. now 25 layers\n",
      "[  0.96032112  59.73499778  64.71695055  29.98968745 106.62431678\n",
      "  64.9525786   97.47981889  28.47424035  67.06595798  57.07569838\n",
      " 165.97987316  61.68203687  70.46982515   9.62224717 110.82330252\n",
      "  88.30439093 120.0433873   68.72334197 138.32124889  61.5019361\n",
      "  88.48605366  83.28173723 101.80545485  65.08886554  33.879903  ]\n",
      "43-th iteration, loss: 0.07684831283251509, 20 gd steps\n",
      "insert gradient: -5.066560643453166e-05\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[9.54494887e-01 5.97278654e+01 6.47001738e+01 3.00097712e+01\n",
      " 1.06629955e+02 0.00000000e+00 2.48689958e-14 6.49537536e+01\n",
      " 9.74801318e+01 2.84751463e+01 6.70784776e+01 5.70835906e+01\n",
      " 1.65994451e+02 6.16766444e+01 7.04926709e+01 9.61733072e+00\n",
      " 1.10832895e+02 8.83058185e+01 1.20049306e+02 6.87308390e+01\n",
      " 1.38325348e+02 6.15064324e+01 8.85100549e+01 8.33035505e+01\n",
      " 1.01732472e+02 6.51084858e+01 3.38799030e+01]\n",
      "44-th iteration, loss: 0.07684825262551331, 21 gd steps\n",
      "insert gradient: -5.0677960718638625e-05\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[9.58318777e-01 5.97350916e+01 6.47052916e+01 3.00099581e+01\n",
      " 1.06630129e+02 2.55863902e-03 4.24894358e-04 6.49575902e+01\n",
      " 9.74859918e+01 2.84804865e+01 6.70828575e+01 5.70737873e+01\n",
      " 1.66014772e+02 6.16655191e+01 7.04976621e+01 9.62066361e+00\n",
      " 1.10838231e+02 8.83123905e+01 1.20066736e+02 6.87326579e+01\n",
      " 1.38335895e+02 6.15135684e+01 8.85097075e+01 8.33049637e+01\n",
      " 1.01733868e+02 6.51139013e+01 3.38799030e+01]\n",
      "45-th iteration, loss: 0.07684820577826088, 24 gd steps\n",
      "insert gradient: -3.7200015675674656e-05\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[9.58214728e-01 5.97395222e+01 6.47054879e+01 3.00150730e+01\n",
      " 1.06632693e+02 4.48150370e-03 4.66494414e-04 6.49603788e+01\n",
      " 9.74883883e+01 2.84839623e+01 6.70861515e+01 5.70720157e+01\n",
      " 1.66024440e+02 6.16589129e+01 7.05066353e+01 9.62332149e+00\n",
      " 1.10843733e+02 8.83117084e+01 1.20079608e+02 6.87351902e+01\n",
      " 1.38347528e+02 6.15203551e+01 8.85244003e+01 8.33099329e+01\n",
      " 1.01721533e+02 6.51215323e+01 3.38799030e+01]\n",
      "46-th iteration, loss: 0.07684811978416538, 71 gd steps\n",
      "insert gradient: -2.234804608331178e-05\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[9.58873966e-01 5.97449506e+01 6.47072315e+01 3.00294491e+01\n",
      " 1.06635780e+02 6.49753748e+01 9.74996299e+01 2.84984751e+01\n",
      " 6.70989301e+01 5.70588163e+01 1.66057605e+02 6.16524183e+01\n",
      " 7.05294043e+01 9.62819128e+00 1.10860793e+02 8.83146018e+01\n",
      " 1.20111504e+02 0.00000000e+00 2.48689958e-14 6.87450093e+01\n",
      " 1.38378812e+02 6.15358753e+01 8.85555820e+01 8.33285018e+01\n",
      " 1.01670000e+02 6.51410376e+01 3.38799030e+01]\n",
      "47-th iteration, loss: 0.07684811027167812, 17 gd steps\n",
      "insert gradient: -3.088721405145543e-05\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[  0.95882919  59.7481114   64.70899703  30.03176327 106.6367904\n",
      "  64.97266797  97.50298981  28.49729804  67.10663904  57.05486424\n",
      " 166.06233359  61.65022483  70.53484984   9.62599444 110.86862809\n",
      "  88.31193794 120.1176469   68.75069935 138.39088092  61.53266249\n",
      "  88.57105182  83.33737602 101.63131301  65.15621941  33.879903  ]\n",
      "48-th iteration, loss: 0.07684810345613527, 21 gd steps\n",
      "insert gradient: -1.5639826355916746e-05\n",
      "48-th iteration, new layer inserted. now 25 layers\n",
      "[  0.95967576  59.74822937  64.71056057  30.03384431 106.63598235\n",
      "  64.97508318  97.50398785  28.49997063  67.10607743  57.05444063\n",
      " 166.06738777  61.65014535  70.53667789   9.62708054 110.87020978\n",
      "  88.31388268 120.12150731  68.74938638 138.38938338  61.53745366\n",
      "  88.57291557  83.33997663 101.62898837  65.1550917   33.879903  ]\n",
      "49-th iteration, loss: 0.07684809890131664, 24 gd steps\n",
      "insert gradient: -1.2698920692740129e-05\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[  0.95939912  59.74771048  64.71071054  30.03440303 106.63604718\n",
      "  64.9768417   97.50500583  28.50239379  67.10718895  57.05706607\n",
      " 166.0718029   61.64398246  70.54084363   9.62891074 110.87190288\n",
      "  88.31468071 120.12576405  68.74935913 138.39225481  61.53922538\n",
      "  88.57712255  83.34286154 101.62235346  65.15702881  33.879903  ]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5314150056394724\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.38057759    0.         1716.41339226]\n",
      "1-th iteration, loss: 0.745144539899422, 11 gd steps\n",
      "insert gradient: -0.623171395081928\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.43634346   62.36427952  230.25057701    0.         1486.16281525]\n",
      "2-th iteration, loss: 0.5331164532124876, 28 gd steps\n",
      "insert gradient: -0.3796253170462278\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   1.50849397   51.85366328  186.85267996   52.14289297  333.6283871\n",
      "    0.         1152.53442815]\n",
      "3-th iteration, loss: 0.4599639557968863, 20 gd steps\n",
      "insert gradient: -0.4776395322549298\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   2.07560285   59.88475022  176.83388635   53.5643029   286.18418704\n",
      "   50.83719253  131.71822036    0.         1020.81620779]\n",
      "4-th iteration, loss: 0.37322776241026573, 39 gd steps\n",
      "insert gradient: -0.40208676866601356\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.9084009   50.26324338 188.84365642  50.78066233 256.64775807\n",
      "  60.33953694 102.08236294  69.27695155 642.73613083   0.\n",
      " 378.08007696]\n",
      "5-th iteration, loss: 0.2826809447068314, 21 gd steps\n",
      "insert gradient: -0.25885663081167326\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.69025273  44.13885791  99.43886781   0.          99.43886781\n",
      "  48.74235674 243.49733727  65.75330617 103.86714283  70.86621696\n",
      " 597.68346746  69.06350519 378.08007696]\n",
      "6-th iteration, loss: 0.27291504446968107, 17 gd steps\n",
      "insert gradient: -0.24007799394412413\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.29193726  50.41087015  87.61855102   8.01568186  87.57690602\n",
      "  44.40348413 241.6880283   66.84831734 104.97563106  68.22748219\n",
      " 597.12722779  66.73266169 168.03558976   0.         210.0444872 ]\n",
      "7-th iteration, loss: 0.22669991968255412, 72 gd steps\n",
      "insert gradient: -0.11624518678182572\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.26042475  53.17282574  91.30414522  14.81336809  66.46144665\n",
      "  50.44325446 224.82133872  70.18459802 105.70752701  68.5475202\n",
      " 149.69805857   0.         449.09417571  61.7635883  111.90839765\n",
      "  66.16236791 210.0444872 ]\n",
      "8-th iteration, loss: 0.20243118967116497, 42 gd steps\n",
      "insert gradient: -0.10662643894012826\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  2.83745882  56.17001206  97.46184721  14.70782654  65.12403517\n",
      "  53.49792558 218.53172939  72.77881352 111.04126968  64.06867173\n",
      " 128.61748471  37.75192644 360.5925769    0.          60.09876282\n",
      "  53.99226554 122.4056586   68.3529335  210.0444872 ]\n",
      "9-th iteration, loss: 0.1796328715644104, 36 gd steps\n",
      "insert gradient: -0.08198969725515634\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[7.42635430e-01 0.00000000e+00 6.93889390e-17 5.08233222e+01\n",
      " 9.40809641e+01 2.78939294e+01 6.28891104e+01 5.33860888e+01\n",
      " 2.05429338e+02 7.59896608e+01 1.09166033e+02 7.12830847e+01\n",
      " 1.18171785e+02 5.57586411e+01 3.02465844e+02 3.91421049e+01\n",
      " 3.18631962e+01 5.45288140e+01 1.42059490e+02 6.11368455e+01\n",
      " 2.10044487e+02]\n",
      "10-th iteration, loss: 0.17231692749624394, 404 gd steps\n",
      "insert gradient: -0.04213274655223322\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  3.07592537  55.82991955  90.72116522  30.34811111  71.42391935\n",
      "  51.19206823 194.05802504  77.82724433 114.74996372  65.87936768\n",
      " 127.08288506  56.23996989 285.69593344  53.95916143  30.92022335\n",
      "  51.07827642 136.01323131  58.13434902 175.03707267   0.\n",
      "  35.00741453]\n",
      "11-th iteration, loss: 0.16308640013589168, 25 gd steps\n",
      "insert gradient: -0.029375274247891915\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  2.56305374  54.98982186  93.3296279   33.2188503   64.14436346\n",
      "  53.33957591 188.82869444  80.68307966 115.5805172   68.69156747\n",
      " 119.8823123   60.73102582 278.85690412  57.9756951   25.05575877\n",
      "  51.26240992  41.2684976    0.         110.04932693  50.33200344\n",
      " 159.8397598   28.13683961  35.00741453]\n",
      "12-th iteration, loss: 0.15971840712318602, 20 gd steps\n",
      "insert gradient: -0.033370470380126444\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  2.56474471  56.31317818  90.25077509  31.22633893  74.12151043\n",
      "  52.06920942 188.03890555  78.64855581 117.78437201  67.59906989\n",
      " 123.97932307  58.51312426 199.61708718   0.          85.55018022\n",
      "  56.32341597  25.51262114  37.20813575  38.90405851   9.22126331\n",
      " 108.64592569  62.83340286 148.63631665  28.93098715  35.00741453]\n",
      "13-th iteration, loss: 0.15544235389680267, 31 gd steps\n",
      "insert gradient: -0.02536267159497679\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  3.7251992   54.59140455  90.17264061  35.72149626  69.27788462\n",
      "  52.46519582 184.55216527  82.83906168 116.68228708  70.69708944\n",
      " 119.36172839  61.23474725 174.41860703  12.39556769  73.47559115\n",
      "  56.03302884  29.97064469  22.4808329   53.68510035  13.72023486\n",
      " 118.33692213  69.51489223 135.25803591  39.322993    35.00741453]\n",
      "14-th iteration, loss: 0.15347054684414646, 22 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  0.50985847  54.72501184  88.09628883  34.3656028   76.63854654\n",
      "  51.16897637 184.07988334  84.0842773  115.594693    70.5385317\n",
      " 121.92290534  60.70565256 165.68490285  17.67812487  67.01563147\n",
      "  60.98068915  30.73590613  17.94444705  64.26488927   8.74487842\n",
      "  26.86583168   0.          94.03041089  70.5156614  134.57158278\n",
      "  39.87380529  35.00741453]\n",
      "15-th iteration, loss: 0.14859112368105123, 999 gd steps\n",
      "insert gradient: -0.023961276182461563\n",
      "15-th iteration, new layer inserted. now 26 layers\n",
      "[  2.4369122   55.14217471  88.11213644  36.58292699  74.43322387\n",
      "  51.30413981 186.03750121  87.99113209 112.23771017  73.54334601\n",
      " 117.74889748  63.8429378  151.30817037  30.90114561  44.3596361\n",
      "  61.64395721  26.784292    12.24323639 116.07099711   8.10447347\n",
      "  99.59728683  73.24276805 125.92439365  48.56600351  35.00741453\n",
      "   0.        ]\n",
      "16-th iteration, loss: 0.14123653120062282, 38 gd steps\n",
      "insert gradient: -0.0199935765131994\n",
      "16-th iteration, new layer inserted. now 28 layers\n",
      "[  2.61694956  54.64736224  89.81465024  39.17486096  73.10416497\n",
      "  48.27783239 190.56556007  88.3105616  111.86837111  73.32952945\n",
      " 119.10190797  65.63610274 142.49156839  38.96529783  24.45720704\n",
      "  70.76667208  26.06688073   5.81366108 125.00012631  15.55170905\n",
      "  36.59900906   0.          45.74876133  80.66648837 112.73427377\n",
      "  64.45196302  11.82859604   9.25254903]\n",
      "17-th iteration, loss: 0.13675722917819721, 20 gd steps\n",
      "insert gradient: -0.04655403459242172\n",
      "17-th iteration, new layer inserted. now 28 layers\n",
      "[  3.15225208  55.27879711  90.63515995  38.95569253  72.84644989\n",
      "  50.07818704  86.38685525   0.         107.98356906  86.9155698\n",
      " 112.70369812  70.40814888 118.41754829  68.40496801 137.80932072\n",
      "  42.96612991  15.17524753  72.46064076  25.01491311   3.2073671\n",
      " 126.65838297  24.076233    23.49032091   9.84890619  32.89585849\n",
      "  86.36582032 108.66388683  79.32562626]\n",
      "18-th iteration, loss: 0.12381139241340339, 65 gd steps\n",
      "insert gradient: -0.00834651083315562\n",
      "18-th iteration, new layer inserted. now 26 layers\n",
      "[  2.49370417  60.10208739  96.18717373  34.19345405  64.8659282\n",
      "  50.48352354  88.07036554  26.29339072  80.75973116  84.37387059\n",
      " 109.6621546   74.45407252 120.43166136  73.19225102 130.23905526\n",
      "  42.06038992  13.97535102  63.17999833 157.96657335  75.07638538\n",
      "   5.54684793  32.75685074   0.          65.51370148 122.62791129\n",
      "  78.27138613]\n",
      "19-th iteration, loss: 0.11781488678026085, 42 gd steps\n",
      "insert gradient: -0.01328916387502576\n",
      "19-th iteration, new layer inserted. now 24 layers\n",
      "[  3.04371577  61.61598826  94.76619271  32.89987775  68.67143739\n",
      "  50.21640333  88.66448359  33.55583794  74.45984208  86.64073539\n",
      " 109.03766061  74.63218383 121.39918074  77.45403843 121.72267112\n",
      " 121.34054081 153.80159434 102.38575871  23.79773812  53.15067628\n",
      "  45.11751069   0.         105.27419161  82.44519133]\n",
      "20-th iteration, loss: 0.11306674293837231, 23 gd steps\n",
      "insert gradient: -0.02148975655861229\n",
      "20-th iteration, new layer inserted. now 26 layers\n",
      "[3.10622362e+00 6.35838521e+01 9.12652221e+01 3.28380793e+01\n",
      " 7.55866217e+01 4.78038225e+01 8.88054253e+01 0.00000000e+00\n",
      " 1.06581410e-14 3.24631970e+01 8.34080189e+01 8.37469949e+01\n",
      " 1.10640809e+02 7.41960036e+01 1.23397557e+02 7.73383226e+01\n",
      " 1.27108975e+02 1.16121436e+02 1.53889513e+02 9.72074306e+01\n",
      " 5.87200328e+01 2.60378446e+01 4.15689365e+01 1.17611964e+01\n",
      " 1.00140730e+02 8.86650308e+01]\n",
      "21-th iteration, loss: 0.10504830579852541, 79 gd steps\n",
      "insert gradient: -0.013732899232980563\n",
      "21-th iteration, new layer inserted. now 26 layers\n",
      "[  1.29157568  64.24774989  95.36591505  39.89237875  72.81392871\n",
      "  47.5705588   84.03299595  38.50757638  81.83207663  88.71392857\n",
      " 106.51315209  80.25876925 119.54772571  78.30378104 128.52452486\n",
      " 119.24224129 152.4939822   90.92412613 104.82278295   6.52529567\n",
      "  49.68263796  25.01840006  48.22711701   0.          38.58169361\n",
      "  86.67688234]\n",
      "22-th iteration, loss: 0.09349804494477593, 34 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "22-th iteration, new layer inserted. now 26 layers\n",
      "[  0.57125398  62.49399912  97.11213694  44.26225151  73.3889735\n",
      "  43.89877331  82.92305371  41.25854346  84.42784373  89.67839644\n",
      " 103.28925123  80.95390107 118.18065753  79.33790327 128.20932465\n",
      " 122.91248328  16.36844268   0.         130.94754145  82.69376067\n",
      " 146.46541586  74.14528401  16.59747288  20.77980973  17.72806241\n",
      "  75.40425835]\n",
      "23-th iteration, loss: 0.06622201773573136, 124 gd steps\n",
      "insert gradient: -0.003637952375978736\n",
      "23-th iteration, new layer inserted. now 24 layers\n",
      "[  4.04385863  53.34237614  89.41318092  45.17119196  80.9843752\n",
      "  45.49429132  89.64837937  53.00973824  96.16653486  88.54939321\n",
      "  98.76401407  87.04264328 108.20601279  87.19703403 113.02169983\n",
      "  91.84478862 205.48806905 100.33788124 127.61317461  85.72426505\n",
      " 127.07405325   0.          54.46030854  32.42329996]\n",
      "24-th iteration, loss: 0.06315684503660071, 72 gd steps\n",
      "insert gradient: -0.002445906911320651\n",
      "24-th iteration, new layer inserted. now 26 layers\n",
      "[  1.81812688  54.01645379  89.05115876  43.25268073  78.81828732\n",
      "  47.16094377  89.02856068  53.59497009 122.43152235  74.60860064\n",
      " 101.49426686  86.88027408 107.09860388  89.10878177 109.10255493\n",
      "  95.98662373 133.06701757   0.          66.53350878 100.44707131\n",
      " 112.54081703  91.31272323 120.6472221    8.04744735  30.23614976\n",
      "  58.62295819]\n",
      "25-th iteration, loss: 0.06202710965552701, 48 gd steps\n",
      "insert gradient: -0.002327692188706382\n",
      "25-th iteration, new layer inserted. now 28 layers\n",
      "[  1.7455505   54.81926371  89.14486057  44.08581828  79.7114206\n",
      "  47.42609896  89.27625962  53.46025707 119.82027009  76.89508391\n",
      "  99.19896062  89.03877089 106.09130212  89.64720174 108.94815584\n",
      "  95.39723231 110.2863029   11.89264933  52.04169033  71.04911656\n",
      "   0.          35.52455828 111.4780913   92.93516114 122.74300516\n",
      "   2.97687874  30.73378265  69.53363703]\n",
      "26-th iteration, loss: 0.061210002315914835, 19 gd steps\n",
      "insert gradient: -0.0046312427849495115\n",
      "26-th iteration, new layer inserted. now 28 layers\n",
      "[2.06300236e+00 5.27607861e+01 8.81107707e+01 4.57949956e+01\n",
      " 8.33968289e+01 0.00000000e+00 7.10542736e-15 4.64955942e+01\n",
      " 9.00109466e+01 5.15502997e+01 1.19369937e+02 7.84136959e+01\n",
      " 9.69785450e+01 9.02600064e+01 1.05031480e+02 9.02339358e+01\n",
      " 1.09273560e+02 9.44614034e+01 1.11038455e+02 1.65420680e+01\n",
      " 3.93149415e+01 6.21985318e+01 1.52051360e+01 3.09675981e+01\n",
      " 1.16831106e+02 9.31914858e+01 1.58493456e+02 7.74269398e+01]\n",
      "27-th iteration, loss: 0.060467882207462235, 25 gd steps\n",
      "insert gradient: -0.0017579171610706155\n",
      "27-th iteration, new layer inserted. now 26 layers\n",
      "[  1.88132661  53.8911026   88.65539343  45.1038157   82.74552013\n",
      "  48.48026489  90.28552197  52.46746164 118.74510557  79.29579965\n",
      "  97.60890771  91.13463198 104.86209915  90.75349539 108.76742288\n",
      "  95.2371188  110.94058143  17.79860846  37.53049062  63.0990733\n",
      "  17.52248789  29.99215053 117.90076221  93.46656706 158.18167587\n",
      "  79.77130219]\n",
      "28-th iteration, loss: 0.057541768998725124, 266 gd steps\n",
      "insert gradient: -0.00025381015430443835\n",
      "28-th iteration, new layer inserted. now 26 layers\n",
      "[  1.99272138  53.03301273  90.21502262  47.80031347  88.85876181\n",
      "  49.80844415  91.37996206  51.28205907  99.4253777   30.96253646\n",
      "   0.          61.92507292  93.43482934  95.82165182  99.80044587\n",
      "  93.99459929 104.13534036  95.23844055 104.08488357  37.97762776\n",
      "   0.25227006  62.01297445 196.87070821 100.86968074 160.42806817\n",
      "  92.71380158]\n",
      "29-th iteration, loss: 0.05738448063153792, 418 gd steps\n",
      "insert gradient: -0.00020282491796198824\n",
      "29-th iteration, new layer inserted. now 26 layers\n",
      "[1.83353521e+00 5.41581364e+01 9.00921933e+01 4.68482676e+01\n",
      " 8.73987980e+01 4.98997324e+01 9.29446757e+01 5.18359241e+01\n",
      " 9.19438245e+01 2.93845323e+01 0.00000000e+00 6.21724894e-15\n",
      " 4.72331946e+00 6.12824536e+01 9.30620500e+01 9.49093912e+01\n",
      " 1.00434182e+02 9.29278995e+01 1.05144080e+02 9.40293611e+01\n",
      " 1.06035742e+02 9.91757069e+01 1.95185526e+02 1.00579565e+02\n",
      " 1.59646884e+02 9.19715907e+01]\n",
      "30-th iteration, loss: 0.05733146410822539, 310 gd steps\n",
      "insert gradient: -3.664653571801678e-05\n",
      "30-th iteration, new layer inserted. now 26 layers\n",
      "[1.83274559e+00 5.46149582e+01 8.97713161e+01 4.62594552e+01\n",
      " 8.65697802e+01 5.00896311e+01 9.38288597e+01 5.18654951e+01\n",
      " 9.02195331e+01 2.69133603e+01 0.00000000e+00 4.44089210e-15\n",
      " 1.07500826e+01 5.86571953e+01 9.36034561e+01 9.40406092e+01\n",
      " 1.01342574e+02 9.21745910e+01 1.05964286e+02 9.31736948e+01\n",
      " 1.07563035e+02 9.83980685e+01 1.93653454e+02 1.00713242e+02\n",
      " 1.58780597e+02 9.18901431e+01]\n",
      "31-th iteration, loss: 0.05732885316958401, 71 gd steps\n",
      "insert gradient: -2.51631338219902e-05\n",
      "31-th iteration, new layer inserted. now 24 layers\n",
      "[  1.86013486  54.64880559  89.70973964  46.17702217  86.44022972\n",
      "  50.12858617  93.98202159  51.82679397  90.06106488  26.52303626\n",
      "  11.73103937  58.17957748  93.73403709  93.88749813 101.5046835\n",
      "  92.04966209 106.09955381  93.03069402 107.83771563  98.28029586\n",
      " 193.32624081 100.78766022 158.58352246  91.90744277]\n",
      "32-th iteration, loss: 0.057328551734538595, 22 gd steps\n",
      "insert gradient: -6.315002804519788e-05\n",
      "32-th iteration, new layer inserted. now 26 layers\n",
      "[1.86740917e+00 5.46714367e+01 8.97027636e+01 4.61376067e+01\n",
      " 8.64011122e+01 5.01482191e+01 9.40036463e+01 5.18181904e+01\n",
      " 9.00374781e+01 2.64035370e+01 0.00000000e+00 7.99360578e-15\n",
      " 1.19749423e+01 5.80664119e+01 9.37738526e+01 9.38468353e+01\n",
      " 1.01541768e+02 9.20233465e+01 1.06129086e+02 9.29981039e+01\n",
      " 1.07894114e+02 9.82580028e+01 1.93223193e+02 1.00816324e+02\n",
      " 1.58520538e+02 9.19185411e+01]\n",
      "33-th iteration, loss: 0.057328386107527426, 24 gd steps\n",
      "insert gradient: -2.1033668323299825e-05\n",
      "33-th iteration, new layer inserted. now 26 layers\n",
      "[1.87311245e+00 5.46525593e+01 8.96819102e+01 4.61466683e+01\n",
      " 8.63933326e+01 0.00000000e+00 3.55271368e-15 5.01385642e+01\n",
      " 9.40450152e+01 5.18028335e+01 9.00119283e+01 2.63634683e+01\n",
      " 1.20911706e+01 5.80236600e+01 9.37715116e+01 9.38307071e+01\n",
      " 1.01570345e+02 9.19999535e+01 1.06153079e+02 9.29740994e+01\n",
      " 1.07930156e+02 9.82452725e+01 1.93190656e+02 1.00815118e+02\n",
      " 1.58505687e+02 9.19077932e+01]\n",
      "34-th iteration, loss: 0.057328226283220624, 338 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.142191714638072e-06\n",
      "34-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918102  54.65657098  89.66787147  46.11285311  86.36147719\n",
      "  50.15756948  94.08236742  51.78099538  89.96709902  26.27109908\n",
      "  12.30718181  57.9263722   93.79248128  93.78837283 101.61105505\n",
      "  91.96349045 106.19894036  92.9312219  108.00741372  98.21254766\n",
      " 193.10889282 100.82536383 158.45333134  91.90225027]\n",
      "35-th iteration, loss: 0.05732822628068829, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.141496850698468e-06\n",
      "35-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918141e+00 5.46565710e+01 8.96678710e+01 4.61128524e+01\n",
      " 8.63614763e+01 5.01575696e+01 9.40823685e+01 5.17809949e+01\n",
      " 8.99670978e+01 2.62710965e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23071880e+01 5.79263694e+01 9.37924818e+01 9.37883718e+01\n",
      " 1.01611056e+02 9.19634895e+01 1.06198941e+02 9.29312208e+01\n",
      " 1.08007416e+02 9.82125468e+01 1.93108891e+02 1.00825364e+02\n",
      " 1.58453330e+02 9.19022502e+01]\n",
      "36-th iteration, loss: 0.057328226276842836, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.1240761051945385e-06\n",
      "36-th iteration, new layer inserted. now 24 layers\n",
      "[  1.8891818   54.65657107  89.66787059  46.11285171  86.36147547\n",
      "  50.15756974  94.08236957  51.78099433  89.96709666  26.271094\n",
      "  12.30720023  57.92636662  93.79248242  93.78837071 101.61105734\n",
      "  91.96348853 106.1989426   92.93121977 108.00741751  98.21254594\n",
      " 193.10888877 100.82536437 158.45332877  91.90225016]\n",
      "37-th iteration, loss: 0.057328226274311146, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.123773087511817e-06\n",
      "37-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918219e+00 5.46565711e+01 8.96678702e+01 4.61128510e+01\n",
      " 8.63614746e+01 5.01575699e+01 9.40823707e+01 5.17809938e+01\n",
      " 8.99670955e+01 2.62710914e+01 0.00000000e+00 7.99360578e-15\n",
      " 1.23072064e+01 5.79263638e+01 9.37924830e+01 9.37883696e+01\n",
      " 1.01611058e+02 9.19634876e+01 1.06198944e+02 9.29312187e+01\n",
      " 1.08007419e+02 9.82125451e+01 1.93108887e+02 1.00825365e+02\n",
      " 1.58453327e+02 9.19022501e+01]\n",
      "38-th iteration, loss: 0.05732822627047375, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.106778763054302e-06\n",
      "38-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918258  54.65657118  89.66786973  46.11285034  86.36147377\n",
      "  50.15757002  94.08237173  51.78099328  89.96709429  26.27108888\n",
      "  12.3072186   57.92636101  93.79248354  93.78836856 101.61105962\n",
      "  91.96348659 106.19894482  92.93121763 108.0074213   98.21254422\n",
      " 193.10888473 100.82536492 158.4533262   91.90225006]\n",
      "39-th iteration, loss: 0.057328226267942525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.1068411522978845e-06\n",
      "39-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918297e+00 5.46565712e+01 8.96678693e+01 4.61128497e+01\n",
      " 8.63614729e+01 5.01575702e+01 9.40823728e+01 5.17809928e+01\n",
      " 8.99670931e+01 2.62710863e+01 0.00000000e+00 4.44089210e-15\n",
      " 1.23072247e+01 5.79263582e+01 9.37924841e+01 9.37883675e+01\n",
      " 1.01611061e+02 9.19634856e+01 1.06198946e+02 9.29312166e+01\n",
      " 1.08007423e+02 9.82125434e+01 1.93108883e+02 1.00825365e+02\n",
      " 1.58453325e+02 9.19022500e+01]\n",
      "40-th iteration, loss: 0.057328226264112665, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.090245685453208e-06\n",
      "40-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918336  54.65657131  89.66786889  46.11284901  86.36147209\n",
      "  50.15757034  94.08237391  51.78099223  89.96709191  26.27108372\n",
      "  12.30723692  57.92635536  93.79248465  93.78836638 101.61106188\n",
      "  91.96348463 106.19894704  92.93121548 108.00742508  98.2125425\n",
      " 193.10888068 100.82536546 158.45332363  91.90224996]\n",
      "41-th iteration, loss: 0.057328226261581725, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.090649035168494e-06\n",
      "41-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918376  54.65657139  89.66786848  46.11284836  86.36147126\n",
      "  50.15757052  94.08237501  51.78099171  89.96709072  26.27108112\n",
      "  12.30724301  57.92635252  93.7924852   93.78836528 101.61106301\n",
      "  91.96348365 106.19894815  92.9312144  108.00742698  98.21254164\n",
      " 193.10887866 100.82536574 158.45332235  91.90224992]\n",
      "42-th iteration, loss: 0.057328226259050964, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.091042749480407e-06\n",
      "42-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918416e+00 5.46565715e+01 8.96678681e+01 4.61128477e+01\n",
      " 8.63614704e+01 5.01575707e+01 9.40823761e+01 5.17809912e+01\n",
      " 8.99670895e+01 2.62710785e+01 0.00000000e+00 4.44089210e-15\n",
      " 1.23072491e+01 5.79263497e+01 9.37924857e+01 9.37883642e+01\n",
      " 1.01611064e+02 9.19634827e+01 1.06198949e+02 9.29312133e+01\n",
      " 1.08007429e+02 9.82125408e+01 1.93108877e+02 1.00825366e+02\n",
      " 1.58453321e+02 9.19022499e+01]\n",
      "43-th iteration, loss: 0.057328226255228217, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.074810226962712e-06\n",
      "43-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918455e+00 5.46565716e+01 8.96678677e+01 4.61128471e+01\n",
      " 8.63614696e+01 5.01575709e+01 9.40823772e+01 5.17809907e+01\n",
      " 8.99670883e+01 2.62710759e+01 0.00000000e+00 2.66453526e-15\n",
      " 1.23072613e+01 5.79263468e+01 9.37924863e+01 9.37883631e+01\n",
      " 1.01611065e+02 9.19634817e+01 1.06198950e+02 9.29312123e+01\n",
      " 1.08007431e+02 9.82125399e+01 1.93108875e+02 1.00825366e+02\n",
      " 1.58453320e+02 9.19022498e+01]\n",
      "44-th iteration, loss: 0.05732822625141223, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.058949546164022e-06\n",
      "44-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918495e+00 5.46565716e+01 8.96678673e+01 4.61128464e+01\n",
      " 8.63614688e+01 5.01575711e+01 9.40823783e+01 5.17809902e+01\n",
      " 8.99670871e+01 2.62710733e+01 0.00000000e+00 7.99360578e-15\n",
      " 1.23072734e+01 5.79263440e+01 9.37924868e+01 9.37883620e+01\n",
      " 1.01611066e+02 9.19634807e+01 1.06198951e+02 9.29312112e+01\n",
      " 1.08007433e+02 9.82125391e+01 1.93108873e+02 1.00825367e+02\n",
      " 1.58453319e+02 9.19022498e+01]\n",
      "45-th iteration, loss: 0.057328226247602684, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.043448313976699e-06\n",
      "45-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918535  54.65657175  89.66786685  46.11284581  86.36146796\n",
      "  50.15757125  94.0823794   51.78098964  89.96708593  26.27107067\n",
      "  12.30728554  57.92634112  93.79248738  93.78836083 101.6110675\n",
      "  91.96347968 106.19895257  92.93121009 108.00743453  98.2125382\n",
      " 193.10887058 100.82536684 158.45331722  91.90224973]\n",
      "46-th iteration, loss: 0.057328226245071674, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.044783752167936e-06\n",
      "46-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918576e+00 5.46565719e+01 8.96678665e+01 4.61128452e+01\n",
      " 8.63614672e+01 5.01575715e+01 9.40823805e+01 5.17809891e+01\n",
      " 8.99670847e+01 2.62710680e+01 0.00000000e+00 6.21724894e-15\n",
      " 1.23072916e+01 5.79263382e+01 9.37924879e+01 9.37883597e+01\n",
      " 1.01611069e+02 9.19634787e+01 1.06198954e+02 9.29312090e+01\n",
      " 1.08007436e+02 9.82125373e+01 1.93108869e+02 1.00825367e+02\n",
      " 1.58453316e+02 9.19022497e+01]\n",
      "47-th iteration, loss: 0.05732822624126801, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.029587091604634e-06\n",
      "47-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918616  54.65657198  89.66786608  46.11284459  86.36146635\n",
      "  50.15757168  94.08238162  51.78098861  89.96708352  26.27106536\n",
      "  12.30730367  57.92633535  93.79248844  93.78835856 101.61106973\n",
      "  91.96347767 106.19895476  92.93120792 108.00743831  98.21253647\n",
      " 193.10886654 100.8253674  158.45331467  91.90224965]\n",
      "48-th iteration, loss: 0.05732822623873692, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.0311799959031424e-06\n",
      "48-th iteration, new layer inserted. now 24 layers\n",
      "[  1.88918656  54.6565721   89.66786569  46.112844    86.36146555\n",
      "  50.1575719   94.08238273  51.7809881   89.96708231  26.27106269\n",
      "  12.3073097   57.92633245  93.79248897  93.78835741 101.61107084\n",
      "  91.96347666 106.19895586  92.93120684 108.00744019  98.21253561\n",
      " 193.10886452 100.82536768 158.45331339  91.90224961]\n",
      "49-th iteration, loss: 0.057328226236206106, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.0327255429523085e-06\n",
      "49-th iteration, new layer inserted. now 26 layers\n",
      "[1.88918697e+00 5.46565722e+01 8.96678653e+01 4.61128434e+01\n",
      " 8.63614647e+01 5.01575721e+01 9.40823838e+01 5.17809876e+01\n",
      " 8.99670811e+01 2.62710600e+01 0.00000000e+00 4.44089210e-15\n",
      " 1.23073157e+01 5.79263296e+01 9.37924895e+01 9.37883563e+01\n",
      " 1.01611072e+02 9.19634756e+01 1.06198957e+02 9.29312058e+01\n",
      " 1.08007442e+02 9.82125347e+01 1.93108863e+02 1.00825368e+02\n",
      " 1.58453312e+02 9.19022496e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5343417449201606\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.98601441    0.         1740.93358358]\n",
      "1-th iteration, loss: 0.7463784459565692, 11 gd steps\n",
      "insert gradient: -0.628974348399279\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.8735978    62.36802774  233.53987097    0.         1507.39371261]\n",
      "2-th iteration, loss: 0.6047057671511766, 13 gd steps\n",
      "insert gradient: -0.6505254560114445\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.74058989   77.74289476  219.22890614   41.68050473  246.10509594\n",
      "    0.         1261.28861668]\n",
      "3-th iteration, loss: 0.4353639815583051, 30 gd steps\n",
      "insert gradient: -0.5640978935828983\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  2.39112435  42.31498627 215.74680962  59.42147759 131.58027731\n",
      "  61.36420618 324.33135857   0.         936.9572581 ]\n",
      "4-th iteration, loss: 0.3306826346898562, 46 gd steps\n",
      "insert gradient: -0.24194646384821492\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.77960056  48.57287234  97.22849841   0.         104.70761367\n",
      "  55.73593788 123.81112014  61.24641267 285.75430312  57.17312462\n",
      " 936.9572581 ]\n",
      "5-th iteration, loss: 0.32253745144753165, 22 gd steps\n",
      "insert gradient: -0.23647147354780612\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.25194461  52.77968412  80.81639928   9.77252198  93.07088943\n",
      "  58.51584907 122.57963462  63.27938448 277.54735651  63.54214066\n",
      " 553.65656161   0.         383.3006965 ]\n",
      "6-th iteration, loss: 0.29164036440318547, 58 gd steps\n",
      "insert gradient: -0.20380204790651155\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.59558145  54.28885293  84.35138023   8.27825669  85.24422684\n",
      "  59.39946486 133.38368626  62.2866148  135.38052433   0.\n",
      " 135.38052433  54.62974647 530.90360831  52.90592798 383.3006965 ]\n",
      "7-th iteration, loss: 0.25833014891543377, 18 gd steps\n",
      "insert gradient: -0.21011527168685035\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  4.31861122  58.40988843  84.23373824   1.6102164   88.23289991\n",
      "  61.79144543 146.45147759  56.1684169  115.92491153  23.76896105\n",
      " 111.82881781  47.50018341 200.89299235   0.         334.82165392\n",
      "  52.38715741 383.3006965 ]\n",
      "8-th iteration, loss: 0.19096814523668526, 58 gd steps\n",
      "insert gradient: -0.08807102244930641\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  3.03684527  67.2583715  168.54928599  76.00801108 142.31466539\n",
      "  62.81667478  98.21323026  44.25513223  81.53153159  45.65977106\n",
      " 203.96220585  25.49125953 330.12284238  41.64324596 119.78146766\n",
      "   0.         263.51922884]\n",
      "9-th iteration, loss: 0.17771524195048016, 19 gd steps\n",
      "insert gradient: -0.025461577017350055\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  1.73648542  66.53128772 162.44440644  83.94852464 128.58223929\n",
      "  65.12453582  99.24042293  47.34664668  71.45535834  46.25375984\n",
      " 200.40038758  36.36274921 314.62911888  41.93046361 112.66886849\n",
      "  17.52172978 112.93681236   0.         150.58241648]\n",
      "10-th iteration, loss: 0.17540750719334142, 22 gd steps\n",
      "insert gradient: -0.014563768986462633\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  0.79954075  64.13711582 162.37953162  84.4556844  127.91411439\n",
      "  66.08861832  99.91977971  46.9069126   70.77259945  46.07928139\n",
      " 196.03303533  42.40998567 305.6744918   45.23929016 110.28535826\n",
      "  18.22278245 111.27189016   5.9020875   87.83974295   0.\n",
      "  62.74267353]\n",
      "11-th iteration, loss: 0.1710878477158133, 31 gd steps\n",
      "insert gradient: -0.03427050847933142\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  0.85806506  63.42308025 155.1850663   86.4118784  129.36259498\n",
      "  63.12521234  98.72278992  47.933967    68.96075573  45.51391125\n",
      " 190.94433605  46.6951475  296.41648762  53.36162375 106.57522127\n",
      "  11.94559994 102.16797589  16.62456785  83.79529112  19.74203049\n",
      "  62.74267353]\n",
      "12-th iteration, loss: 0.16855716230493387, 37 gd steps\n",
      "insert gradient: -0.028347562915935494\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  2.30250465  62.19386413 144.43725687  89.83510273 133.94907769\n",
      "  62.29053317  97.27623347  47.32223123  69.21660494  43.62040623\n",
      " 188.35869723  49.95735743 288.33438521  59.60714566 104.42209534\n",
      "   9.42664157  94.09055145  21.58432247  72.36100903  28.63431615\n",
      "  62.74267353]\n",
      "13-th iteration, loss: 0.16662547323330346, 28 gd steps\n",
      "insert gradient: -0.012412731505546476\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[1.09490883e+00 6.11272374e+01 1.38442270e+02 0.00000000e+00\n",
      " 3.19744231e-14 9.13965212e+01 1.35290077e+02 6.47105530e+01\n",
      " 9.76859282e+01 4.58642711e+01 6.68435470e+01 4.60100000e+01\n",
      " 1.87071834e+02 4.77544347e+01 2.85514797e+02 6.15420097e+01\n",
      " 1.03566552e+02 9.88411794e+00 9.09791352e+01 2.29161608e+01\n",
      " 6.74303840e+01 3.23145171e+01 6.27426735e+01]\n",
      "14-th iteration, loss: 0.16528109892178836, 16 gd steps\n",
      "insert gradient: -0.018392467080850624\n",
      "14-th iteration, new layer inserted. now 21 layers\n",
      "[  2.05961666  59.72310436 129.53659902  93.88157434 137.93250949\n",
      "  66.24582663  97.63735633  46.01074526  66.8091193   45.11237869\n",
      " 186.66250746  48.16389049 283.63158338  62.32255418 101.61304731\n",
      "  11.19077214  85.14617393  25.83015182  60.40536622  37.35015694\n",
      "  62.74267353]\n",
      "15-th iteration, loss: 0.16406939347374566, 17 gd steps\n",
      "insert gradient: -0.021351446493416717\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  2.71151102  62.34372801 121.1628674   96.36272882 134.66708555\n",
      "  69.31035051  98.74231588  45.7466298   63.88986602  46.20970318\n",
      " 118.05135302   0.          67.45791601  48.77015423 281.22141346\n",
      "  62.86418804  99.71342186  12.82253965  77.28018699  30.12721707\n",
      "  53.73115393  42.08673407  62.74267353]\n",
      "16-th iteration, loss: 0.16083914139992841, 16 gd steps\n",
      "insert gradient: -0.03603157942943834\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[  4.33582726  59.58909194 116.84695765  98.89329816 130.1230053\n",
      "  74.85116698 102.00695992  41.90627077  70.27956981  50.28367969\n",
      "  89.65085911  14.43297043  47.17268774  53.12118903 278.25596435\n",
      "  64.59637213  97.30285535  18.79759954  66.32717023  35.84955483\n",
      "  44.71574372  43.66774668  62.74267353]\n",
      "17-th iteration, loss: 0.15554921281678627, 25 gd steps\n",
      "insert gradient: -0.04074591014700108\n",
      "17-th iteration, new layer inserted. now 25 layers\n",
      "[  3.30183884  64.10691251 119.56823459  97.68040662 133.02026342\n",
      "  71.52478805 104.20388901  48.93756483  64.95394436  49.01549486\n",
      "  94.96734349  20.64847959  36.34310717  52.40928733 107.44922689\n",
      "   0.         161.17384034  69.66708299  98.32793589  22.52094028\n",
      "  62.34931409  37.85464973  40.10516564  43.1778026   62.74267353]\n",
      "18-th iteration, loss: 0.13825124727157495, 26 gd steps\n",
      "insert gradient: -0.006385152199744773\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[2.84105044e+00 6.47715619e+01 1.23636117e+02 9.26610429e+01\n",
      " 1.31961774e+02 7.29189882e+01 1.04462971e+02 5.06896891e+01\n",
      " 6.82572045e+01 0.00000000e+00 1.59872116e-14 4.51846681e+01\n",
      " 1.00449415e+02 5.09466808e+01 3.29627126e-01 5.93261435e+01\n",
      " 8.49804004e+01 1.07465551e+01 1.22585103e+02 7.95505451e+01\n",
      " 1.02242915e+02 2.83452826e+01 6.02008313e+01 4.80411577e+01\n",
      " 2.52464813e+01 3.49313834e+01 6.27426735e+01]\n",
      "19-th iteration, loss: 0.13221518465428184, 27 gd steps\n",
      "insert gradient: -0.0045874803068593276\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[  2.78851768  67.41225591 123.83111997  95.04412363 132.59426686\n",
      "  71.83998662 105.4877168   54.01603847  71.24470915  44.6255886\n",
      "  96.08991323 114.25570588  92.3767639   24.7464864   72.00028589\n",
      "  60.56341999   0.          25.95575142 109.48417223  32.53770905\n",
      "  47.73503221  55.87869019  24.63622001  29.02582093  62.74267353]\n",
      "20-th iteration, loss: 0.1297829693604482, 72 gd steps\n",
      "insert gradient: -0.010714427014790287\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[  2.77828192  68.17422793 121.800541    99.10946282 129.00647882\n",
      "  72.39126078 108.33580374  53.89648507  72.83040423  44.9581565\n",
      "  95.55189313  62.43663351   0.          49.94930681  90.06523435\n",
      "  40.37865052  53.2231673   58.76059893  16.39955929  17.62670795\n",
      " 104.88926551  43.94042944  21.94712493  55.4376445   41.96716628\n",
      "  28.03407715  62.74267353]\n",
      "21-th iteration, loss: 0.12646955077621314, 29 gd steps\n",
      "insert gradient: -0.00750219080982106\n",
      "21-th iteration, new layer inserted. now 29 layers\n",
      "[2.40221702e+00 7.00974879e+01 1.24001978e+02 0.00000000e+00\n",
      " 1.06581410e-14 1.01993806e+02 1.29040748e+02 7.23998533e+01\n",
      " 1.10544155e+02 5.82356005e+01 7.39314313e+01 4.42249370e+01\n",
      " 9.70859841e+01 6.30384482e+01 7.62652330e+00 4.42429107e+01\n",
      " 9.44800326e+01 4.58980363e+01 5.53821497e+01 5.18553890e+01\n",
      " 3.89323275e+01 9.69523014e+00 1.06195667e+02 4.84978239e+01\n",
      " 1.98253773e+01 4.92397001e+01 4.78328930e+01 2.85720925e+01\n",
      " 6.27426735e+01]\n",
      "22-th iteration, loss: 0.11543837686089868, 43 gd steps\n",
      "insert gradient: -0.01698215842357826\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[1.54228383e+00 8.10488151e+01 1.29743910e+02 1.07833109e+02\n",
      " 1.24168646e+02 6.96138313e+01 1.20602729e+02 6.51610262e+01\n",
      " 8.81516596e+01 4.39996942e+01 9.16244045e+01 0.00000000e+00\n",
      " 3.55271368e-15 6.10087905e+01 4.86182657e+01 2.17856968e+01\n",
      " 9.67709636e+01 5.55272926e+01 6.88056213e+01 3.23499415e+01\n",
      " 1.66796211e+02 5.61775993e+01 3.06440861e+01 3.99383918e+01\n",
      " 6.03792158e+01 3.30774481e+01 6.27426735e+01]\n",
      "23-th iteration, loss: 0.11202114369603615, 15 gd steps\n",
      "insert gradient: -0.03835960033322204\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[  1.15663384  83.49227154 133.95509724 107.45527169 125.08010695\n",
      "  65.52050237 124.10451886  66.74750343  95.42950072  44.86410552\n",
      "  88.36075801  60.29664899  69.23293574  14.85037912  93.96260696\n",
      "  56.4860872   76.79007003  26.4189634   91.34478671   0.\n",
      "  73.07582937  58.33325588  41.52090509  36.21751977  59.20068108\n",
      "  36.30944119  62.74267353]\n",
      "24-th iteration, loss: 0.10431652595426062, 31 gd steps\n",
      "insert gradient: -0.013056589876477337\n",
      "24-th iteration, new layer inserted. now 29 layers\n",
      "[2.51519063e-01 7.99053702e+01 1.36353358e+02 1.10503642e+02\n",
      " 1.26433301e+02 6.95295917e+01 1.25086108e+02 6.80816992e+01\n",
      " 9.87967561e+01 4.66319286e+01 8.53324509e+01 5.76963986e+01\n",
      " 8.61333968e+01 1.48524991e+01 8.60597784e+01 5.59355608e+01\n",
      " 8.80709904e+01 2.93955763e+01 8.19254253e+01 0.00000000e+00\n",
      " 1.06581410e-14 6.73394974e+00 6.07664483e+01 5.43387695e+01\n",
      " 5.55974601e+01 3.52341444e+01 5.37846975e+01 3.95877590e+01\n",
      " 6.27426735e+01]\n",
      "25-th iteration, loss: 0.053417805493001916, 164 gd steps\n",
      "insert gradient: -0.0005225473415750415\n",
      "25-th iteration, new layer inserted. now 29 layers\n",
      "[1.78069036e+00 7.91344319e+01 1.42736484e+02 8.68430568e+01\n",
      " 1.80962862e+02 7.87568737e+01 1.20520839e+02 7.62457267e+01\n",
      " 1.32170309e+02 7.37499306e+01 9.97150800e+01 4.75539904e+01\n",
      " 9.75056768e+01 6.11267672e+01 1.03969807e+02 4.61955082e+01\n",
      " 5.80358307e+01 4.28767544e+01 8.59119055e+01 4.35473207e+01\n",
      " 6.95554314e+01 0.00000000e+00 1.06581410e-14 2.42878340e+01\n",
      " 5.23770827e+01 3.47915505e+01 9.81353167e+01 5.44679864e+01\n",
      " 6.27426735e+01]\n",
      "26-th iteration, loss: 0.05256828102580757, 28 gd steps\n",
      "insert gradient: -0.00046757302066353213\n",
      "26-th iteration, new layer inserted. now 27 layers\n",
      "[  1.60181259  83.14031439 137.12059115  86.07813998 182.58780377\n",
      "  75.48732368 125.4843798   77.41342424 128.62272926  72.72169535\n",
      " 102.57504007  47.20204859  95.91779824  63.25893839 100.83521925\n",
      "  46.43288194  66.80178427  37.40503783  90.07727018  44.67255653\n",
      "  67.42150765  34.63558312  40.27251966  32.24283581  92.9610501\n",
      "  56.64579511  62.74267353]\n",
      "27-th iteration, loss: 0.05187049479321047, 42 gd steps\n",
      "insert gradient: -0.00026614894130985224\n",
      "27-th iteration, new layer inserted. now 29 layers\n",
      "[7.52194468e-01 8.45867551e+01 1.29645479e+02 8.63002545e+01\n",
      " 1.78170140e+02 7.77442895e+01 1.27410185e+02 7.86715327e+01\n",
      " 1.24146582e+02 7.16513194e+01 1.02862197e+02 4.65586033e+01\n",
      " 9.55883800e+01 6.35878413e+01 1.02904943e+02 4.67808043e+01\n",
      " 6.57301862e+01 3.86051247e+01 8.80810189e+01 4.33735724e+01\n",
      " 7.21214817e+01 4.57966739e+01 0.00000000e+00 8.88178420e-15\n",
      " 4.03737473e+01 2.19227197e+01 9.06767491e+01 5.37548035e+01\n",
      " 6.27426735e+01]\n",
      "28-th iteration, loss: 0.05130520286607709, 140 gd steps\n",
      "insert gradient: -0.00033482986846358417\n",
      "28-th iteration, new layer inserted. now 29 layers\n",
      "[9.87440190e-01 8.42592965e+01 1.27432180e+02 8.70452956e+01\n",
      " 1.73444525e+02 7.85158552e+01 1.29813993e+02 7.87894969e+01\n",
      " 1.21241876e+02 7.03717195e+01 1.04144462e+02 4.62789983e+01\n",
      " 9.46157395e+01 6.40463160e+01 1.04096667e+02 4.69151375e+01\n",
      " 6.72746192e+01 3.78122959e+01 8.72402238e+01 4.31117524e+01\n",
      " 7.47370296e+01 4.52852427e+01 0.00000000e+00 1.77635684e-15\n",
      " 6.15986687e+01 1.16294301e+01 8.99254033e+01 5.26513849e+01\n",
      " 6.27426735e+01]\n",
      "29-th iteration, loss: 0.05116740753197049, 117 gd steps\n",
      "insert gradient: -0.00019227896650163127\n",
      "29-th iteration, new layer inserted. now 29 layers\n",
      "[1.06382087e+00 8.39802597e+01 1.27118061e+02 8.75762192e+01\n",
      " 1.71435160e+02 7.89916786e+01 1.30367745e+02 7.86169588e+01\n",
      " 1.20573628e+02 7.00210161e+01 1.04654382e+02 4.61435378e+01\n",
      " 9.42870000e+01 6.42058044e+01 1.04436008e+02 4.70270390e+01\n",
      " 6.74905529e+01 3.77037729e+01 8.69486162e+01 4.30761305e+01\n",
      " 7.50642863e+01 4.44411196e+01 0.00000000e+00 5.32907052e-15\n",
      " 7.07903112e+01 9.05380159e+00 8.84221075e+01 5.17731265e+01\n",
      " 6.27426735e+01]\n",
      "30-th iteration, loss: 0.05112329893592722, 93 gd steps\n",
      "insert gradient: -9.929154504082722e-05\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[  1.14375659  83.83203446 127.01073306  87.81708983 170.41942273\n",
      "  79.29075116 130.58163359  78.51936791 120.3195578   69.8492219\n",
      " 104.80500176  46.10026377  94.17967401  64.25262551 104.63169455\n",
      "  47.05283185  67.60366127  37.63958888  86.82722244  43.03878465\n",
      "  75.30753837  43.85708873  75.931285     8.00696777  86.75864676\n",
      "  51.53658262  62.74267353]\n",
      "31-th iteration, loss: 0.051092048057315875, 25 gd steps\n",
      "insert gradient: -0.0002621767183802635\n",
      "31-th iteration, new layer inserted. now 27 layers\n",
      "[  1.37223319  83.70615041 126.97176123  88.12208047 169.26661849\n",
      "  79.52295534 130.91559504  78.33327527 119.99464085  69.6312662\n",
      " 105.02821869  45.93575202  94.10057547  64.55247826 104.86398768\n",
      "  46.86556914  68.04337992  37.53017686  86.76790969  42.80140046\n",
      "  76.15787157  43.30987922  83.74270609   7.73202395  79.50516301\n",
      "  52.10321216  62.74267353]\n",
      "32-th iteration, loss: 0.05109045454601541, 128 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.727363455745138e-07\n",
      "32-th iteration, new layer inserted. now 29 layers\n",
      "[1.27118290e+00 8.36763084e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704663e+01 1.30899568e+02 7.83760754e+01\n",
      " 1.19914170e+02 6.96842360e+01 1.05007009e+02 4.59332841e+01\n",
      " 9.40574880e+01 6.44746409e+01 1.04953500e+02 4.70077545e+01\n",
      " 6.79324382e+01 3.75618517e+01 8.66609476e+01 4.28272414e+01\n",
      " 7.57809442e+01 4.34667433e+01 8.35064683e+01 0.00000000e+00\n",
      " 3.55271368e-15 7.80994234e+00 7.93129730e+01 5.22738759e+01\n",
      " 6.27426735e+01]\n",
      "33-th iteration, loss: 0.051090454546001515, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7226044901850203e-07\n",
      "33-th iteration, new layer inserted. now 31 layers\n",
      "[1.27118291e+00 8.36763085e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704663e+01 1.30899568e+02 7.83760755e+01\n",
      " 1.19914170e+02 6.96842359e+01 1.05007009e+02 4.59332841e+01\n",
      " 9.40574879e+01 6.44746408e+01 1.04953500e+02 4.70077545e+01\n",
      " 6.79324381e+01 3.75618514e+01 8.66609475e+01 4.28272410e+01\n",
      " 7.57809440e+01 4.34667431e+01 8.35064683e+01 1.72885631e-07\n",
      " 5.79755707e-08 0.00000000e+00 4.96308368e-24 7.80994251e+00\n",
      " 7.93129730e+01 5.22738758e+01 6.27426735e+01]\n",
      "34-th iteration, loss: 0.051090454545986874, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.7107126929931464e-07\n",
      "34-th iteration, new layer inserted. now 31 layers\n",
      "[1.27118292e+00 8.36763085e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704663e+01 1.30899568e+02 7.83760755e+01\n",
      " 1.19914170e+02 6.96842358e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746407e+01 1.04953500e+02 4.70077545e+01\n",
      " 6.79324380e+01 3.75618512e+01 8.66609473e+01 4.28272407e+01\n",
      " 7.57809438e+01 4.34667430e+01 8.35064684e+01 3.44953498e-07\n",
      " 1.16302061e-07 1.72070556e-07 5.83264899e-08 7.80994269e+00\n",
      " 7.93129729e+01 5.22738758e+01 6.27426735e+01]\n",
      "35-th iteration, loss: 0.05109045454597279, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6987307198287686e-07\n",
      "35-th iteration, new layer inserted. now 33 layers\n",
      "[1.27118293e+00 8.36763085e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704663e+01 1.30899568e+02 7.83760755e+01\n",
      " 1.19914170e+02 6.96842357e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746406e+01 1.04953500e+02 4.70077545e+01\n",
      " 6.79324380e+01 3.75618510e+01 8.66609472e+01 4.28272403e+01\n",
      " 7.57809437e+01 4.34667428e+01 8.35064684e+01 5.15807654e-07\n",
      " 1.74823636e-07 3.42930105e-07 1.16840085e-07 0.00000000e+00\n",
      " 1.32348898e-23 7.80994286e+00 7.93129729e+01 5.22738757e+01\n",
      " 6.27426735e+01]\n",
      "36-th iteration, loss: 0.05109045454595796, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6798618585336438e-07\n",
      "36-th iteration, new layer inserted. now 33 layers\n",
      "[1.27118294e+00 8.36763086e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760756e+01\n",
      " 1.19914170e+02 6.96842356e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746405e+01 1.04953500e+02 4.70077546e+01\n",
      " 6.79324379e+01 3.75618508e+01 8.66609471e+01 4.28272400e+01\n",
      " 7.57809435e+01 4.34667426e+01 8.35064685e+01 6.85131732e-07\n",
      " 2.33412003e-07 5.12262290e-07 1.75412550e-07 1.69337603e-07\n",
      " 5.85724644e-08 7.80994303e+00 7.93129729e+01 5.22738757e+01\n",
      " 6.27426735e+01]\n",
      "37-th iteration, loss: 0.05109045454594359, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6610922158015722e-07\n",
      "37-th iteration, new layer inserted. now 35 layers\n",
      "[1.27118294e+00 8.36763086e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760756e+01\n",
      " 1.19914170e+02 6.96842355e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746404e+01 1.04953500e+02 4.70077546e+01\n",
      " 6.79324378e+01 3.75618507e+01 8.66609470e+01 4.28272397e+01\n",
      " 7.57809434e+01 4.34667425e+01 8.35064685e+01 8.52551739e-07\n",
      " 2.91921288e-07 6.79693122e-07 2.33898079e-07 3.36776570e-07\n",
      " 1.17050140e-07 0.00000000e+00 2.64697796e-23 7.80994319e+00\n",
      " 7.93129729e+01 5.22738756e+01 6.27426735e+01]\n",
      "38-th iteration, loss: 0.05109045454592852, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6357845323254844e-07\n",
      "38-th iteration, new layer inserted. now 37 layers\n",
      "[1.27118295e+00 8.36763087e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760756e+01\n",
      " 1.19914170e+02 6.96842355e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746403e+01 1.04953500e+02 4.70077547e+01\n",
      " 6.79324378e+01 3.75618505e+01 8.66609468e+01 4.28272394e+01\n",
      " 7.57809433e+01 4.34667423e+01 8.35064686e+01 1.01777742e-06\n",
      " 3.50234427e-07 8.44932344e-07 2.92179697e-07 5.02026640e-07\n",
      " 1.75316141e-07 1.65255498e-07 5.82660008e-08 0.00000000e+00\n",
      " 6.61744490e-24 7.80994336e+00 7.93129729e+01 5.22738755e+01\n",
      " 6.27426735e+01]\n",
      "39-th iteration, loss: 0.05109045454591279, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.6042813098214943e-07\n",
      "39-th iteration, new layer inserted. now 39 layers\n",
      "[1.27118296e+00 8.36763087e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760757e+01\n",
      " 1.19914170e+02 6.96842354e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746402e+01 1.04953500e+02 4.70077547e+01\n",
      " 6.79324377e+01 3.75618503e+01 8.66609467e+01 4.28272391e+01\n",
      " 7.57809431e+01 4.34667421e+01 8.35064687e+01 1.18016858e-06\n",
      " 4.08121117e-07 1.00733974e-06 3.50027204e-07 6.64447588e-07\n",
      " 2.33140367e-07 3.27684576e-07 1.16082563e-07 1.62431780e-07\n",
      " 5.78165619e-08 0.00000000e+00 6.61744490e-24 7.80994352e+00\n",
      " 7.93129728e+01 5.22738755e+01 6.27426735e+01]\n",
      "40-th iteration, loss: 0.051090454545896495, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5669910840487157e-07\n",
      "40-th iteration, new layer inserted. now 41 layers\n",
      "[1.27118297e+00 8.36763087e+01 1.26901084e+02 8.81463814e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760757e+01\n",
      " 1.19914170e+02 6.96842353e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746402e+01 1.04953500e+02 4.70077548e+01\n",
      " 6.79324376e+01 3.75618501e+01 8.66609466e+01 4.28272388e+01\n",
      " 7.57809430e+01 4.34667420e+01 8.35064687e+01 1.33912227e-06\n",
      " 4.65364622e-07 1.16631236e-06 4.07223993e-07 8.23436439e-07\n",
      " 2.90306342e-07 4.86684239e-07 1.73233342e-07 3.21436827e-07\n",
      " 1.14959809e-07 1.59007728e-07 5.71432470e-08 0.00000000e+00\n",
      " 3.30872245e-24 7.80994368e+00 7.93129728e+01 5.22738754e+01\n",
      " 6.27426735e+01]\n",
      "41-th iteration, loss: 0.05109045454587975, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.5243802609535692e-07\n",
      "41-th iteration, new layer inserted. now 41 layers\n",
      "[1.27118298e+00 8.36763088e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899568e+02 7.83760757e+01\n",
      " 1.19914170e+02 6.96842352e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746401e+01 1.04953500e+02 4.70077548e+01\n",
      " 6.79324376e+01 3.75618500e+01 8.66609465e+01 4.28272385e+01\n",
      " 7.57809428e+01 4.34667418e+01 8.35064688e+01 1.49407910e-06\n",
      " 5.21763726e-07 1.32129077e-06 4.63569011e-07 9.78433732e-07\n",
      " 3.46613173e-07 6.41694995e-07 2.29517603e-07 4.76455616e-07\n",
      " 1.71229164e-07 3.14031849e-07 1.13405228e-07 1.55026771e-07\n",
      " 5.62619808e-08 7.80994383e+00 7.93129728e+01 5.22738754e+01\n",
      " 6.27426735e+01]\n",
      "42-th iteration, loss: 0.05109045454586366, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.4829979312121056e-07\n",
      "42-th iteration, new layer inserted. now 43 layers\n",
      "[1.27118299e+00 8.36763088e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760758e+01\n",
      " 1.19914170e+02 6.96842352e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746400e+01 1.04953500e+02 4.70077549e+01\n",
      " 6.79324375e+01 3.75618498e+01 8.66609464e+01 4.28272382e+01\n",
      " 7.57809427e+01 4.34667416e+01 8.35064688e+01 1.64480205e-06\n",
      " 5.77223774e-07 1.47203791e-06 5.18967784e-07 1.12920237e-06\n",
      " 4.01966572e-07 7.92479708e-07 2.84841244e-07 6.27250974e-07\n",
      " 2.26530709e-07 4.64835146e-07 1.68692210e-07 3.05835328e-07\n",
      " 1.11541774e-07 0.00000000e+00 6.61744490e-24 7.80994399e+00\n",
      " 7.93129728e+01 5.22738753e+01 6.27426735e+01]\n",
      "43-th iteration, loss: 0.05109045454584724, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.436933741053386e-07\n",
      "43-th iteration, new layer inserted. now 45 layers\n",
      "[1.27118300e+00 8.36763089e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760758e+01\n",
      " 1.19914170e+02 6.96842351e+01 1.05007009e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746399e+01 1.04953500e+02 4.70077550e+01\n",
      " 6.79324374e+01 3.75618497e+01 8.66609463e+01 4.28272379e+01\n",
      " 7.57809426e+01 4.34667415e+01 8.35064689e+01 1.79114682e-06\n",
      " 6.31681564e-07 1.61840945e-06 5.73357310e-07 1.27559798e-06\n",
      " 4.56303732e-07 9.38893959e-07 3.39141653e-07 7.73678434e-07\n",
      " 2.80802029e-07 6.11273112e-07 2.22941974e-07 4.52281116e-07\n",
      " 1.65777355e-07 1.46450961e-07 5.42355808e-08 0.00000000e+00\n",
      " 3.30872245e-24 7.80994413e+00 7.93129728e+01 5.22738753e+01\n",
      " 6.27426735e+01]\n",
      "44-th iteration, loss: 0.05109045454583066, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3867265455945349e-07\n",
      "44-th iteration, new layer inserted. now 45 layers\n",
      "[1.27118301e+00 8.36763089e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760758e+01\n",
      " 1.19914170e+02 6.96842350e+01 1.05007008e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746399e+01 1.04953499e+02 4.70077551e+01\n",
      " 6.79324374e+01 3.75618495e+01 8.66609462e+01 4.28272376e+01\n",
      " 7.57809424e+01 4.34667413e+01 8.35064689e+01 1.93266802e-06\n",
      " 6.84976058e-07 1.75995995e-06 6.26576751e-07 1.41717507e-06\n",
      " 5.09464018e-07 1.08049221e-06 3.92258398e-07 9.15292409e-07\n",
      " 3.33882896e-07 7.52900109e-07 2.75994493e-07 5.93918452e-07\n",
      " 2.18808900e-07 2.88095985e-07 1.07260334e-07 1.41647539e-07\n",
      " 5.30247534e-08 7.80994427e+00 7.93129727e+01 5.22738752e+01\n",
      " 6.27426735e+01]\n",
      "45-th iteration, loss: 0.0510904545458148, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.3384118305957544e-07\n",
      "45-th iteration, new layer inserted. now 45 layers\n",
      "[1.27118302e+00 8.36763089e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760759e+01\n",
      " 1.19914171e+02 6.96842350e+01 1.05007008e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746398e+01 1.04953499e+02 4.70077551e+01\n",
      " 6.79324373e+01 3.75618494e+01 8.66609461e+01 4.28272373e+01\n",
      " 7.57809423e+01 4.34667412e+01 8.35064690e+01 2.06922338e-06\n",
      " 7.37046307e-07 1.89654707e-06 6.78565383e-07 1.55379124e-06\n",
      " 5.61386929e-07 1.21713201e-06 4.44131202e-07 1.05195040e-06\n",
      " 3.85713254e-07 8.89573584e-07 3.27789936e-07 7.30604726e-07\n",
      " 2.70576800e-07 4.24792406e-07 1.59014874e-07 2.78348935e-07\n",
      " 1.04772724e-07 7.80994441e+00 7.93129727e+01 5.22738752e+01\n",
      " 6.27426735e+01]\n",
      "46-th iteration, loss: 0.05109045454579963, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2919172906455263e-07\n",
      "46-th iteration, new layer inserted. now 45 layers\n",
      "[1.27118303e+00 8.36763090e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760759e+01\n",
      " 1.19914171e+02 6.96842349e+01 1.05007008e+02 4.59332840e+01\n",
      " 9.40574879e+01 6.44746398e+01 1.04953499e+02 4.70077552e+01\n",
      " 6.79324373e+01 3.75618492e+01 8.66609460e+01 4.28272370e+01\n",
      " 7.57809422e+01 4.34667410e+01 8.35064690e+01 2.20099969e-06\n",
      " 7.87939972e-07 2.02835756e-06 7.29371096e-07 1.68563320e-06\n",
      " 6.12120587e-07 1.34900000e-06 4.94808417e-07 1.18383899e-06\n",
      " 4.36341684e-07 1.02148006e-06 3.78377113e-07 8.62526403e-07\n",
      " 3.21130096e-07 5.56726630e-07 2.09548471e-07 4.10290533e-07\n",
      " 1.55293413e-07 7.80994454e+00 7.93129727e+01 5.22738751e+01\n",
      " 6.27426735e+01]\n",
      "47-th iteration, loss: 0.0510904545457851, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2471734041429303e-07\n",
      "47-th iteration, new layer inserted. now 45 layers\n",
      "[1.27118303e+00 8.36763090e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760760e+01\n",
      " 1.19914171e+02 6.96842348e+01 1.05007008e+02 4.59332840e+01\n",
      " 9.40574880e+01 6.44746397e+01 1.04953499e+02 4.70077553e+01\n",
      " 6.79324372e+01 3.75618491e+01 8.66609459e+01 4.28272367e+01\n",
      " 7.57809420e+01 4.34667409e+01 8.35064691e+01 2.32817663e-06\n",
      " 8.37702865e-07 2.15557104e-06 7.79039926e-07 1.81288051e-06\n",
      " 6.61711246e-07 1.47627569e-06 5.44336517e-07 1.31113763e-06\n",
      " 4.85814885e-07 1.14879893e-06 4.27802944e-07 9.89862823e-07\n",
      " 3.70515927e-07 6.84077943e-07 2.58908484e-07 5.37651563e-07\n",
      " 2.04634400e-07 7.80994467e+00 7.93129727e+01 5.22738751e+01\n",
      " 6.27426735e+01]\n",
      "48-th iteration, loss: 0.051090454545771165, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.2041133201300338e-07\n",
      "48-th iteration, new layer inserted. now 47 layers\n",
      "[1.27118304e+00 8.36763091e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760760e+01\n",
      " 1.19914171e+02 6.96842347e+01 1.05007008e+02 4.59332841e+01\n",
      " 9.40574880e+01 6.44746396e+01 1.04953499e+02 4.70077554e+01\n",
      " 6.79324372e+01 3.75618489e+01 8.66609458e+01 4.28272364e+01\n",
      " 7.57809419e+01 4.34667407e+01 8.35064691e+01 2.45092701e-06\n",
      " 8.86379021e-07 2.27836027e-06 8.27616118e-07 1.93570587e-06\n",
      " 7.10203367e-07 1.59913174e-06 5.92760177e-07 1.43401892e-06\n",
      " 5.34177742e-07 1.27170275e-06 4.76112526e-07 1.11278648e-06\n",
      " 4.18779606e-07 8.07018782e-07 3.07140439e-07 6.60604409e-07\n",
      " 2.52841421e-07 0.00000000e+00 1.32348898e-23 7.80994479e+00\n",
      " 7.93129726e+01 5.22738750e+01 6.27426735e+01]\n",
      "49-th iteration, loss: 0.051090454545757176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -1.1578988923961568e-07\n",
      "49-th iteration, new layer inserted. now 47 layers\n",
      "[1.27118305e+00 8.36763091e+01 1.26901084e+02 8.81463813e+01\n",
      " 1.69015194e+02 7.96704662e+01 1.30899567e+02 7.83760760e+01\n",
      " 1.19914171e+02 6.96842347e+01 1.05007008e+02 4.59332841e+01\n",
      " 9.40574880e+01 6.44746396e+01 1.04953499e+02 4.70077555e+01\n",
      " 6.79324371e+01 3.75618488e+01 8.66609457e+01 4.28272362e+01\n",
      " 7.57809418e+01 4.34667405e+01 8.35064692e+01 2.56920069e-06\n",
      " 9.33939956e-07 2.39667506e-06 8.75071396e-07 2.05405903e-06\n",
      " 7.57568879e-07 1.71751784e-06 6.40051530e-07 1.55243251e-06\n",
      " 5.81402593e-07 1.39014111e-06 5.23278403e-07 1.23124692e-06\n",
      " 4.65893878e-07 9.25498644e-07 3.54217287e-07 7.79098514e-07\n",
      " 2.99887634e-07 1.18505830e-07 4.70462127e-08 7.80994491e+00\n",
      " 7.93129726e+01 5.22738749e+01 6.27426735e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5361568410224664\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.59145123    0.         1765.4537749 ]\n",
      "1-th iteration, loss: 0.747573652308036, 11 gd steps\n",
      "insert gradient: -0.6299407406615269\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.31049111   62.35738846  236.82916493    0.         1528.62460997]\n",
      "2-th iteration, loss: 0.6056669912845173, 13 gd steps\n",
      "insert gradient: -0.6569129424581444\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.46006063   77.32336463  221.81217224   40.77406684  249.57136489\n",
      "    0.         1279.05324508]\n",
      "3-th iteration, loss: 0.4547488892047187, 21 gd steps\n",
      "insert gradient: -0.502286778310418\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   3.92670554   53.55604196  224.069182     76.61953331   96.7207341\n",
      "   72.55370547  146.17751372    0.         1132.87573136]\n",
      "4-th iteration, loss: 0.3631759923416439, 30 gd steps\n",
      "insert gradient: -0.673954205032164\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   3.38444954   47.28235998  108.86686528    0.          117.24123953\n",
      "   71.4246781   117.07766234   70.2052639   127.50674378   54.62267859\n",
      " 1132.87573136]\n",
      "5-th iteration, loss: 0.24140514220442116, 119 gd steps\n",
      "insert gradient: -0.20480438566737952\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.8826782   47.86024493  75.43966371  41.95005612  94.54809238\n",
      "  79.28605334 118.50567164  82.28921317 128.25340202  82.83288035\n",
      " 205.9774057    0.         926.89832566]\n",
      "6-th iteration, loss: 0.19078937709118934, 28 gd steps\n",
      "insert gradient: -0.08454194420705259\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.72453702  50.15080389  84.26297188  42.20412845  85.30743056\n",
      "  82.9221909  110.89833565  85.05189684 123.52202357  83.23283883\n",
      " 164.64934499  66.07344237 205.9774057    0.         720.92091995]\n",
      "7-th iteration, loss: 0.1787999681375493, 43 gd steps\n",
      "insert gradient: -0.0417968541852098\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.23710001  50.77881055  86.31544851  44.33717939  83.57903152\n",
      "  86.33605696 107.06298643  85.83689759 119.54511452  87.63011165\n",
      " 154.1457144   68.02909452 191.62130484  23.49739674 225.28778749\n",
      "   0.         495.63313247]\n",
      "8-th iteration, loss: 0.17310175690988294, 33 gd steps\n",
      "insert gradient: -0.04552508255225105\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  0.70072278  50.80515964  88.29963194  48.12294516  83.58931118\n",
      "  90.36745663 105.19450865  88.78339682 116.07086316  90.04900043\n",
      " 148.15851048  73.48455124 182.13413038  33.84846755 219.98635625\n",
      "  13.18672028 106.20709981   0.         389.42603265]\n",
      "9-th iteration, loss: 0.16323088828759327, 53 gd steps\n",
      "insert gradient: -0.04344230698273567\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  0.77696267  49.11570903  87.51983679  48.72971018  86.04071823\n",
      "  90.87289463 102.96724036  89.62499952 114.58801     89.17871964\n",
      " 134.90492089  84.77052643 176.3022832   45.57890019 203.58645196\n",
      "  24.13320082  57.82455343  16.27253194 389.42603265]\n",
      "10-th iteration, loss: 0.16260494738217113, 9 gd steps\n",
      "insert gradient: -0.0291611383604579\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.0694698   50.31653113  88.19595126  49.71134992  86.18624695\n",
      "  90.48398124 102.94755315  89.33839009 114.64203727  89.03096229\n",
      " 134.67745024  85.01403314 176.36386775  45.65795422 203.41812946\n",
      "  24.26688384  57.48296591  16.48146416 259.6173551    0.\n",
      " 129.80867755]\n",
      "11-th iteration, loss: 0.1568467514384592, 20 gd steps\n",
      "insert gradient: -0.035294003315083845\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  1.13000667  48.89870317  88.21966314  49.82395229  88.12300072\n",
      "  88.10668842 105.64924057  85.12804974 121.50968417  88.25250454\n",
      " 127.92728014  89.40903954 127.6884773    0.          47.88317899\n",
      "  50.72176794 190.28732146  35.2668157   34.83319601  24.317875\n",
      " 251.88684996   5.87904551 129.80867755]\n",
      "12-th iteration, loss: 0.15432014356530804, 16 gd steps\n",
      "insert gradient: -0.058265493627248706\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  0.45210118  47.20805757  86.56126184  50.85046838  84.21987519\n",
      "  88.79896853 106.76393014  84.26927702 124.10132509  87.98422307\n",
      " 125.77514038  91.46195175 120.95872803   6.63276419  40.07529822\n",
      "  50.73591616 187.94959458  39.58397898  29.63312885  25.84868416\n",
      " 247.43339664   6.75785656 129.80867755]\n",
      "13-th iteration, loss: 0.15291834904537627, 16 gd steps\n",
      "insert gradient: -0.023346907751682695\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  1.1583348   49.6744069   87.47631814  50.11619519  84.81529135\n",
      "  87.98274799 106.76656258  83.87909447 124.67416441  87.57783676\n",
      " 125.72334745  91.50499927 120.33853391   8.30548296  38.95190616\n",
      "  51.02578952 187.796564    40.51038286  29.25376216  26.32386903\n",
      " 246.62453941   6.95940171 103.84694204   0.          25.96173551]\n",
      "14-th iteration, loss: 0.15132897619876348, 16 gd steps\n",
      "insert gradient: -0.016317163407265727\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[1.07251867e+00 4.90626577e+01 8.72924879e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.88542901e+01 8.57908693e+01 8.79428556e+01\n",
      " 1.06934592e+02 8.37983639e+01 1.26515947e+02 8.72548077e+01\n",
      " 1.26611753e+02 9.23023262e+01 1.19117046e+02 9.85597699e+00\n",
      " 3.58665647e+01 5.21219251e+01 1.87776291e+02 4.25403103e+01\n",
      " 2.79749441e+01 2.73703128e+01 2.45730026e+02 7.87737857e+00\n",
      " 1.03574089e+02 5.40167899e+00 2.59617355e+01]\n",
      "15-th iteration, loss: 0.14808483656385776, 130 gd steps\n",
      "insert gradient: -0.009166733698712012\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  1.18628295  48.81900963  86.91638012  48.12859114  85.68085612\n",
      "  86.03309342 107.29428308  79.56024908 132.12681001  83.91911123\n",
      " 130.60782755  90.24144235 115.52945285  13.58671922  22.65121236\n",
      "  62.35036756 178.33429124  52.9212877   16.44193713  32.6718937\n",
      " 228.49253495  14.62894335  73.81676799  11.46479819  25.96173551]\n",
      "16-th iteration, loss: 0.14797562691727556, 15 gd steps\n",
      "insert gradient: -0.006517647446038242\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  1.16589556  48.68787552  86.88580978  48.59119188  85.49694259\n",
      "  66.76638329   0.          19.07610951 107.15936964  79.47038025\n",
      " 132.05577653  83.74073121 130.60842648  90.09250277 115.47315568\n",
      "  13.5144662   22.51560134  62.27177839 178.12107087  52.87871251\n",
      "  16.2545456   32.5660463  228.1533747   14.66867982  72.92525461\n",
      "  11.65547021  25.96173551]\n",
      "17-th iteration, loss: 0.14684079692720753, 73 gd steps\n",
      "insert gradient: -0.006475468978913829\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[1.16759871e+00 0.00000000e+00 1.66533454e-16 4.79299600e+01\n",
      " 8.75179810e+01 4.92152045e+01 8.57986236e+01 6.08737962e+01\n",
      " 1.01824628e+01 1.92936606e+01 9.99890929e+01 7.97196950e+01\n",
      " 1.32078191e+02 8.33439285e+01 1.30202664e+02 8.93198414e+01\n",
      " 1.16937684e+02 1.28091196e+01 1.98388457e+01 6.47333819e+01\n",
      " 1.74495108e+02 5.58842388e+01 1.37012187e+01 3.24094792e+01\n",
      " 2.20926394e+02 1.88413398e+01 5.87524993e+01 1.55442733e+01\n",
      " 2.59617355e+01]\n",
      "18-th iteration, loss: 0.14672946351648009, 47 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[  1.46724951  48.49813579  87.38503861  48.69513941  86.3140282\n",
      "  60.57018939  10.7891352   19.01286999  99.78382782  79.36597109\n",
      "  16.51926265   0.         115.63483854  83.23354335 129.90742757\n",
      "  89.02639776 117.28956786  12.4684392   19.55754233  64.97017017\n",
      " 173.7686433   56.08540159  13.18953962  32.31690857 219.48719821\n",
      "  19.23160094  57.23619944  16.05192655  25.96173551]\n",
      "19-th iteration, loss: 0.14582223949873355, 35 gd steps\n",
      "insert gradient: -0.00925346214831403\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[  1.58586527  48.74741878  88.10156271  48.72095969  87.4397364\n",
      "  60.88476309  13.66353495  17.40289576 100.74114861  69.64187121\n",
      "  15.66637679   8.44187903 113.92675658  84.86513797 129.72899818\n",
      "  88.86172356 118.98586367  11.53181876  18.51731376  67.31817715\n",
      " 171.85765201  57.98712801  10.9280769   32.23986927 215.74441255\n",
      "  19.92135308  54.69824078  15.86398709  25.96173551]\n",
      "20-th iteration, loss: 0.145674800415468, 21 gd steps\n",
      "insert gradient: -0.004725142536312253\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  1.60342208  48.87971562  88.31965856  49.11780826  87.20235416\n",
      "  60.32643564  14.50518262  17.32007342 100.56251545  68.2013804\n",
      "  16.35060004   8.84726834 113.84061157  84.91519454 129.52886102\n",
      "  88.83613415 119.16377036  11.35298445  18.37739226  67.51526062\n",
      " 171.24230889  58.19744281  10.51000781  32.26795308 161.33903898\n",
      "   0.          53.77967966  19.85149904  54.77619897  15.55329399\n",
      "  25.96173551]\n",
      "21-th iteration, loss: 0.14468203094273505, 30 gd steps\n",
      "insert gradient: -0.004710785708356932\n",
      "21-th iteration, new layer inserted. now 33 layers\n",
      "[  1.84003722  49.38441682  89.55382283  49.41094729  87.98515548\n",
      "  58.27908975  19.72758777  16.79439387  99.97413429  62.6005555\n",
      "  19.78945329  11.49481456 112.19450566  73.63653497   0.\n",
      "  12.27275583 128.6032324   89.15804809 120.2555208   10.74991534\n",
      "  17.99914877  68.90508075 168.76890053  59.71475046   9.66071772\n",
      "  29.00399336 156.2100003    6.82894694  48.82267581  23.80602114\n",
      "  53.23326916  13.52893286  25.96173551]\n",
      "22-th iteration, loss: 0.14376720158873332, 18 gd steps\n",
      "insert gradient: -0.015123376510320407\n",
      "22-th iteration, new layer inserted. now 35 layers\n",
      "[1.92539091e+00 4.97737076e+01 9.14927030e+01 4.95292192e+01\n",
      " 8.92716648e+01 5.76827630e+01 2.35430820e+01 1.57836367e+01\n",
      " 9.82766863e+01 6.35923083e+01 2.49155244e+01 0.00000000e+00\n",
      " 8.88178420e-16 9.92752893e+00 1.09088406e+02 7.23895156e+01\n",
      " 5.49848623e+00 1.18003836e+01 1.23808054e+02 9.06843791e+01\n",
      " 1.19989229e+02 1.17279376e+01 1.72454980e+01 6.91259441e+01\n",
      " 1.68057761e+02 6.15224458e+01 1.06614889e+01 2.46834583e+01\n",
      " 1.49751055e+02 1.23652811e+01 4.22563287e+01 2.93517099e+01\n",
      " 5.03094437e+01 1.29834541e+01 2.59617355e+01]\n",
      "23-th iteration, loss: 0.1411302508801516, 31 gd steps\n",
      "insert gradient: -0.005619391880722071\n",
      "23-th iteration, new layer inserted. now 35 layers\n",
      "[2.15080596e+00 4.99096303e+01 9.11572406e+01 5.02481519e+01\n",
      " 8.96917706e+01 5.75702366e+01 2.99778054e+01 1.30164852e+01\n",
      " 9.92969424e+01 6.03381245e+01 0.00000000e+00 1.77635684e-14\n",
      " 3.51980596e+01 7.76655826e+00 1.06080635e+02 6.62121697e+01\n",
      " 1.47615423e+01 1.40980691e+01 1.17579585e+02 9.28625702e+01\n",
      " 1.19110657e+02 1.43363971e+01 1.32986271e+01 6.93216445e+01\n",
      " 1.67215586e+02 6.53180335e+01 1.41712736e+01 1.75373929e+01\n",
      " 1.45700288e+02 2.10370853e+01 3.16915227e+01 3.62191396e+01\n",
      " 4.68318278e+01 1.11037550e+01 2.59617355e+01]\n",
      "24-th iteration, loss: 0.1364668434218447, 72 gd steps\n",
      "insert gradient: -0.013347987849499204\n",
      "24-th iteration, new layer inserted. now 35 layers\n",
      "[  2.51616474  50.94396324  91.92118195  50.35774187  90.26007245\n",
      "  54.75224957  44.42751885   8.14377473  99.86424389  56.07689948\n",
      "  54.20937638   2.22131034 107.02509425  60.56128783  30.78938321\n",
      "  10.07165606 109.20024051  93.70805412 119.71165883  19.00702814\n",
      "   2.7128793   70.66332071 140.57787534   0.          23.42964589\n",
      "  73.26934943  17.18392012   6.88807097 149.74754777  34.51693775\n",
      "  18.83184538  39.12721047  44.93440706   8.11050482  25.96173551]\n",
      "25-th iteration, loss: 0.13257245775783946, 512 gd steps\n",
      "insert gradient: -0.013074939638445691\n",
      "25-th iteration, new layer inserted. now 33 layers\n",
      "[  2.61629375  50.91337226  91.71686108  49.9247484   90.12770301\n",
      "  54.04748307  50.02653668   5.98846917  99.23675833  54.71267194\n",
      " 168.20933722  59.69116838  38.77303419   7.36231129 106.54315977\n",
      "  65.24874678   0.          26.09949871 121.40748635  91.89289825\n",
      " 133.54207817   8.72844648  17.45657564  70.68093933  19.75845179\n",
      "   2.07691437 151.28913066  40.1040983   15.19325452  39.56539701\n",
      "  45.35453206   6.80051948  25.96173551]\n",
      "26-th iteration, loss: 0.12606128316813967, 20 gd steps\n",
      "insert gradient: -0.0408822467765077\n",
      "26-th iteration, new layer inserted. now 31 layers\n",
      "[  2.42257709  50.85325466  91.60214425  50.68130652  92.23987544\n",
      "  54.4020986   62.8102946    1.42993078 100.59531972  51.69092457\n",
      " 103.32400373   0.          61.99440224  62.38584143 168.16957921\n",
      "  59.82056547  26.17888083  11.40182517 116.86135661  93.18116484\n",
      " 125.62810973  16.02214228  10.31524933  70.959031   175.89222662\n",
      "  52.53016489   8.51354245  39.01291423  45.47429046   6.52634063\n",
      "  25.96173551]\n",
      "27-th iteration, loss: 0.12455112929046046, 19 gd steps\n",
      "insert gradient: -0.006314675994662737\n",
      "27-th iteration, new layer inserted. now 31 layers\n",
      "[2.16199052e+00 4.98829149e+01 9.21254067e+01 5.11165157e+01\n",
      " 8.98282095e+01 5.19088649e+01 6.45087952e+01 8.59142914e-02\n",
      " 1.01642430e+02 5.72781700e+01 1.65641754e+02 5.91348596e+01\n",
      " 1.67025288e+02 6.13268185e+01 2.97850010e+01 9.46913421e+00\n",
      " 1.15814943e+02 7.03532374e+01 0.00000000e+00 2.34510791e+01\n",
      " 1.25425864e+02 1.71595096e+01 9.09430591e+00 7.11832212e+01\n",
      " 1.75646978e+02 5.37705828e+01 8.94901984e+00 3.84979428e+01\n",
      " 4.51249593e+01 7.16662933e+00 2.59617355e+01]\n",
      "28-th iteration, loss: 0.12382346520693997, 25 gd steps\n",
      "insert gradient: -0.005026097754447764\n",
      "28-th iteration, new layer inserted. now 31 layers\n",
      "[  2.42527205  50.7629356   91.71890149  50.36585974  90.55546834\n",
      "  52.40846396 168.00174599  54.77068293 166.24917223  60.11277809\n",
      " 167.26758628  62.11336901  32.63226124   7.90263796 114.95139685\n",
      "  69.09237515   3.50320537  22.23568807 124.72532274  18.6154606\n",
      "   7.79386657  71.4448903  109.93785155   0.          65.96271093\n",
      "  54.97672317   9.25587482  38.03442973  45.05718839   7.3390093\n",
      "  25.96173551]\n",
      "29-th iteration, loss: 0.12299728079960781, 16 gd steps\n",
      "insert gradient: -0.01562135991815273\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[2.01007619e+00 5.19293432e+01 9.16946707e+01 5.09793404e+01\n",
      " 8.85830236e+01 0.00000000e+00 1.42108547e-14 5.05981602e+01\n",
      " 1.68990579e+02 5.61507211e+01 1.66389764e+02 5.93584611e+01\n",
      " 1.66286475e+02 6.45658018e+01 4.18837064e+01 3.86100021e+00\n",
      " 1.13489087e+02 6.39690697e+01 1.38261068e+01 1.87561240e+01\n",
      " 1.18863007e+02 2.47669207e+01 2.73486468e+00 7.04289229e+01\n",
      " 1.04580517e+02 3.77700185e+00 6.00500342e+01 5.92014755e+01\n",
      " 1.08240942e+01 3.40716882e+01 4.68014487e+01 6.95342748e+00\n",
      " 2.59617355e+01]\n",
      "30-th iteration, loss: 0.12193781050072627, 254 gd steps\n",
      "insert gradient: -0.005833576028617474\n",
      "30-th iteration, new layer inserted. now 31 layers\n",
      "[  2.04099246  51.27860274  92.09416201  51.02035495  90.83070272\n",
      "  52.85041829 168.83796477  55.33762833 104.92049018   0.\n",
      "  62.95229411  59.19735914 168.49140755  61.18312358  48.02568839\n",
      "   1.14987093 117.77515649  63.47731299  19.72341228  15.61719605\n",
      " 118.24485912  99.49329649 103.52229066   5.13283405  56.21795917\n",
      "  61.63858182  11.94618655  31.14270187  48.08565288   5.64856935\n",
      "  25.96173551]\n",
      "31-th iteration, loss: 0.12122279416716472, 104 gd steps\n",
      "insert gradient: -0.0007392844021546639\n",
      "31-th iteration, new layer inserted. now 31 layers\n",
      "[1.95619352e+00 5.20752694e+01 9.30313685e+01 5.17609828e+01\n",
      " 9.18150216e+01 5.31632772e+01 1.70619589e+02 5.61680050e+01\n",
      " 1.04344002e+02 4.86656174e+00 5.29521865e+01 5.88294849e+01\n",
      " 0.00000000e+00 5.32907052e-15 1.71354924e+02 5.91142912e+01\n",
      " 1.73990537e+02 6.20353664e+01 2.52672877e+01 1.42743975e+01\n",
      " 1.14839215e+02 1.00131536e+02 1.04083997e+02 6.63199546e+00\n",
      " 4.90432988e+01 6.66163101e+01 1.44330270e+01 2.34779151e+01\n",
      " 4.75604424e+01 2.47841297e+00 2.59617355e+01]\n",
      "32-th iteration, loss: 0.12106385871598978, 71 gd steps\n",
      "insert gradient: -0.0012653075760219264\n",
      "32-th iteration, new layer inserted. now 29 layers\n",
      "[  2.03244702  52.26935466  93.18422365  51.82295596  91.94741515\n",
      "  53.17842519 171.34899061  55.83094178 105.91991218   5.64668063\n",
      "  49.96930551  58.60013645 172.53014507  58.92963479 173.51740257\n",
      "  62.20838526  25.88223027  14.08751394 114.29175817 100.24482435\n",
      " 105.66242023   7.26312812  45.65264749  69.37267577  17.71699604\n",
      "  20.22240123  47.1888532    0.52724117  25.96173551]\n",
      "33-th iteration, loss: 0.12101851122686053, 85 gd steps\n",
      "insert gradient: -0.0002396326638291448\n",
      "33-th iteration, new layer inserted. now 29 layers\n",
      "[2.04644378e+00 5.21976595e+01 9.31781696e+01 5.17372704e+01\n",
      " 9.19755287e+01 5.31793484e+01 1.71228997e+02 5.58186996e+01\n",
      " 1.06582405e+02 5.68240653e+00 4.92448536e+01 5.87161523e+01\n",
      " 1.72482935e+02 5.88363808e+01 1.73559250e+02 6.20935052e+01\n",
      " 2.59994290e+01 1.41360143e+01 1.14091900e+02 1.00217600e+02\n",
      " 0.00000000e+00 3.55271368e-15 1.06543193e+02 7.48696566e+00\n",
      " 4.43678128e+01 7.03878440e+01 1.95767548e+01 1.89729856e+01\n",
      " 7.31129663e+01]\n",
      "34-th iteration, loss: 0.12099524460871594, 209 gd steps\n",
      "insert gradient: -0.00014472742562025916\n",
      "34-th iteration, new layer inserted. now 29 layers\n",
      "[2.04990837e+00 5.22191874e+01 9.31850433e+01 5.18310561e+01\n",
      " 9.19691523e+01 5.31397267e+01 1.71467592e+02 5.56745883e+01\n",
      " 1.07253445e+02 5.80831693e+00 4.86457969e+01 5.85355250e+01\n",
      " 1.72782968e+02 5.86534380e+01 1.73826928e+02 6.19863178e+01\n",
      " 2.62156793e+01 1.40810302e+01 1.13942014e+02 1.00021027e+02\n",
      " 0.00000000e+00 3.55271368e-15 1.09117653e+02 7.43660769e+00\n",
      " 4.23960668e+01 7.12408724e+01 1.98385911e+01 1.84279141e+01\n",
      " 7.31129663e+01]\n",
      "35-th iteration, loss: 0.12098503635866525, 20 gd steps\n",
      "insert gradient: -0.0007098563800505379\n",
      "35-th iteration, new layer inserted. now 27 layers\n",
      "[  2.06904275  52.33139521  93.21331664  51.78298462  92.0035629\n",
      "  53.14099187 171.65008742  55.5103528  107.70622079   5.86892473\n",
      "  48.35372013  58.42234609 172.97902729  58.49437852 173.9851059\n",
      "  61.82437663  26.3818272   14.04685215 113.78569824  99.83706561\n",
      " 112.55359203   7.21173314  40.30384171  72.14012846  20.005179\n",
      "  17.88844143  73.11296626]\n",
      "36-th iteration, loss: 0.12098228510859377, 36 gd steps\n",
      "insert gradient: -0.00013260966687233402\n",
      "36-th iteration, new layer inserted. now 27 layers\n",
      "[  2.04853937  52.24430288  93.23750917  51.88342901  91.98766745\n",
      "  53.06758335 171.73281426  55.50423388 107.60165443   5.89846739\n",
      "  48.3486286   58.41845542 173.00631724  58.49422234 174.0056664\n",
      "  61.89546632  26.43608043  14.01163865 113.80235658  99.65788145\n",
      " 112.63372288   7.25959454  40.04494979  72.1645051   19.97885223\n",
      "  17.81590407  73.11296626]\n",
      "37-th iteration, loss: 0.12098104243221473, 103 gd steps\n",
      "insert gradient: -0.0002469350813093057\n",
      "37-th iteration, new layer inserted. now 27 layers\n",
      "[  2.03867623  52.23169917  93.25142304  51.93651642  91.98927142\n",
      "  53.01975722 171.84434393  55.47245184 107.39232728   5.95418389\n",
      "  48.44771433  58.39457922 173.10372898  58.43337785 174.09404373\n",
      "  61.86597332  26.49011276  13.98168995 113.87106204  99.57077071\n",
      " 113.41970584   7.2253725   39.4581519   72.47929284  19.99996836\n",
      "  17.6344886   73.11296626]\n",
      "38-th iteration, loss: 0.12098032054692202, 157 gd steps\n",
      "insert gradient: -6.901945303498619e-05\n",
      "38-th iteration, new layer inserted. now 27 layers\n",
      "[  2.03726249  52.26365703  93.28406806  51.91647348  92.01274531\n",
      "  53.02002759 171.9033912   55.43165937 107.30993176   5.98151144\n",
      "  48.5030834   58.3701927  173.15472857  58.40086383 174.11324223\n",
      "  61.85115193  26.51873104  13.96361343 113.88660229  99.46630174\n",
      " 114.26016354   7.15787331  38.92702865  72.70689204  19.99033895\n",
      "  17.48710796  73.11296626]\n",
      "39-th iteration, loss: 0.12098028724928477, 36 gd steps\n",
      "insert gradient: -0.00020227489711055902\n",
      "39-th iteration, new layer inserted. now 27 layers\n",
      "[  2.03042178  52.25368299  93.28785151  51.92536043  92.01190264\n",
      "  53.0152963  171.92473356  55.42594696 107.29730743   5.98845583\n",
      "  48.51532855  58.36367688 173.17198657  58.38582072 174.14397071\n",
      "  61.84799638  26.52901808  13.96030587 113.88422903  99.45483024\n",
      " 114.41839285   7.1515679   38.81935304  72.73337638  19.99131218\n",
      "  17.45926428  73.11296626]\n",
      "40-th iteration, loss: 0.12098023927624578, 89 gd steps\n",
      "insert gradient: -1.0985847575074142e-05\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[  2.04115003  52.26836371  93.28211565  51.92654194  92.00627829\n",
      "  53.01471776 171.93209389  55.42200826 107.28748572   5.99377856\n",
      "  48.51726256  58.35560179 173.17626553  58.38897211 174.13556907\n",
      "  61.84669502  26.53384274  13.95624002 113.88547954  99.44106741\n",
      " 114.5024926    7.14498744  38.76208206  72.7501324   19.98640134\n",
      "  17.44652112  73.11296626]\n",
      "41-th iteration, loss: 0.12098022079698285, 108 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.828427684716733e-06\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[  2.04132006  52.26993409  93.28371439  51.92753536  92.00598475\n",
      "  53.01169002 171.93998022  55.41784664 107.27881724   5.99656281\n",
      "  48.52382829  58.35261541 173.18162973  58.38585233 174.13947482\n",
      "  61.84497081  26.53610867  13.95476925 113.88709693  99.42856383\n",
      " 114.62357815   7.13735556  38.68846715  72.77082946  19.98826811\n",
      "  17.42938647  73.11296626]\n",
      "42-th iteration, loss: 0.12098022079449743, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.848344037908073e-06\n",
      "42-th iteration, new layer inserted. now 29 layers\n",
      "[2.04132044e+00 5.22699359e+01 9.32837160e+01 5.19275388e+01\n",
      " 9.20059862e+01 5.30116902e+01 1.71939980e+02 5.54178433e+01\n",
      " 1.07278814e+02 5.99655791e+00 4.85238259e+01 5.83526106e+01\n",
      " 1.73181629e+02 5.83858481e+01 1.74139474e+02 6.18449685e+01\n",
      " 2.65361080e+01 1.39547672e+01 1.13887096e+02 9.94285623e+01\n",
      " 0.00000000e+00 1.77635684e-14 1.14623582e+02 7.13735520e+00\n",
      " 3.86884638e+01 7.27708298e+01 1.99882674e+01 1.74293861e+01\n",
      " 7.31129663e+01]\n",
      "43-th iteration, loss: 0.12098022079207144, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.864627795974046e-06\n",
      "43-th iteration, new layer inserted. now 29 layers\n",
      "[2.04132078e+00 5.22699375e+01 9.32837176e+01 5.19275420e+01\n",
      " 9.20059876e+01 5.30116903e+01 1.71939980e+02 5.54178402e+01\n",
      " 1.07278812e+02 5.99655341e+00 4.85238238e+01 5.83526061e+01\n",
      " 1.73181627e+02 5.83858442e+01 1.74139473e+02 6.18449663e+01\n",
      " 2.65361074e+01 1.39547653e+01 1.13887095e+02 9.94285608e+01\n",
      " 0.00000000e+00 3.55271368e-15 1.14623590e+02 7.13735485e+00\n",
      " 3.86884605e+01 7.27708302e+01 1.99882667e+01 1.74293856e+01\n",
      " 7.31129663e+01]\n",
      "44-th iteration, loss: 0.12098022078989945, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.87945647725437e-06\n",
      "44-th iteration, new layer inserted. now 27 layers\n",
      "[  2.0413211   52.26993901  93.28371898  51.92754486  92.00598885\n",
      "  53.01169038 171.93997959  55.41783739 107.27880922   5.99654929\n",
      "  48.52382177  58.35260201 173.18162658  58.38584054 174.13947257\n",
      "  61.84496433  26.53610697  13.95476357 113.8870943   99.42855939\n",
      " 114.62359743   7.13735452  38.68845714  72.77083054  19.98826606\n",
      "  17.42938517  73.11296626]\n",
      "45-th iteration, loss: 0.12098022078819196, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.894993023307754e-06\n",
      "45-th iteration, new layer inserted. now 29 layers\n",
      "[2.04132138e+00 5.22699404e+01 9.32837203e+01 5.19275475e+01\n",
      " 9.20059900e+01 5.30116905e+01 1.71939980e+02 5.54178347e+01\n",
      " 1.07278807e+02 5.99654550e+00 4.85238199e+01 5.83525982e+01\n",
      " 1.73181626e+02 5.83858372e+01 1.74139472e+02 6.18449625e+01\n",
      " 2.65361066e+01 1.39547619e+01 1.13887094e+02 9.94285581e+01\n",
      " 0.00000000e+00 1.06581410e-14 1.14623601e+02 7.13735419e+00\n",
      " 3.86884538e+01 7.27708309e+01 1.99882654e+01 1.74293847e+01\n",
      " 7.31129663e+01]\n",
      "46-th iteration, loss: 0.12098022078641729, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.907229897901884e-06\n",
      "46-th iteration, new layer inserted. now 29 layers\n",
      "[2.04132164e+00 5.22699417e+01 9.32837215e+01 5.19275500e+01\n",
      " 9.20059911e+01 5.30116905e+01 1.71939980e+02 5.54178323e+01\n",
      " 1.07278805e+02 5.99654203e+00 4.85238183e+01 5.83525946e+01\n",
      " 1.73181625e+02 5.83858341e+01 1.74139472e+02 6.18449608e+01\n",
      " 2.65361062e+01 1.39547604e+01 1.13887093e+02 9.94285568e+01\n",
      " 0.00000000e+00 1.77635684e-14 1.14623609e+02 7.13735388e+00\n",
      " 3.86884505e+01 7.27708313e+01 1.99882647e+01 1.74293842e+01\n",
      " 7.31129663e+01]\n",
      "47-th iteration, loss: 0.12098022078479545, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.91834078893815e-06\n",
      "47-th iteration, new layer inserted. now 29 layers\n",
      "[2.04132187e+00 5.22699428e+01 9.32837226e+01 5.19275522e+01\n",
      " 9.20059920e+01 5.30116906e+01 1.71939980e+02 5.54178300e+01\n",
      " 1.07278803e+02 5.99653886e+00 4.85238168e+01 5.83525913e+01\n",
      " 1.73181625e+02 5.83858312e+01 1.74139471e+02 6.18449593e+01\n",
      " 2.65361060e+01 1.39547590e+01 1.13887092e+02 9.94285556e+01\n",
      " 0.00000000e+00 1.06581410e-14 1.14623617e+02 7.13735358e+00\n",
      " 3.86884472e+01 7.27708318e+01 1.99882640e+01 1.74293837e+01\n",
      " 7.31129663e+01]\n",
      "48-th iteration, loss: 0.12098022078330273, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.928417664681137e-06\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[  2.04132208  52.26994393  93.28372364  51.92755427  92.00599292\n",
      "  53.01169061 171.93997983  55.41782785 107.27880073   5.99653594\n",
      "  48.52381538  58.35258826 173.18162423  58.38582852 174.13947092\n",
      "  61.84495787  26.53610575  13.95475765 113.88709177  99.42855445\n",
      " 114.62362477   7.13735328  38.68844384  72.77083219  19.98826332\n",
      "  17.42938321  73.11296626]\n",
      "49-th iteration, loss: 0.12098022078217506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.939613186520855e-06\n",
      "49-th iteration, new layer inserted. now 27 layers\n",
      "[  2.04132226  52.26994492  93.2837246   51.92755617  92.00599375\n",
      "  53.01169063 171.93998004  55.41782586 107.27879892   5.99653327\n",
      "  48.52381411  58.35258541 173.18162391  58.38582604 174.1394707\n",
      "  61.84495656  26.5361056   13.9547564  113.88709127  99.42855336\n",
      " 114.62362871   7.13735299  38.68844052  72.77083264  19.98826263\n",
      "  17.42938269  73.11296626]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5368637482248393\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.19688805    0.         1789.97396622]\n",
      "1-th iteration, loss: 0.7487306396172311, 11 gd steps\n",
      "insert gradient: -0.6252748469438001\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.74722856   62.33225191  240.11845888    0.         1549.85550733]\n",
      "2-th iteration, loss: 0.6069736229758301, 13 gd steps\n",
      "insert gradient: -0.6539885402321906\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.17206376   76.8706847   224.37049407   39.55553825  253.03763385\n",
      "    0.         1296.81787348]\n",
      "3-th iteration, loss: 0.4528786387801591, 20 gd steps\n",
      "insert gradient: -0.6198571202205533\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           35.85756438  113.58863835    0.          120.27032296\n",
      "   79.77832592  105.13081187   71.66457524 1296.81787348]\n",
      "4-th iteration, loss: 0.3639196724904893, 29 gd steps\n",
      "insert gradient: -0.4392507982873351\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   0.           45.15570848   51.72962572   49.18731203  106.33017401\n",
      "   77.5275434   122.9690034    80.6292223   192.12116644    0.\n",
      " 1104.69670704]\n",
      "5-th iteration, loss: 0.24140514221070833, 129 gd steps\n",
      "insert gradient: -0.20603954110577258\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.88249951  47.86032982  75.43949335  41.95011418  94.54806065\n",
      "  79.28588229 118.5058353   82.28941506 128.25279419  82.83308222\n",
      " 200.85394673   0.         903.84276031]\n",
      "6-th iteration, loss: 0.18563008890287308, 76 gd steps\n",
      "insert gradient: -0.0531782297353487\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[2.77655071e+00 4.70377969e+01 8.40451431e+01 4.41010004e+01\n",
      " 8.74615421e+01 8.30766359e+01 1.09013050e+02 8.44172949e+01\n",
      " 1.26040351e+02 8.25126802e+01 1.54039206e+02 8.79898990e+01\n",
      " 9.03842760e+02 0.00000000e+00 3.97903932e-13]\n",
      "7-th iteration, loss: 0.1821439195737043, 28 gd steps\n",
      "insert gradient: -0.05522236544573251\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[2.13449309e+00 4.62309970e+01 8.40988603e+01 4.48876744e+01\n",
      " 8.60664388e+01 8.29544857e+01 1.08448070e+02 8.35931113e+01\n",
      " 1.23689177e+02 8.43653580e+01 1.52516948e+02 8.60124450e+01\n",
      " 1.70174999e+02 0.00000000e+00 7.37424998e+02 1.21545081e+01\n",
      " 3.62122003e-13]\n",
      "8-th iteration, loss: 0.17660372551924639, 24 gd steps\n",
      "insert gradient: -0.04040346886343173\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[2.59420306e+00 4.88015070e+01 8.51647982e+01 4.33169203e+01\n",
      " 8.37991101e+01 8.25916588e+01 1.09890563e+02 8.28295995e+01\n",
      " 1.24081297e+02 8.55826358e+01 1.49627266e+02 7.95268014e+01\n",
      " 1.71027934e+02 1.34011556e+01 7.34085299e+02 1.24012410e+01\n",
      " 3.47359953e-13]\n",
      "9-th iteration, loss: 0.17514620672484948, 38 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[2.11070229e+00 4.97146720e+01 8.60532935e+01 4.44640837e+01\n",
      " 8.23062270e+01 8.32847499e+01 1.10247894e+02 8.25712027e+01\n",
      " 1.22822409e+02 8.70657106e+01 1.48003290e+02 7.71931567e+01\n",
      " 1.75348720e+02 1.92219561e+01 5.17788802e+01 0.00000000e+00\n",
      " 6.73125443e+02 1.29158575e+01 2.84466882e-13]\n",
      "10-th iteration, loss: 0.1607306471473643, 22 gd steps\n",
      "insert gradient: -0.02093392453153336\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[8.48137545e-01 4.86832101e+01 8.43711324e+01 0.00000000e+00\n",
      " 2.30926389e-14 4.66931915e+01 8.19139565e+01 8.45176910e+01\n",
      " 1.06064244e+02 8.39512838e+01 1.20687474e+02 8.45148814e+01\n",
      " 1.47485984e+02 7.75245484e+01 1.64643711e+02 4.79431480e+01\n",
      " 1.80480392e+01 2.61627014e+01 6.34476221e+02 7.84773318e+00\n",
      " 2.04410580e-13]\n",
      "11-th iteration, loss: 0.1591251570607492, 39 gd steps\n",
      "insert gradient: -0.02666172030683099\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[1.92274007e+00 4.80662185e+01 8.59346711e+01 2.78956899e-01\n",
      " 2.59439114e-01 4.58536572e+01 8.39727794e+01 8.51140778e+01\n",
      " 1.05267246e+02 8.36902758e+01 1.21399528e+02 8.45531271e+01\n",
      " 1.45410106e+02 7.72518975e+01 1.61455285e+02 5.54022041e+01\n",
      " 8.93229218e+00 2.85202751e+01 2.29084941e+02 0.00000000e+00\n",
      " 4.00898647e+02 6.50296156e+00 1.65475642e-13]\n",
      "12-th iteration, loss: 0.15793448481171218, 19 gd steps\n",
      "insert gradient: -0.011850265539500953\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[2.04716259e+00 4.82158242e+01 8.62022780e+01 8.51117521e-02\n",
      " 2.31042935e-01 4.62183533e+01 8.41856728e+01 8.50744800e+01\n",
      " 1.05078883e+02 8.36509434e+01 1.21196919e+02 8.50033170e+01\n",
      " 1.44630526e+02 7.78236281e+01 1.60390097e+02 5.68846970e+01\n",
      " 7.69252187e+00 2.90077736e+01 2.26687698e+02 4.56741017e+00\n",
      " 3.58775619e+02 0.00000000e+00 3.98639577e+01 5.56595890e+00\n",
      " 1.54232985e-13]\n",
      "13-th iteration, loss: 0.15684845229752847, 20 gd steps\n",
      "insert gradient: -0.01567115867307259\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[1.89573620e+00 4.81784130e+01 8.60543407e+01 4.63138119e+01\n",
      " 8.46588836e+01 8.53383456e+01 1.03829553e+02 8.39066216e+01\n",
      " 1.20285492e+02 8.49271632e+01 1.43878580e+02 7.85229293e+01\n",
      " 1.59803620e+02 6.02755552e+01 5.80228209e+00 2.91658589e+01\n",
      " 2.22478142e+02 7.07420178e+00 1.07266883e+02 0.00000000e+00\n",
      " 2.50289395e+02 6.38293646e+00 3.88078934e+01 5.25130169e+00\n",
      " 1.33897934e-13]\n",
      "14-th iteration, loss: 0.1561345992590772, 21 gd steps\n",
      "insert gradient: -0.011450855647458577\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[1.74885762e+00 4.76211677e+01 8.57422206e+01 4.62866427e+01\n",
      " 8.47317413e+01 8.53435863e+01 1.04026918e+02 8.35908929e+01\n",
      " 1.21181259e+02 8.43821742e+01 1.41719866e+02 8.02636025e+01\n",
      " 1.59486896e+02 6.27341903e+01 5.19295375e+00 2.82865026e+01\n",
      " 2.20062405e+02 7.29659622e+00 1.05351859e+02 3.36217689e+00\n",
      " 2.21067408e+02 0.00000000e+00 2.76334260e+01 8.18471872e+00\n",
      " 4.21108293e+01 5.16751009e+00 1.21779939e-13]\n",
      "15-th iteration, loss: 0.15353860230771313, 47 gd steps\n",
      "insert gradient: -0.005667347095210875\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[1.78169242e+00 4.76536741e+01 8.55622961e+01 4.63613273e+01\n",
      " 8.41449876e+01 8.43651754e+01 1.04658524e+02 8.20583087e+01\n",
      " 1.22959927e+02 8.32414036e+01 1.39762161e+02 8.13819435e+01\n",
      " 1.59862511e+02 6.44067848e+01 6.43790327e+00 2.65623697e+01\n",
      " 2.18388032e+02 7.14654972e+00 3.81116930e+01 0.00000000e+00\n",
      " 6.35194884e+01 5.94820757e+00 2.19557982e+02 9.93212448e+00\n",
      " 2.65404698e+01 1.61283777e+01 4.10852586e+01 1.20089088e+01\n",
      " 9.37701767e-14]\n",
      "16-th iteration, loss: 0.1496448030122985, 79 gd steps\n",
      "insert gradient: -0.005203250552408569\n",
      "16-th iteration, new layer inserted. now 31 layers\n",
      "[1.67792072e+00 4.69646327e+01 8.41215584e+01 4.55785626e+01\n",
      " 8.26787072e+01 8.11998721e+01 1.06856553e+02 7.87289132e+01\n",
      " 1.25670677e+02 8.10379999e+01 1.34731126e+02 8.25885891e+01\n",
      " 1.59018740e+02 6.93707389e+01 1.22380866e+01 1.95890081e+01\n",
      " 1.56384039e+02 0.00000000e+00 5.21280131e+01 5.04591010e+00\n",
      " 2.55534573e+01 1.22063554e+01 4.98133246e+01 1.72645957e+01\n",
      " 2.01422801e+02 2.49903552e+01 1.71215698e+01 3.48238526e+01\n",
      " 2.36518606e+01 1.17168764e+01 9.59892744e-14]\n",
      "17-th iteration, loss: 0.14950468100361577, 16 gd steps\n",
      "insert gradient: -0.003959071105965764\n",
      "17-th iteration, new layer inserted. now 33 layers\n",
      "[1.64015517e+00 4.67552676e+01 8.40040422e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.53855518e+01 8.26080743e+01 8.12626594e+01\n",
      " 1.07045280e+02 7.86766564e+01 1.25828004e+02 8.10414278e+01\n",
      " 1.34595812e+02 8.26690262e+01 1.58887600e+02 6.94498805e+01\n",
      " 1.24060702e+01 1.92550609e+01 1.56181409e+02 7.23405176e-01\n",
      " 5.19239716e+01 5.04446928e+00 2.53439502e+01 1.25675778e+01\n",
      " 4.95031034e+01 1.74517792e+01 2.00779026e+02 2.53037900e+01\n",
      " 1.66692786e+01 3.53922123e+01 2.36363277e+01 1.15511738e+01\n",
      " 9.57719619e-14]\n",
      "18-th iteration, loss: 0.1481114063691887, 24 gd steps\n",
      "insert gradient: -0.0023277531234720834\n",
      "18-th iteration, new layer inserted. now 35 layers\n",
      "[1.70415780e+00 4.67239929e+01 8.39466727e+01 2.49947162e-01\n",
      " 5.91930672e-02 4.53948155e+01 8.18958037e+01 8.09426710e+01\n",
      " 1.07300547e+02 7.79943706e+01 1.26288674e+02 8.05232898e+01\n",
      " 1.34077323e+02 8.23811179e+01 1.12830819e+02 0.00000000e+00\n",
      " 4.51323278e+01 7.17276682e+01 1.47746958e+01 1.53340003e+01\n",
      " 1.51520969e+02 5.58896775e+00 4.66188926e+01 6.42819476e+00\n",
      " 1.99796756e+01 1.83956476e+01 4.25411339e+01 2.09750221e+01\n",
      " 1.92990681e+02 2.94532403e+01 1.01772211e+01 4.44636938e+01\n",
      " 2.34131307e+01 1.02189483e+01 9.97745876e-14]\n",
      "19-th iteration, loss: 0.14649461704634886, 31 gd steps\n",
      "insert gradient: -0.01312567110761292\n",
      "19-th iteration, new layer inserted. now 33 layers\n",
      "[1.58232842e+00 4.60831551e+01 8.35063525e+01 4.57658089e+01\n",
      " 8.13267113e+01 8.03547390e+01 1.07562729e+02 7.72849429e+01\n",
      " 1.26911283e+02 8.01559893e+01 1.33533633e+02 8.27522365e+01\n",
      " 1.10146211e+02 1.40973757e+00 4.19433372e+01 7.50081667e+01\n",
      " 1.89781993e+01 1.10854155e+01 1.48166183e+02 1.12234395e+01\n",
      " 3.90344782e+01 9.05906066e+00 1.17780499e+01 2.63657756e+01\n",
      " 3.17343702e+01 2.58805731e+01 1.85071062e+02 3.28187872e+01\n",
      " 3.63140395e+00 5.34945888e+01 2.39293483e+01 8.37934415e+00\n",
      " 7.75134841e-14]\n",
      "20-th iteration, loss: 0.14565336361407236, 55 gd steps\n",
      "insert gradient: -0.0039052555921720174\n",
      "20-th iteration, new layer inserted. now 35 layers\n",
      "[1.71430432e+00 4.66189382e+01 8.40426242e+01 4.53626733e+01\n",
      " 8.15016974e+01 7.99618410e+01 1.07810473e+02 7.66055932e+01\n",
      " 1.27499610e+02 7.98146860e+01 1.32988381e+02 8.30789597e+01\n",
      " 1.09288464e+02 1.57842227e+00 4.01094709e+01 7.63039932e+01\n",
      " 2.11929247e+01 8.98330065e+00 1.47414407e+02 1.49312394e+01\n",
      " 3.54820655e+01 1.08112494e+01 6.01999614e+00 3.08720213e+01\n",
      " 2.63449208e+01 2.81073096e+01 1.55308092e+02 0.00000000e+00\n",
      " 2.58846820e+01 3.40293177e+01 9.58446055e-01 5.58820113e+01\n",
      " 2.49483835e+01 7.61545810e+00 6.46728699e-14]\n",
      "21-th iteration, loss: 0.14553301264239604, 14 gd steps\n",
      "insert gradient: -0.008573793623664715\n",
      "21-th iteration, new layer inserted. now 35 layers\n",
      "[1.64925728e+00 4.63128031e+01 8.38483962e+01 4.50444640e+01\n",
      " 8.14073929e+01 7.99847580e+01 1.07839483e+02 7.66111171e+01\n",
      " 1.27561760e+02 7.98530835e+01 1.32977427e+02 8.31091755e+01\n",
      " 1.09258968e+02 1.59937288e+00 3.99736610e+01 7.64222711e+01\n",
      " 2.13565486e+01 8.79893307e+00 1.47352815e+02 1.53057510e+01\n",
      " 3.51979293e+01 1.09925241e+01 5.41384295e+00 3.12522497e+01\n",
      " 2.58184055e+01 2.82149225e+01 1.54979679e+02 6.83531375e-01\n",
      " 2.55559956e+01 3.41681693e+01 6.13892131e-01 5.60549082e+01\n",
      " 2.50172632e+01 7.55612436e+00 6.46828588e-14]\n",
      "22-th iteration, loss: 0.14541803303085166, 16 gd steps\n",
      "insert gradient: -0.001659424905225873\n",
      "22-th iteration, new layer inserted. now 37 layers\n",
      "[1.67557220e+00 4.64144912e+01 8.39680694e+01 4.52110893e+01\n",
      " 8.14436154e+01 7.98644210e+01 1.07837435e+02 7.64989936e+01\n",
      " 1.27575522e+02 7.97801941e+01 1.32949089e+02 8.30333855e+01\n",
      " 1.09225689e+02 1.57877404e+00 3.98330201e+01 7.65450131e+01\n",
      " 2.15200850e+01 8.61666399e+00 1.47300001e+02 0.00000000e+00\n",
      " 2.13162821e-14 1.56202067e+01 3.49028818e+01 1.11648345e+01\n",
      " 4.78869040e+00 3.16024346e+01 2.52577185e+01 2.82550520e+01\n",
      " 1.54654203e+02 1.13735399e+00 2.52258528e+01 3.43249208e+01\n",
      " 1.62011206e-01 5.62271219e+01 2.50583450e+01 7.53520798e+00\n",
      " 6.54203737e-14]\n",
      "23-th iteration, loss: 0.145062634657948, 22 gd steps\n",
      "insert gradient: -0.002816700045825729\n",
      "23-th iteration, new layer inserted. now 35 layers\n",
      "[1.69722267e+00 4.63080749e+01 8.39052290e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.51107628e+01 8.13146188e+01 7.97921124e+01\n",
      " 1.07921407e+02 7.63648914e+01 1.27726206e+02 7.97651255e+01\n",
      " 1.32911379e+02 8.29309978e+01 1.09185787e+02 1.53161683e+00\n",
      " 3.92412550e+01 7.72065649e+01 2.22448059e+01 7.51950175e+00\n",
      " 1.47099136e+02 1.78326144e+01 3.31933842e+01 1.23961377e+01\n",
      " 1.41744624e+00 3.34429080e+01 2.28755179e+01 2.89088951e+01\n",
      " 1.53519553e+02 1.98636904e+00 2.40579767e+01 9.18453987e+01\n",
      " 2.51180769e+01 7.33959003e+00 7.62734217e-14]\n",
      "24-th iteration, loss: 0.1445447549797789, 100 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[1.76033058e+00 4.62151455e+01 8.39001926e+01 4.50706841e+01\n",
      " 8.09560894e+01 7.96247477e+01 1.07647959e+02 7.61250049e+01\n",
      " 1.27595928e+02 7.96139899e+01 1.32875090e+02 8.23896805e+01\n",
      " 1.09779878e+02 1.13617227e+00 1.09297302e+01 0.00000000e+00\n",
      " 2.73243256e+01 7.86024279e+01 2.31414674e+01 5.93321771e+00\n",
      " 1.45693085e+02 1.97588641e+01 2.81965839e+01 5.51102603e+01\n",
      " 1.52586320e+01 3.17846560e+01 1.51854494e+02 1.09998842e+00\n",
      " 2.25276469e+01 9.14996333e+01 2.56366194e+01 7.16460844e+00\n",
      " 7.02343903e-14]\n",
      "25-th iteration, loss: 0.14442816066228298, 19 gd steps\n",
      "insert gradient: -0.0050472557780903305\n",
      "25-th iteration, new layer inserted. now 33 layers\n",
      "[1.63465471e+00 0.00000000e+00 5.55111512e-17 4.57532849e+01\n",
      " 8.37778722e+01 4.54370998e+01 8.12304249e+01 7.94942979e+01\n",
      " 1.07659188e+02 7.60525900e+01 1.27601582e+02 7.94945908e+01\n",
      " 1.32887315e+02 8.22119652e+01 1.21334747e+02 1.92974290e+00\n",
      " 2.71991073e+01 7.78793641e+01 2.36380447e+01 6.27554644e+00\n",
      " 1.44039451e+02 1.92775388e+01 2.71227648e+01 5.99140683e+01\n",
      " 1.20514758e+01 3.24276104e+01 1.50717094e+02 1.14073566e+00\n",
      " 2.15017871e+01 9.12917136e+01 2.58593369e+01 7.28246602e+00\n",
      " 5.59305312e-14]\n",
      "26-th iteration, loss: 0.1443790733421785, 14 gd steps\n",
      "insert gradient: -0.0008899918566054723\n",
      "26-th iteration, new layer inserted. now 33 layers\n",
      "[1.70246781e+00 4.62056672e+01 8.37843222e+01 4.50282954e+01\n",
      " 8.10751204e+01 7.94245905e+01 1.07651061e+02 7.60085322e+01\n",
      " 1.27602198e+02 7.94710465e+01 1.32875839e+02 8.22193150e+01\n",
      " 1.21261349e+02 1.76202981e+00 2.71003228e+01 7.78133210e+01\n",
      " 2.36295119e+01 6.31329159e+00 1.43928408e+02 1.92290563e+01\n",
      " 2.70888054e+01 0.00000000e+00 2.66453526e-15 6.00520160e+01\n",
      " 1.19190337e+01 3.24146145e+01 1.50681054e+02 1.14609692e+00\n",
      " 2.14710883e+01 9.12970615e+01 2.58619579e+01 7.28293593e+00\n",
      " 5.68960956e-14]\n",
      "27-th iteration, loss: 0.1443680696877277, 15 gd steps\n",
      "insert gradient: -0.0024660367935406347\n",
      "27-th iteration, new layer inserted. now 31 layers\n",
      "[1.70640262e+00 4.61028676e+01 8.37950416e+01 4.49391261e+01\n",
      " 8.09888205e+01 7.94013404e+01 1.07698400e+02 7.59797845e+01\n",
      " 1.27650554e+02 7.94678380e+01 1.32886988e+02 8.22789397e+01\n",
      " 1.21209904e+02 1.74974015e+00 2.70127684e+01 7.77910603e+01\n",
      " 2.36662280e+01 6.35920072e+00 1.43768310e+02 1.91833380e+01\n",
      " 2.70375583e+01 6.05916978e+01 1.16604954e+01 3.23873599e+01\n",
      " 1.50588657e+02 1.14161665e+00 2.13877903e+01 9.12817876e+01\n",
      " 2.58822917e+01 7.27435827e+00 5.66025448e-14]\n",
      "28-th iteration, loss: 0.14436355396857373, 13 gd steps\n",
      "insert gradient: -0.001007165177147982\n",
      "28-th iteration, new layer inserted. now 31 layers\n",
      "[1.73162952e+00 4.61705236e+01 8.38554496e+01 4.50197398e+01\n",
      " 8.09912412e+01 7.93546106e+01 1.07726445e+02 7.59309901e+01\n",
      " 1.27670473e+02 7.94341091e+01 1.32881314e+02 8.22824856e+01\n",
      " 1.21185744e+02 1.76858405e+00 2.69624370e+01 7.77890567e+01\n",
      " 2.37042314e+01 6.39031963e+00 1.43655291e+02 1.91327969e+01\n",
      " 2.70028564e+01 6.07930749e+01 1.14773480e+01 3.23781547e+01\n",
      " 1.50518936e+02 1.14460234e+00 2.13251624e+01 9.12708071e+01\n",
      " 2.58981875e+01 7.28273917e+00 5.77332738e-14]\n",
      "29-th iteration, loss: 0.14431054880829264, 39 gd steps\n",
      "insert gradient: -0.0017979567534684236\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[1.72288459e+00 0.00000000e+00 5.55111512e-17 4.60503503e+01\n",
      " 8.37419910e+01 4.49233781e+01 8.08919370e+01 7.92024664e+01\n",
      " 1.07969721e+02 7.57680753e+01 1.27821585e+02 7.93515194e+01\n",
      " 1.32839642e+02 8.23247000e+01 1.21216251e+02 1.92162794e+00\n",
      " 2.66674971e+01 7.77239894e+01 2.42901352e+01 6.57333968e+00\n",
      " 1.42247543e+02 1.86935267e+01 2.69513736e+01 6.35019896e+01\n",
      " 9.79680603e+00 3.24827816e+01 1.49911768e+02 1.09833070e+00\n",
      " 2.08092067e+01 9.12272882e+01 2.60129068e+01 7.30166661e+00\n",
      " 7.02731356e-14]\n",
      "30-th iteration, loss: 0.14427884308375133, 39 gd steps\n",
      "insert gradient: -0.0014030101445473215\n",
      "30-th iteration, new layer inserted. now 31 layers\n",
      "[1.69994012e+00 4.59428325e+01 8.37564225e+01 4.49524599e+01\n",
      " 8.09706353e+01 7.90542282e+01 1.08103793e+02 7.56511450e+01\n",
      " 1.27874539e+02 7.92691932e+01 1.32845957e+02 8.22850216e+01\n",
      " 1.21382536e+02 1.95749337e+00 2.64392945e+01 7.78760385e+01\n",
      " 2.49926479e+01 6.67776365e+00 1.40755253e+02 1.82394328e+01\n",
      " 2.70886738e+01 6.57688317e+01 8.49403649e+00 3.23218058e+01\n",
      " 1.49426980e+02 1.05263708e+00 2.04146393e+01 9.12299648e+01\n",
      " 2.61445014e+01 7.32265528e+00 6.73687920e-14]\n",
      "31-th iteration, loss: 0.14425370919723238, 67 gd steps\n",
      "insert gradient: -0.00035483823689946907\n",
      "31-th iteration, new layer inserted. now 33 layers\n",
      "[1.73728948e+00 4.60673450e+01 8.36941111e+01 4.48904794e+01\n",
      " 8.08544271e+01 7.89810178e+01 1.08186402e+02 7.55691578e+01\n",
      " 1.27973355e+02 7.92357516e+01 1.32792005e+02 8.23311178e+01\n",
      " 1.21500264e+02 2.09159387e+00 2.60501425e+01 7.79869624e+01\n",
      " 2.59851001e+01 6.74977587e+00 1.38995528e+02 1.79011064e+01\n",
      " 2.74276574e+01 0.00000000e+00 4.44089210e-15 6.75306399e+01\n",
      " 7.54421876e+00 3.21315927e+01 1.49045105e+02 9.91906056e-01\n",
      " 2.01338221e+01 9.12589871e+01 2.63335376e+01 7.31421368e+00\n",
      " 5.93426844e-14]\n",
      "32-th iteration, loss: 0.14425287676280882, 11 gd steps\n",
      "insert gradient: -0.00028254961059637906\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[1.73531523e+00 4.60595595e+01 8.36890362e+01 4.48777332e+01\n",
      " 8.08495833e+01 7.89789835e+01 1.08190619e+02 7.55680133e+01\n",
      " 1.27976846e+02 7.92351645e+01 1.32792557e+02 8.23326613e+01\n",
      " 1.21503366e+02 2.09263111e+00 2.60427642e+01 7.79888562e+01\n",
      " 2.60025492e+01 6.75143768e+00 1.38959511e+02 1.78928477e+01\n",
      " 2.74326493e+01 2.44073635e-02 4.87255501e-03 0.00000000e+00\n",
      " 8.67361738e-19 6.75550800e+01 7.52669512e+00 3.21246510e+01\n",
      " 1.49038837e+02 9.90753389e-01 2.01294367e+01 9.12602559e+01\n",
      " 2.63370066e+01 7.31320594e+00 5.94458099e-14]\n",
      "33-th iteration, loss: 0.14422777278420396, 68 gd steps\n",
      "insert gradient: -0.001595969728009611\n",
      "33-th iteration, new layer inserted. now 31 layers\n",
      "[1.72307667e+00 4.59970652e+01 8.36188161e+01 4.48132410e+01\n",
      " 8.08549295e+01 7.88447729e+01 1.08314800e+02 7.54818015e+01\n",
      " 1.28053152e+02 7.91976087e+01 1.32802075e+02 8.23415014e+01\n",
      " 1.21870122e+02 2.18828542e+00 2.55717030e+01 7.82174666e+01\n",
      " 2.73259364e+01 6.85362916e+00 1.36121629e+02 1.74504365e+01\n",
      " 2.79338288e+01 7.05349411e+01 6.41074445e+00 3.12299594e+01\n",
      " 1.48595703e+02 9.33558557e-01 1.98275782e+01 9.13803265e+01\n",
      " 2.66313469e+01 7.29134232e+00 5.92046611e-14]\n",
      "34-th iteration, loss: 0.144226405679713, 18 gd steps\n",
      "insert gradient: -0.0008625022899302557\n",
      "34-th iteration, new layer inserted. now 33 layers\n",
      "[1.72782332e+00 4.60174266e+01 8.36270896e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.48143874e+01 8.08316834e+01 7.88635040e+01\n",
      " 1.08331862e+02 7.54670480e+01 1.28058713e+02 7.91782749e+01\n",
      " 1.32790916e+02 8.23397807e+01 1.21873947e+02 2.19540112e+00\n",
      " 2.55435969e+01 7.82260160e+01 2.73861842e+01 6.87163611e+00\n",
      " 1.36028973e+02 1.74241780e+01 2.79229004e+01 7.05496993e+01\n",
      " 6.38495341e+00 3.12208570e+01 1.48583705e+02 9.32626007e-01\n",
      " 1.98209027e+01 9.13799523e+01 2.66448367e+01 7.28878814e+00\n",
      " 5.89420867e-14]\n",
      "35-th iteration, loss: 0.1442191916163885, 28 gd steps\n",
      "insert gradient: -0.00016821073174104988\n",
      "35-th iteration, new layer inserted. now 35 layers\n",
      "[1.72799441e+00 4.60206222e+01 8.36738906e+01 3.94958435e-02\n",
      " 1.13082326e-02 4.47679382e+01 8.08099497e+01 7.88423565e+01\n",
      " 1.08395696e+02 7.54235616e+01 1.28090716e+02 7.91529368e+01\n",
      " 1.32785074e+02 8.23302555e+01 1.22051111e+02 2.22415405e+00\n",
      " 2.53066886e+01 7.83979737e+01 0.00000000e+00 1.06581410e-14\n",
      " 2.80520658e+01 6.87087148e+00 1.34846387e+02 1.73467823e+01\n",
      " 2.80958513e+01 7.08661748e+01 6.20324742e+00 3.12015111e+01\n",
      " 1.48496283e+02 9.00079552e-01 1.97983566e+01 9.14465644e+01\n",
      " 2.67631224e+01 7.27248868e+00 7.01616819e-14]\n",
      "36-th iteration, loss: 0.14420884823345154, 323 gd steps\n",
      "insert gradient: -0.0003336247166770858\n",
      "36-th iteration, new layer inserted. now 31 layers\n",
      "[1.73057305e+00 4.60199094e+01 8.36151689e+01 4.48027442e+01\n",
      " 8.07778291e+01 7.87694447e+01 1.08498773e+02 7.53589891e+01\n",
      " 1.28191313e+02 7.91318926e+01 1.32776182e+02 8.23641674e+01\n",
      " 1.22267807e+02 2.31630670e+00 2.48902514e+01 7.85459672e+01\n",
      " 2.91500538e+01 6.88209631e+00 1.33034658e+02 1.72531733e+01\n",
      " 2.84446505e+01 7.11827998e+01 5.95053838e+00 3.12412413e+01\n",
      " 1.48398760e+02 8.32223181e-01 1.97928354e+01 9.15449017e+01\n",
      " 2.69122467e+01 7.23475657e+00 7.17908273e-14]\n",
      "37-th iteration, loss: 0.14419420772051836, 71 gd steps\n",
      "insert gradient: -0.0011877746239162643\n",
      "37-th iteration, new layer inserted. now 33 layers\n",
      "[1.71080146e+00 4.60033264e+01 8.36713763e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.47509763e+01 8.07205808e+01 7.87226453e+01\n",
      " 1.08608891e+02 7.52912443e+01 1.28278662e+02 7.90982052e+01\n",
      " 1.32778593e+02 8.23759819e+01 1.22912157e+02 2.42963547e+00\n",
      " 2.41050637e+01 7.88497169e+01 3.07841766e+01 6.94270066e+00\n",
      " 1.29734784e+02 1.71625417e+01 2.89175017e+01 7.17576661e+01\n",
      " 5.58735664e+00 3.13156995e+01 1.48276375e+02 6.94296590e-01\n",
      " 1.98462777e+01 9.18385679e+01 2.73043461e+01 7.15485766e+00\n",
      " 6.21992993e-14]\n",
      "38-th iteration, loss: 0.14419357715638462, 19 gd steps\n",
      "insert gradient: -0.00011029390504362923\n",
      "38-th iteration, new layer inserted. now 33 layers\n",
      "[1.72265492e+00 4.60192827e+01 8.36325258e+01 4.47969046e+01\n",
      " 8.07409116e+01 7.87134466e+01 1.08611523e+02 7.52865600e+01\n",
      " 1.28283406e+02 7.90975796e+01 1.32779483e+02 8.23782707e+01\n",
      " 1.22920409e+02 2.43346463e+00 2.40880758e+01 7.88534647e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.08117622e+01 6.94541205e+00\n",
      " 1.29685346e+02 1.71619048e+01 2.89276397e+01 7.17632139e+01\n",
      " 5.57947082e+00 3.13163985e+01 1.48272298e+02 6.90829596e-01\n",
      " 1.98449389e+01 9.18413474e+01 2.73115314e+01 7.15314079e+00\n",
      " 6.31716285e-14]\n",
      "39-th iteration, loss: 0.1441902996210677, 25 gd steps\n",
      "insert gradient: -0.0004999986271998388\n",
      "39-th iteration, new layer inserted. now 33 layers\n",
      "[1.71919759e+00 4.60249192e+01 8.36678356e+01 4.47317985e+01\n",
      " 8.07684896e+01 7.86592243e+01 1.08654746e+02 7.52605033e+01\n",
      " 1.28307156e+02 7.90890039e+01 1.32795258e+02 8.23657889e+01\n",
      " 1.23131077e+02 2.43623379e+00 2.38705425e+01 7.89402991e+01\n",
      " 2.58640559e-01 5.57142764e-02 3.10749126e+01 6.94869137e+00\n",
      " 1.28768625e+02 1.71150494e+01 2.91035214e+01 7.18540686e+01\n",
      " 5.47540243e+00 3.13499987e+01 1.48232616e+02 6.61521400e-01\n",
      " 1.98523192e+01 9.19149574e+01 2.74089302e+01 7.13579119e+00\n",
      " 1.12212341e-13]\n",
      "40-th iteration, loss: 0.14418799408183, 35 gd steps\n",
      "insert gradient: -0.00010879127454138143\n",
      "40-th iteration, new layer inserted. now 33 layers\n",
      "[1.71218707e+00 0.00000000e+00 1.66533454e-16 4.59737196e+01\n",
      " 8.36352695e+01 4.48245618e+01 8.07806237e+01 7.86531301e+01\n",
      " 1.08695068e+02 7.52412223e+01 1.28353753e+02 7.90830066e+01\n",
      " 1.32784603e+02 8.23931740e+01 1.23252666e+02 2.50356247e+00\n",
      " 2.36524792e+01 7.89939498e+01 3.16819588e+01 6.98017015e+00\n",
      " 1.28092425e+02 1.71132904e+01 2.92255959e+01 7.19302926e+01\n",
      " 5.39771442e+00 3.14049405e+01 1.48208929e+02 6.23294855e-01\n",
      " 1.98638613e+01 9.19674736e+01 2.74885613e+01 7.11257248e+00\n",
      " 1.23383555e-13]\n",
      "41-th iteration, loss: 0.14418743651469476, 11 gd steps\n",
      "insert gradient: -0.00017116665906637121\n",
      "41-th iteration, new layer inserted. now 33 layers\n",
      "[1.72068362e+00 2.50592597e-02 7.33972625e-03 4.59992893e+01\n",
      " 8.36133154e+01 4.47667629e+01 8.07618739e+01 7.86524208e+01\n",
      " 1.08692977e+02 7.52388991e+01 1.28351719e+02 7.90812562e+01\n",
      " 1.32782945e+02 8.23906837e+01 1.23256455e+02 2.50233661e+00\n",
      " 2.36489484e+01 7.89922447e+01 3.16851715e+01 6.98051762e+00\n",
      " 1.28076875e+02 1.71133237e+01 2.92276371e+01 7.19323137e+01\n",
      " 5.39707724e+00 3.14069656e+01 1.48209944e+02 6.23648637e-01\n",
      " 1.98656817e+01 9.19699566e+01 2.74898800e+01 7.11335635e+00\n",
      " 1.22791448e-13]\n",
      "42-th iteration, loss: 0.14418653767990597, 49 gd steps\n",
      "insert gradient: -9.192548803363282e-05\n",
      "42-th iteration, new layer inserted. now 33 layers\n",
      "[1.73094004e+00 4.60328470e+01 8.35933466e+01 4.47889930e+01\n",
      " 8.07466405e+01 0.00000000e+00 3.55271368e-15 7.86542869e+01\n",
      " 1.08708078e+02 7.52284613e+01 1.28361981e+02 7.90743926e+01\n",
      " 1.32780064e+02 8.23900219e+01 1.23318719e+02 2.51798860e+00\n",
      " 2.35714953e+01 7.90036761e+01 3.17849730e+01 7.00073245e+00\n",
      " 1.27819150e+02 1.71021015e+01 2.92766761e+01 7.19583625e+01\n",
      " 5.35927325e+00 3.14254910e+01 1.48196979e+02 6.08568411e-01\n",
      " 1.98665712e+01 9.19926620e+01 2.75234019e+01 7.10807271e+00\n",
      " 1.26253253e-13]\n",
      "43-th iteration, loss: 0.14418639533907032, 13 gd steps\n",
      "insert gradient: -0.0004723323114578157\n",
      "43-th iteration, new layer inserted. now 33 layers\n",
      "[1.72749535e+00 4.60159579e+01 8.35843404e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.47775718e+01 8.07428077e+01 7.86561532e+01\n",
      " 1.08710257e+02 7.52292610e+01 1.28364480e+02 7.90756829e+01\n",
      " 1.32780095e+02 8.23918953e+01 1.23329845e+02 2.52116242e+00\n",
      " 2.35584349e+01 7.90053806e+01 3.18042166e+01 7.00322691e+00\n",
      " 1.27776052e+02 1.71023190e+01 2.92856616e+01 7.19643570e+01\n",
      " 5.35443413e+00 3.14302801e+01 1.48195625e+02 6.06447514e-01\n",
      " 1.98675984e+01 9.19970373e+01 2.75291873e+01 7.10652246e+00\n",
      " 1.27854022e-13]\n",
      "44-th iteration, loss: 0.14418377510598412, 37 gd steps\n",
      "insert gradient: -0.00060664709478416\n",
      "44-th iteration, new layer inserted. now 31 layers\n",
      "[1.72696298e+00 4.59956220e+01 8.35506835e+01 4.48177760e+01\n",
      " 8.07307647e+01 7.86314413e+01 1.08744946e+02 7.52079170e+01\n",
      " 1.28388696e+02 7.90652893e+01 1.32784844e+02 8.23910387e+01\n",
      " 1.23564793e+02 2.55417908e+00 2.33019294e+01 7.90747920e+01\n",
      " 3.21962574e+01 7.03829113e+00 1.26942003e+02 1.70730033e+01\n",
      " 2.94337951e+01 7.20589604e+01 5.25210885e+00 3.14814483e+01\n",
      " 1.48161700e+02 5.65674818e-01 1.98798312e+01 9.20853261e+01\n",
      " 2.76509503e+01 7.08557389e+00 1.22356050e-13]\n",
      "45-th iteration, loss: 0.14417834656186798, 112 gd steps\n",
      "insert gradient: -0.0001457766422107635\n",
      "45-th iteration, new layer inserted. now 33 layers\n",
      "[1.72396062e+00 0.00000000e+00 1.66533454e-16 4.60140190e+01\n",
      " 8.35867590e+01 4.47644582e+01 8.07324463e+01 7.85841047e+01\n",
      " 1.08820555e+02 7.51620192e+01 1.28450883e+02 7.90471538e+01\n",
      " 1.32793005e+02 8.23954487e+01 1.24131900e+02 2.64206492e+00\n",
      " 2.26684332e+01 7.91866062e+01 3.30667718e+01 7.11003267e+00\n",
      " 1.25125750e+02 1.70427374e+01 2.97430400e+01 7.22305521e+01\n",
      " 5.04925309e+00 3.16111175e+01 1.48098427e+02 4.73973118e-01\n",
      " 1.99200653e+01 9.22820472e+01 2.79361321e+01 7.03212841e+00\n",
      " 9.55550808e-14]\n",
      "46-th iteration, loss: 0.14417803268169757, 12 gd steps\n",
      "insert gradient: -6.657902592837738e-05\n",
      "46-th iteration, new layer inserted. now 33 layers\n",
      "[1.73466291e+00 4.60325437e+01 8.35630332e+01 4.47800796e+01\n",
      " 8.07222843e+01 7.85914823e+01 1.08829756e+02 7.51584135e+01\n",
      " 1.28458011e+02 7.90434475e+01 1.32786860e+02 8.23991058e+01\n",
      " 1.24168440e+02 2.65283443e+00 2.26134639e+01 7.91889071e+01\n",
      " 0.00000000e+00 1.06581410e-14 3.31315705e+01 7.11539413e+00\n",
      " 1.25001962e+02 1.70446049e+01 2.97680814e+01 7.22414947e+01\n",
      " 5.03352049e+00 3.16225344e+01 1.48092727e+02 4.64794357e-01\n",
      " 1.99218430e+01 9.22922324e+01 2.79577546e+01 7.02594077e+00\n",
      " 9.94580776e-14]\n",
      "47-th iteration, loss: 0.14417730893246594, 25 gd steps\n",
      "insert gradient: -6.490453458827058e-05\n",
      "47-th iteration, new layer inserted. now 33 layers\n",
      "[1.72838392e+00 4.60315256e+01 8.35932978e+01 4.47641526e+01\n",
      " 8.07261901e+01 7.85885318e+01 1.08838307e+02 7.51516358e+01\n",
      " 1.28466989e+02 7.90408242e+01 1.32786221e+02 8.23988434e+01\n",
      " 1.24261676e+02 2.66738965e+00 2.25026504e+01 7.92015558e+01\n",
      " 8.94164860e-02 5.93084242e-03 3.32209871e+01 7.11657940e+00\n",
      " 1.24708831e+02 1.70451852e+01 2.98207510e+01 7.22587035e+01\n",
      " 5.00222261e+00 3.16480166e+01 1.48082517e+02 4.49866220e-01\n",
      " 1.99280906e+01 9.23200214e+01 2.80001469e+01 7.01633241e+00\n",
      " 1.04255826e-13]\n",
      "48-th iteration, loss: 0.14417724031815018, 10 gd steps\n",
      "insert gradient: -5.07114247112246e-05\n",
      "48-th iteration, new layer inserted. now 33 layers\n",
      "[1.72785681e+00 4.60261354e+01 8.35871010e+01 4.47660355e+01\n",
      " 8.07249195e+01 7.85860912e+01 1.08837696e+02 7.51500246e+01\n",
      " 1.28466670e+02 7.90398529e+01 1.32785461e+02 8.23977597e+01\n",
      " 1.24265344e+02 2.66778982e+00 2.24971771e+01 7.92012555e+01\n",
      " 9.29069370e-02 3.94979902e-03 3.32245793e+01 7.11742879e+00\n",
      " 1.24695764e+02 1.70455091e+01 2.98230201e+01 7.22594614e+01\n",
      " 5.00059545e+00 3.16493632e+01 1.48082442e+02 4.49094677e-01\n",
      " 1.99287530e+01 9.23213807e+01 2.80019631e+01 7.01645028e+00\n",
      " 1.04065696e-13]\n",
      "49-th iteration, loss: 0.14417714797570058, 15 gd steps\n",
      "insert gradient: -8.90458675529886e-05\n",
      "49-th iteration, new layer inserted. now 35 layers\n",
      "[1.72850068e+00 4.60228336e+01 8.35771002e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.47637520e+01 8.07214492e+01 7.85854251e+01\n",
      " 1.08839462e+02 7.51505537e+01 1.28469128e+02 7.90413697e+01\n",
      " 1.32785968e+02 8.23989833e+01 1.24275121e+02 2.67053347e+00\n",
      " 2.24852353e+01 7.92024163e+01 1.00820626e-01 9.34568103e-04\n",
      " 3.32326390e+01 7.11921379e+00 1.24664906e+02 1.70461579e+01\n",
      " 2.98285619e+01 7.22612918e+01 4.99723388e+00 3.16525530e+01\n",
      " 1.48081691e+02 4.47246039e-01 1.99297411e+01 9.23241967e+01\n",
      " 2.80063459e+01 7.01549672e+00 1.05258231e-13]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.53646652041829\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  44.80232488    0.         1814.49415753]\n",
      "1-th iteration, loss: 0.7498498981810913, 11 gd steps\n",
      "insert gradient: -0.614251204140616\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  43.18401986   62.29250037  243.40775284    0.         1571.08640469]\n",
      "2-th iteration, loss: 0.6089410267357657, 14 gd steps\n",
      "insert gradient: -0.6452192028766961\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.89257665   76.47071447  226.95092798   37.98108443  256.50390281\n",
      "    0.         1314.58250189]\n",
      "3-th iteration, loss: 0.45366339920287774, 20 gd steps\n",
      "insert gradient: -0.7624262960403181\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           43.15474977  114.1933679     0.          107.84929191\n",
      "   84.53736023  118.63788493   69.18145114 1314.58250189]\n",
      "4-th iteration, loss: 0.3632810602383317, 80 gd steps\n",
      "insert gradient: -0.4644145672014258\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   0.           44.40928439   55.21426241   48.92337998  106.82199704\n",
      "   76.82013469  123.12825712   80.14237833  194.75296324    0.\n",
      " 1119.82953864]\n",
      "5-th iteration, loss: 0.24388213742357978, 39 gd steps\n",
      "insert gradient: -0.1998445663489944\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.35789937  44.16199127  76.01442107  44.12063282  95.53554002\n",
      "  79.76313423 118.23869816  80.096938   140.22899673  76.35783216\n",
      " 203.60537066   0.         916.22416798]\n",
      "6-th iteration, loss: 0.21015766089593502, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          42.21238782  87.50749446  49.29062631  83.27034147\n",
      "  87.29616269 113.42699489  84.66457952 125.72082851  81.4101128\n",
      " 180.20406489  37.95243506 916.22416798]\n",
      "7-th iteration, loss: 0.18790396993936895, 40 gd steps\n",
      "insert gradient: -0.06255486653596246\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[3.95260929e+00 4.51827211e+01 8.49596840e+01 4.41731178e+01\n",
      " 8.54572657e+01 8.28232518e+01 1.07930511e+02 8.47311432e+01\n",
      " 1.23304650e+02 8.29383971e+01 1.54908771e+02 7.74115170e+01\n",
      " 9.16224168e+02 0.00000000e+00 2.98427949e-13]\n",
      "8-th iteration, loss: 0.18176173064362822, 37 gd steps\n",
      "insert gradient: -0.0596122718332395\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[2.60231604e+00 4.74478625e+01 8.42437819e+01 4.42337937e+01\n",
      " 8.58099081e+01 8.24879029e+01 1.09110722e+02 8.37307607e+01\n",
      " 1.22458221e+02 8.44897651e+01 1.52729831e+02 8.15916064e+01\n",
      " 1.71971755e+02 0.00000000e+00 7.45210939e+02 1.31746358e+01\n",
      " 2.79194024e-13]\n",
      "9-th iteration, loss: 0.17553204633133157, 44 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[2.42955205e+00 4.94911051e+01 8.58578621e+01 4.44797444e+01\n",
      " 8.30667382e+01 8.32787084e+01 1.10311491e+02 8.25878980e+01\n",
      " 1.23031664e+02 8.69086868e+01 1.48832034e+02 7.69172538e+01\n",
      " 1.74600110e+02 1.75618369e+01 5.22520105e+01 0.00000000e+00\n",
      " 6.79276137e+02 1.19847148e+01 2.29880227e-13]\n",
      "10-th iteration, loss: 0.1640312347406675, 17 gd steps\n",
      "insert gradient: -0.018876945101857726\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[3.81686006e+00 5.45912853e+01 8.20009706e+01 4.43605131e+01\n",
      " 8.44915190e+01 8.60945094e+01 1.06073945e+02 8.31083194e+01\n",
      " 1.19933374e+02 8.43468572e+01 1.47067708e+02 8.02358083e+01\n",
      " 1.61624753e+02 5.12234057e+01 1.20472231e+01 2.85210769e+01\n",
      " 3.66143788e+02 0.00000000e+00 2.61531277e+02 7.63310711e+00\n",
      " 6.26304547e-14]\n",
      "11-th iteration, loss: 0.15821968501888808, 47 gd steps\n",
      "insert gradient: -0.021486624545215022\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[2.02491474e+00 4.86826891e+01 8.65063406e+01 4.53554211e+01\n",
      " 8.37897364e+01 8.51002822e+01 1.05171001e+02 8.35767241e+01\n",
      " 1.21715888e+02 8.46040857e+01 1.43256300e+02 7.90464629e+01\n",
      " 1.60115771e+02 5.93917722e+01 6.02636710e+00 2.90032448e+01\n",
      " 2.29864807e+02 0.00000000e+00 1.31351318e+02 3.82468304e+00\n",
      " 2.54410659e+02 6.45067488e+00 3.21401282e-15]\n",
      "12-th iteration, loss: 0.15702525339053555, 22 gd steps\n",
      "insert gradient: -0.0073762512224456196\n",
      "12-th iteration, new layer inserted. now 24 layers\n",
      "[1.96563064e+00 4.86964046e+01 8.65031010e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.56800060e+01 8.45574276e+01 8.56515115e+01\n",
      " 1.04163624e+02 8.39616577e+01 1.20937416e+02 8.52066320e+01\n",
      " 1.41873315e+02 8.01927857e+01 1.59570043e+02 6.19061210e+01\n",
      " 5.43493085e+00 2.80472473e+01 2.24095303e+02 5.68751055e+00\n",
      " 1.26552788e+02 2.15298324e+00 2.50686763e+02 6.31888423e+00]\n",
      "13-th iteration, loss: 0.1566745460966018, 400 gd steps\n",
      "insert gradient: -0.005712889361619283\n",
      "13-th iteration, new layer inserted. now 24 layers\n",
      "[  1.86952013  47.97278893  86.12027404  46.4189607   84.54123478\n",
      "  85.57396055 103.94251154  83.78128774 121.11979764  84.86913844\n",
      " 140.89377009  80.70436621 159.38577709  63.84052092   5.09635564\n",
      "  27.29015207 219.43292161   6.72700697 123.41675243   2.38868128\n",
      " 199.0946566    0.          49.77366415   6.33752352]\n",
      "14-th iteration, loss: 0.1559910135198065, 64 gd steps\n",
      "insert gradient: -0.009654568971891056\n",
      "14-th iteration, new layer inserted. now 26 layers\n",
      "[  2.04608131  47.83250646  85.46785368  45.56208548  83.67849506\n",
      "  84.26126834 104.39813148  82.42045362 121.96155963  83.77039821\n",
      " 139.28599967  80.74268395 158.1810676   67.75725938   5.33982423\n",
      "  24.26964131 214.85288938   6.03893194  65.67892204   0.\n",
      "  52.54313763   3.37910561 201.09162505   9.8547304   47.39289601\n",
      "  13.00746008]\n",
      "15-th iteration, loss: 0.1522726633425253, 65 gd steps\n",
      "insert gradient: -0.006895347371370255\n",
      "15-th iteration, new layer inserted. now 28 layers\n",
      "[1.69582853e+00 0.00000000e+00 1.66533454e-16 4.69803648e+01\n",
      " 8.47931325e+01 4.61985567e+01 8.35845276e+01 8.32888993e+01\n",
      " 1.05728218e+02 8.07632641e+01 1.24222511e+02 8.23527818e+01\n",
      " 1.37703858e+02 8.19450522e+01 1.59477301e+02 6.84775054e+01\n",
      " 8.11900816e+00 2.28334132e+01 2.12349976e+02 1.10613414e+01\n",
      " 5.90132762e+01 1.21495736e+01 4.42676494e+01 9.20438064e+00\n",
      " 2.00461698e+02 2.15546806e+01 4.10060222e+01 1.77197062e+01]\n",
      "16-th iteration, loss: 0.1521071460220519, 14 gd steps\n",
      "insert gradient: -0.005864788948638699\n",
      "16-th iteration, new layer inserted. now 28 layers\n",
      "[  1.63937454  46.81301235  85.1713732   46.52743904  83.73619924\n",
      "  82.85652168 105.96173594  80.45270076 124.52738686  82.08754763\n",
      " 137.21808815  82.11218252 159.59643653  68.61059922   8.57361986\n",
      "  22.46737403 212.1665885   11.05804601  56.95352975  12.77128254\n",
      "  42.63325998  10.23734257 155.59335452   0.          44.45524415\n",
      "  22.59213341  39.45621703  18.41332659]\n",
      "17-th iteration, loss: 0.15150168469637398, 29 gd steps\n",
      "insert gradient: -0.004676830211609772\n",
      "17-th iteration, new layer inserted. now 30 layers\n",
      "[  2.03121565  48.4451505   84.51874324  44.95993249  83.09788257\n",
      "  82.70007159 106.48663386  79.92046751 124.97166029  81.69232442\n",
      " 136.22603587  82.23679255 159.57115642  68.92386646   9.62298084\n",
      "  21.41882215 132.35555319   0.          79.41333192  10.80713814\n",
      "  52.25859746  14.06421626  40.92124544  11.74911929 154.41685319\n",
      "   3.48382862  43.13122129  26.50236537  35.5520432   18.75806042]\n",
      "18-th iteration, loss: 0.1491424575134772, 50 gd steps\n",
      "insert gradient: -0.006711805563398081\n",
      "18-th iteration, new layer inserted. now 30 layers\n",
      "[  1.59057765  46.98407397  83.81725583  45.6355501   82.03717131\n",
      "  81.17574294 106.91787047  78.51586789 125.71312257  80.85209467\n",
      " 134.59175658  82.35052355 158.3024499   70.88959036  12.27373277\n",
      "  17.22630135 176.89196134   0.          25.27028019   9.51819994\n",
      "  41.83328738  22.33803431  38.75496895  18.57391615 150.4930964\n",
      "   6.54589217  35.49925904  40.39138769  24.21703946  21.60134697]\n",
      "19-th iteration, loss: 0.1481690170085364, 26 gd steps\n",
      "insert gradient: -0.009557172077636431\n",
      "19-th iteration, new layer inserted. now 30 layers\n",
      "[  1.78147628  47.07053168  83.74419077  44.91477202  81.65369333\n",
      "  80.67256704 107.58580149  77.86654185 126.53650972  80.67322943\n",
      " 133.72468033  82.46647525 156.475973    70.68353645  12.5648648\n",
      "  14.90493591 172.98426067   3.81113662  21.37230666   9.13390879\n",
      "  39.18841082  26.77408589  36.1954183   20.60662821 149.3945701\n",
      "   7.31435782  32.7283347   45.69573807  21.42135166  21.66098405]\n",
      "20-th iteration, loss: 0.14494545465883477, 75 gd steps\n",
      "insert gradient: -0.002571248269587123\n",
      "20-th iteration, new layer inserted. now 32 layers\n",
      "[1.61807498e+00 4.63480491e+01 8.47192055e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.48561080e+01 8.13470339e+01 8.02540486e+01\n",
      " 1.06337906e+02 7.69286545e+01 1.26616590e+02 7.99984698e+01\n",
      " 1.32979109e+02 8.18998407e+01 1.51393123e+02 7.65089992e+01\n",
      " 1.16217531e+01 6.43990726e+00 1.58478080e+02 1.13853239e+01\n",
      " 3.71460021e-01 1.24204165e+01 2.32047870e+01 5.48241400e+01\n",
      " 1.97909550e+01 2.72903443e+01 1.48211600e+02 4.58544565e+00\n",
      " 2.60129711e+01 7.15129566e+01 1.15942101e+01 1.76416936e+01]\n",
      "21-th iteration, loss: 0.14488377071243433, 18 gd steps\n",
      "insert gradient: -0.004687024055375187\n",
      "21-th iteration, new layer inserted. now 30 layers\n",
      "[1.68514943e+00 4.61035047e+01 8.42170353e+01 4.51714933e+01\n",
      " 8.12542180e+01 8.02791240e+01 1.06433721e+02 7.68368944e+01\n",
      " 1.26715437e+02 7.99426525e+01 1.32943862e+02 8.19103084e+01\n",
      " 1.51165065e+02 7.66344478e+01 1.18682280e+01 6.36558213e+00\n",
      " 1.58475031e+02 1.12696468e+01 3.19809419e-02 1.23009459e+01\n",
      " 2.32287674e+01 5.56725297e+01 1.94400926e+01 2.71256690e+01\n",
      " 1.48115300e+02 4.62373696e+00 2.59712938e+01 7.17567755e+01\n",
      " 1.20948572e+01 1.71931936e+01]\n",
      "22-th iteration, loss: 0.14485675614079357, 15 gd steps\n",
      "insert gradient: -0.002387088885311092\n",
      "22-th iteration, new layer inserted. now 30 layers\n",
      "[  1.73716287  46.2376607   84.23146483  45.20731583  81.22030181\n",
      "  80.21833986 106.45947031  76.78514456 126.75713245  79.92104854\n",
      " 132.93292286  81.90592756 132.18257421   0.          18.88322489\n",
      "  76.69528986  11.98890974   6.32236765 158.45390601  23.4306378\n",
      "  23.21889882  56.09946764  19.22780914  27.05294534 148.04082388\n",
      "   4.65123731  25.91684153  71.91817233  12.33058193  16.99461297]\n",
      "23-th iteration, loss: 0.14467530292551622, 17 gd steps\n",
      "insert gradient: -0.005834266900919589\n",
      "23-th iteration, new layer inserted. now 32 layers\n",
      "[1.59166404e+00 0.00000000e+00 5.55111512e-17 4.56070311e+01\n",
      " 8.43687408e+01 4.55019440e+01 8.12415464e+01 7.98823640e+01\n",
      " 1.06819382e+02 7.65075702e+01 1.27044010e+02 7.98429244e+01\n",
      " 1.32910225e+02 8.19421916e+01 1.31478310e+02 1.35348164e+00\n",
      " 1.82653586e+01 7.53528611e+01 1.35820643e+01 6.41919402e+00\n",
      " 1.56536722e+02 2.25314966e+01 2.23740486e+01 6.16594497e+01\n",
      " 1.61572358e+01 2.72674078e+01 1.46783406e+02 4.54994240e+00\n",
      " 2.49163201e+01 7.47468686e+01 1.37544469e+01 1.51496784e+01]\n",
      "24-th iteration, loss: 0.14462234692866505, 14 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 32 layers\n",
      "[  1.69406969  46.15857991  84.21215175  45.06860733  81.10806821\n",
      "  79.90832056 106.8420126   76.48507727 127.05979686  79.81465121\n",
      " 132.89127628  81.95210075 131.45406559   1.3914899   18.25028462\n",
      "  75.34315791  13.64836085   6.44583177  22.35096774   0.\n",
      " 134.10580644  22.50703777  22.36723421  61.78855057  16.03595346\n",
      "  27.2600596  146.73605202   4.54151965  24.8847379   74.84020539\n",
      "  13.82167673  15.08021828]\n",
      "25-th iteration, loss: 0.1443890273025988, 27 gd steps\n",
      "insert gradient: -0.0007729120478025906\n",
      "25-th iteration, new layer inserted. now 34 layers\n",
      "[1.70486694e+00 0.00000000e+00 2.77555756e-16 4.60858703e+01\n",
      " 8.38855343e+01 4.51577893e+01 8.11434269e+01 7.95322875e+01\n",
      " 1.07628039e+02 7.60338117e+01 1.27639714e+02 7.95699228e+01\n",
      " 1.32832655e+02 8.22391768e+01 1.30730602e+02 2.62130152e+00\n",
      " 1.77300560e+01 7.61196391e+01 1.31388212e+01 2.61990458e+00\n",
      " 1.94185684e+01 3.73192000e+00 1.31227836e+02 2.10308657e+01\n",
      " 2.32206743e+01 6.76094963e+01 1.27358268e+01 2.80593095e+01\n",
      " 1.45752111e+02 3.71825105e+00 2.42822266e+01 7.90885129e+01\n",
      " 1.50319313e+01 1.28787633e+01]\n",
      "26-th iteration, loss: 0.1443752375098843, 20 gd steps\n",
      "insert gradient: -0.001467192464665476\n",
      "26-th iteration, new layer inserted. now 36 layers\n",
      "[1.72759190e+00 9.46310060e-02 2.93087599e-02 4.61003849e+01\n",
      " 8.39647280e+01 0.00000000e+00 1.06581410e-14 4.49560496e+01\n",
      " 8.09608561e+01 7.94436462e+01 1.07667385e+02 7.59852797e+01\n",
      " 1.27720463e+02 7.95696170e+01 1.32819651e+02 8.22567628e+01\n",
      " 1.30665176e+02 2.66632779e+00 1.76771033e+01 7.62430611e+01\n",
      " 1.31253696e+01 2.30229003e+00 1.93812956e+01 3.94951366e+00\n",
      " 1.31171772e+02 2.08927807e+01 2.34275934e+01 6.77943954e+01\n",
      " 1.24996831e+01 2.81300789e+01 1.45668024e+02 3.66901656e+00\n",
      " 2.42317754e+01 7.93077225e+01 1.52060543e+01 1.27686167e+01]\n",
      "27-th iteration, loss: 0.14437050915820965, 13 gd steps\n",
      "insert gradient: -0.0005643790687971554\n",
      "27-th iteration, new layer inserted. now 36 layers\n",
      "[1.74474433e+00 1.10638596e-01 1.02591663e-02 4.61384046e+01\n",
      " 8.39781513e+01 4.49513053e+01 8.09498517e+01 0.00000000e+00\n",
      " 3.55271368e-15 7.94348099e+01 1.07703101e+02 7.59550340e+01\n",
      " 1.27755661e+02 7.95539592e+01 1.32810558e+02 8.22603592e+01\n",
      " 1.30632262e+02 2.69416092e+00 1.76486751e+01 7.63106394e+01\n",
      " 1.31157188e+01 2.15861389e+00 1.93629503e+01 4.05290154e+00\n",
      " 1.31144721e+02 2.08318454e+01 2.35294547e+01 6.78794067e+01\n",
      " 1.23861222e+01 2.81648438e+01 1.45623424e+02 3.64387496e+00\n",
      " 2.42016860e+01 7.94087372e+01 1.52779216e+01 1.27031940e+01]\n",
      "28-th iteration, loss: 0.14433443027234583, 32 gd steps\n",
      "insert gradient: -0.0010628932227242254\n",
      "28-th iteration, new layer inserted. now 34 layers\n",
      "[1.75285330e+00 0.00000000e+00 3.33066907e-16 4.60808441e+01\n",
      " 8.39273291e+01 4.50095628e+01 8.08997846e+01 7.93508099e+01\n",
      " 1.07877780e+02 7.58349687e+01 1.27896817e+02 7.95032097e+01\n",
      " 1.32778510e+02 8.22823689e+01 1.30515451e+02 2.82485088e+00\n",
      " 1.75588878e+01 7.68011933e+01 1.29576499e+01 1.19922950e+00\n",
      " 1.91563191e+01 4.75649574e+00 1.30900499e+02 2.04168262e+01\n",
      " 2.41781639e+01 6.84769928e+01 1.16705859e+01 2.84624740e+01\n",
      " 1.45373959e+02 3.46366618e+00 2.40537505e+01 8.01714831e+01\n",
      " 1.56647736e+01 1.22989512e+01]\n",
      "29-th iteration, loss: 0.14429404989708045, 22 gd steps\n",
      "insert gradient: -0.0005575533017728714\n",
      "29-th iteration, new layer inserted. now 32 layers\n",
      "[1.73390780e+00 4.63006308e+01 8.37175968e+01 4.50167414e+01\n",
      " 8.08191686e+01 7.92898857e+01 1.08059703e+02 7.56877451e+01\n",
      " 1.28009837e+02 0.00000000e+00 3.55271368e-14 7.94020017e+01\n",
      " 1.32684187e+02 8.23112534e+01 1.30328436e+02 2.99190460e+00\n",
      " 1.74204678e+01 7.75876910e+01 3.14403862e+01 5.81857284e+00\n",
      " 1.30432286e+02 1.98272372e+01 2.50555209e+01 6.93837082e+01\n",
      " 1.05538677e+01 2.89559806e+01 1.45083839e+02 3.05557438e+00\n",
      " 2.39221830e+01 8.15348152e+01 1.61932541e+01 1.15944031e+01]\n",
      "30-th iteration, loss: 0.14428994528646072, 11 gd steps\n",
      "insert gradient: -0.0003948147050114938\n",
      "30-th iteration, new layer inserted. now 34 layers\n",
      "[1.71317707e+00 4.61780629e+01 8.37801326e+01 4.50547301e+01\n",
      " 8.08316507e+01 7.92222021e+01 1.08053862e+02 7.56811917e+01\n",
      " 1.28023244e+02 4.60714922e-03 1.10348385e-02 7.94080259e+01\n",
      " 1.32696161e+02 8.23156192e+01 1.30320392e+02 2.98764731e+00\n",
      " 1.74185284e+01 7.75898034e+01 3.14486597e+01 5.79711092e+00\n",
      " 1.30419444e+02 1.97789573e+01 2.50899319e+01 6.94043774e+01\n",
      " 1.04905273e+01 2.89642161e+01 1.45070908e+02 3.03879672e+00\n",
      " 2.39134732e+01 0.00000000e+00 2.66453526e-15 8.15901153e+01\n",
      " 1.62277304e+01 1.15772923e+01]\n",
      "31-th iteration, loss: 0.14428730151126498, 11 gd steps\n",
      "insert gradient: -0.0003477462883246478\n",
      "31-th iteration, new layer inserted. now 32 layers\n",
      "[1.71622819e+00 4.61550759e+01 8.38009000e+01 4.50118096e+01\n",
      " 8.08277093e+01 7.91957069e+01 1.08057809e+02 7.56727849e+01\n",
      " 1.28048613e+02 7.94057600e+01 1.32702728e+02 8.23158390e+01\n",
      " 1.30315556e+02 2.98563739e+00 1.74178993e+01 7.75967368e+01\n",
      " 3.14574403e+01 5.79399215e+00 1.30411479e+02 1.97493734e+01\n",
      " 2.51189129e+01 6.94226622e+01 1.04452439e+01 2.89711872e+01\n",
      " 1.45059386e+02 3.02773084e+00 2.39052802e+01 0.00000000e+00\n",
      " 4.44089210e-15 8.16701128e+01 1.62536130e+01 1.15543024e+01]\n",
      "32-th iteration, loss: 0.14428384818524886, 15 gd steps\n",
      "insert gradient: -0.0006814017229069041\n",
      "32-th iteration, new layer inserted. now 30 layers\n",
      "[  1.72013928  46.09504315  83.82536085  44.95169303  80.83131241\n",
      "  79.17466277 108.08230975  75.6579066  128.07026123  79.40129511\n",
      " 132.71548842  82.3172303  130.30390169   2.98427928  17.41636862\n",
      "  77.62054317  31.48392433   5.80391743 130.39506455  19.69513732\n",
      "  25.20095329  69.47800373  10.34546481  29.00025932 145.02904766\n",
      "   2.9986828   23.88409598  81.84967635  16.32089074  11.47688075]\n",
      "33-th iteration, loss: 0.14427516517113248, 16 gd steps\n",
      "insert gradient: -0.000338825208007056\n",
      "33-th iteration, new layer inserted. now 30 layers\n",
      "[  1.75123617  46.06790271  83.88986277  44.92592328  80.87916237\n",
      "  79.09758289 108.16309296  75.59138539 128.10985713  79.35893892\n",
      " 132.74308346  82.28494062 130.27805131   2.97457295  17.42939547\n",
      "  77.72814043  31.58173891   5.86558853 130.32229222  19.47258114\n",
      "  25.51402567  69.6960612    9.96919057  29.11107648 144.90800015\n",
      "   2.90025587  23.79490802  82.24262144  16.59492532  11.24855966]\n",
      "34-th iteration, loss: 0.14426043379194195, 26 gd steps\n",
      "insert gradient: -0.0003406351474345144\n",
      "34-th iteration, new layer inserted. now 32 layers\n",
      "[1.78321524e+00 4.62143421e+01 8.38433147e+01 4.48206253e+01\n",
      " 8.08190573e+01 7.90420755e+01 1.08278843e+02 7.55061858e+01\n",
      " 1.28148191e+02 7.92992299e+01 1.32726214e+02 8.22679248e+01\n",
      " 1.30190695e+02 2.97212869e+00 1.74731506e+01 7.78715289e+01\n",
      " 3.17693531e+01 6.02397160e+00 1.30023083e+02 1.90192748e+01\n",
      " 2.62254405e+01 7.02311853e+01 9.12501507e+00 2.94110819e+01\n",
      " 1.44656783e+02 2.64159658e+00 2.36197456e+01 0.00000000e+00\n",
      " 8.88178420e-16 8.33229385e+01 1.72766585e+01 1.06817381e+01]\n",
      "35-th iteration, loss: 0.1442558336912172, 11 gd steps\n",
      "insert gradient: -0.0003100468660854264\n",
      "35-th iteration, new layer inserted. now 32 layers\n",
      "[1.74553867e+00 4.60767320e+01 8.38201107e+01 4.49067254e+01\n",
      " 8.08271644e+01 7.90117521e+01 1.08275331e+02 7.55012213e+01\n",
      " 1.28152315e+02 7.92961599e+01 1.32724836e+02 8.22657215e+01\n",
      " 1.30177119e+02 2.96823710e+00 1.74641334e+01 7.78690178e+01\n",
      " 3.17739103e+01 6.02438009e+00 1.30008392e+02 1.90018512e+01\n",
      " 2.62463391e+01 7.02441411e+01 9.08998351e+00 2.94181504e+01\n",
      " 1.44649613e+02 2.62650894e+00 2.36149679e+01 0.00000000e+00\n",
      " 8.88178420e-16 8.33949737e+01 1.73052776e+01 1.06636790e+01]\n",
      "36-th iteration, loss: 0.14425367294426725, 14 gd steps\n",
      "insert gradient: -0.0008320249690893921\n",
      "36-th iteration, new layer inserted. now 32 layers\n",
      "[1.73783296e+00 4.60466831e+01 8.37802179e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.48704528e+01 8.08000654e+01 7.89859815e+01\n",
      " 1.08282723e+02 7.55009334e+01 1.28168574e+02 7.93011467e+01\n",
      " 1.32731061e+02 8.22741178e+01 1.30161507e+02 2.96877370e+00\n",
      " 1.74590981e+01 7.78737642e+01 3.17826366e+01 6.03677972e+00\n",
      " 1.29976856e+02 1.89689453e+01 2.62931717e+01 7.02772652e+01\n",
      " 9.02590481e+00 2.94393571e+01 1.44634490e+02 2.59977601e+00\n",
      " 2.36061396e+01 8.35374355e+01 1.73684080e+01 1.06098113e+01]\n",
      "37-th iteration, loss: 0.14423417649318057, 29 gd steps\n",
      "insert gradient: -0.0021084826124975\n",
      "37-th iteration, new layer inserted. now 30 layers\n",
      "[  1.75167823  46.11375988  83.72853951  44.71970572  80.84226167\n",
      "  78.89830924 108.38502804  75.40761327 128.180973    79.23226334\n",
      " 132.75173007  82.22940121 130.05943063   2.9864979   17.57068337\n",
      "  78.05956643  32.01724953   6.27880833 129.35859964  18.41472714\n",
      "  27.1363089   70.93809606   8.0288896   29.7689026  144.37643323\n",
      "   2.24204489  23.45394336  85.10906769  18.55162564   9.8312293 ]\n",
      "38-th iteration, loss: 0.14422937263060373, 19 gd steps\n",
      "insert gradient: -0.0008838464999185891\n",
      "38-th iteration, new layer inserted. now 32 layers\n",
      "[1.73344853e+00 0.00000000e+00 1.66533454e-16 4.60064467e+01\n",
      " 8.36993502e+01 4.48655361e+01 8.07408585e+01 7.88627042e+01\n",
      " 1.08431654e+02 7.53887798e+01 1.28235021e+02 7.92110162e+01\n",
      " 1.32748416e+02 8.22530747e+01 1.29989738e+02 2.99029418e+00\n",
      " 1.75385967e+01 7.80525724e+01 3.20513934e+01 6.29864325e+00\n",
      " 1.29237822e+02 1.83584273e+01 2.72451009e+01 7.10206487e+01\n",
      " 7.88218148e+00 2.98235363e+01 1.44352795e+02 2.16769440e+00\n",
      " 2.34450043e+01 8.53392935e+01 1.87683836e+01 9.71454149e+00]\n",
      "39-th iteration, loss: 0.1442230776041708, 20 gd steps\n",
      "insert gradient: -0.00025204320436463424\n",
      "39-th iteration, new layer inserted. now 30 layers\n",
      "[  1.7403229   46.00523908  83.74750079  44.84333914  80.79205389\n",
      "  78.79135285 108.49799203  75.34068174 128.27867195  79.19210499\n",
      " 132.75178286  82.25803613 129.92582898   3.0163871   17.57042579\n",
      "  78.07222445  32.14181939   6.38483945 128.94691185  18.20101676\n",
      "  27.50314521  71.20372479   7.58764907  29.93541601 144.29313133\n",
      "   2.04245666  23.41852443  85.90379199  19.25735554   9.45433388]\n",
      "40-th iteration, loss: 0.14422223412393337, 14 gd steps\n",
      "insert gradient: -0.0012586880867151706\n",
      "40-th iteration, new layer inserted. now 32 layers\n",
      "[1.74028390e+00 4.60117301e+01 8.36952892e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.47956207e+01 8.07554077e+01 7.87938848e+01\n",
      " 1.08504443e+02 7.53357472e+01 1.28284339e+02 7.91863830e+01\n",
      " 1.32746528e+02 8.22602923e+01 1.29909990e+02 3.02188111e+00\n",
      " 1.75680763e+01 7.80718677e+01 3.21561671e+01 6.39399999e+00\n",
      " 1.28907008e+02 1.81881337e+01 2.75370853e+01 7.12262188e+01\n",
      " 7.54754238e+00 2.99507467e+01 1.44286378e+02 2.02001693e+00\n",
      " 2.34160218e+01 8.59749582e+01 1.93259908e+01 9.41967940e+00]\n",
      "41-th iteration, loss: 0.1441868250638244, 999 gd steps\n",
      "insert gradient: -0.0001561440854742773\n",
      "41-th iteration, new layer inserted. now 30 layers\n",
      "[  1.73724842  46.01264855  83.60968051  44.77127498  80.69704108\n",
      "  78.60994041 108.76094195  75.1653327  128.4385271   79.07135118\n",
      " 132.75848392  82.29284897 129.3546687    3.14462055  17.87965779\n",
      "  78.32723387  33.16109199   6.88351305 126.02156835  17.40406474\n",
      "  29.10676654  72.14384858   5.85053913  30.81495506 144.08916269\n",
      "   1.09067165  23.479959    89.73290919  23.59431508   7.85279741]\n",
      "42-th iteration, loss: 0.14417239683956354, 50 gd steps\n",
      "insert gradient: -0.00017593631728859933\n",
      "42-th iteration, new layer inserted. now 30 layers\n",
      "[  1.73923699  46.05038605  83.59022454  44.77400472  80.71946606\n",
      "  78.58702551 108.84930489  75.13744958 128.50852567  79.04777107\n",
      " 132.79108225  82.34735085 128.74578014   3.18975229  18.34139923\n",
      "  78.62308025  34.4598341    7.16365745 122.28452887  17.29216038\n",
      "  29.7281616   72.46914971   5.20371644  31.52322557 144.16749131\n",
      "   0.46274893  23.90266227  92.18172684  27.42114827   7.05932248]\n",
      "43-th iteration, loss: 0.14417218489281236, 17 gd steps\n",
      "insert gradient: -8.659677205291519e-05\n",
      "43-th iteration, new layer inserted. now 32 layers\n",
      "[1.73352324e+00 4.60333510e+01 8.35916775e+01 4.47729862e+01\n",
      " 8.07167215e+01 7.85862036e+01 1.08855196e+02 7.51355121e+01\n",
      " 1.28510309e+02 7.90456296e+01 1.32792587e+02 8.23452745e+01\n",
      " 1.28741055e+02 3.18684840e+00 1.83406751e+01 7.86354245e+01\n",
      " 3.44732775e+01 7.16594785e+00 1.22271689e+02 1.72831743e+01\n",
      " 0.00000000e+00 1.77635684e-15 2.97510487e+01 7.24681588e+01\n",
      " 5.18611867e+00 3.15309201e+01 1.44161437e+02 4.60549278e-01\n",
      " 2.38985191e+01 9.21746184e+01 2.74384624e+01 7.05830001e+00]\n",
      "44-th iteration, loss: 0.14417208440462673, 15 gd steps\n",
      "insert gradient: -8.570850591658033e-05\n",
      "44-th iteration, new layer inserted. now 32 layers\n",
      "[1.73206167e+00 4.60282717e+01 8.35888816e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.47658599e+01 8.07128237e+01 7.85852878e+01\n",
      " 1.08859756e+02 7.51345104e+01 1.28512395e+02 7.90453767e+01\n",
      " 1.32793952e+02 8.23452165e+01 1.28737724e+02 3.18536370e+00\n",
      " 1.83410514e+01 7.86453608e+01 3.44848019e+01 7.16894610e+00\n",
      " 1.22260353e+02 1.72744110e+01 2.97840079e+01 7.24670110e+01\n",
      " 5.17075809e+00 3.15372611e+01 1.44156590e+02 4.58730377e-01\n",
      " 2.38952528e+01 9.21717100e+01 2.74516831e+01 7.05676070e+00]\n",
      "45-th iteration, loss: 0.14417145415400595, 17 gd steps\n",
      "insert gradient: -0.0001794355951145874\n",
      "45-th iteration, new layer inserted. now 32 layers\n",
      "[1.73891867e+00 4.60470278e+01 8.35653140e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.47464215e+01 8.06965861e+01 7.85663013e+01\n",
      " 1.08885766e+02 7.51127188e+01 1.28510037e+02 7.90299367e+01\n",
      " 1.32796324e+02 8.23268792e+01 1.28707670e+02 3.15685884e+00\n",
      " 1.83714301e+01 7.87820238e+01 3.46184650e+01 7.19819300e+00\n",
      " 1.22060559e+02 1.71804007e+01 2.99925591e+01 7.24583530e+01\n",
      " 5.00750171e+00 3.16304981e+01 1.44104167e+02 4.25342518e-01\n",
      " 2.38653841e+01 9.22307350e+01 2.76004547e+01 7.04121479e+00]\n",
      "46-th iteration, loss: 0.14417121866728447, 41 gd steps\n",
      "insert gradient: -9.088887920071537e-05\n",
      "46-th iteration, new layer inserted. now 32 layers\n",
      "[1.72966071e+00 0.00000000e+00 1.66533454e-16 4.60129431e+01\n",
      " 8.35755394e+01 4.47666827e+01 8.06987131e+01 7.85478352e+01\n",
      " 1.08903284e+02 7.51026137e+01 1.28528529e+02 7.90245936e+01\n",
      " 1.32796314e+02 8.23314712e+01 1.28677433e+02 3.15833924e+00\n",
      " 1.83646138e+01 7.87912202e+01 3.46545922e+01 7.20961555e+00\n",
      " 1.21983648e+02 1.71592752e+01 3.00502639e+01 7.24577287e+01\n",
      " 4.95811116e+00 3.16610749e+01 1.44086878e+02 4.12024064e-01\n",
      " 2.38551894e+01 9.22509192e+01 2.76523134e+01 7.03457754e+00]\n",
      "47-th iteration, loss: 0.14417114324133462, 20 gd steps\n",
      "insert gradient: -0.00012114119643001063\n",
      "47-th iteration, new layer inserted. now 32 layers\n",
      "[1.73227189e+00 4.60259094e+01 8.35736055e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.47491312e+01 8.06937106e+01 7.85436217e+01\n",
      " 1.08907665e+02 7.51000093e+01 1.28532556e+02 7.90242009e+01\n",
      " 1.32796299e+02 8.23329590e+01 1.28671004e+02 3.15987302e+00\n",
      " 1.83655153e+01 7.87936828e+01 3.46648314e+01 7.21352355e+00\n",
      " 1.21960387e+02 1.71537396e+01 3.00667308e+01 7.24580538e+01\n",
      " 4.94534875e+00 3.16703184e+01 1.44083166e+02 4.08806913e-01\n",
      " 2.38535664e+01 9.22583048e+01 2.76668704e+01 7.03238124e+00]\n",
      "48-th iteration, loss: 0.14417110054527058, 12 gd steps\n",
      "insert gradient: -3.7941840412053115e-05\n",
      "48-th iteration, new layer inserted. now 32 layers\n",
      "[1.73201721e+00 4.60224759e+01 8.35748575e+01 4.47534934e+01\n",
      " 8.06946587e+01 7.85408507e+01 1.08911172e+02 7.50973480e+01\n",
      " 1.28535031e+02 7.90227919e+01 1.32796052e+02 8.23331264e+01\n",
      " 1.28666237e+02 3.16151998e+00 1.83662907e+01 7.87959847e+01\n",
      " 3.46729250e+01 7.21681602e+00 1.21942705e+02 1.71489466e+01\n",
      " 0.00000000e+00 2.66453526e-15 3.00790743e+01 7.24579978e+01\n",
      " 4.93539749e+00 3.16768615e+01 1.44080193e+02 4.06084406e-01\n",
      " 2.38521728e+01 9.22639575e+01 2.76779234e+01 7.03106906e+00]\n",
      "49-th iteration, loss: 0.14417032319480022, 25 gd steps\n",
      "insert gradient: -4.487245481727325e-05\n",
      "49-th iteration, new layer inserted. now 32 layers\n",
      "[1.71906867e+00 4.60330538e+01 8.35550707e+01 4.47487473e+01\n",
      " 8.06999244e+01 0.00000000e+00 1.06581410e-14 7.85143150e+01\n",
      " 1.08951914e+02 7.50758276e+01 1.28556440e+02 7.90110454e+01\n",
      " 1.32797990e+02 8.23389080e+01 1.28577318e+02 3.16364731e+00\n",
      " 1.84256420e+01 7.88795338e+01 3.48720321e+01 7.27541614e+00\n",
      " 1.21409346e+02 1.70944180e+01 3.02934373e+01 7.24490864e+01\n",
      " 4.77614408e+00 3.18265953e+01 1.44050367e+02 3.41916253e-01\n",
      " 2.38672657e+01 9.24471090e+01 2.79871942e+01 6.98441649e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5349698070010542\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  45.4077617     0.         1839.01434885]\n",
      "1-th iteration, loss: 0.7509319290812663, 11 gd steps\n",
      "insert gradient: -0.5962539479000885\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  43.62107906   62.23800861  246.6970468     0.         1592.31730206]\n",
      "2-th iteration, loss: 0.6119887249061261, 14 gd steps\n",
      "insert gradient: -0.6365788896015144\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.64441142   76.22263247  229.66057099   36.01382744  259.97017176\n",
      "    0.         1332.34713029]\n",
      "3-th iteration, loss: 0.46220792462344373, 20 gd steps\n",
      "insert gradient: -0.5422852566965942\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[  4.3689229   52.48797128 237.10825062  73.21925037 110.27665663\n",
      "  74.2628277  380.67060865   0.         951.67652164]\n",
      "4-th iteration, loss: 0.3764552308248561, 20 gd steps\n",
      "insert gradient: -0.25293123344298146\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.04804701  50.36684588 228.94092483  78.36487561 109.26234846\n",
      "  66.29275451 352.04517799  48.50368311 599.20373585   0.\n",
      " 352.47278579]\n",
      "5-th iteration, loss: 0.3218718804260975, 18 gd steps\n",
      "insert gradient: -0.2869123697712034\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.03210536  62.40428712 206.79407321  84.11185243 108.23971829\n",
      "  63.3655957  240.0937653    0.         112.04375714  42.71535729\n",
      " 593.7530421   38.05955873 352.47278579]\n",
      "6-th iteration, loss: 0.279448249589511, 23 gd steps\n",
      "insert gradient: -0.11201800809308106\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          63.57324002 210.29529498  77.10806466 110.31419588\n",
      "  58.82712665 188.2082448   44.04283059  76.95582884  47.92863708\n",
      " 576.95157754  39.28393516 234.98185719   0.         117.4909286 ]\n",
      "7-th iteration, loss: 0.26181596179234495, 18 gd steps\n",
      "insert gradient: -0.16675236463580403\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.81826168  67.87375811 212.36756509  70.97894782 114.02524583\n",
      "  61.1005625  172.25151555  56.33508288  65.26731817  47.68845268\n",
      " 218.37127946   0.         363.95213243  23.43668382 222.10002412\n",
      "  28.01496937 117.4909286 ]\n",
      "8-th iteration, loss: 0.23119999443492004, 21 gd steps\n",
      "insert gradient: -0.08221499850698556\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[2.87467250e-01 6.18085969e+01 1.24074656e+02 0.00000000e+00\n",
      " 9.65025099e+01 6.99984236e+01 1.11182698e+02 6.69173331e+01\n",
      " 1.61119311e+02 5.55352285e+01 7.57986874e+01 3.31533952e+01\n",
      " 2.10936808e+02 2.89466542e+01 5.82289861e+02 4.65738550e+01\n",
      " 1.17490929e+02]\n",
      "9-th iteration, loss: 0.22299417358916293, 16 gd steps\n",
      "insert gradient: -0.06775502708330686\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          62.24430237 106.20445265  12.91844708  79.82427646\n",
      "  69.85286225 118.42943627  63.13184797 159.21734519  65.30631032\n",
      "  69.09528173  30.32999701 115.08673029   0.          86.31504771\n",
      "  48.04158741 570.07053185  32.95555424 117.4909286 ]\n",
      "10-th iteration, loss: 0.17474443391055294, 66 gd steps\n",
      "insert gradient: -0.15058662632595013\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[7.12655360e-01 6.21968379e+01 1.86353197e+02 7.35501374e+01\n",
      " 1.40431757e+02 7.36211243e+01 1.35677955e+02 4.94631601e+01\n",
      " 8.36639589e+01 4.54532917e+01 8.11189019e+01 5.63337755e+01\n",
      " 4.24380151e-01 3.45880803e+01 5.79969447e+02 4.34471154e+01\n",
      " 1.17490929e+02 0.00000000e+00 1.77635684e-14]\n",
      "11-th iteration, loss: 0.15093434946900053, 23 gd steps\n",
      "insert gradient: -0.023936356663885522\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[1.45670530e+00 6.01058778e+01 1.87732275e+02 7.21273320e+01\n",
      " 1.46152978e+02 7.52919418e+01 1.27928219e+02 5.51061816e+01\n",
      " 8.43190344e+01 4.40092024e+01 8.58371641e+01 5.72363264e+01\n",
      " 2.87975474e-01 3.49354219e+01 1.45124775e+02 0.00000000e+00\n",
      " 4.35374326e+02 4.31177180e+01 9.39809836e+01 3.26956755e+01\n",
      " 5.33609047e-15]\n",
      "12-th iteration, loss: 0.1469039380391578, 18 gd steps\n",
      "insert gradient: -0.07249886949794532\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[0.00000000e+00 5.47554646e+01 1.83921287e+02 7.09255911e+01\n",
      " 1.54648166e+02 8.23758094e+01 1.16128945e+02 6.16431578e+01\n",
      " 8.18164978e+01 4.16788076e+01 1.01216329e+02 5.46285825e+01\n",
      " 1.26469089e+00 3.23202936e+01 1.04435172e+02 2.44584827e+01\n",
      " 1.18043885e+02 0.00000000e+00 3.14783694e+02 4.11742776e+01\n",
      " 9.00750682e+01 4.20637670e+01 1.75191159e-14]\n",
      "13-th iteration, loss: 0.10941307702495313, 30 gd steps\n",
      "insert gradient: -0.019978543404327365\n",
      "13-th iteration, new layer inserted. now 22 layers\n",
      "[  3.02589013  74.56895241 178.67082655  75.07640019 162.27237654\n",
      "  67.52640448 132.08177754  76.01892302 100.8842337   42.21891146\n",
      "  80.62265238 101.14852693  89.11772912  44.92529388  86.19609364\n",
      "  35.61871499  98.32032412   0.         196.64064825  35.51395676\n",
      "  85.24606174  49.84297726]\n",
      "14-th iteration, loss: 0.07944729138468629, 133 gd steps\n",
      "insert gradient: -0.004044499930867428\n",
      "14-th iteration, new layer inserted. now 22 layers\n",
      "[  2.47376272  74.73191369 142.23262194 107.19099369 140.814713\n",
      "  79.46720985 125.32978734  75.37997747 120.91519513  48.95618507\n",
      "  72.53859728  90.07376151  90.71885083  47.72909242  83.35307623\n",
      "  30.87646262  71.71284215  49.98873395 126.07646286  38.00872201\n",
      "  87.30153356  48.60952541]\n",
      "15-th iteration, loss: 0.07762655154079762, 37 gd steps\n",
      "insert gradient: -0.008001985233623783\n",
      "15-th iteration, new layer inserted. now 24 layers\n",
      "[  2.24914482  78.0065466  146.98795245 109.18872229 134.68479189\n",
      "  82.53414752 123.22203467  76.1117711  124.25276688  53.81234276\n",
      "  71.39378545  54.29353048   0.          36.19568698  90.22330355\n",
      "  48.94918887  84.89549761  32.25624548  69.18437166  50.7535559\n",
      " 128.92887357  40.07331486  84.24668216  47.09196745]\n",
      "16-th iteration, loss: 0.0742292072780814, 47 gd steps\n",
      "insert gradient: -0.0024407370308554954\n",
      "16-th iteration, new layer inserted. now 24 layers\n",
      "[  0.67955134  81.62903824 144.61032201 114.98322888 134.14901932\n",
      "  85.64309849 119.46661979  78.94528607 123.89098462  59.92262719\n",
      "  81.81780906  45.16438078  12.41707784  34.81854195  88.2472849\n",
      "  50.13247087  89.77353387  37.29785251  60.34325889  51.7893967\n",
      " 130.23649438  47.15313289  77.59819446  47.07596046]\n",
      "17-th iteration, loss: 0.07379501947732661, 39 gd steps\n",
      "insert gradient: -0.001504808280114943\n",
      "17-th iteration, new layer inserted. now 26 layers\n",
      "[9.62819757e-01 8.22291554e+01 1.41979259e+02 1.17694794e+02\n",
      " 1.35487767e+02 8.59767951e+01 1.18997055e+02 7.96608816e+01\n",
      " 1.24389172e+02 6.08827566e+01 8.75128559e+01 4.17246228e+01\n",
      " 1.49055521e+01 3.57127057e+01 8.69149409e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.92289411e+01 8.98745226e+01 3.82473732e+01\n",
      " 5.99151908e+01 5.29783432e+01 1.28768979e+02 4.98680194e+01\n",
      " 7.68122483e+01 4.67544203e+01]\n",
      "18-th iteration, loss: 0.07374666047786178, 17 gd steps\n",
      "insert gradient: -0.001174804752833552\n",
      "18-th iteration, new layer inserted. now 24 layers\n",
      "[  1.36613128  82.52692348 140.61650753 118.25752301 136.43003497\n",
      "  86.003495   118.75306255  79.93663043 124.32164265  60.73619436\n",
      "  89.59769969  41.14713124  15.0484149   36.19008656  86.29205683\n",
      "  49.16355484  89.88167863  38.10993544  60.78856367  52.8306004\n",
      " 128.44101139  50.65941096  76.9811903   46.37573273]\n",
      "19-th iteration, loss: 0.07370684983726022, 54 gd steps\n",
      "insert gradient: -0.0004067765724488018\n",
      "19-th iteration, new layer inserted. now 26 layers\n",
      "[1.32193732e+00 8.23994642e+01 1.40177170e+02 1.18919816e+02\n",
      " 1.37076200e+02 8.63275506e+01 1.18629848e+02 8.01450988e+01\n",
      " 1.24415464e+02 6.15113672e+01 8.98201797e+01 4.11494811e+01\n",
      " 1.51172134e+01 3.64173807e+01 8.60296095e+01 4.89644223e+01\n",
      " 8.98377317e+01 3.85722090e+01 6.09466078e+01 5.24651670e+01\n",
      " 1.28773706e+02 0.00000000e+00 2.48689958e-14 5.12327075e+01\n",
      " 7.72779102e+01 4.60458781e+01]\n",
      "20-th iteration, loss: 0.0737012371262073, 73 gd steps\n",
      "insert gradient: -3.6443006104189674e-05\n",
      "20-th iteration, new layer inserted. now 26 layers\n",
      "[1.45883779e+00 8.24490822e+01 1.39674072e+02 0.00000000e+00\n",
      " 3.19744231e-14 1.19078010e+02 1.37801393e+02 8.63795865e+01\n",
      " 1.18486192e+02 8.03315002e+01 1.24289157e+02 6.16380122e+01\n",
      " 9.03818258e+01 4.09631300e+01 1.49801604e+01 3.67336551e+01\n",
      " 8.57302666e+01 4.88919679e+01 8.98197512e+01 3.86165769e+01\n",
      " 6.13014113e+01 5.22532195e+01 1.28970306e+02 5.15196070e+01\n",
      " 7.74401821e+01 4.58418337e+01]\n",
      "21-th iteration, loss: 0.07370102638980026, 24 gd steps\n",
      "insert gradient: -7.314994377290538e-05\n",
      "21-th iteration, new layer inserted. now 24 layers\n",
      "[  1.4944092   82.45009141 139.60472137 119.08594596 137.94487863\n",
      "  86.38717247 118.46349256  80.36972699 124.24073358  61.6542749\n",
      "  90.48728451  40.90923818  14.94515025  36.82442335  85.66876356\n",
      "  48.87739371  89.82800772  38.60862446  61.3687816   52.22600034\n",
      " 129.00517374  51.54645254  77.47274348  45.81359254]\n",
      "22-th iteration, loss: 0.07370096914766594, 38 gd steps\n",
      "insert gradient: -1.3273964789107595e-05\n",
      "22-th iteration, new layer inserted. now 24 layers\n",
      "[  1.49324209  82.45332927 139.58634391 119.09624572 137.98618183\n",
      "  86.39294803 118.45289457  80.37306832 124.25088913  61.67159671\n",
      "  90.50222193  40.9232021   14.92991641  36.82956946  85.66458938\n",
      "  48.87136213  89.81699942  38.62785922  61.37278697  52.20792413\n",
      " 129.01286168  51.57558239  77.48549115  45.80065978]\n",
      "23-th iteration, loss: 0.07370096081990182, 20 gd steps\n",
      "insert gradient: -1.2486948880963004e-05\n",
      "23-th iteration, new layer inserted. now 24 layers\n",
      "[  1.49676912  82.4552738  139.57156725 119.09533853 138.01101406\n",
      "  86.3928711  118.44629975  80.37747719 124.24718009  61.67242113\n",
      "  90.51617216  40.92377538  14.92366667  36.83727142  85.65491301\n",
      "  48.87368743  89.81541181  38.62828263  61.38291024  52.19693715\n",
      " 129.0183854   51.58152256  77.49082189  45.79348871]\n",
      "24-th iteration, loss: 0.07370095672367276, 23 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.627879878439504e-06\n",
      "24-th iteration, new layer inserted. now 26 layers\n",
      "[1.49734974e+00 8.24552521e+01 1.39568033e+02 1.19096635e+02\n",
      " 1.38020947e+02 8.63941587e+01 1.18446516e+02 8.03797869e+01\n",
      " 1.24246161e+02 6.16756061e+01 9.05189886e+01 4.09225660e+01\n",
      " 1.49215370e+01 3.68413530e+01 8.56532589e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.88697230e+01 8.98161866e+01 3.86302726e+01\n",
      " 6.13841253e+01 5.21967671e+01 1.29022774e+02 5.15844209e+01\n",
      " 7.74912121e+01 4.57929170e+01]\n",
      "25-th iteration, loss: 0.0737009567124774, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.167738217735606e-06\n",
      "25-th iteration, new layer inserted. now 28 layers\n",
      "[1.49735033e+00 8.24552542e+01 1.39568033e+02 1.19096639e+02\n",
      " 1.38020952e+02 8.63941614e+01 1.18446517e+02 8.03797903e+01\n",
      " 1.24246164e+02 6.16756120e+01 9.05189929e+01 4.09225721e+01\n",
      " 1.49215405e+01 3.68413602e+01 8.56532623e+01 7.47735917e-06\n",
      " 3.31967932e-06 0.00000000e+00 4.23516474e-22 4.88697305e+01\n",
      " 8.98161902e+01 3.86302782e+01 6.13841274e+01 5.21967660e+01\n",
      " 1.29022772e+02 5.15844198e+01 7.74912123e+01 4.57929165e+01]\n",
      "26-th iteration, loss: 0.07370095670057332, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.8082471679472945e-06\n",
      "26-th iteration, new layer inserted. now 30 layers\n",
      "[1.49735091e+00 8.24552563e+01 1.39568034e+02 1.19096643e+02\n",
      " 1.38020956e+02 8.63941642e+01 1.18446519e+02 8.03797937e+01\n",
      " 1.24246166e+02 6.16756179e+01 9.05189971e+01 4.09225780e+01\n",
      " 1.49215439e+01 0.00000000e+00 1.33226763e-15 3.68413672e+01\n",
      " 8.56532654e+01 1.44612009e-05 6.44578010e-06 6.98839789e-06\n",
      " 3.12610078e-06 4.88697374e+01 8.98161936e+01 3.86302836e+01\n",
      " 6.13841295e+01 5.21967649e+01 1.29022770e+02 5.15844187e+01\n",
      " 7.74912126e+01 4.57929160e+01]\n",
      "27-th iteration, loss: 0.07370095668833868, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.508467822338026e-06\n",
      "27-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735149e+00 8.24552583e+01 1.39568034e+02 1.19096647e+02\n",
      " 1.38020961e+02 8.63941669e+01 1.18446520e+02 8.03797970e+01\n",
      " 1.24246168e+02 6.16756237e+01 9.05190012e+01 4.09225836e+01\n",
      " 1.49215471e+01 6.68901929e-06 3.22684470e-06 0.00000000e+00\n",
      " 1.05879118e-22 3.68413739e+01 8.56532683e+01 2.08637184e-05\n",
      " 9.34859460e-06 1.33997621e-05 6.01932385e-06 4.88697439e+01\n",
      " 8.98161966e+01 3.86302886e+01 6.13841315e+01 5.21967638e+01\n",
      " 1.29022768e+02 5.15844177e+01 7.74912128e+01 4.57929154e+01]\n",
      "28-th iteration, loss: 0.0737009566760076, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.170596032761725e-06\n",
      "28-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735206e+00 8.24552602e+01 1.39568034e+02 1.19096651e+02\n",
      " 1.38020965e+02 8.63941695e+01 1.18446522e+02 8.03798002e+01\n",
      " 1.24246170e+02 6.16756294e+01 9.05190052e+01 4.09225891e+01\n",
      " 1.49215501e+01 1.30649637e-05 6.27124557e-06 6.37638035e-06\n",
      " 3.04440086e-06 3.68413802e+01 8.56532710e+01 2.66914134e-05\n",
      " 1.20189926e-05 1.92402879e-05 8.67133081e-06 4.88697497e+01\n",
      " 8.98161995e+01 3.86302934e+01 6.13841334e+01 5.21967626e+01\n",
      " 1.29022767e+02 5.15844167e+01 7.74912130e+01 4.57929149e+01]\n",
      "29-th iteration, loss: 0.07370095666496475, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.854965541390154e-06\n",
      "29-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735262e+00 8.24552622e+01 1.39568035e+02 1.19096656e+02\n",
      " 1.38020970e+02 8.63941722e+01 1.18446523e+02 8.03798034e+01\n",
      " 1.24246172e+02 6.16756350e+01 9.05190091e+01 4.09225943e+01\n",
      " 1.49215530e+01 1.91102392e-05 9.11675149e-06 1.24225031e-05\n",
      " 5.88904534e-06 3.68413863e+01 8.56532735e+01 3.19657505e-05\n",
      " 1.44580628e-05 2.45311210e-05 1.10839939e-05 4.88697550e+01\n",
      " 8.98162021e+01 3.86302980e+01 6.13841353e+01 5.21967614e+01\n",
      " 1.29022765e+02 5.15844158e+01 7.74912133e+01 4.57929143e+01]\n",
      "30-th iteration, loss: 0.07370095665503613, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.560026461663249e-06\n",
      "30-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735318e+00 8.24552640e+01 1.39568035e+02 1.19096660e+02\n",
      " 1.38020974e+02 8.63941748e+01 1.18446524e+02 8.03798065e+01\n",
      " 1.24246174e+02 6.16756405e+01 9.05190129e+01 4.09225992e+01\n",
      " 1.49215556e+01 2.48465734e-05 1.17752838e-05 1.81600689e-05\n",
      " 8.54589947e-06 3.68413920e+01 8.56532758e+01 3.67299526e-05\n",
      " 1.66828298e-05 2.93151668e-05 1.32750919e-05 4.88697598e+01\n",
      " 8.98162045e+01 3.86303023e+01 6.13841371e+01 5.21967602e+01\n",
      " 1.29022764e+02 5.15844150e+01 7.74912135e+01 4.57929137e+01]\n",
      "31-th iteration, loss: 0.07370095664607215, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.430163180277842e-06\n",
      "31-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735373e+00 8.24552659e+01 1.39568035e+02 1.19096664e+02\n",
      " 1.38020978e+02 8.63941774e+01 1.18446525e+02 8.03798095e+01\n",
      " 1.24246176e+02 6.16756460e+01 9.05190166e+01 4.09226039e+01\n",
      " 1.49215581e+01 3.02941822e-05 1.42579879e-05 2.36092684e-05\n",
      " 1.10261503e-05 3.68413975e+01 8.56532778e+01 4.10239007e-05\n",
      " 1.87090480e-05 3.36320122e-05 1.52610749e-05 4.88697641e+01\n",
      " 8.98162067e+01 3.86303064e+01 6.13841389e+01 5.21967591e+01\n",
      " 1.29022762e+02 5.15844142e+01 7.74912138e+01 4.57929131e+01]\n",
      "32-th iteration, loss: 0.07370095663794429, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.353652104847181e-06\n",
      "32-th iteration, new layer inserted. now 32 layers\n",
      "[1.49735428e+00 8.24552677e+01 1.39568036e+02 1.19096668e+02\n",
      " 1.38020983e+02 8.63941801e+01 1.18446527e+02 8.03798125e+01\n",
      " 1.24246177e+02 6.16756513e+01 9.05190202e+01 4.09226084e+01\n",
      " 1.49215604e+01 3.54718793e-05 1.65752865e-05 2.87888916e-05\n",
      " 1.33402596e-05 3.68414026e+01 8.56532797e+01 4.48843914e-05\n",
      " 2.05512973e-05 3.75181812e-05 1.70571640e-05 4.88697681e+01\n",
      " 8.98162087e+01 3.86303103e+01 6.13841406e+01 5.21967579e+01\n",
      " 1.29022761e+02 5.15844135e+01 7.74912141e+01 4.57929125e+01]\n",
      "33-th iteration, loss: 0.07370095663054188, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.2806021587311126e-06\n",
      "33-th iteration, new layer inserted. now 34 layers\n",
      "[1.49735482e+00 8.24552694e+01 1.39568036e+02 1.19096672e+02\n",
      " 1.38020987e+02 8.63941826e+01 1.18446528e+02 8.03798154e+01\n",
      " 1.24246179e+02 0.00000000e+00 2.13162821e-14 6.16756566e+01\n",
      " 9.05190238e+01 4.09226126e+01 1.49215626e+01 4.03971777e-05\n",
      " 1.87369302e-05 3.37164290e-05 1.54980144e-05 3.68414076e+01\n",
      " 8.56532815e+01 4.83453738e-05 2.22230718e-05 4.10073705e-05\n",
      " 1.86774442e-05 4.88697716e+01 8.98162106e+01 3.86303141e+01\n",
      " 6.13841423e+01 5.21967568e+01 1.29022760e+02 5.15844128e+01\n",
      " 7.74912145e+01 4.57929119e+01]\n",
      "34-th iteration, loss: 0.07370095662295967, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.186838348664349e-06\n",
      "34-th iteration, new layer inserted. now 36 layers\n",
      "[1.49735536e+00 8.24552712e+01 1.39568036e+02 1.19096676e+02\n",
      " 1.38020992e+02 8.63941852e+01 1.18446529e+02 8.03798183e+01\n",
      " 1.24246181e+02 5.21153969e-06 1.76494490e-06 0.00000000e+00\n",
      " 3.17637355e-22 6.16756619e+01 9.05190273e+01 4.09226167e+01\n",
      " 1.49215646e+01 4.50813054e-05 2.07485754e-05 3.84030878e-05\n",
      " 1.75051052e-05 3.68414123e+01 8.56532830e+01 5.14353065e-05\n",
      " 2.37345535e-05 4.41278048e-05 2.01326425e-05 4.88697747e+01\n",
      " 8.98162123e+01 3.86303177e+01 6.13841439e+01 5.21967557e+01\n",
      " 1.29022759e+02 5.15844123e+01 7.74912148e+01 4.57929113e+01]\n",
      "35-th iteration, loss: 0.07370095661520452, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.073604009072665e-06\n",
      "35-th iteration, new layer inserted. now 36 layers\n",
      "[1.49735589e+00 8.24552729e+01 1.39568036e+02 1.19096680e+02\n",
      " 1.38020996e+02 8.63941877e+01 1.18446530e+02 8.03798210e+01\n",
      " 1.24246183e+02 1.03182507e-05 3.47079647e-06 5.10871393e-06\n",
      " 1.70585157e-06 6.16756670e+01 9.05190306e+01 4.09226206e+01\n",
      " 1.49215665e+01 4.95329393e-05 2.26157191e-05 4.28575242e-05\n",
      " 1.93670616e-05 3.68414167e+01 8.56532845e+01 5.41788899e-05\n",
      " 2.50930815e-05 4.69039644e-05 2.14306041e-05 4.88697775e+01\n",
      " 8.98162138e+01 3.86303212e+01 6.13841456e+01 5.21967546e+01\n",
      " 1.29022758e+02 5.15844118e+01 7.74912152e+01 4.57929107e+01]\n",
      "36-th iteration, loss: 0.07370095660803627, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.965174678544709e-06\n",
      "36-th iteration, new layer inserted. now 38 layers\n",
      "[1.49735642e+00 8.24552745e+01 1.39568036e+02 1.19096684e+02\n",
      " 1.38021000e+02 8.63941902e+01 1.18446531e+02 8.03798237e+01\n",
      " 1.24246184e+02 1.53107590e-05 5.11554631e-06 1.01051611e-05\n",
      " 3.34480380e-06 0.00000000e+00 6.35274710e-22 6.16756720e+01\n",
      " 9.05190339e+01 4.09226243e+01 1.49215682e+01 5.37653082e-05\n",
      " 2.43471087e-05 4.70929479e-05 2.10926623e-05 3.68414210e+01\n",
      " 8.56532858e+01 5.66017368e-05 2.63077217e-05 4.93612522e-05\n",
      " 2.25808673e-05 4.88697800e+01 8.98162153e+01 3.86303245e+01\n",
      " 6.13841472e+01 5.21967535e+01 1.29022757e+02 5.15844114e+01\n",
      " 7.74912155e+01 4.57929101e+01]\n",
      "37-th iteration, loss: 0.07370095660068599, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.839084206779329e-06\n",
      "37-th iteration, new layer inserted. now 40 layers\n",
      "[1.49735694e+00 8.24552762e+01 1.39568037e+02 1.19096688e+02\n",
      " 1.38021005e+02 8.63941927e+01 1.18446532e+02 8.03798263e+01\n",
      " 1.24246186e+02 2.01828388e-05 6.69539473e-06 1.49830463e-05\n",
      " 4.91318444e-06 4.88168100e-06 1.56838064e-06 0.00000000e+00\n",
      " 2.11758237e-22 6.16756768e+01 9.05190372e+01 4.09226278e+01\n",
      " 1.49215698e+01 5.77880193e-05 2.59477993e-05 5.11189475e-05\n",
      " 2.26869921e-05 3.68414250e+01 8.56532869e+01 5.87263911e-05\n",
      " 2.73867928e-05 5.15220147e-05 2.35921885e-05 4.88697822e+01\n",
      " 8.98162166e+01 3.86303278e+01 6.13841488e+01 5.21967526e+01\n",
      " 1.29022756e+02 5.15844111e+01 7.74912160e+01 4.57929096e+01]\n",
      "38-th iteration, loss: 0.07370095659320319, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.697037886617572e-06\n",
      "38-th iteration, new layer inserted. now 40 layers\n",
      "[1.49735746e+00 8.24552778e+01 1.39568037e+02 1.19096692e+02\n",
      " 1.38021009e+02 8.63941951e+01 1.18446533e+02 8.03798287e+01\n",
      " 1.24246187e+02 2.49159930e-05 8.20301167e-06 1.97237986e-05\n",
      " 6.40379794e-06 9.62800904e-06 3.05345419e-06 4.74810792e-06\n",
      " 1.48507355e-06 6.16756816e+01 9.05190403e+01 4.09226312e+01\n",
      " 1.49215713e+01 6.16085073e-05 2.74228073e-05 5.49429402e-05\n",
      " 2.41550956e-05 3.68414288e+01 8.56532880e+01 6.05722862e-05\n",
      " 2.83361239e-05 5.34054990e-05 2.44708038e-05 4.88697841e+01\n",
      " 8.98162178e+01 3.86303310e+01 6.13841504e+01 5.21967517e+01\n",
      " 1.29022756e+02 5.15844109e+01 7.74912164e+01 4.57929090e+01]\n",
      "39-th iteration, loss: 0.07370095658625482, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.561630948840577e-06\n",
      "39-th iteration, new layer inserted. now 42 layers\n",
      "[1.49735798e+00 8.24552793e+01 1.39568037e+02 1.19096695e+02\n",
      " 1.38021013e+02 8.63941975e+01 1.18446534e+02 8.03798311e+01\n",
      " 1.24246189e+02 2.95039731e-05 9.63774884e-06 2.43210879e-05\n",
      " 7.81615167e-06 1.42325656e-05 4.45488162e-06 9.35612971e-06\n",
      " 2.88111262e-06 0.00000000e+00 2.11758237e-22 6.16756862e+01\n",
      " 9.05190434e+01 4.09226344e+01 1.49215727e+01 6.52385528e-05\n",
      " 2.87801817e-05 5.85766896e-05 2.55050491e-05 3.68414325e+01\n",
      " 8.56532889e+01 6.21600285e-05 2.91632927e-05 5.50321337e-05\n",
      " 2.52246715e-05 4.88697858e+01 8.98162189e+01 3.86303342e+01\n",
      " 6.13841520e+01 5.21967508e+01 1.29022755e+02 5.15844108e+01\n",
      " 7.74912169e+01 4.57929084e+01]\n",
      "40-th iteration, loss: 0.07370095657920894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.412484752975264e-06\n",
      "40-th iteration, new layer inserted. now 44 layers\n",
      "[1.49735849e+00 8.24552809e+01 1.39568037e+02 1.19096699e+02\n",
      " 1.38021017e+02 8.63941999e+01 1.18446534e+02 8.03798334e+01\n",
      " 1.24246190e+02 3.39436884e-05 1.09974278e-05 2.87717406e-05\n",
      " 9.14822996e-06 1.86920885e-05 5.77080815e-06 1.38207081e-05\n",
      " 4.18642139e-06 4.46784803e-06 1.30530877e-06 0.00000000e+00\n",
      " 1.05879118e-22 6.16756907e+01 9.05190464e+01 4.09226375e+01\n",
      " 1.49215739e+01 6.86867667e-05 3.00246406e-05 6.20287905e-05\n",
      " 2.67415965e-05 3.68414359e+01 8.56532897e+01 6.35077009e-05\n",
      " 2.98753273e-05 5.64198340e-05 2.58611722e-05 4.88697872e+01\n",
      " 8.98162200e+01 3.86303373e+01 6.13841536e+01 5.21967501e+01\n",
      " 1.29022755e+02 5.15844108e+01 7.74912174e+01 4.57929079e+01]\n",
      "41-th iteration, loss: 0.07370095657212937, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2773344296287635e-06\n",
      "41-th iteration, new layer inserted. now 44 layers\n",
      "[1.49735899e+00 8.24552824e+01 1.39568037e+02 1.19096703e+02\n",
      " 1.38021022e+02 8.63942023e+01 1.18446535e+02 8.03798355e+01\n",
      " 1.24246192e+02 3.82212334e-05 1.22768377e-05 3.30617659e-05\n",
      " 1.03949884e-05 2.29924956e-05 6.99635397e-06 1.81276642e-05\n",
      " 5.39628285e-06 8.77955507e-06 2.51009991e-06 4.31318837e-06\n",
      " 1.20479114e-06 6.16756950e+01 9.05190493e+01 4.09226404e+01\n",
      " 1.49215751e+01 7.19599049e-05 3.11609309e-05 6.53059839e-05\n",
      " 2.78695091e-05 3.68414392e+01 8.56532903e+01 6.46308199e-05\n",
      " 3.04770927e-05 5.75839581e-05 2.63854988e-05 4.88697884e+01\n",
      " 8.98162210e+01 3.86303403e+01 6.13841553e+01 5.21967494e+01\n",
      " 1.29022754e+02 5.15844110e+01 7.74912180e+01 4.57929073e+01]\n",
      "42-th iteration, loss: 0.07370095656556033, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.2631469443170045e-06\n",
      "42-th iteration, new layer inserted. now 44 layers\n",
      "[1.49735949e+00 8.24552839e+01 1.39568037e+02 1.19096707e+02\n",
      " 1.38021026e+02 8.63942046e+01 1.18446536e+02 8.03798376e+01\n",
      " 1.24246193e+02 4.23339930e-05 1.34768358e-05 3.71884578e-05\n",
      " 1.15574665e-05 2.71309842e-05 8.13273913e-06 2.22740926e-05\n",
      " 6.51209589e-06 1.29321074e-05 3.61594951e-06 8.46858932e-06\n",
      " 2.30574593e-06 6.16756991e+01 9.05190521e+01 4.09226431e+01\n",
      " 1.49215761e+01 7.50686899e-05 3.21965115e-05 6.84189775e-05\n",
      " 2.88962695e-05 3.68414423e+01 8.56532909e+01 6.55461605e-05\n",
      " 3.09751192e-05 5.85411300e-05 2.68044885e-05 4.88697894e+01\n",
      " 8.98162219e+01 3.86303434e+01 6.13841570e+01 5.21967488e+01\n",
      " 1.29022754e+02 5.15844112e+01 7.74912186e+01 4.57929068e+01]\n",
      "43-th iteration, loss: 0.07370095655944127, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.249667429159587e-06\n",
      "43-th iteration, new layer inserted. now 46 layers\n",
      "[1.49735999e+00 8.24552853e+01 1.39568037e+02 1.19096711e+02\n",
      " 0.00000000e+00 7.10542736e-15 1.38021030e+02 8.63942069e+01\n",
      " 1.18446536e+02 8.03798395e+01 1.24246194e+02 4.62909312e-05\n",
      " 1.46017692e-05 4.11606901e-05 1.26401966e-05 3.11163325e-05\n",
      " 9.18467976e-06 2.62686703e-05 7.53875884e-06 1.69340754e-05\n",
      " 4.62793650e-06 1.24746608e-05 3.30812237e-06 6.16757031e+01\n",
      " 9.05190548e+01 4.09226457e+01 1.49215771e+01 7.80247869e-05\n",
      " 3.31383888e-05 7.13794227e-05 2.98289063e-05 3.68414453e+01\n",
      " 8.56532914e+01 6.62705064e-05 3.13772171e-05 5.93079908e-05\n",
      " 2.71262356e-05 4.88697902e+01 8.98162227e+01 3.86303464e+01\n",
      " 6.13841587e+01 5.21967483e+01 1.29022754e+02 5.15844116e+01\n",
      " 7.74912192e+01 4.57929063e+01]\n",
      "44-th iteration, loss: 0.07370095655282231, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.22427365351717e-06\n",
      "44-th iteration, new layer inserted. now 48 layers\n",
      "[1.49736049e+00 8.24552868e+01 1.39568037e+02 1.19096715e+02\n",
      " 4.20183033e-06 3.92613829e-06 0.00000000e+00 4.23516474e-22\n",
      " 1.38021034e+02 8.63942091e+01 1.18446537e+02 8.03798413e+01\n",
      " 1.24246195e+02 5.00954025e-05 1.56517730e-05 4.49817322e-05\n",
      " 1.36434894e-05 3.49517193e-05 1.01526602e-05 3.01144803e-05\n",
      " 8.47692840e-06 2.07884407e-05 5.54688846e-06 1.63342781e-05\n",
      " 4.21291750e-06 6.16757070e+01 9.05190575e+01 4.09226483e+01\n",
      " 1.49215780e+01 8.08373965e-05 3.39917350e-05 7.41965079e-05\n",
      " 3.06726123e-05 3.68414481e+01 8.56532918e+01 6.68166176e-05\n",
      " 3.16895778e-05 5.98971689e-05 2.73571929e-05 4.88697908e+01\n",
      " 8.98162235e+01 3.86303495e+01 6.13841604e+01 5.21967479e+01\n",
      " 1.29022754e+02 5.15844122e+01 7.74912198e+01 4.57929058e+01]\n",
      "45-th iteration, loss: 0.07370095654568888, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.187340836922287e-06\n",
      "45-th iteration, new layer inserted. now 48 layers\n",
      "[1.49736098e+00 8.24552882e+01 1.39568037e+02 1.19096719e+02\n",
      " 8.37104570e-06 7.81370127e-06 4.16993185e-06 3.88756298e-06\n",
      " 1.38021039e+02 8.63942113e+01 1.18446537e+02 8.03798430e+01\n",
      " 1.24246196e+02 5.37580944e-05 1.66303627e-05 4.86621866e-05\n",
      " 1.45710316e-05 3.86476569e-05 1.10405376e-05 3.38219397e-05\n",
      " 9.33063036e-06 2.45055201e-05 6.37699870e-06 2.00576524e-05\n",
      " 5.02449081e-06 6.16757107e+01 9.05190601e+01 4.09226506e+01\n",
      " 1.49215787e+01 8.35213804e-05 3.47663160e-05 7.68850825e-05\n",
      " 3.14371727e-05 3.68414508e+01 8.56532922e+01 6.71941350e-05\n",
      " 3.19193045e-05 6.03181820e-05 2.75047072e-05 4.88697913e+01\n",
      " 8.98162242e+01 3.86303525e+01 6.13841621e+01 5.21967476e+01\n",
      " 1.29022754e+02 5.15844128e+01 7.74912205e+01 4.57929053e+01]\n",
      "46-th iteration, loss: 0.0737009565389048, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.151645633804102e-06\n",
      "46-th iteration, new layer inserted. now 48 layers\n",
      "[1.49736146e+00 8.24552895e+01 1.39568037e+02 1.19096723e+02\n",
      " 1.25036556e-05 1.16533232e-05 8.30396769e-06 7.72642398e-06\n",
      " 1.38021043e+02 8.63942135e+01 1.18446538e+02 8.03798446e+01\n",
      " 1.24246197e+02 5.72942444e-05 1.75448041e-05 5.22172094e-05\n",
      " 1.54302489e-05 4.22192157e-05 1.18558956e-05 3.74060278e-05\n",
      " 1.01076056e-05 2.81001971e-05 7.12616384e-06 2.36595663e-05\n",
      " 5.75089355e-06 6.16757143e+01 9.05190627e+01 4.09226529e+01\n",
      " 1.49215795e+01 8.60922985e-05 3.54726634e-05 7.94606958e-05\n",
      " 3.21331364e-05 3.68414534e+01 8.56532924e+01 6.74143037e-05\n",
      " 3.20739787e-05 6.05821619e-05 2.75765911e-05 4.88697916e+01\n",
      " 8.98162249e+01 3.86303555e+01 6.13841639e+01 5.21967474e+01\n",
      " 1.29022754e+02 5.15844136e+01 7.74912213e+01 4.57929049e+01]\n",
      "47-th iteration, loss: 0.07370095653243229, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.117138828278133e-06\n",
      "47-th iteration, new layer inserted. now 48 layers\n",
      "[1.49736193e+00 8.24552908e+01 1.39568037e+02 1.19096727e+02\n",
      " 1.66008240e-05 1.54459706e-05 1.24032626e-05 1.15175559e-05\n",
      " 1.38021047e+02 8.63942156e+01 1.18446538e+02 8.03798461e+01\n",
      " 1.24246198e+02 6.07109281e-05 1.83986500e-05 5.56538039e-05\n",
      " 1.62248361e-05 4.56733212e-05 1.26025703e-05 4.08735879e-05\n",
      " 1.08118301e-05 3.15792278e-05 7.79849889e-06 2.71466842e-05\n",
      " 6.39637855e-06 6.16757178e+01 9.05190652e+01 4.09226551e+01\n",
      " 1.49215801e+01 8.85585113e-05 3.61158013e-05 8.19316994e-05\n",
      " 3.27655430e-05 3.68414558e+01 8.56532926e+01 6.74892783e-05\n",
      " 3.21592499e-05 6.07011599e-05 2.75787096e-05 4.88697917e+01\n",
      " 8.98162256e+01 3.86303585e+01 6.13841658e+01 5.21967473e+01\n",
      " 1.29022755e+02 5.15844145e+01 7.74912220e+01 4.57929044e+01]\n",
      "48-th iteration, loss: 0.07370095652623808, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.0837732980975165e-06\n",
      "48-th iteration, new layer inserted. now 48 layers\n",
      "[1.49736239e+00 8.24552921e+01 1.39568037e+02 1.19096730e+02\n",
      " 2.06636703e-05 1.91925942e-05 1.64689276e-05 1.52619161e-05\n",
      " 1.38021051e+02 8.63942176e+01 1.18446539e+02 8.03798476e+01\n",
      " 1.24246199e+02 6.40148157e-05 1.91952538e-05 5.89785711e-05\n",
      " 1.69582812e-05 4.90165012e-05 1.32841831e-05 4.42310700e-05\n",
      " 1.14470575e-05 3.49489796e-05 8.39788857e-06 3.05252861e-05\n",
      " 6.96496080e-06 6.16757212e+01 9.05190676e+01 4.09226572e+01\n",
      " 1.49215807e+01 9.09277809e-05 3.67004021e-05 8.43058464e-05\n",
      " 3.33390787e-05 3.68414582e+01 8.56532927e+01 6.74302746e-05\n",
      " 3.21803493e-05 6.06862969e-05 2.75164927e-05 4.88697918e+01\n",
      " 8.98162262e+01 3.86303616e+01 6.13841676e+01 5.21967473e+01\n",
      " 1.29022755e+02 5.15844155e+01 7.74912229e+01 4.57929040e+01]\n",
      "49-th iteration, loss: 0.07370095652029278, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.051503925085071e-06\n",
      "49-th iteration, new layer inserted. now 50 layers\n",
      "[1.49736285e+00 8.24552934e+01 1.39568037e+02 1.19096734e+02\n",
      " 2.46932717e-05 2.28941289e-05 2.05020314e-05 1.89604455e-05\n",
      " 0.00000000e+00 1.69406589e-21 1.38021055e+02 8.63942197e+01\n",
      " 1.18446539e+02 8.03798489e+01 1.24246200e+02 6.72121948e-05\n",
      " 1.99377815e-05 6.21977340e-05 1.76338771e-05 5.22549091e-05\n",
      " 1.39041526e-05 4.74845535e-05 1.20168313e-05 3.82154539e-05\n",
      " 8.92800052e-06 3.38012907e-05 7.46043110e-06 6.16757245e+01\n",
      " 9.05190701e+01 4.09226593e+01 1.49215813e+01 9.32073128e-05\n",
      " 3.72308102e-05 8.65903347e-05 3.38581013e-05 3.68414605e+01\n",
      " 8.56532927e+01 6.72476431e-05 3.21421208e-05 6.05478350e-05\n",
      " 2.73949681e-05 4.88697917e+01 8.98162268e+01 3.86303646e+01\n",
      " 6.13841695e+01 5.21967474e+01 1.29022756e+02 5.15844166e+01\n",
      " 7.74912237e+01 4.57929036e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5323788484040266\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.01319852    0.         1863.53454017]\n",
      "1-th iteration, loss: 0.7519772466020608, 11 gd steps\n",
      "insert gradient: -0.6140803251599061\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.05862423   62.16864421  227.26030978    0.         1636.27423039]\n",
      "2-th iteration, loss: 0.601661687281514, 13 gd steps\n",
      "insert gradient: -0.5935751135100985\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.30806535   78.12680833  213.73632702   43.46952547  233.75346148\n",
      "    0.         1402.52076891]\n",
      "3-th iteration, loss: 0.46654972711189185, 19 gd steps\n",
      "insert gradient: -0.730852708419782\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           57.34483502  221.073772     57.74579546  160.21882071\n",
      "   47.87822166  360.64819772    0.         1041.87257119]\n",
      "4-th iteration, loss: 0.36993940719111573, 13 gd steps\n",
      "insert gradient: -0.30459836246614796\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.39779506  63.24090324 221.52488159  49.83241276 145.80673157\n",
      "  52.4845547  341.40532459  47.02786561 617.40596811   0.\n",
      " 424.46660308]\n",
      "5-th iteration, loss: 0.31092730871924373, 34 gd steps\n",
      "insert gradient: -0.21381313610030447\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          60.82626781 207.23741565  68.1402505  130.46931838\n",
      "  51.72985438 233.62043608   0.         109.02287017  44.04389544\n",
      " 565.36478925  41.19136113 424.46660308]\n",
      "6-th iteration, loss: 0.2760567019229985, 23 gd steps\n",
      "insert gradient: -0.08916541564641096\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[1.07056166e-01 6.62922509e+01 1.96925803e+02 7.19407708e+01\n",
      " 1.15963405e+02 5.45788902e+01 1.92336068e+02 3.40058703e+01\n",
      " 6.76901830e+01 5.21445067e+01 5.55154142e+02 4.32915244e+01\n",
      " 4.00885125e+02 0.00000000e+00 2.35814779e+01]\n",
      "7-th iteration, loss: 0.26773130797076744, 23 gd steps\n",
      "insert gradient: -0.07105106929383721\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.          62.61655081 112.83356068   0.          87.75943608\n",
      "  69.49936067 112.47342043  58.92712256 180.72193963  37.93847931\n",
      "  67.0612905   49.92728133 552.86104794  37.96083055 391.81536688\n",
      "  12.10327957  23.58147795]\n",
      "8-th iteration, loss: 0.2649819384631086, 25 gd steps\n",
      "insert gradient: -0.087663001157553\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[3.38490571e-01 6.12869313e+01 2.00358391e+02 7.05013816e+01\n",
      " 1.11245863e+02 5.92052455e+01 1.77250692e+02 3.89766984e+01\n",
      " 6.38038413e+01 5.12506181e+01 2.05380498e+02 0.00000000e+00\n",
      " 3.42300830e+02 3.99314056e+01 3.74366815e+02 1.75184934e+01\n",
      " 2.35814779e+01]\n",
      "9-th iteration, loss: 0.2553118359212187, 22 gd steps\n",
      "insert gradient: -0.16604116010113815\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.58076969  64.4266808  204.37934764  68.83266231 113.46057653\n",
      "  54.79083267 172.13598888  49.72470356  59.39025026  47.16529545\n",
      " 196.57760398  20.73305063 175.05391374   0.         131.2904353\n",
      "  41.43614084 360.36521585  23.31706418  23.58147795]\n",
      "10-th iteration, loss: 0.1639003027454008, 33 gd steps\n",
      "insert gradient: -0.09526074469064459\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  3.32855615  59.39979897 187.31039475  65.04804561 121.29649847\n",
      "  68.77194589  75.2938316    0.         105.41136424  53.5254241\n",
      "  69.31679103  34.27613727 144.1524392   64.44380573 143.60298743\n",
      "  25.83099559  88.43633275  50.01429549 335.93789397  32.88410691\n",
      "  23.58147795]\n",
      "11-th iteration, loss: 0.15607097869577397, 37 gd steps\n",
      "insert gradient: -0.050246598160876045\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  2.55547064  63.67796707 173.03319133  71.68819581 129.28412132\n",
      "  61.38932796  61.80445737  12.04623304  99.7621101   51.7990729\n",
      "  74.58230035  36.01404486 133.1189513   66.19188113  93.17486847\n",
      "   0.          53.24278198  31.70215302  76.742358    55.45821459\n",
      " 341.8189029   23.22680655  23.58147795]\n",
      "12-th iteration, loss: 0.15569763713579657, 14 gd steps\n",
      "insert gradient: -0.0412134559688332\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  2.46197769  62.81554579 172.05294394  71.86170384 130.30944038\n",
      "  62.65779898  61.34306644  12.13736247 100.44253151  50.90615785\n",
      "  74.98419521  36.05675207 131.71244981  66.29259791  92.59503086\n",
      "   1.07946101  52.60765735  32.32628482  76.16442671  55.47248031\n",
      " 205.65674193   0.         137.10449462  23.37232706  23.58147795]\n",
      "13-th iteration, loss: 0.09999944607089603, 271 gd steps\n",
      "insert gradient: -0.03443250506735769\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  2.93870833  64.94632619 184.85440268  81.30256234 110.42154062\n",
      "  70.47329832 206.76327309  59.33840482 113.99354223  55.43998719\n",
      "  77.17559954  43.96954263 113.08184402   1.67402405  74.45547338\n",
      "  47.83453822  87.50911418  39.69344994 102.71926805   0.\n",
      " 102.71926805  27.35478334 100.67565281  61.84555067  23.58147795]\n",
      "14-th iteration, loss: 0.09308248144810409, 35 gd steps\n",
      "insert gradient: -0.0030456131012702066\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[3.47949918e+00 7.07860723e+01 1.82879197e+02 7.87460267e+01\n",
      " 1.11531911e+02 0.00000000e+00 1.77635684e-14 7.04157377e+01\n",
      " 2.03466348e+02 6.10446640e+01 1.11230375e+02 5.67447914e+01\n",
      " 8.28762826e+01 4.35023196e+01 1.01752745e+02 7.08411188e+00\n",
      " 6.78597733e+01 4.75290727e+01 9.45787092e+01 4.40051712e+01\n",
      " 8.05200608e+01 1.45124211e+01 8.21805038e+01 2.34037294e+01\n",
      " 9.84279530e+01 7.48368275e+01 2.35814779e+01]\n",
      "15-th iteration, loss: 0.08247875975522209, 220 gd steps\n",
      "insert gradient: -0.001644908809744164\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[7.84693785e-01 7.70057348e+01 1.79478985e+02 7.51887300e+01\n",
      " 1.20331494e+02 6.53294889e+01 2.01256989e+02 7.03018873e+01\n",
      " 1.03400207e+02 6.04212806e+01 9.24315265e+01 4.86260404e+01\n",
      " 9.35670614e+01 0.00000000e+00 3.55271368e-15 4.49393835e+01\n",
      " 1.46636231e+01 4.60776264e+01 9.13278227e+01 4.66150346e+01\n",
      " 7.94869765e+01 3.66430224e+01 5.21745701e+01 2.53350107e+01\n",
      " 7.53792036e+01 7.32460562e+01 2.35814779e+01]\n",
      "16-th iteration, loss: 0.07694665429923797, 49 gd steps\n",
      "insert gradient: -0.001353710645226684\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[1.51916791e+00 7.31298071e+01 1.74784092e+02 8.07751432e+01\n",
      " 1.18585654e+02 0.00000000e+00 3.19744231e-14 6.97934812e+01\n",
      " 1.95401195e+02 6.86548876e+01 1.09170400e+02 5.43551385e+01\n",
      " 8.68725154e+01 5.61611722e+01 9.72340055e+01 6.01505716e+01\n",
      " 7.53554174e+00 3.59782022e+01 9.25910060e+01 4.61665561e+01\n",
      " 8.28608176e+01 4.43288082e+01 5.88962310e+01 2.27242670e+01\n",
      " 7.03262453e+01 4.73971710e+01 2.35814779e+01]\n",
      "17-th iteration, loss: 0.07639093243523365, 96 gd steps\n",
      "insert gradient: -0.0005765252652126836\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[1.87463573e+00 0.00000000e+00 2.22044605e-16 7.44797407e+01\n",
      " 1.70926796e+02 8.18826955e+01 1.17452592e+02 7.09965619e+01\n",
      " 1.94430047e+02 6.80672566e+01 1.09338286e+02 5.54127285e+01\n",
      " 8.45527169e+01 5.59079341e+01 1.00792335e+02 6.20772828e+01\n",
      " 1.89528957e+01 2.43073627e+01 9.58355713e+01 4.66937618e+01\n",
      " 8.27145491e+01 4.39272625e+01 7.38361062e+01 1.92692018e+01\n",
      " 6.39223728e+01 4.64057242e+01 2.35814779e+01]\n",
      "18-th iteration, loss: 0.07614877844551492, 23 gd steps\n",
      "insert gradient: -0.0012428967884119187\n",
      "18-th iteration, new layer inserted. now 25 layers\n",
      "[  1.92427694  74.78791157 170.36405347  82.00057655 117.43632844\n",
      "  71.47424766 194.35286911  68.03542251 109.74513801  55.59914995\n",
      "  85.3538155   55.50656436 101.42085812  61.74978973  25.13258611\n",
      "  20.87441981  96.44646342  47.14440351  82.08430168  44.03185304\n",
      "  76.93080693  19.94501346  59.5706605   46.80591938  23.58147795]\n",
      "19-th iteration, loss: 0.05997466717758198, 251 gd steps\n",
      "insert gradient: -0.002624179346516512\n",
      "19-th iteration, new layer inserted. now 27 layers\n",
      "[1.26530066e+00 8.72876618e+01 1.56747867e+02 7.79703742e+01\n",
      " 1.35557999e+02 0.00000000e+00 2.48689958e-14 7.24570144e+01\n",
      " 1.50806899e+02 8.83648993e+01 1.13504712e+02 7.01435348e+01\n",
      " 8.98593152e+01 4.47547227e+01 1.01664480e+02 6.84395761e+01\n",
      " 1.00734782e+02 4.49092258e+01 7.65693840e+01 4.45265657e+01\n",
      " 5.98747906e+01 2.41578628e+01 8.90327302e+01 5.00148684e+01\n",
      " 9.13174142e+01 2.92930202e+01 2.35814779e+01]\n",
      "20-th iteration, loss: 0.05905574697480738, 515 gd steps\n",
      "insert gradient: -5.2212908594336644e-05\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[2.56429140e+00 8.31501627e+01 1.39858584e+02 0.00000000e+00\n",
      " 2.48689958e-14 8.89696114e+01 1.34118009e+02 7.32788683e+01\n",
      " 1.46554641e+02 9.08843515e+01 1.14379967e+02 7.16278870e+01\n",
      " 9.20068864e+01 4.38114565e+01 9.92189204e+01 6.94423652e+01\n",
      " 1.02001589e+02 4.46909096e+01 7.98391846e+01 4.33202174e+01\n",
      " 7.58269341e+01 1.66515134e+01 8.64747220e+01 4.92686102e+01\n",
      " 8.83658116e+01 3.52982511e+01 2.35814779e+01]\n",
      "21-th iteration, loss: 0.05905079044422567, 264 gd steps\n",
      "insert gradient: -1.6841709768423936e-05\n",
      "21-th iteration, new layer inserted. now 27 layers\n",
      "[2.78370243e+00 8.29751151e+01 1.38756080e+02 8.96234470e+01\n",
      " 1.34338819e+02 7.31648450e+01 1.46213736e+02 9.12739420e+01\n",
      " 1.14378886e+02 7.17752946e+01 9.26156934e+01 4.36002631e+01\n",
      " 9.89489202e+01 6.95890062e+01 1.02111191e+02 4.47388429e+01\n",
      " 7.99122663e+01 4.31323241e+01 7.60944558e+01 1.66169856e+01\n",
      " 8.62556171e+01 4.92842855e+01 0.00000000e+00 5.32907052e-15\n",
      " 8.89486833e+01 3.55733091e+01 2.35814779e+01]\n",
      "22-th iteration, loss: 0.05905037502439026, 100 gd steps\n",
      "insert gradient: -1.5552620138532058e-05\n",
      "22-th iteration, new layer inserted. now 25 layers\n",
      "[  2.75766825  82.99154504 138.54103525  89.74044584 134.30688573\n",
      "  73.16615902 146.19773628  91.35112413 114.30706751  71.84030359\n",
      "  92.64776799  43.59075147  98.88161504  69.64399641 102.08941439\n",
      "  44.75834665  79.89532953  43.10780986  76.09008924  16.6333353\n",
      "  86.21476939  49.29411695  89.12334841  35.60114282  23.58147795]\n",
      "23-th iteration, loss: 0.05905019485113033, 38 gd steps\n",
      "insert gradient: -3.408105103630299e-05\n",
      "23-th iteration, new layer inserted. now 27 layers\n",
      "[2.75009678e+00 8.29842205e+01 1.38376714e+02 8.98421581e+01\n",
      " 1.34338452e+02 7.31424313e+01 1.46163961e+02 9.14091408e+01\n",
      " 1.14282193e+02 7.18822178e+01 9.26959509e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.35605297e+01 9.88333876e+01 6.96790499e+01\n",
      " 1.02090920e+02 4.47649937e+01 7.99192581e+01 4.30961868e+01\n",
      " 7.60734420e+01 1.66399717e+01 8.61961200e+01 4.92987467e+01\n",
      " 8.91765084e+01 3.56762027e+01 2.35814779e+01]\n",
      "24-th iteration, loss: 0.059050098915732104, 228 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.247813392175529e-06\n",
      "24-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040582e+00 8.29689193e+01 1.38250770e+02 8.99178990e+01\n",
      " 1.34359946e+02 7.31216821e+01 1.46148559e+02 9.14668823e+01\n",
      " 1.14257236e+02 7.19237337e+01 9.27562718e+01 4.35423055e+01\n",
      " 9.87936058e+01 6.97165028e+01 1.02091516e+02 4.47686787e+01\n",
      " 7.99239425e+01 4.30692652e+01 7.60582108e+01 1.66546030e+01\n",
      " 8.61879748e+01 4.93018596e+01 0.00000000e+00 5.32907052e-15\n",
      " 8.92757504e+01 3.57207605e+01 2.35814779e+01]\n",
      "25-th iteration, loss: 0.05905009891369161, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.245281366520803e-06\n",
      "25-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040589e+00 8.29689187e+01 1.38250766e+02 8.99179016e+01\n",
      " 1.34359947e+02 7.31216814e+01 1.46148558e+02 9.14668841e+01\n",
      " 1.14257235e+02 7.19237350e+01 9.27562725e+01 4.35423048e+01\n",
      " 9.87936044e+01 6.97165038e+01 1.02091516e+02 4.47686790e+01\n",
      " 7.99239427e+01 4.30692645e+01 7.60582101e+01 1.66546034e+01\n",
      " 8.61879745e+01 4.93018597e+01 3.24667726e-06 1.05861037e-07\n",
      " 8.92757537e+01 3.57207620e+01 2.35814779e+01]\n",
      "26-th iteration, loss: 0.05905009891165187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2428227672272596e-06\n",
      "26-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040595e+00 8.29689182e+01 1.38250762e+02 8.99179041e+01\n",
      " 1.34359948e+02 7.31216807e+01 1.46148558e+02 9.14668860e+01\n",
      " 1.14257234e+02 7.19237364e+01 9.27562732e+01 4.35423040e+01\n",
      " 9.87936031e+01 6.97165048e+01 1.02091516e+02 4.47686792e+01\n",
      " 7.99239428e+01 4.30692637e+01 7.60582094e+01 1.66546039e+01\n",
      " 8.61879741e+01 4.93018598e+01 6.49075270e-06 2.03661807e-07\n",
      " 8.92757569e+01 3.57207634e+01 2.35814779e+01]\n",
      "27-th iteration, loss: 0.059050098909612894, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2404337163778252e-06\n",
      "27-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040602e+00 8.29689177e+01 1.38250758e+02 8.99179067e+01\n",
      " 1.34359949e+02 7.31216799e+01 1.46148557e+02 9.14668879e+01\n",
      " 1.14257234e+02 7.19237378e+01 9.27562739e+01 4.35423033e+01\n",
      " 9.87936017e+01 6.97165058e+01 1.02091516e+02 4.47686794e+01\n",
      " 7.99239430e+01 4.30692629e+01 7.60582088e+01 1.66546043e+01\n",
      " 8.61879738e+01 4.93018599e+01 9.73230564e-06 2.93633505e-07\n",
      " 8.92757602e+01 3.57207649e+01 2.35814779e+01]\n",
      "28-th iteration, loss: 0.059050098907574684, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2381105559897493e-06\n",
      "28-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040609e+00 8.29689172e+01 1.38250754e+02 8.99179092e+01\n",
      " 1.34359950e+02 7.31216792e+01 1.46148556e+02 9.14668897e+01\n",
      " 1.14257233e+02 7.19237392e+01 9.27562745e+01 4.35423025e+01\n",
      " 9.87936003e+01 6.97165068e+01 1.02091516e+02 4.47686796e+01\n",
      " 7.99239432e+01 4.30692622e+01 7.60582081e+01 1.66546047e+01\n",
      " 8.61879734e+01 4.93018600e+01 1.29714114e-05 3.75995083e-07\n",
      " 8.92757634e+01 3.57207664e+01 2.35814779e+01]\n",
      "29-th iteration, loss: 0.05905009890553711, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.235849836183617e-06\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040615e+00 8.29689166e+01 1.38250749e+02 8.99179118e+01\n",
      " 1.34359951e+02 7.31216785e+01 1.46148556e+02 9.14668916e+01\n",
      " 1.14257232e+02 7.19237406e+01 9.27562752e+01 4.35423018e+01\n",
      " 9.87935990e+01 6.97165077e+01 1.02091516e+02 4.47686798e+01\n",
      " 7.99239434e+01 4.30692614e+01 7.60582074e+01 1.66546051e+01\n",
      " 8.61879731e+01 4.93018601e+01 1.62081415e-05 4.50953953e-07\n",
      " 8.92757666e+01 3.57207678e+01 2.35814779e+01]\n",
      "30-th iteration, loss: 0.05905009890350027, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.233648302755921e-06\n",
      "30-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040622e+00 8.29689161e+01 1.38250745e+02 8.99179144e+01\n",
      " 1.34359952e+02 7.31216778e+01 1.46148555e+02 9.14668935e+01\n",
      " 1.14257231e+02 7.19237420e+01 9.27562759e+01 4.35423011e+01\n",
      " 9.87935976e+01 6.97165087e+01 1.02091516e+02 4.47686800e+01\n",
      " 7.99239436e+01 4.30692606e+01 7.60582067e+01 1.66546055e+01\n",
      " 8.61879727e+01 4.93018602e+01 1.94425639e-05 5.18706637e-07\n",
      " 0.00000000e+00 7.94093388e-23 8.92757699e+01 3.57207693e+01\n",
      " 2.35814779e+01]\n",
      "31-th iteration, loss: 0.059050098901110536, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2286015867428537e-06\n",
      "31-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040629e+00 8.29689156e+01 1.38250741e+02 8.99179169e+01\n",
      " 1.34359953e+02 7.31216771e+01 1.46148554e+02 9.14668954e+01\n",
      " 1.14257230e+02 7.19237434e+01 9.27562766e+01 4.35423003e+01\n",
      " 9.87935962e+01 6.97165097e+01 1.02091516e+02 4.47686801e+01\n",
      " 7.99239437e+01 4.30692598e+01 7.60582061e+01 1.66546060e+01\n",
      " 8.61879724e+01 4.93018602e+01 2.26738179e-05 5.77119041e-07\n",
      " 3.23171789e-06 5.84124041e-08 8.92757731e+01 3.57207707e+01\n",
      " 2.35814779e+01]\n",
      "32-th iteration, loss: 0.05905009889872305, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2237499703524436e-06\n",
      "32-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040635e+00 8.29689150e+01 1.38250737e+02 8.99179195e+01\n",
      " 1.34359954e+02 7.31216764e+01 1.46148554e+02 9.14668973e+01\n",
      " 1.14257229e+02 7.19237448e+01 9.27562773e+01 4.35422996e+01\n",
      " 9.87935949e+01 6.97165107e+01 1.02091516e+02 4.47686803e+01\n",
      " 7.99239439e+01 4.30692590e+01 7.60582054e+01 1.66546064e+01\n",
      " 8.61879720e+01 4.93018603e+01 2.58999865e-05 6.22020423e-07\n",
      " 6.45840248e-06 1.00424017e-07 8.92757763e+01 3.57207722e+01\n",
      " 2.35814779e+01]\n",
      "33-th iteration, loss: 0.05905009889633766, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.219081724572155e-06\n",
      "33-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040642e+00 8.29689145e+01 1.38250733e+02 8.99179220e+01\n",
      " 1.34359955e+02 7.31216757e+01 1.46148553e+02 9.14668992e+01\n",
      " 1.14257228e+02 7.19237462e+01 9.27562779e+01 4.35422988e+01\n",
      " 9.87935935e+01 6.97165117e+01 1.02091516e+02 4.47686805e+01\n",
      " 7.99239441e+01 4.30692582e+01 7.60582047e+01 1.66546068e+01\n",
      " 8.61879716e+01 4.93018604e+01 2.91212873e-05 6.54021991e-07\n",
      " 9.68025955e-06 1.26650547e-07 8.92757796e+01 3.57207736e+01\n",
      " 2.35814779e+01]\n",
      "34-th iteration, loss: 0.05905009889395425, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2145858587893714e-06\n",
      "34-th iteration, new layer inserted. now 31 layers\n",
      "[2.75040648e+00 8.29689140e+01 1.38250729e+02 8.99179246e+01\n",
      " 1.34359956e+02 7.31216750e+01 1.46148552e+02 9.14669011e+01\n",
      " 1.14257227e+02 7.19237476e+01 9.27562786e+01 4.35422981e+01\n",
      " 9.87935921e+01 6.97165127e+01 1.02091516e+02 4.47686806e+01\n",
      " 7.99239442e+01 4.30692574e+01 7.60582040e+01 1.66546072e+01\n",
      " 8.61879713e+01 4.93018604e+01 3.23379259e-05 6.73698020e-07\n",
      " 1.28974829e-05 1.37670585e-07 0.00000000e+00 2.64697796e-23\n",
      " 8.92757828e+01 3.57207751e+01 2.35814779e+01]\n",
      "35-th iteration, loss: 0.059050098891223785, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2074887424365026e-06\n",
      "35-th iteration, new layer inserted. now 31 layers\n",
      "[2.75040655e+00 8.29689135e+01 1.38250725e+02 8.99179272e+01\n",
      " 1.34359957e+02 7.31216744e+01 1.46148552e+02 9.14669030e+01\n",
      " 1.14257227e+02 7.19237491e+01 9.27562793e+01 4.35422973e+01\n",
      " 9.87935907e+01 6.97165137e+01 1.02091516e+02 4.47686808e+01\n",
      " 7.99239444e+01 4.30692566e+01 7.60582033e+01 1.66546075e+01\n",
      " 8.61879709e+01 4.93018604e+01 3.55492214e-05 6.79410616e-07\n",
      " 1.61093808e-05 1.31850381e-07 0.00000000e+00 1.98523347e-23\n",
      " 8.92757892e+01 3.57207765e+01 2.35814779e+01]\n",
      "36-th iteration, loss: 0.05905009888849745, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2006625627372774e-06\n",
      "36-th iteration, new layer inserted. now 31 layers\n",
      "[2.75040661e+00 8.29689129e+01 1.38250721e+02 8.99179297e+01\n",
      " 1.34359958e+02 7.31216737e+01 1.46148551e+02 9.14669049e+01\n",
      " 1.14257226e+02 7.19237505e+01 9.27562800e+01 4.35422965e+01\n",
      " 9.87935893e+01 6.97165147e+01 1.02091516e+02 4.47686809e+01\n",
      " 7.99239445e+01 4.30692558e+01 7.60582026e+01 1.66546079e+01\n",
      " 8.61879705e+01 4.93018605e+01 3.87535247e-05 6.67641266e-07\n",
      " 1.93142916e-05 1.05676181e-07 0.00000000e+00 1.65436123e-23\n",
      " 8.92757956e+01 3.57207780e+01 2.35814779e+01]\n",
      "37-th iteration, loss: 0.059050098885775, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1940909000636525e-06\n",
      "37-th iteration, new layer inserted. now 31 layers\n",
      "[2.75040668e+00 8.29689124e+01 1.38250717e+02 8.99179323e+01\n",
      " 1.34359958e+02 7.31216730e+01 1.46148550e+02 9.14669068e+01\n",
      " 1.14257225e+02 7.19237519e+01 9.27562807e+01 4.35422958e+01\n",
      " 9.87935880e+01 6.97165157e+01 1.02091516e+02 4.47686810e+01\n",
      " 7.99239447e+01 4.30692549e+01 7.60582019e+01 1.66546083e+01\n",
      " 8.61879701e+01 4.93018605e+01 4.19511345e-05 6.39228201e-07\n",
      " 2.25124984e-05 5.99924669e-08 0.00000000e+00 1.32348898e-23\n",
      " 8.92758020e+01 3.57207794e+01 2.35814779e+01]\n",
      "38-th iteration, loss: 0.05905009888305619, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1877494846925103e-06\n",
      "38-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040675e+00 8.29689119e+01 1.38250712e+02 8.99179348e+01\n",
      " 1.34359959e+02 7.31216723e+01 1.46148550e+02 9.14669087e+01\n",
      " 1.14257224e+02 7.19237533e+01 9.27562813e+01 4.35422950e+01\n",
      " 9.87935866e+01 6.97165166e+01 1.02091516e+02 4.47686811e+01\n",
      " 7.99239448e+01 4.30692540e+01 7.60582012e+01 1.66546086e+01\n",
      " 8.61879697e+01 4.93018605e+01 4.51423319e-05 5.94958311e-07\n",
      " 0.00000000e+00 5.29395592e-23 8.92758341e+01 3.57207808e+01\n",
      " 2.35814779e+01]\n",
      "39-th iteration, loss: 0.05905009888068431, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.184223956481176e-06\n",
      "39-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040681e+00 8.29689113e+01 1.38250708e+02 8.99179374e+01\n",
      " 1.34359960e+02 7.31216716e+01 1.46148549e+02 9.14669106e+01\n",
      " 1.14257223e+02 7.19237547e+01 9.27562820e+01 4.35422942e+01\n",
      " 9.87935852e+01 6.97165176e+01 1.02091516e+02 4.47686812e+01\n",
      " 7.99239449e+01 4.30692532e+01 7.60582005e+01 1.66546090e+01\n",
      " 8.61879693e+01 4.93018605e+01 4.83281680e-05 5.37507793e-07\n",
      " 0.00000000e+00 7.94093388e-23 8.92758405e+01 3.57207822e+01\n",
      " 2.35814779e+01]\n",
      "40-th iteration, loss: 0.059050098878313875, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.180778418915311e-06\n",
      "40-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040687e+00 8.29689108e+01 1.38250704e+02 8.99179400e+01\n",
      " 1.34359961e+02 7.31216709e+01 1.46148549e+02 9.14669125e+01\n",
      " 1.14257222e+02 7.19237562e+01 9.27562827e+01 4.35422935e+01\n",
      " 9.87935838e+01 6.97165186e+01 1.02091516e+02 4.47686813e+01\n",
      " 7.99239450e+01 4.30692523e+01 7.60581998e+01 1.66546093e+01\n",
      " 8.61879689e+01 4.93018604e+01 5.15105628e-05 4.71258570e-07\n",
      " 8.92758468e+01 3.57207837e+01 2.35814779e+01]\n",
      "41-th iteration, loss: 0.05905009887628711, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.180151175556181e-06\n",
      "41-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040694e+00 8.29689103e+01 1.38250700e+02 8.99179425e+01\n",
      " 1.34359962e+02 7.31216702e+01 1.46148548e+02 9.14669145e+01\n",
      " 1.14257221e+02 7.19237576e+01 9.27562834e+01 4.35422927e+01\n",
      " 9.87935824e+01 6.97165196e+01 1.02091516e+02 4.47686814e+01\n",
      " 7.99239451e+01 4.30692514e+01 7.60581991e+01 1.66546097e+01\n",
      " 8.61879685e+01 4.93018604e+01 5.46904224e-05 3.98481854e-07\n",
      " 8.92758500e+01 3.57207851e+01 2.35814779e+01]\n",
      "42-th iteration, loss: 0.059050098874260826, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1794955147747076e-06\n",
      "42-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040700e+00 8.29689097e+01 1.38250696e+02 8.99179451e+01\n",
      " 1.34359963e+02 7.31216696e+01 1.46148547e+02 9.14669164e+01\n",
      " 1.14257221e+02 7.19237590e+01 9.27562841e+01 4.35422919e+01\n",
      " 9.87935810e+01 6.97165205e+01 1.02091516e+02 4.47686815e+01\n",
      " 7.99239452e+01 4.30692505e+01 7.60581983e+01 1.66546100e+01\n",
      " 8.61879681e+01 4.93018604e+01 5.78697103e-05 3.23585314e-07\n",
      " 8.92758532e+01 3.57207865e+01 2.35814779e+01]\n",
      "43-th iteration, loss: 0.05905009887223506, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1788133656256457e-06\n",
      "43-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040707e+00 8.29689092e+01 1.38250692e+02 8.99179476e+01\n",
      " 1.34359964e+02 7.31216689e+01 1.46148547e+02 9.14669183e+01\n",
      " 1.14257220e+02 7.19237604e+01 9.27562848e+01 4.35422911e+01\n",
      " 9.87935796e+01 6.97165215e+01 1.02091516e+02 4.47686816e+01\n",
      " 7.99239454e+01 4.30692496e+01 7.60581976e+01 1.66546104e+01\n",
      " 8.61879677e+01 4.93018603e+01 6.10484007e-05 2.46479613e-07\n",
      " 8.92758564e+01 3.57207879e+01 2.35814779e+01]\n",
      "44-th iteration, loss: 0.05905009887020978, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1781065430602933e-06\n",
      "44-th iteration, new layer inserted. now 29 layers\n",
      "[2.75040713e+00 8.29689087e+01 1.38250688e+02 8.99179502e+01\n",
      " 1.34359965e+02 7.31216682e+01 1.46148546e+02 9.14669202e+01\n",
      " 1.14257219e+02 7.19237619e+01 9.27562854e+01 4.35422904e+01\n",
      " 9.87935782e+01 6.97165225e+01 1.02091516e+02 4.47686817e+01\n",
      " 7.99239455e+01 4.30692488e+01 7.60581969e+01 1.66546107e+01\n",
      " 8.61879673e+01 4.93018603e+01 6.42264696e-05 1.67081430e-07\n",
      " 0.00000000e+00 1.98523347e-23 8.92758595e+01 3.57207893e+01\n",
      " 2.35814779e+01]\n",
      "45-th iteration, loss: 0.0590500988678433, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1746364004903363e-06\n",
      "45-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040720e+00 8.29689081e+01 1.38250684e+02 8.99179528e+01\n",
      " 1.34359966e+02 7.31216675e+01 1.46148545e+02 9.14669222e+01\n",
      " 1.14257218e+02 7.19237633e+01 9.27562861e+01 4.35422896e+01\n",
      " 9.87935769e+01 6.97165235e+01 1.02091516e+02 4.47686818e+01\n",
      " 7.99239456e+01 4.30692479e+01 7.60581962e+01 1.66546111e+01\n",
      " 8.61879669e+01 4.93018603e+01 6.74030820e-05 8.33059419e-08\n",
      " 8.92758659e+01 3.57207907e+01 2.35814779e+01]\n",
      "46-th iteration, loss: 0.059050098865819176, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1739686072618494e-06\n",
      "46-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040726e+00 8.29689076e+01 1.38250679e+02 8.99179553e+01\n",
      " 1.34359967e+02 7.31216668e+01 1.46148545e+02 9.14669241e+01\n",
      " 1.14257217e+02 7.19237647e+01 9.27562868e+01 4.35422889e+01\n",
      " 9.87935755e+01 6.97165245e+01 1.02091516e+02 4.47686819e+01\n",
      " 7.99239457e+01 4.30692470e+01 7.60581955e+01 1.66546114e+01\n",
      " 8.61879665e+01 4.93018603e+01 0.00000000e+00 5.32907052e-15\n",
      " 8.92759397e+01 3.57207922e+01 2.35814779e+01]\n",
      "47-th iteration, loss: 0.05905009886379577, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.173105285461233e-06\n",
      "47-th iteration, new layer inserted. now 25 layers\n",
      "[  2.75040732  82.96890704 138.25067535  89.91795787 134.35996787\n",
      "  73.12166616 146.14854422  91.46692601 114.25721625  71.92376617\n",
      "  92.7562875   43.54228811  98.7935741   69.71652549 102.09151607\n",
      "  44.76868195  79.92394585  43.06924613  76.05819476  16.6546118\n",
      "  86.18796613  49.30186024  89.275946    35.72079356  23.58147795]\n",
      "48-th iteration, loss: 0.05905009886211394, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.174961691000033e-06\n",
      "48-th iteration, new layer inserted. now 27 layers\n",
      "[2.75040739e+00 8.29689065e+01 1.38250671e+02 8.99179604e+01\n",
      " 1.34359969e+02 7.31216655e+01 1.46148544e+02 9.14669279e+01\n",
      " 1.14257215e+02 7.19237676e+01 9.27562882e+01 4.35422874e+01\n",
      " 9.87935727e+01 6.97165265e+01 1.02091516e+02 4.47686821e+01\n",
      " 7.99239460e+01 4.30692453e+01 7.60581940e+01 1.66546122e+01\n",
      " 8.61879657e+01 4.93018602e+01 0.00000000e+00 1.59872116e-14\n",
      " 8.92759492e+01 3.57207950e+01 2.35814779e+01]\n",
      "49-th iteration, loss: 0.05905009886009134, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.1739738806743646e-06\n",
      "49-th iteration, new layer inserted. now 25 layers\n",
      "[  2.75040745  82.96890596 138.2506671   89.91796298 134.35996974\n",
      "  73.1216648  146.14854298  91.46692987 114.25721454  71.92376905\n",
      "  92.75628889  43.54228661  98.79357135  69.71652749 102.09151608\n",
      "  44.76868217  79.9239461   43.06924439  76.05819333  16.65461251\n",
      "  86.18796533  49.30186019  89.27595552  35.72079639  23.58147795]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.528699471250537\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  46.61863535    0.         1888.05473149]\n",
      "1-th iteration, loss: 0.7529863819158965, 11 gd steps\n",
      "insert gradient: -0.6313887019727018\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.49687716   62.08426795  230.25057701    0.         1657.80415448]\n",
      "2-th iteration, loss: 0.6016582804213604, 13 gd steps\n",
      "insert gradient: -0.6432336494936581\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.91690677   77.59782542  216.14103196   43.18783982  236.82916493\n",
      "    0.         1420.97498955]\n",
      "3-th iteration, loss: 0.4671391551199765, 20 gd steps\n",
      "insert gradient: -0.7440787383244131\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           59.14793483  223.32392981   55.74248605  162.877654\n",
      "   48.86272745  365.39356874    0.         1055.58142081]\n",
      "4-th iteration, loss: 0.37305213918984115, 13 gd steps\n",
      "insert gradient: -0.2972886547384708\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[4.15198753e-01 6.37252824e+01 2.22628825e+02 5.47698171e+01\n",
      " 1.47091159e+02 4.97910498e+01 3.39553754e+02 4.93259596e+01\n",
      " 5.86434123e+02 0.00000000e+00 4.69147298e+02]\n",
      "5-th iteration, loss: 0.3111960856638316, 18 gd steps\n",
      "insert gradient: -0.29910493127899324\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.19306674  58.65009525 207.08945408  71.32348476 121.29583767\n",
      "  57.98997192 227.70119921   0.         106.26055963  44.88947867\n",
      " 569.36990447  40.23165814 469.14729814]\n",
      "6-th iteration, loss: 0.2761773872416396, 24 gd steps\n",
      "insert gradient: -0.10778603003753273\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          62.69851627 199.58693873  72.86343615 110.97269652\n",
      "  60.7598375  192.13069122  27.44934683  74.43369483  49.45116094\n",
      " 554.44223379  42.02852398 390.95608178   0.          78.19121636]\n",
      "7-th iteration, loss: 0.26379360516488715, 64 gd steps\n",
      "insert gradient: -0.04822990928626837\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[3.32709157e-01 6.02283289e+01 1.97430041e+02 7.10539572e+01\n",
      " 1.09530245e+02 5.91061487e+01 1.75794638e+02 3.76519852e+01\n",
      " 6.27437978e+01 5.30247706e+01 2.03538661e+02 0.00000000e+00\n",
      " 3.39231102e+02 4.07209151e+01 3.67591822e+02 1.90433280e+01\n",
      " 7.81912164e+01]\n",
      "8-th iteration, loss: 0.259275180420806, 17 gd steps\n",
      "insert gradient: -0.20998763752968647\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          58.0900366  112.12024829   0.          84.09018622\n",
      "  70.75212233 106.82455138  60.79297282 174.93299183  39.66719427\n",
      "  60.81262382  50.44111951 194.31449191  10.07292767 324.60888141\n",
      "  41.13600358 359.54451311  22.44451524  78.19121636]\n",
      "9-th iteration, loss: 0.18034236426112954, 69 gd steps\n",
      "insert gradient: -0.13136763462486192\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.50681594  60.83613472 185.24092741  66.53170119 113.92045301\n",
      "  69.26457018 184.96532051  53.46744615  65.59073872  37.89709849\n",
      " 161.57995114  54.68575405 138.57127491   0.         138.57127491\n",
      "  47.30318874 334.78753925  37.89302823  78.19121636]\n",
      "10-th iteration, loss: 0.16390759039679478, 22 gd steps\n",
      "insert gradient: -0.06119346114184923\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  2.70535104  64.01348908 178.89012305  69.89343812 117.53963386\n",
      "  62.37043353  82.09050255   0.         114.92670357  49.8189645\n",
      "  64.01828683  36.57026895 157.07577092  61.75313709 122.61201166\n",
      "  16.63705137 124.19681591  54.77338412 329.18052472  33.4373751\n",
      "  78.19121636]\n",
      "11-th iteration, loss: 0.139700414945197, 151 gd steps\n",
      "insert gradient: -0.002014447984908321\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[7.22640565e-01 6.37274504e+01 1.61263444e+02 7.03658680e+01\n",
      " 1.16053233e+02 1.02176634e+02 9.69665787e+01 4.62784277e+01\n",
      " 7.13024785e+01 4.07294490e+01 1.46077916e+02 5.63609662e+01\n",
      " 8.95372688e+01 2.34241908e+01 1.27256791e+02 0.00000000e+00\n",
      " 4.26325641e-14 5.47866401e+01 3.11438967e+02 2.68711563e+01\n",
      " 7.81912164e+01]\n",
      "12-th iteration, loss: 0.13640749275494174, 69 gd steps\n",
      "insert gradient: -0.02512536561878578\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  1.93052293  61.6800327  160.27610221  68.36401218 116.00733812\n",
      " 100.07824765  98.19893397  45.96381619  68.42577977  41.46022634\n",
      " 136.04811503  62.91419754  84.29365061  26.10264692 102.35285494\n",
      "  71.53986299 240.88908691   0.          48.17781738  32.67632288\n",
      "  78.19121636]\n",
      "13-th iteration, loss: 0.1276350649895681, 249 gd steps\n",
      "insert gradient: -0.02797877379898397\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  0.68006289  71.56968782 153.94250927  69.97293421 116.79579088\n",
      " 100.13523052 106.55029375  49.43946597  76.12019108  43.13982236\n",
      "  92.47206821  86.7584891   84.22840292  34.01576881  72.20333598\n",
      "  79.47696202 132.79276451   0.         110.66063709  17.05223952\n",
      "  29.46786068  36.71791596  78.19121636]\n",
      "14-th iteration, loss: 0.118879380958099, 124 gd steps\n",
      "insert gradient: -0.004282345234745494\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  0.35834243  70.13032237 158.89033455  72.11330375 118.75084359\n",
      " 103.41097945 108.59089153  51.18489337  82.54176803  42.48373791\n",
      "  87.98678108  88.52085556  92.00925094  42.37488472  57.69224926\n",
      "  60.83949958   0.          15.2098749  121.65751698  36.44464299\n",
      "  20.69489965  41.38791685  24.04259584  44.31899381  78.19121636]\n",
      "15-th iteration, loss: 0.1089723371382118, 93 gd steps\n",
      "insert gradient: -0.0010845901533534895\n",
      "15-th iteration, new layer inserted. now 23 layers\n",
      "[  0.77935404  66.9183878  168.56183986  71.25455719 110.4896181\n",
      " 102.58240244 114.54458486  59.71826556  84.13704352  40.52885363\n",
      "  86.49288586  87.1366071   99.82628083  45.93256043  62.93436087\n",
      "  49.40332105 164.2800268   62.11319451  23.2925771   34.01082965\n",
      "   1.39343167  26.61406661  78.19121636]\n",
      "16-th iteration, loss: 0.10754293438823083, 167 gd steps\n",
      "insert gradient: -0.0020786033277696613\n",
      "16-th iteration, new layer inserted. now 23 layers\n",
      "[  1.18691176  66.7975887  171.86203786  69.86184824 112.66162019\n",
      " 103.43997793 113.19758271  59.81844765  88.92269871  39.91499558\n",
      "  85.58977595  85.68972282 106.08979914  46.57609182  67.29320497\n",
      "  42.78744382 108.30875657   0.          61.89071804  58.93786053\n",
      "  51.19056864  31.99280179  78.19121636]\n",
      "17-th iteration, loss: 0.10747864643538273, 57 gd steps\n",
      "insert gradient: -0.0016244832128436365\n",
      "17-th iteration, new layer inserted. now 23 layers\n",
      "[  1.28428345  67.07816483 172.18189618  70.1470825  113.11473845\n",
      " 103.31544858 113.31534069  59.82689285  89.6510719   40.03880129\n",
      "  85.46885881  85.95222664 106.13376543  46.19816582  69.44991071\n",
      "  42.31828716 106.49809006   1.98564148  59.28085453  57.03282218\n",
      "  56.9587053   28.6237278   78.19121636]\n",
      "18-th iteration, loss: 0.10746780549894237, 22 gd steps\n",
      "insert gradient: -0.00048609162277549\n",
      "18-th iteration, new layer inserted. now 25 layers\n",
      "[1.21844324e+00 6.69434398e+01 1.72298027e+02 6.99917022e+01\n",
      " 1.13062017e+02 0.00000000e+00 1.06581410e-14 1.03517189e+02\n",
      " 1.13307870e+02 6.00439775e+01 8.96219935e+01 3.99650310e+01\n",
      " 8.54014156e+01 8.60426807e+01 1.06006128e+02 4.62408013e+01\n",
      " 6.94993192e+01 4.22552529e+01 1.06500395e+02 2.01033852e+00\n",
      " 5.92027926e+01 5.71228216e+01 5.72278394e+01 2.84420732e+01\n",
      " 7.81912164e+01]\n",
      "19-th iteration, loss: 0.10746208611046157, 558 gd steps\n",
      "insert gradient: -6.86739047080846e-05\n",
      "19-th iteration, new layer inserted. now 25 layers\n",
      "[1.20353029e+00 6.69432750e+01 1.72404602e+02 6.99748787e+01\n",
      " 1.13144899e+02 1.03648199e+02 1.13329405e+02 6.01397984e+01\n",
      " 8.97782542e+01 3.99894498e+01 8.52510752e+01 8.62506830e+01\n",
      " 1.05675431e+02 4.62991352e+01 6.99736038e+01 4.19649573e+01\n",
      " 1.06637638e+02 2.21311746e+00 5.88798882e+01 5.67433103e+01\n",
      " 0.00000000e+00 5.32907052e-15 5.87256606e+01 2.76145435e+01\n",
      " 7.81912164e+01]\n",
      "20-th iteration, loss: 0.10746090529026774, 103 gd steps\n",
      "insert gradient: -1.415884782496966e-05\n",
      "20-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20101661  66.96095999 172.45177303  69.97975758 113.17087161\n",
      " 103.69303867 113.31815283  60.16390679  89.86238304  39.99001051\n",
      "  85.21570773  86.30229306 105.64486099  46.29593659  70.16221057\n",
      "  41.85226277 106.81491179   2.26921012  58.79909088  56.40390675\n",
      "  59.62444459  27.17154964  78.19121636]\n",
      "21-th iteration, loss: 0.10746075312623267, 23 gd steps\n",
      "insert gradient: -6.043983772314309e-05\n",
      "21-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20118407  66.97101808 172.46151893  69.98295743 113.17266247\n",
      " 103.70562712 113.33279069  60.15920457  89.88500623  40.00131484\n",
      "  85.19171576  86.3335684  105.58174636  46.30976192  70.24064164\n",
      "  41.78178187 106.97913058   2.27362743  58.71408113  56.3304392\n",
      "  59.8491817   27.04523008  78.19121636]\n",
      "22-th iteration, loss: 0.10746071574500493, 58 gd steps\n",
      "insert gradient: -4.262173436695796e-05\n",
      "22-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20177215  66.97308976 172.47267528  69.9851297  113.18875081\n",
      " 103.70351758 113.32849611  60.16133951  89.90245558  40.00109667\n",
      "  85.17736813  86.34603796 105.57090391  46.30919427  70.2774487\n",
      "  41.76531215 107.05940402   2.28331338  58.64710713  56.28604856\n",
      "  59.95690843  26.99905819  78.19121636]\n",
      "23-th iteration, loss: 0.10746070574544253, 35 gd steps\n",
      "insert gradient: -1.347897784491574e-05\n",
      "23-th iteration, new layer inserted. now 25 layers\n",
      "[1.20121022e+00 6.69710486e+01 1.72476668e+02 6.99824404e+01\n",
      " 1.13186951e+02 0.00000000e+00 1.42108547e-14 1.03708200e+02\n",
      " 1.13326650e+02 6.01678699e+01 8.99064805e+01 4.00003084e+01\n",
      " 8.51729813e+01 8.63498360e+01 1.05568834e+02 4.63089640e+01\n",
      " 7.02823201e+01 4.17612010e+01 1.07082135e+02 2.28552626e+00\n",
      " 5.86294749e+01 5.62800193e+01 5.99759219e+01 2.69884717e+01\n",
      " 7.81912164e+01]\n",
      "24-th iteration, loss: 0.10746069972899434, 37 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.937727629239363e-06\n",
      "24-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20095364  66.97066785 172.47694336  69.98204475 113.18808131\n",
      " 103.71102241 113.32749238  60.16822174  89.90857326  40.00088888\n",
      "  85.17176573  86.35183883 105.56642814  46.30869519  70.28898894\n",
      "  41.75629834 107.11660922   2.28633015  58.60312676  56.27054622\n",
      "  59.99878905  26.97758426  78.19121636]\n",
      "25-th iteration, loss: 0.10746069972637119, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.89621326171373e-06\n",
      "25-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20095484  66.97067249 172.47694526  69.98204908 113.18808295\n",
      " 103.71102178 113.32749345  60.16822429  89.90857215  40.00088391\n",
      "  85.1717637   86.35183699 105.56642821  46.30869698  70.28899051\n",
      "  41.75630025 107.11661413   2.28633257  58.60312522  56.27054756\n",
      "  59.99879185  26.97758353  78.19121636]\n",
      "26-th iteration, loss: 0.10746069972384897, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.855937384760497e-06\n",
      "26-th iteration, new layer inserted. now 23 layers\n",
      "[  1.20095601  66.97067703 172.47694709  69.98205329 113.18808453\n",
      " 103.71102108 113.32749449  60.16822685  89.90857109  40.00087904\n",
      "  85.17176173  86.35183528 105.5664283   46.30869875  70.28899206\n",
      "  41.7563021  107.116619     2.28633489  58.60312364  56.27054885\n",
      "  59.99879463  26.97758279  78.19121636]\n",
      "27-th iteration, loss: 0.10746069972142135, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.816876784402814e-06\n",
      "27-th iteration, new layer inserted. now 25 layers\n",
      "[1.20095716e+00 6.69706815e+01 1.72476949e+02 6.99820574e+01\n",
      " 1.13188086e+02 1.03711020e+02 1.13327496e+02 6.01682294e+01\n",
      " 8.99085701e+01 4.00008743e+01 8.51717598e+01 8.63518337e+01\n",
      " 1.05566428e+02 4.63087005e+01 7.02889936e+01 4.17563039e+01\n",
      " 0.00000000e+00 7.10542736e-15 1.07116624e+02 2.28633710e+00\n",
      " 5.86031220e+01 5.62705501e+01 5.99987974e+01 2.69775820e+01\n",
      " 7.81912164e+01]\n",
      "28-th iteration, loss: 0.10746069971860683, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.758249562728597e-06\n",
      "28-th iteration, new layer inserted. now 25 layers\n",
      "[1.20095829e+00 6.69706858e+01 1.72476951e+02 6.99820614e+01\n",
      " 1.13188088e+02 1.03711020e+02 1.13327496e+02 6.01682320e+01\n",
      " 8.99085691e+01 4.00008696e+01 8.51717580e+01 8.63518322e+01\n",
      " 1.05566429e+02 4.63087022e+01 7.02889951e+01 4.17563056e+01\n",
      " 4.77461910e-06 1.73035452e-06 1.07116629e+02 2.28633918e+00\n",
      " 5.86031203e+01 5.62705513e+01 5.99988002e+01 2.69775813e+01\n",
      " 7.81912164e+01]\n",
      "29-th iteration, loss: 0.1074606997159002, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.702233759900706e-06\n",
      "29-th iteration, new layer inserted. now 27 layers\n",
      "[1.20095940e+00 6.69706901e+01 1.72476952e+02 6.99820653e+01\n",
      " 1.13188089e+02 1.03711019e+02 1.13327497e+02 6.01682346e+01\n",
      " 8.99085681e+01 4.00008651e+01 8.51717561e+01 8.63518308e+01\n",
      " 1.05566429e+02 4.63087039e+01 7.02889966e+01 4.17563073e+01\n",
      " 9.49130711e-06 3.37331189e-06 0.00000000e+00 7.41153829e-22\n",
      " 1.07116633e+02 2.28634111e+00 5.86031186e+01 5.62705524e+01\n",
      " 5.99988029e+01 2.69775805e+01 7.81912164e+01]\n",
      "30-th iteration, loss: 0.10746069971285398, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.629174815605564e-06\n",
      "30-th iteration, new layer inserted. now 27 layers\n",
      "[1.20096048e+00 6.69706943e+01 1.72476954e+02 6.99820691e+01\n",
      " 1.13188090e+02 1.03711018e+02 1.13327498e+02 6.01682372e+01\n",
      " 8.99085672e+01 4.00008606e+01 8.51717544e+01 8.63518295e+01\n",
      " 1.05566429e+02 4.63087055e+01 7.02889980e+01 4.17563088e+01\n",
      " 1.41383679e-05 4.91351720e-06 4.64921637e-06 1.54020531e-06\n",
      " 1.07116638e+02 2.28634287e+00 5.86031168e+01 5.62705534e+01\n",
      " 5.99988056e+01 2.69775798e+01 7.81912164e+01]\n",
      "31-th iteration, loss: 0.10746069970993181, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.56019095779184e-06\n",
      "31-th iteration, new layer inserted. now 29 layers\n",
      "[1.20096155e+00 6.69706984e+01 1.72476955e+02 6.99820728e+01\n",
      " 1.13188092e+02 1.03711017e+02 1.13327499e+02 6.01682398e+01\n",
      " 8.99085664e+01 4.00008562e+01 8.51717527e+01 8.63518282e+01\n",
      " 1.05566429e+02 4.63087070e+01 7.02889994e+01 4.17563103e+01\n",
      " 1.87132578e-05 6.34337467e-06 9.22724601e-06 2.96709192e-06\n",
      " 0.00000000e+00 5.29395592e-22 1.07116643e+02 2.28634445e+00\n",
      " 5.86031149e+01 5.62705544e+01 5.99988083e+01 2.69775790e+01\n",
      " 7.81912164e+01]\n",
      "32-th iteration, loss: 0.1074606997067225, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.476961661014932e-06\n",
      "32-th iteration, new layer inserted. now 31 layers\n",
      "[1.20096259e+00 6.69707024e+01 1.72476957e+02 6.99820765e+01\n",
      " 1.13188093e+02 1.03711016e+02 1.13327500e+02 6.01682424e+01\n",
      " 8.99085655e+01 4.00008519e+01 8.51717510e+01 8.63518271e+01\n",
      " 1.05566429e+02 4.63087084e+01 7.02890007e+01 4.17563116e+01\n",
      " 2.32067680e-05 7.65185755e-06 1.37248096e-05 4.26967857e-06\n",
      " 4.49945956e-06 1.30258665e-06 0.00000000e+00 2.64697796e-22\n",
      " 1.07116647e+02 2.28634583e+00 5.86031129e+01 5.62705552e+01\n",
      " 5.99988110e+01 2.69775782e+01 7.81912164e+01]\n",
      "33-th iteration, loss: 0.10746069970327225, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.382047492972649e-06\n",
      "33-th iteration, new layer inserted. now 33 layers\n",
      "[1.20096361e+00 6.69707063e+01 1.72476958e+02 6.99820800e+01\n",
      " 1.13188094e+02 1.03711015e+02 1.13327501e+02 6.01682451e+01\n",
      " 8.99085647e+01 4.00008477e+01 8.51717493e+01 8.63518260e+01\n",
      " 1.05566429e+02 4.63087097e+01 7.02890020e+01 4.17563128e+01\n",
      " 2.76057980e-05 8.81903319e-06 1.81287292e-05 5.42808403e-06\n",
      " 8.90610749e-06 2.45811694e-06 4.40748029e-06 1.15553030e-06\n",
      " 0.00000000e+00 1.58818678e-22 1.07116651e+02 2.28634697e+00\n",
      " 5.86031109e+01 5.62705560e+01 5.99988136e+01 2.69775774e+01\n",
      " 7.81912164e+01]\n",
      "34-th iteration, loss: 0.10746069969962652, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.278106279131487e-06\n",
      "34-th iteration, new layer inserted. now 35 layers\n",
      "[1.20096462e+00 6.69707101e+01 1.72476960e+02 6.99820835e+01\n",
      " 1.13188096e+02 1.03711014e+02 1.13327502e+02 6.01682478e+01\n",
      " 8.99085639e+01 4.00008435e+01 8.51717477e+01 8.63518249e+01\n",
      " 1.05566429e+02 4.63087109e+01 7.02890032e+01 4.17563138e+01\n",
      " 3.19000857e-05 9.83011395e-06 2.24286522e-05 6.42758050e-06\n",
      " 1.32094991e-05 3.45192240e-06 8.71244260e-06 2.14651937e-06\n",
      " 4.30570070e-06 9.90989069e-07 0.00000000e+00 2.11758237e-22\n",
      " 1.07116656e+02 2.28634786e+00 5.86031087e+01 5.62705566e+01\n",
      " 5.99988162e+01 2.69775766e+01 7.81912164e+01]\n",
      "35-th iteration, loss: 0.10746069969582632, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.167774204207606e-06\n",
      "35-th iteration, new layer inserted. now 37 layers\n",
      "[1.20096561e+00 6.69707139e+01 1.72476961e+02 6.99820869e+01\n",
      " 1.13188097e+02 1.03711013e+02 1.13327503e+02 6.01682505e+01\n",
      " 8.99085632e+01 4.00008394e+01 8.51717461e+01 8.63518239e+01\n",
      " 1.05566429e+02 4.63087119e+01 7.02890043e+01 4.17563147e+01\n",
      " 3.60822457e-05 1.06755175e-05 2.66170937e-05 7.25865213e-06\n",
      " 1.74020478e-05 4.27455314e-06 1.29071971e-05 2.96358284e-06\n",
      " 8.50182686e-06 1.80530119e-06 4.19675940e-06 8.14312126e-07\n",
      " 0.00000000e+00 1.58818678e-22 1.07116660e+02 2.28634847e+00\n",
      " 5.86031065e+01 5.62705571e+01 5.99988188e+01 2.69775757e+01\n",
      " 7.81912164e+01]\n",
      "36-th iteration, loss: 0.10746069969190591, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.053559707904885e-06\n",
      "36-th iteration, new layer inserted. now 39 layers\n",
      "[1.20096658e+00 6.69707176e+01 1.72476963e+02 6.99820903e+01\n",
      " 1.13188098e+02 1.03711012e+02 1.13327504e+02 6.01682533e+01\n",
      " 8.99085625e+01 4.00008354e+01 8.51717446e+01 8.63518230e+01\n",
      " 1.05566429e+02 4.63087129e+01 7.02890054e+01 4.17563154e+01\n",
      " 4.01476859e-05 1.13506988e-05 3.06893555e-05 7.91682502e-06\n",
      " 2.14789479e-05 4.92160609e-06 1.69868287e-05 3.60238808e-06\n",
      " 1.25833521e-05 2.43867375e-06 8.27943827e-06 1.44500295e-06\n",
      " 4.08319922e-06 6.30690822e-07 0.00000000e+00 1.32348898e-22\n",
      " 1.07116664e+02 2.28634880e+00 5.86031041e+01 5.62705575e+01\n",
      " 5.99988213e+01 2.69775749e+01 7.81912164e+01]\n",
      "37-th iteration, loss: 0.10746069968789185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.937755569038807e-06\n",
      "37-th iteration, new layer inserted. now 41 layers\n",
      "[1.20096753e+00 6.69707212e+01 1.72476964e+02 6.99820936e+01\n",
      " 1.13188099e+02 1.03711011e+02 1.13327505e+02 6.01682560e+01\n",
      " 8.99085618e+01 4.00008315e+01 8.51717430e+01 8.63518221e+01\n",
      " 1.05566429e+02 4.63087137e+01 7.02890064e+01 4.17563159e+01\n",
      " 4.40944164e-05 1.18557851e-05 3.46433392e-05 8.40230078e-06\n",
      " 2.54379904e-05 5.39335678e-06 2.09490161e-05 4.06328416e-06\n",
      " 1.65478415e-05 2.89152906e-06 1.22454859e-05 1.89256769e-06\n",
      " 8.05017021e-06 1.07564640e-06 3.96737401e-06 4.44955576e-07\n",
      " 0.00000000e+00 2.64697796e-23 1.07116668e+02 2.28634883e+00\n",
      " 5.86031015e+01 5.62705577e+01 5.99988238e+01 2.69775740e+01\n",
      " 7.81912164e+01]\n",
      "38-th iteration, loss: 0.1074606996838026, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.8223726552243295e-06\n",
      "38-th iteration, new layer inserted. now 41 layers\n",
      "[1.20096847e+00 6.69707248e+01 1.72476965e+02 6.99820968e+01\n",
      " 1.13188100e+02 1.03711010e+02 1.13327506e+02 6.01682588e+01\n",
      " 8.99085612e+01 4.00008276e+01 8.51717415e+01 8.63518213e+01\n",
      " 1.05566429e+02 4.63087144e+01 7.02890073e+01 4.17563163e+01\n",
      " 4.79227760e-05 1.21950562e-05 3.84792746e-05 8.71943481e-06\n",
      " 2.92792949e-05 5.69423591e-06 2.47937670e-05 4.35077685e-06\n",
      " 2.03951888e-05 3.16844767e-06 1.60946809e-05 2.16166142e-06\n",
      " 1.19005746e-05 1.33959607e-06 7.81846573e-06 7.06370092e-07\n",
      " 3.85137605e-06 2.61414515e-07 1.07116672e+02 2.28634856e+00\n",
      " 5.86030989e+01 5.62705578e+01 5.99988263e+01 2.69775731e+01\n",
      " 7.81912164e+01]\n",
      "39-th iteration, loss: 0.10746069967990293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.7198475083236614e-06\n",
      "39-th iteration, new layer inserted. now 41 layers\n",
      "[1.20096939e+00 6.69707282e+01 1.72476967e+02 6.99821000e+01\n",
      " 1.13188102e+02 1.03711009e+02 1.13327506e+02 6.01682617e+01\n",
      " 8.99085606e+01 4.00008238e+01 8.51717401e+01 8.63518206e+01\n",
      " 1.05566429e+02 4.63087150e+01 7.02890081e+01 4.17563165e+01\n",
      " 5.16431934e-05 1.23860042e-05 4.22074847e-05 8.88579446e-06\n",
      " 3.30130768e-05 5.84188607e-06 2.85311874e-05 4.48258379e-06\n",
      " 2.41353894e-05 3.28722207e-06 1.98369062e-05 2.27015132e-06\n",
      " 1.56441811e-05 1.44048149e-06 1.15629283e-05 8.02259513e-07\n",
      " 7.59628996e-06 3.54842907e-07 1.07116676e+02 2.28634802e+00\n",
      " 5.86030962e+01 5.62705578e+01 5.99988287e+01 2.69775722e+01\n",
      " 7.81912164e+01]\n",
      "40-th iteration, loss: 0.10746069967615939, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6284918624944597e-06\n",
      "40-th iteration, new layer inserted. now 41 layers\n",
      "[1.20097030e+00 6.69707317e+01 1.72476968e+02 6.99821031e+01\n",
      " 1.13188103e+02 1.03711008e+02 1.13327507e+02 6.01682646e+01\n",
      " 8.99085601e+01 4.00008201e+01 8.51717387e+01 8.63518199e+01\n",
      " 1.05566429e+02 4.63087155e+01 7.02890089e+01 4.17563166e+01\n",
      " 5.52682353e-05 1.24502259e-05 4.58404413e-05 8.92304556e-06\n",
      " 3.66517114e-05 5.85804179e-06 3.21735550e-05 4.48050807e-06\n",
      " 2.77806214e-05 3.26972384e-06 2.34842387e-05 2.23997730e-06\n",
      " 1.92929643e-05 1.40031084e-06 1.52126319e-05 7.54700154e-07\n",
      " 1.12465062e-05 3.02429520e-07 1.07116679e+02 2.28634723e+00\n",
      " 5.86030933e+01 5.62705576e+01 5.99988311e+01 2.69775713e+01\n",
      " 7.81912164e+01]\n",
      "41-th iteration, loss: 0.10746069967254589, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.546854792436061e-06\n",
      "41-th iteration, new layer inserted. now 41 layers\n",
      "[1.20097119e+00 6.69707350e+01 1.72476969e+02 6.99821062e+01\n",
      " 1.13188104e+02 1.03711006e+02 1.13327508e+02 6.01682675e+01\n",
      " 8.99085595e+01 4.00008164e+01 8.51717373e+01 8.63518193e+01\n",
      " 1.05566428e+02 4.63087160e+01 7.02890096e+01 4.17563166e+01\n",
      " 5.88088239e-05 1.24061992e-05 4.93889856e-05 8.84972676e-06\n",
      " 4.02059575e-05 5.76130255e-06 3.57315444e-05 4.36320993e-06\n",
      " 3.13414738e-05 3.13467388e-06 2.70471805e-05 2.08992092e-06\n",
      " 2.28573374e-05 1.23792627e-06 1.87778998e-05 5.82594712e-07\n",
      " 1.48122564e-05 1.23137566e-07 1.07116683e+02 2.28634622e+00\n",
      " 5.86030904e+01 5.62705574e+01 5.99988334e+01 2.69775704e+01\n",
      " 7.81912164e+01]\n",
      "42-th iteration, loss: 0.10746069966904305, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.473019659473153e-06\n",
      "42-th iteration, new layer inserted. now 39 layers\n",
      "[1.20097207e+00 6.69707383e+01 1.72476970e+02 6.99821091e+01\n",
      " 1.13188105e+02 1.03711005e+02 1.13327509e+02 6.01682704e+01\n",
      " 8.99085591e+01 4.00008129e+01 8.51717360e+01 8.63518189e+01\n",
      " 1.05566428e+02 4.63087164e+01 7.02890104e+01 4.17563165e+01\n",
      " 6.22744678e-05 1.22697388e-05 5.28625571e-05 8.68170683e-06\n",
      " 4.36851840e-05 5.56759102e-06 3.92144523e-05 4.14666595e-06\n",
      " 3.48271698e-05 2.89810272e-06 3.05348795e-05 1.83606663e-06\n",
      " 2.63463719e-05 9.69466150e-07 2.22677254e-05 3.02135534e-07\n",
      " 1.07116705e+02 2.28634502e+00 5.86030874e+01 5.62705571e+01\n",
      " 5.99988358e+01 2.69775694e+01 7.81912164e+01]\n",
      "43-th iteration, loss: 0.10746069966585158, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.4152612152362364e-06\n",
      "43-th iteration, new layer inserted. now 39 layers\n",
      "[1.20097293e+00 6.69707415e+01 1.72476971e+02 6.99821121e+01\n",
      " 1.13188106e+02 1.03711004e+02 1.13327510e+02 6.01682733e+01\n",
      " 8.99085586e+01 4.00008094e+01 8.51717347e+01 8.63518185e+01\n",
      " 1.05566428e+02 4.63087168e+01 7.02890111e+01 4.17563163e+01\n",
      " 6.56790394e-05 1.20599219e-05 5.62749690e-05 8.43811077e-06\n",
      " 4.71031435e-05 5.29608014e-06 4.26359696e-05 3.85009706e-06\n",
      " 3.82513368e-05 2.57927929e-06 3.39608984e-05 1.49773144e-06\n",
      " 2.97735640e-05 6.14295655e-07 0.00000000e+00 1.32348898e-22\n",
      " 1.07116734e+02 2.28634365e+00 5.86030843e+01 5.62705567e+01\n",
      " 5.99988381e+01 2.69775685e+01 7.81912164e+01]\n",
      "44-th iteration, loss: 0.10746069966273293, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.361251441641608e-06\n",
      "44-th iteration, new layer inserted. now 37 layers\n",
      "[1.20097378e+00 6.69707447e+01 1.72476972e+02 6.99821149e+01\n",
      " 1.13188107e+02 1.03711003e+02 1.13327511e+02 6.01682763e+01\n",
      " 8.99085582e+01 4.00008060e+01 8.51717334e+01 8.63518182e+01\n",
      " 1.05566428e+02 4.63087172e+01 7.02890117e+01 4.17563161e+01\n",
      " 6.90309944e-05 1.17900759e-05 5.96346303e-05 8.13230508e-06\n",
      " 5.04681967e-05 4.96017557e-06 4.60044070e-05 3.48694814e-06\n",
      " 4.16222344e-05 2.19168781e-06 3.73334442e-05 1.08843898e-06\n",
      " 3.31470668e-05 1.85977932e-07 1.07116740e+02 2.28634214e+00\n",
      " 5.86030812e+01 5.62705563e+01 5.99988404e+01 2.69775675e+01\n",
      " 7.81912164e+01]\n",
      "45-th iteration, loss: 0.10746069965988407, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.320151309916477e-06\n",
      "45-th iteration, new layer inserted. now 35 layers\n",
      "[1.20097461e+00 6.69707478e+01 1.72476974e+02 6.99821177e+01\n",
      " 1.13188108e+02 1.03711002e+02 1.13327512e+02 6.01682792e+01\n",
      " 8.99085578e+01 4.00008027e+01 8.51717322e+01 8.63518180e+01\n",
      " 1.05566428e+02 4.63087176e+01 7.02890124e+01 4.17563158e+01\n",
      " 7.23397856e-05 1.14712497e-05 6.29509553e-05 7.77537242e-06\n",
      " 5.37897183e-05 4.57099378e-06 4.93290981e-05 3.06836958e-06\n",
      " 4.49491537e-05 1.74651269e-06 4.06617639e-05 6.19407849e-07\n",
      " 1.07116780e+02 2.28634051e+00 5.86030780e+01 5.62705558e+01\n",
      " 5.99988427e+01 2.69775665e+01 7.81912164e+01]\n",
      "46-th iteration, loss: 0.10746069965728525, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2908578869435223e-06\n",
      "46-th iteration, new layer inserted. now 37 layers\n",
      "[1.20097543e+00 6.69707509e+01 1.72476975e+02 6.99821204e+01\n",
      " 1.13188109e+02 1.03711001e+02 1.13327512e+02 6.01682822e+01\n",
      " 8.99085574e+01 4.00007994e+01 8.51717311e+01 8.63518178e+01\n",
      " 1.05566428e+02 4.63087180e+01 7.02890131e+01 4.17563155e+01\n",
      " 7.56170821e-05 1.11183667e-05 6.62355820e-05 7.38226379e-06\n",
      " 5.70793134e-05 4.14351357e-06 5.26216141e-05 2.60936814e-06\n",
      " 4.82436304e-05 1.25878880e-06 4.39573566e-05 1.05701155e-07\n",
      " 0.00000000e+00 2.64697796e-23 1.07116784e+02 2.28633880e+00\n",
      " 5.86030748e+01 5.62705553e+01 5.99988450e+01 2.69775656e+01\n",
      " 7.81912164e+01]\n",
      "47-th iteration, loss: 0.1074606996545311, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2523086136868798e-06\n",
      "47-th iteration, new layer inserted. now 35 layers\n",
      "[1.20097623e+00 6.69707539e+01 1.72476976e+02 6.99821231e+01\n",
      " 1.13188110e+02 1.03711000e+02 1.13327513e+02 6.01682851e+01\n",
      " 8.99085570e+01 4.00007962e+01 8.51717299e+01 8.63518178e+01\n",
      " 1.05566428e+02 4.63087184e+01 7.02890138e+01 4.17563152e+01\n",
      " 7.88625532e-05 1.07331586e-05 6.94881578e-05 6.95473116e-06\n",
      " 6.03366064e-05 3.67950733e-06 5.58815548e-05 2.11173677e-06\n",
      " 5.15052384e-05 7.30329830e-07 0.00000000e+00 1.32348898e-22\n",
      " 1.07116837e+02 2.28633700e+00 5.86030715e+01 5.62705547e+01\n",
      " 5.99988472e+01 2.69775646e+01 7.81912164e+01]\n",
      "48-th iteration, loss: 0.10746069965201276, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.225421544753724e-06\n",
      "48-th iteration, new layer inserted. now 33 layers\n",
      "[1.20097703e+00 6.69707569e+01 1.72476977e+02 6.99821257e+01\n",
      " 1.13188111e+02 1.03710998e+02 1.13327514e+02 6.01682881e+01\n",
      " 8.99085567e+01 4.00007931e+01 8.51717289e+01 8.63518178e+01\n",
      " 1.05566428e+02 4.63087188e+01 7.02890144e+01 4.17563148e+01\n",
      " 8.20784978e-05 1.03144246e-05 7.27109608e-05 6.49159436e-06\n",
      " 6.35638536e-05 3.17781552e-06 5.91111531e-05 1.57433672e-06\n",
      " 5.47361862e-05 1.60018028e-07 1.07116844e+02 2.28633512e+00\n",
      " 5.86030683e+01 5.62705542e+01 5.99988495e+01 2.69775636e+01\n",
      " 7.81912164e+01]\n",
      "49-th iteration, loss: 0.10746069964972187, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.2077782825268043e-06\n",
      "49-th iteration, new layer inserted. now 33 layers\n",
      "[1.20097781e+00 6.69707598e+01 1.72476978e+02 6.99821282e+01\n",
      " 1.13188112e+02 1.03710997e+02 1.13327515e+02 6.01682910e+01\n",
      " 8.99085564e+01 4.00007900e+01 8.51717278e+01 8.63518179e+01\n",
      " 1.05566428e+02 4.63087192e+01 7.02890151e+01 4.17563144e+01\n",
      " 8.52757759e-05 9.87613777e-06 7.59148298e-05 6.00684528e-06\n",
      " 6.67718707e-05 2.65244922e-06 6.23212009e-05 1.01119849e-06\n",
      " 0.00000000e+00 2.11758237e-22 1.07116905e+02 2.28633319e+00\n",
      " 5.86030650e+01 5.62705535e+01 5.99988518e+01 2.69775626e+01\n",
      " 7.81912164e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.52393808315439\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  47.22407217    0.         1912.57492281]\n",
      "1-th iteration, loss: 0.7539598880564631, 11 gd steps\n",
      "insert gradient: -0.6451563394401445\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  44.93606301   61.9847341   116.62042212    0.         1795.95450068]\n",
      "2-th iteration, loss: 0.5889695722482216, 15 gd steps\n",
      "insert gradient: -0.7033809708695182\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   4.88869433  104.88029881  103.09422569   84.57946344  109.956398\n",
      "    0.         1685.99810268]\n",
      "3-th iteration, loss: 0.4545752343045659, 12 gd steps\n",
      "insert gradient: -0.45262122992299014\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.          111.14225003   97.97450825   66.02758311  120.28365864\n",
      "   40.70857632  337.19962054    0.         1348.79848215]\n",
      "4-th iteration, loss: 0.3741531975546431, 13 gd steps\n",
      "insert gradient: -0.213710051592688\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  1.23459148 120.82777797  87.7313255   69.36001583 112.88634806\n",
      "  52.90416532 310.43671507  48.52651225 599.46599207   0.\n",
      " 749.33249008]\n",
      "5-th iteration, loss: 0.29168833891613277, 49 gd steps\n",
      "insert gradient: -0.41535386145068076\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.         105.34095233  99.39112713  91.05527398  99.88756516\n",
      "  53.74351249 133.14916384   0.         133.14916384  52.56165592\n",
      " 542.36701284  49.14284807 749.33249008]\n",
      "6-th iteration, loss: 0.251517202764068, 19 gd steps\n",
      "insert gradient: -0.08342626007405034\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.57998511 100.03979931  99.34951217 102.13428146  98.93696188\n",
      "  55.11127397 114.20625229  23.48418668 114.02367494  51.00352666\n",
      " 538.05880936  47.01775179 333.03666226   0.         416.29582782]\n",
      "7-th iteration, loss: 0.24200464041421885, 35 gd steps\n",
      "insert gradient: -0.06693008890667404\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  0.         100.35574781  99.60399137 104.6634528   99.62508908\n",
      "  58.52332813 108.44101395  32.79283389  90.92119808  58.79490196\n",
      " 532.55359525  44.13918428 325.41693637  18.31023669 416.29582782]\n",
      "8-th iteration, loss: 0.1881605743299559, 57 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "8-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          72.13670116 121.52088845  91.19906813 118.04944565\n",
      "  80.76274017  90.66055912  41.36203727  75.30280216  47.00827283\n",
      " 530.45868939  47.00936715 303.10768043  32.29441552 416.29582782]\n",
      "9-th iteration, loss: 0.18575562323565348, 36 gd steps\n",
      "insert gradient: -0.06802715708508848\n",
      "9-th iteration, new layer inserted. now 17 layers\n",
      "[2.28418061e-01 7.12161345e+01 1.19340750e+02 9.35558754e+01\n",
      " 1.13030279e+02 8.23937933e+01 9.05930318e+01 4.09364607e+01\n",
      " 7.29619177e+01 4.87457771e+01 1.30996468e+02 0.00000000e+00\n",
      " 3.92989404e+02 5.24585753e+01 2.87648677e+02 3.71724135e+01\n",
      " 4.16295828e+02]\n",
      "10-th iteration, loss: 0.17045565704181556, 46 gd steps\n",
      "insert gradient: -0.06079669283887955\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[5.70043193e-02 7.77561831e+01 1.14016526e+02 9.53982527e+01\n",
      " 1.09298170e+02 8.62178013e+01 9.59779903e+01 4.03782275e+01\n",
      " 7.40154958e+01 5.01777115e+01 1.10105159e+02 3.44270028e+01\n",
      " 7.50479450e+01 0.00000000e+00 2.75175798e+02 5.76153224e+01\n",
      " 2.87191475e+02 3.51026046e+01 4.16295828e+02]\n",
      "11-th iteration, loss: 0.1555955051152522, 15 gd steps\n",
      "insert gradient: -0.08727740828436478\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  0.          82.49017987 109.63666013  98.58863015 103.565447\n",
      "  83.87489904 102.02897889  47.74896552  71.4160714   40.02948872\n",
      " 110.99965582  63.46714335  36.35019236  22.53223891 238.33310809\n",
      "  49.32697737 289.21620408  30.5715298  416.29582782]\n",
      "12-th iteration, loss: 0.15024768350164439, 24 gd steps\n",
      "insert gradient: -0.03286523564520072\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  0.          84.59291863 109.9480599   97.54288171 107.17915974\n",
      "  81.78318392 102.64874588  45.84118232  72.75337575  47.29053152\n",
      " 100.26074292  69.34309396  33.7545249   27.20973872 230.39796662\n",
      "  48.74888571 142.42725378   0.         142.42725378  27.03851288\n",
      " 416.29582782]\n",
      "13-th iteration, loss: 0.14816198225057167, 57 gd steps\n",
      "insert gradient: -0.016736528422706787\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  0.          84.08876995 109.50126004  97.60931517 106.95094473\n",
      "  81.63347768 103.63483364  45.08274924  72.9049684   48.33017752\n",
      "  99.86081534  69.04983629  33.24508806  30.14370384 224.73567325\n",
      "  48.82693421 134.76503829   7.82424645 135.72813356  26.22223369\n",
      " 227.07045154   0.         189.22537628]\n",
      "14-th iteration, loss: 0.1469758045476081, 47 gd steps\n",
      "insert gradient: -0.03035448994392673\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          84.63098156 109.22062643  98.171725   107.55918027\n",
      "  81.33987306 103.4540049   45.65133923  72.78409678  48.02122055\n",
      " 100.18437917  70.24998148  31.09512936  29.01843834 136.1440798\n",
      "   0.          90.76271987  49.50060291 136.729733     7.90779777\n",
      " 141.46861279  18.89039026 222.42482902   8.93505181 189.22537628]\n",
      "15-th iteration, loss: 0.14373100518692405, 57 gd steps\n",
      "insert gradient: -0.016494837812700888\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  0.          85.21668196 110.54984692  99.89020949 106.76841356\n",
      "  80.50641136 103.17460194  46.55457655  75.66024061  46.36844202\n",
      " 100.07352456  72.31578633  35.93040387  21.36587422 122.89001662\n",
      "  16.14951597  66.75628458  58.19986007 140.63240933   9.58347719\n",
      " 139.16608149  11.39290966 226.97491768  17.0353015  189.22537628]\n",
      "16-th iteration, loss: 0.14248572669097778, 37 gd steps\n",
      "insert gradient: -0.03260026076340473\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 8.58007170e+01 1.03727748e+02 9.97375755e+01\n",
      " 1.11533464e+02 8.46329755e+01 1.08359733e+02 4.31587449e+01\n",
      " 7.91375840e+01 4.77605014e+01 9.60290592e+01 0.00000000e+00\n",
      " 2.48689958e-14 6.67501098e+01 6.01706391e+01 1.46149427e+01\n",
      " 1.16151661e+02 2.72560223e+01 4.25201785e+01 6.30439623e+01\n",
      " 1.45277985e+02 1.61317011e+01 1.13875461e+02 1.22871099e+01\n",
      " 2.30968808e+02 1.75322836e+01 1.89225376e+02]\n",
      "17-th iteration, loss: 0.1364118550678875, 24 gd steps\n",
      "insert gradient: -0.028768344721274784\n",
      "17-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          88.31511702 108.9124788  100.92685462 107.51111026\n",
      "  83.67597849 107.40632684  49.06878466  79.17375287  45.63966761\n",
      "  96.70380656  69.30852967  68.31800807  11.95554548 115.29873624\n",
      "  31.24967147  37.72073614  60.65060397  66.65330796   0.\n",
      "  83.31663495  21.39894797 109.34286578   7.77112117 226.43130111\n",
      "  18.29298925 189.22537628]\n",
      "18-th iteration, loss: 0.12657576048260905, 124 gd steps\n",
      "insert gradient: -0.012381292773981943\n",
      "18-th iteration, new layer inserted. now 27 layers\n",
      "[0.00000000e+00 9.20807424e+01 1.06348519e+02 1.03189224e+02\n",
      " 1.04805251e+02 9.06722127e+01 1.09678144e+02 0.00000000e+00\n",
      " 1.42108547e-14 5.13641590e+01 8.47187597e+01 4.52040690e+01\n",
      " 9.35336721e+01 6.26716560e+01 1.15040525e+02 2.22827129e+01\n",
      " 6.01173634e+01 4.93462423e+01 3.69454852e+01 2.84526969e+01\n",
      " 1.74317579e+02 4.78646061e+01 8.03787622e+01 1.09503559e+01\n",
      " 2.10852510e+02 2.38987634e+01 1.89225376e+02]\n",
      "19-th iteration, loss: 0.12552768404375417, 199 gd steps\n",
      "insert gradient: -0.009553771620845846\n",
      "19-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          93.17674479 105.44054329 103.82088491 104.65078841\n",
      "  90.98349639 110.90257233  51.5409707   85.85202017  45.22951747\n",
      "  93.16263428  63.11649503 109.46153721  29.68181807  46.69137384\n",
      "  53.86106968  37.77299526  24.32779549 174.11993241  55.95194533\n",
      "  73.71549037  12.56600017 139.61799843   0.          69.80899922\n",
      "  23.74545366 189.22537628]\n",
      "20-th iteration, loss: 0.1206091154216461, 95 gd steps\n",
      "insert gradient: -0.03692607838728937\n",
      "20-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          95.92533547 105.11189756  99.92162131 106.35218888\n",
      "  89.2437363  113.06450425  51.18002611  86.40113445  44.44414576\n",
      "  93.68063695  59.87316172 110.23043057  41.76423693  34.50377823\n",
      "  54.34799386  48.65918296   9.84205402 145.97379851  88.04455125\n",
      "  80.57163562   8.82245598 116.30955584  22.65940036  30.86240073\n",
      "  42.3164221  189.22537628]\n",
      "21-th iteration, loss: 0.11183696383368029, 558 gd steps\n",
      "insert gradient: -0.026948487095794667\n",
      "21-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          94.11633179 108.85096578 103.67490998 108.03037604\n",
      "  58.71357982 157.66399303  61.03139371  84.9774954   39.33880904\n",
      "  88.7867766   62.58182129 112.91535865  50.00934004  32.85013079\n",
      "  55.73118784  75.21877552   2.90732564 116.82129707  87.96621296\n",
      " 101.36373072  13.23093141  89.9864507   38.71998767   7.1400485\n",
      "  44.07678394 189.22537628]\n",
      "22-th iteration, loss: 0.10889277775559718, 80 gd steps\n",
      "insert gradient: -0.011876424565689644\n",
      "22-th iteration, new layer inserted. now 27 layers\n",
      "[  0.          95.46200563 108.96455745 102.95003681 110.49858033\n",
      "  62.42013694 158.87131598  57.19667925  87.92909297  43.41661458\n",
      "  86.37906477  61.22681525 114.10643041  55.83415283  35.75298974\n",
      "  48.13381637 133.19872668   0.          66.59936334  92.44412871\n",
      " 102.03221334  16.30011704  82.39542793  37.23722203  11.26538031\n",
      "  41.47211771 189.22537628]\n",
      "23-th iteration, loss: 0.10595188116331422, 41 gd steps\n",
      "insert gradient: -0.006741340169782462\n",
      "23-th iteration, new layer inserted. now 29 layers\n",
      "[  0.          97.82483447 110.05073159 103.01589129 111.22231516\n",
      "  62.76354929 159.85800822  58.64197984  89.96043021  43.52173723\n",
      "  87.6850803   59.95181138 115.99611733  56.97177701  45.26039453\n",
      "  41.67159473 111.24476998  11.48203638  48.4331399  102.46101401\n",
      " 103.65186067  22.7247667   61.81588428  40.40142583  12.61202011\n",
      "  40.63017012 118.26586018   0.          70.95951611]\n",
      "24-th iteration, loss: 0.10275356675949172, 70 gd steps\n",
      "insert gradient: -0.00746652462793832\n",
      "24-th iteration, new layer inserted. now 31 layers\n",
      "[  0.          99.87143219 110.59432636 107.20770021 111.01041668\n",
      "  62.90836418 163.55924579  60.37387376  95.23541836  46.12264537\n",
      "  85.03935497  58.81580547 118.4023243   62.12071578  55.12048295\n",
      "  35.13106431  93.00643054  22.15608608  30.73910683  83.86920017\n",
      "   0.          27.95640006 107.56696641  39.97900386  41.68081172\n",
      "  49.42501603  16.52033556  29.20913024 117.89674219  12.22697489\n",
      "  70.95951611]\n",
      "25-th iteration, loss: 0.0984427204632612, 22 gd steps\n",
      "insert gradient: -0.013401533582941058\n",
      "25-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         101.94954377 111.01447641 104.93625167 114.48795046\n",
      "  58.58520826 166.90994174  64.73803264  97.66269429  45.57622323\n",
      "  85.49990354  56.64635403 114.53539915  66.90542983  66.35672825\n",
      "  33.64098192  92.50188138  28.98083554  14.44904875  74.55748599\n",
      "  30.93373967  17.84294523 106.26982414  49.05237873  35.82557634\n",
      "  50.23246889  29.25936003  16.95330181 119.44159725  19.75721862\n",
      "  70.95951611]\n",
      "26-th iteration, loss: 0.09631466461084275, 19 gd steps\n",
      "insert gradient: -0.005016211957684302\n",
      "26-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.02204246e+02 1.08878273e+02 1.05787267e+02\n",
      " 1.13480705e+02 6.22905659e+01 1.68652294e+02 6.21680147e+01\n",
      " 9.65144485e+01 0.00000000e+00 2.13162821e-14 4.79066966e+01\n",
      " 8.51975061e+01 5.55873484e+01 1.14540580e+02 6.86787729e+01\n",
      " 7.03999065e+01 3.24815194e+01 9.12620814e+01 3.33393006e+01\n",
      " 9.36294122e+00 7.36833699e+01 3.71241107e+01 1.56103003e+01\n",
      " 1.05994070e+02 5.11837025e+01 3.54371344e+01 4.92676744e+01\n",
      " 3.50495952e+01 1.46894265e+01 1.19231804e+02 2.00819855e+01\n",
      " 7.09595161e+01]\n",
      "27-th iteration, loss: 0.09322083966044058, 49 gd steps\n",
      "insert gradient: -0.002164525406924514\n",
      "27-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         102.35103145 108.30301913 106.20263162 113.83217649\n",
      "  62.4393255  171.17048088  62.12003412  98.08648061  49.50671368\n",
      "  85.5470841   53.88154229 110.42709631  70.54114963  87.23334786\n",
      "  27.92081262  80.46726566 115.92476268  60.21086801   7.06519579\n",
      " 104.06589307  58.01869118  37.02843954  40.60586819  56.80020752\n",
      "   9.01165995  43.61730053   0.          72.69550088  22.74945289\n",
      "  70.95951611]\n",
      "28-th iteration, loss: 0.09276187003769099, 19 gd steps\n",
      "insert gradient: -0.00244153300829008\n",
      "28-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         102.15180401 107.00637069 106.98517111 112.7764785\n",
      "  62.1063117  171.57284594  62.28020152  98.10837671  50.04268803\n",
      "  85.04883792  53.4247814  110.06139369  71.0417341   88.36642605\n",
      "  28.87233144  74.52117411 115.18872899  68.45262579   4.77803733\n",
      " 103.5571571   59.46295837  40.71282873  35.08916777  59.70711988\n",
      "   6.59051271  41.31132915   3.8486201   70.57082012  27.23268151\n",
      "  70.95951611]\n",
      "29-th iteration, loss: 0.09248364097459373, 925 gd steps\n",
      "insert gradient: -0.004021393720256997\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[  0.         101.17279341 107.44162311 106.98511381 112.42030316\n",
      "  62.36952192 171.63864544  62.01821479  98.11342476  49.77093361\n",
      "  84.90374981  53.15112686 108.76366608  71.94430766  88.85500379\n",
      "  28.7339498   71.83858379 114.27874991  76.67273434   1.71534848\n",
      "  44.83618072   0.          59.7815743   60.67922965  45.45892987\n",
      "  29.82244344  60.84625327   2.95010508  40.23438023   7.05896343\n",
      "  70.93568804  31.22469132  70.95951611]\n",
      "30-th iteration, loss: 0.09147239309040542, 26 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 31 layers\n",
      "[  0.          99.16369843 108.22248254 105.83588821 112.10019733\n",
      "  62.82295281 177.99954879  60.90879254  98.13916934  50.71460238\n",
      "  85.54164908  52.35599634 104.84446658  74.70337443  94.14387314\n",
      "  28.98259429  69.22980596 110.58925726 118.41418272  11.15816317\n",
      "  54.37386314  60.35065717  45.10316561  23.92220887  62.55282434\n",
      "   0.51083479  41.42858026  10.93674662  71.90311302  36.01318459\n",
      "  70.95951611]\n",
      "31-th iteration, loss: 0.08640608011295335, 407 gd steps\n",
      "insert gradient: -0.002091455622703421\n",
      "31-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         102.35633585 108.89041519  96.37305224 119.83806785\n",
      "  63.68494902 189.74055145  58.70932541  99.35519599  51.12247881\n",
      "  86.99393524  50.03297575  99.08958733  77.21301382 108.81301563\n",
      "  29.46921619  62.1872203   54.11166371   0.          54.11166371\n",
      " 102.55818806  46.90611329  30.15079231  44.96072564  39.16904818\n",
      "  23.34161928 115.40113156  31.36882169  46.80483644  24.56370598\n",
      "  70.95951611]\n",
      "32-th iteration, loss: 0.08593468753729706, 17 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "32-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         102.62858598 108.33068636  94.36615543 125.40764126\n",
      "  63.52286837 191.43222146  57.10889168  99.28735317  52.59078249\n",
      "  87.22148285  49.24297807  99.19226765  76.79195724 111.65517929\n",
      "  30.07786515  60.44146282  51.54159165   7.52024321  50.43628376\n",
      "  98.04581884  56.40964644  25.20618449  39.96837599  44.43361643\n",
      "  21.34835392 114.40726057  37.65870753  38.43368676  26.12220938\n",
      "  70.95951611]\n",
      "33-th iteration, loss: 0.08524578114664855, 72 gd steps\n",
      "insert gradient: -0.0016992005275693258\n",
      "33-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         105.62057712 107.76829953  92.21357743 127.53398534\n",
      "  63.75161203 191.99886069  58.11237446 100.18836552  52.12042376\n",
      "  87.89679309  49.48110526  98.56686333  77.2985494  110.08238613\n",
      "  32.56396664  55.43866909  51.29931755  13.74981134  47.76233141\n",
      "  96.94978913  58.5216707   23.68786525  33.98158976  55.12118898\n",
      "  20.27599466 110.75573853  42.96809909  31.87220537  27.816365\n",
      "  70.95951611]\n",
      "34-th iteration, loss: 0.0830575985893212, 133 gd steps\n",
      "insert gradient: -0.0071896240887199085\n",
      "34-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.09369373e+02 1.08546061e+02 7.30210664e+01\n",
      " 1.53733089e+02 6.63498102e+01 1.90401826e+02 5.99866632e+01\n",
      " 1.01984544e+02 5.27709757e+01 8.79518810e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.94541301e+01 9.79129047e+01 7.66880430e+01\n",
      " 1.06748231e+02 3.75228698e+01 4.78802846e+01 5.07461724e+01\n",
      " 2.45383926e+01 4.39566932e+01 9.61484362e+01 6.15679969e+01\n",
      " 1.79527198e+01 2.50825685e+01 7.91915990e+01 1.87877316e+01\n",
      " 9.98855100e+01 5.37434159e+01 2.15915354e+01 2.94788381e+01\n",
      " 7.09595161e+01]\n",
      "35-th iteration, loss: 0.08179377695995928, 26 gd steps\n",
      "insert gradient: -0.0055183072333897815\n",
      "35-th iteration, new layer inserted. now 33 layers\n",
      "[  0.         109.31676942 111.0353973   69.07695512 163.3557878\n",
      "  65.12015733 132.52645983   0.          53.01058393  63.25702328\n",
      " 100.21024429  53.62067727  88.57734953  51.02023133  98.81532761\n",
      "  72.97335758 110.06101831  42.3353956   47.50259991  54.25895123\n",
      "  26.85064094  36.1739293   94.70468478  64.03194678  16.93775198\n",
      "  25.00985758  83.31678518  19.67887872  96.61435389  54.82147244\n",
      "  23.09382143  26.3862526   70.95951611]\n",
      "36-th iteration, loss: 0.08009081910894647, 29 gd steps\n",
      "insert gradient: -0.005664335939522279\n",
      "36-th iteration, new layer inserted. now 35 layers\n",
      "[0.00000000e+00 1.07181653e+02 1.13170236e+02 6.90021078e+01\n",
      " 1.61612652e+02 6.60786321e+01 1.25956398e+02 9.98160808e+00\n",
      " 4.56582773e+01 6.49473986e+01 1.02336442e+02 5.38585384e+01\n",
      " 8.79114001e+01 5.04020109e+01 9.98575874e+01 0.00000000e+00\n",
      " 1.06581410e-14 7.27766641e+01 1.10664914e+02 4.59381479e+01\n",
      " 4.90292582e+01 5.55675280e+01 3.04436076e+01 2.89344395e+01\n",
      " 9.30630184e+01 6.51959646e+01 1.82737359e+01 2.52129391e+01\n",
      " 8.60537120e+01 2.07018449e+01 9.29833200e+01 5.33227067e+01\n",
      " 2.32224896e+01 2.23259631e+01 7.09595161e+01]\n",
      "37-th iteration, loss: 0.07788093586502931, 17 gd steps\n",
      "insert gradient: -0.015868832970531415\n",
      "37-th iteration, new layer inserted. now 37 layers\n",
      "[0.00000000e+00 1.07525972e+02 1.18851699e+02 7.11267415e+01\n",
      " 1.61474091e+02 6.87274826e+01 1.19366384e+02 0.00000000e+00\n",
      " 1.42108547e-14 1.76579413e+01 2.96384208e+01 6.57233007e+01\n",
      " 1.03391286e+02 5.24924511e+01 8.63355761e+01 5.01702737e+01\n",
      " 1.01338366e+02 8.42422950e-02 7.96750986e-01 7.31100322e+01\n",
      " 1.09608516e+02 4.85253441e+01 5.12009936e+01 5.53358762e+01\n",
      " 3.35450128e+01 2.43825116e+01 9.22136700e+01 6.60594460e+01\n",
      " 2.18763301e+01 2.40536100e+01 8.79849197e+01 2.16417049e+01\n",
      " 8.95897077e+01 5.16188269e+01 2.19754546e+01 2.11184563e+01\n",
      " 7.09595161e+01]\n",
      "38-th iteration, loss: 0.07160110139290515, 22 gd steps\n",
      "insert gradient: -0.007879203224625944\n",
      "38-th iteration, new layer inserted. now 33 layers\n",
      "[  0.23427522 104.13977462 121.69742574  69.83767547 160.44861156\n",
      "  68.05203829 117.61079804  39.06031414  12.97125641  64.19028694\n",
      " 103.52414244  54.45985124  84.48503182  46.84630935 100.51126147\n",
      "  76.25436644 108.87744207  49.90914429  53.7262788   55.06398773\n",
      "  37.12225234  19.90775173  91.21782012  66.81518335  28.04329325\n",
      "  22.76188007  89.03790209  21.79236622  86.57573367  49.9273514\n",
      "  20.5010532   19.81724878  70.95951611]\n",
      "39-th iteration, loss: 0.06560474752552685, 140 gd steps\n",
      "insert gradient: -0.0009905099033899558\n",
      "39-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.00581228e+02 1.24210720e+02 6.77314662e+01\n",
      " 1.72401627e+02 6.71872535e+01 1.09503757e+02 1.14915322e+02\n",
      " 1.02411402e+02 5.47193499e+01 8.57610578e+01 4.82882891e+01\n",
      " 9.66404846e+01 7.67677832e+01 1.08132832e+02 4.84636293e+01\n",
      " 6.80284617e+01 4.98476934e+01 0.00000000e+00 1.77635684e-15\n",
      " 6.14111979e+01 5.51150397e+00 9.06597726e+01 6.01849866e+01\n",
      " 5.67921548e+01 2.34216032e+01 8.73353158e+01 1.75262129e+01\n",
      " 7.00906155e+01 4.84601202e+01 2.46960051e+01 1.53715919e+01\n",
      " 7.09595161e+01]\n",
      "40-th iteration, loss: 0.06471087483897267, 127 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "40-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.01383458e+02 1.21969067e+02 6.79640458e+01\n",
      " 1.73730646e+02 6.67059132e+01 1.10493500e+02 1.13747724e+02\n",
      " 1.03415888e+02 5.46498170e+01 8.63318851e+01 4.81225238e+01\n",
      " 9.66151644e+01 7.60906556e+01 1.10238469e+02 4.77896553e+01\n",
      " 7.16371023e+01 4.51036502e+01 7.86045388e+01 8.02229543e-02\n",
      " 2.66081780e+01 0.00000000e+00 6.65204450e+01 5.69312607e+01\n",
      " 6.36014437e+01 2.39713164e+01 8.71946647e+01 1.90818534e+01\n",
      " 6.09892380e+01 4.99237732e+01 2.94362366e+01 1.25419772e+01\n",
      " 7.09595161e+01]\n",
      "41-th iteration, loss: 0.06460914897124517, 125 gd steps\n",
      "insert gradient: -0.00036808661604776635\n",
      "41-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         101.56076636 121.60145602  68.07647046 173.87204364\n",
      "  66.66133156 110.61935778 113.60468718 103.61038615  54.73025009\n",
      "  86.3948215   48.1198298   96.64802841  75.98558217 110.48766475\n",
      "  47.82304383  72.18345328  44.50334274 104.61414219   0.66180707\n",
      "  66.24708739  56.50569318  65.15106479  23.70501803  87.14642744\n",
      "  19.82324174  58.61604251  50.5321494   30.62757596  11.82272137\n",
      "  70.95951611]\n",
      "42-th iteration, loss: 0.06416892173803355, 80 gd steps\n",
      "insert gradient: -0.0004817547102101994\n",
      "42-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.02973908e+02 1.19310557e+02 6.81942376e+01\n",
      " 1.75029356e+02 6.63495092e+01 1.11753197e+02 1.12086823e+02\n",
      " 1.04540460e+02 5.49027915e+01 8.74167595e+01 4.77766030e+01\n",
      " 9.63548054e+01 0.00000000e+00 7.10542736e-15 7.60995984e+01\n",
      " 1.10995884e+02 4.66821614e+01 7.62391345e+01 4.34542109e+01\n",
      " 1.00102420e+02 3.17663312e+00 6.07783430e+01 5.37706901e+01\n",
      " 8.35121351e+01 1.97674130e+01 8.62314523e+01 2.61000897e+01\n",
      " 3.66387989e+01 6.22445245e+01 3.95992270e+01 8.24404870e+00\n",
      " 7.09595161e+01]\n",
      "43-th iteration, loss: 0.06416565235059367, 420 gd steps\n",
      "insert gradient: -5.1275020171908367e-05\n",
      "43-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.03003194e+02 1.19230056e+02 6.82326622e+01\n",
      " 1.75080119e+02 6.63427973e+01 1.11756235e+02 1.12062823e+02\n",
      " 1.04585867e+02 5.48903281e+01 8.74178033e+01 4.77787457e+01\n",
      " 9.63806777e+01 7.61765417e+01 1.10848896e+02 4.67111620e+01\n",
      " 7.62849790e+01 4.34601973e+01 1.00288489e+02 3.07487745e+00\n",
      " 6.07570428e+01 5.37060351e+01 8.37765512e+01 1.99165539e+01\n",
      " 8.59562616e+01 2.61321190e+01 3.66523809e+01 6.18053169e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.98838656e+01 8.20209325e+00\n",
      " 7.09595161e+01]\n",
      "44-th iteration, loss: 0.06416560544915145, 315 gd steps\n",
      "insert gradient: -5.1017876571493454e-05\n",
      "44-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         103.00367089 119.22882283  68.23281183 175.08102916\n",
      "  66.34269286 111.7562333  112.0626257  104.58659569  54.89038734\n",
      "  87.41779391  47.77863851  96.38112164  76.1770153  110.8469484\n",
      "  46.71154339  76.28561143  43.45997753 100.2916275    3.07357278\n",
      "  60.7568141   53.70493084  83.78055573  19.91863149  85.95200893\n",
      "  26.13274361  36.65268254  61.79934413  39.89369458   8.20115782\n",
      "  70.95951611]\n",
      "45-th iteration, loss: 0.06416558948832599, 130 gd steps\n",
      "insert gradient: -5.104302712402193e-05\n",
      "45-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.03003864e+02 1.19228318e+02 6.82328745e+01\n",
      " 1.75081404e+02 6.63426473e+01 1.11756231e+02 1.12062542e+02\n",
      " 1.04586891e+02 5.48904042e+01 8.74177874e+01 4.77785968e+01\n",
      " 9.63813116e+01 7.61772313e+01 1.10846160e+02 4.67116997e+01\n",
      " 7.62858686e+01 4.34598834e+01 1.00292913e+02 3.07302815e+00\n",
      " 6.07567177e+01 5.37044782e+01 8.37821987e+01 1.99194885e+01\n",
      " 8.59502693e+01 2.61330017e+01 3.66528065e+01 6.17969041e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.98957081e+01 8.20076635e+00\n",
      " 7.09595161e+01]\n",
      "46-th iteration, loss: 0.06416554621088685, 294 gd steps\n",
      "insert gradient: -5.0822061009843284e-05\n",
      "46-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         103.00429927 119.22717925  68.23302314 175.08226225\n",
      "  66.34254463 111.7562248  112.06235375 104.58755643  54.89043213\n",
      "  87.41776728  47.7785019   96.38175133  76.17774462 110.84438964\n",
      "  46.71205159  76.28644832  43.45966724 100.295827     3.07178543\n",
      "  60.7564982   53.70345787  83.78592481  19.92144401  85.94633751\n",
      "  26.13359049  36.65308759  61.79139562  39.90482528   8.19986397\n",
      "  70.95951611]\n",
      "47-th iteration, loss: 0.06416553121275477, 123 gd steps\n",
      "insert gradient: -5.084908767018523e-05\n",
      "47-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         103.00448059 119.22670488  68.23308737 175.08262315\n",
      "  66.34250077 111.7562222  112.0622756  104.58783236  54.8904389\n",
      "  87.41775659  47.77846062  96.38193637  76.17796255 110.84365317\n",
      "  46.71219219  76.28668733  43.45957355 100.2970436    3.07126245\n",
      "  60.7564058   53.70303218  83.78748085  19.9222636   85.94469881\n",
      "  26.13383582  36.65320385  61.78910287  39.90672736   8.19948168\n",
      "  70.95951611]\n",
      "48-th iteration, loss: 0.06416551648842145, 121 gd steps\n",
      "insert gradient: -5.086996297199914e-05\n",
      "48-th iteration, new layer inserted. now 33 layers\n",
      "[0.00000000e+00 1.03004659e+02 1.19226239e+02 6.82331494e+01\n",
      " 1.75082978e+02 6.63424559e+01 1.11756219e+02 1.12062199e+02\n",
      " 1.04588102e+02 5.48904429e+01 8.74177452e+01 4.77784191e+01\n",
      " 9.63821191e+01 7.61781810e+01 1.10842930e+02 4.67123235e+01\n",
      " 7.62869190e+01 4.34594771e+01 1.00298241e+02 3.07075019e+00\n",
      " 6.07563151e+01 5.37026105e+01 8.37890085e+01 1.99230633e+01\n",
      " 8.59430852e+01 2.61340727e+01 3.66533158e+01 6.17868498e+01\n",
      " 0.00000000e+00 1.77635684e-14 3.99086007e+01 8.19910627e+00\n",
      " 7.09595161e+01]\n",
      "49-th iteration, loss: 0.06416547688094434, 272 gd steps\n",
      "insert gradient: -5.06658776950267e-05\n",
      "49-th iteration, new layer inserted. now 31 layers\n",
      "[  0.         103.00505806 119.225195    68.23329342 175.08378119\n",
      "  66.34235848 111.75621468 112.06203058 104.58870894  54.89045271\n",
      "  87.41771846  47.77832793  96.38253514  76.17868134 110.84131764\n",
      "  46.71262578  76.2874425   43.45926699 100.30093277   3.06959879\n",
      "  60.75611386  53.70167321  83.79244267  19.92487168  85.93947087\n",
      "  26.13460728  36.65356664  61.78180841  39.91700618   8.19825107\n",
      "  70.95951611]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5181016671604137\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  47.82950899    0.         1937.09511412]\n",
      "1-th iteration, loss: 0.7548983462167986, 11 gd steps\n",
      "insert gradient: -0.6658215687676553\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  45.37641      61.86989079  118.11555574    0.         1818.97955839]\n",
      "2-th iteration, loss: 0.5091173697592798, 55 gd steps\n",
      "insert gradient: -0.6504328949059671\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   4.60119281   53.49616089   99.00339967   99.41684399  222.73219082\n",
      "    0.         1596.24736756]\n",
      "3-th iteration, loss: 0.4353686157745709, 13 gd steps\n",
      "insert gradient: -0.5681873563082831\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           65.94275488   94.35592826   91.73787068  110.69647209\n",
      "    0.           93.21808176   50.0780836  1596.24736756]\n",
      "4-th iteration, loss: 0.34023907788073104, 47 gd steps\n",
      "insert gradient: -0.3371900958061859\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   1.16803437   83.78413825  125.55537408   77.31829589   98.59979888\n",
      "   44.68023943   67.06281642   48.40407118  532.08245585    0.\n",
      " 1064.16491171]\n",
      "5-th iteration, loss: 0.3060265343404453, 16 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[   0.           82.49157331  128.74037276   75.93355412   98.78733207\n",
      "   45.68754193   61.75731922   47.06253728  514.07013361   39.19516946\n",
      " 1064.16491171]\n",
      "6-th iteration, loss: 0.2905679530262813, 43 gd steps\n",
      "insert gradient: -0.13190780100663796\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  1.01300997  70.3889371  135.0408181   82.5837316  101.41012041\n",
      "  46.25962265  66.01850534  41.93667275 478.20861399  59.97671113\n",
      " 677.19585291   0.         386.9690588 ]\n",
      "7-th iteration, loss: 0.2781886444405395, 73 gd steps\n",
      "insert gradient: -0.1473185529498367\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  1.2035083   66.2327349  135.44985413  81.13757517 106.70968348\n",
      "  48.48142655  57.86983649  43.78310623 131.2842292    0.\n",
      " 341.33899592  54.35903257 652.87145008  24.40230633 386.9690588 ]\n",
      "8-th iteration, loss: 0.24866642082357357, 17 gd steps\n",
      "insert gradient: -0.2600156680955722\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  2.12186597  63.2388675  135.66502741  78.85899842 124.65443804\n",
      "  53.35158512  58.37261935  39.34032773  96.8718443   33.4233679\n",
      "  77.54908851   0.         232.64726554  58.3019413  653.27501804\n",
      "  22.96571115 386.9690588 ]\n",
      "9-th iteration, loss: 0.22249142931447977, 73 gd steps\n",
      "insert gradient: -0.10287741211871085\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.58760607  67.56999746 130.93966281  82.14082809 118.97680263\n",
      "  59.41585846  72.75184777  34.74318401  89.12261177  57.52173309\n",
      "  38.88235207  17.3996605  219.63318236  50.74555175 346.42788566\n",
      "   0.         346.42788566  16.01297339 386.9690588 ]\n",
      "10-th iteration, loss: 0.20888282980253273, 46 gd steps\n",
      "insert gradient: -0.09118644123074243\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  0.92837353  68.77714757 131.53995455  81.79080005 125.59248891\n",
      "  59.51962038  72.94992181  34.22736904  90.63689052  59.39275763\n",
      "  49.66062875  13.71438538 146.85647513   0.          73.42823756\n",
      "  45.77405219 295.61712988  25.65195912 361.85689311  12.171292\n",
      " 386.9690588 ]\n",
      "11-th iteration, loss: 0.18966902235964314, 105 gd steps\n",
      "insert gradient: -0.07361130051372274\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  1.8801436   68.25339832 134.36874368  86.34156695 127.21928502\n",
      "  64.69204885  90.40876654  36.63534651  81.21277008  53.17850984\n",
      " 197.02141316  40.88448357  26.34018845  64.06275751 199.9783299\n",
      "   0.          99.98916495  26.16040622 377.2573678   14.25711972\n",
      " 386.9690588 ]\n",
      "12-th iteration, loss: 0.1682547988846449, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  4.12562619  61.88303231 132.63365938  92.21133765 130.77978477\n",
      "  65.42731282  96.85719712  44.80379283  66.23696318  46.08090772\n",
      " 182.48366539  68.53437548  44.90585262  30.05213731 162.74694939\n",
      "  34.58128134 102.59887981  14.37751591  74.15730052   0.\n",
      " 333.70785234  27.23087051 386.9690588 ]\n",
      "13-th iteration, loss: 0.1585792016154052, 19 gd steps\n",
      "insert gradient: -0.042724553535073796\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[  0.82792511  61.46648606 118.68665358  99.23018336 127.24984197\n",
      "  71.62030012 100.73236192  45.03512622  60.06849946  46.80678232\n",
      " 184.30273462  60.87952622  66.71989578  22.79939493 158.92198373\n",
      "  41.21403668 107.12851038   5.55225668  72.1818673    9.86828355\n",
      " 333.95821536  31.00152899 386.9690588 ]\n",
      "14-th iteration, loss: 0.15451693735758035, 30 gd steps\n",
      "insert gradient: -0.0567218831766679\n",
      "14-th iteration, new layer inserted. now 25 layers\n",
      "[  1.27095745  67.46677188 116.71377493 103.47014648 118.86400819\n",
      "  71.35151803 102.31747021  47.0876351   59.88430185  46.62976619\n",
      " 112.6128286    0.          75.07521907  56.19285621  81.96769939\n",
      "  20.07085927 152.08205095  38.54485377 110.48863625   3.92910909\n",
      "  72.1934027   14.89222913 335.23134197  33.32491364 386.9690588 ]\n",
      "15-th iteration, loss: 0.1443278029791087, 39 gd steps\n",
      "insert gradient: -0.04042504527883766\n",
      "15-th iteration, new layer inserted. now 27 layers\n",
      "[  0.87750389  70.63523269 121.44931494 103.56193633 115.5100512\n",
      "  72.18283875 109.20360655  49.72359739  65.91495465  46.67453715\n",
      "  97.60441617  21.56545465  37.30922244  56.80025319 102.48135268\n",
      "  14.41682968  80.55196222   0.          64.44156977  39.9246028\n",
      " 113.43543866   8.57446372  70.17747803  19.23721832 327.84077788\n",
      "  29.55313708 386.9690588 ]\n",
      "16-th iteration, loss: 0.1304005709448718, 27 gd steps\n",
      "insert gradient: -0.020775940729955956\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  0.72182675  74.19523227 122.81441346 105.31112666 114.01558068\n",
      "  72.09233618 116.77239792  52.79938268  75.56813925  41.73756822\n",
      "  94.91318756  46.83139419  10.75374455  55.24565008 102.84587888\n",
      "  23.93739412  58.17102821  11.96628795  41.8826251   44.15581921\n",
      " 109.88324227  18.81087689  53.89310344  27.58601137 317.02167608\n",
      "  30.08535198 386.9690588 ]\n",
      "17-th iteration, loss: 0.12365094529512163, 18 gd steps\n",
      "insert gradient: -0.03605190765414683\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[2.22886670e+00 7.33185141e+01 1.26339842e+02 1.05729060e+02\n",
      " 1.19204860e+02 0.00000000e+00 1.77635684e-14 6.47481875e+01\n",
      " 1.23240051e+02 5.31303309e+01 8.26452353e+01 4.15492011e+01\n",
      " 8.45925800e+01 5.97290215e+01 1.71808699e+01 3.75229832e+01\n",
      " 9.46036305e+01 5.92037866e+01 2.89343890e+01 3.10010600e+01\n",
      " 1.35284586e+01 3.71248298e+01 9.58340458e+01 3.38023089e+01\n",
      " 2.07634030e+01 5.02700807e+01 3.00449018e+02 3.06337468e+01\n",
      " 3.86969059e+02]\n",
      "18-th iteration, loss: 0.12018732199301446, 332 gd steps\n",
      "insert gradient: -0.023623737617056096\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[3.03202003e-01 7.45207048e+01 1.26010997e+02 1.09224993e+02\n",
      " 1.15289319e+02 6.84795862e+01 1.25169332e+02 5.82369905e+01\n",
      " 8.22510975e+01 4.14153860e+01 8.71101814e+01 5.62521016e+01\n",
      " 2.41123506e+01 3.69124005e+01 9.58505515e+01 6.16093574e+01\n",
      " 2.75644459e+01 2.62552486e+01 1.85796433e+01 3.99743493e+01\n",
      " 9.15496971e+01 4.07216084e+01 1.83035399e+01 5.07256325e+01\n",
      " 2.30168294e+02 0.00000000e+00 7.67227646e+01 2.84565174e+01\n",
      " 3.86969059e+02]\n",
      "19-th iteration, loss: 0.11737292167075615, 21 gd steps\n",
      "insert gradient: -0.014737578463882836\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[4.85244257e-01 7.70749437e+01 1.22863103e+02 1.10181972e+02\n",
      " 1.14498940e+02 6.98626506e+01 1.24101209e+02 5.99670463e+01\n",
      " 8.51132816e+01 4.18208988e+01 8.68830679e+01 5.56270304e+01\n",
      " 3.60341301e+01 0.00000000e+00 7.10542736e-15 2.72436735e+01\n",
      " 9.63826158e+01 6.48290444e+01 2.74389260e+01 2.29377063e+01\n",
      " 2.91595732e+01 3.39288380e+01 9.01404672e+01 6.20574845e+01\n",
      " 4.77134541e+00 4.27490706e+01 2.19245663e+02 1.27247940e+01\n",
      " 5.59927460e+01 3.73739448e+01 3.86969059e+02]\n",
      "20-th iteration, loss: 0.11433539381038971, 689 gd steps\n",
      "insert gradient: -0.019460692173024773\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  1.2655231   68.73574099 132.25998722 101.1272022  127.88709915\n",
      "  71.97988128 118.55601343  58.36826813  84.86857261  42.87357594\n",
      "  88.67738294  56.37655464  45.76354336  21.64296386  95.97289984\n",
      "  55.85381919  45.33237254  30.37720962  38.2099675   23.65523165\n",
      "  84.09718895  64.97383157  14.36109822  19.9605898  218.71405256\n",
      "  27.46650621  33.35632098  54.20407749 145.11339705   0.\n",
      " 241.85566175]\n",
      "21-th iteration, loss: 0.10648012860184858, 70 gd steps\n",
      "insert gradient: -0.02387990707170603\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[  2.65968445  64.32870852 123.09686009  96.05850731 144.45762302\n",
      "  75.78736446 117.24931984  65.27170164  90.44788353  41.8698229\n",
      "  84.98812441  51.98670486  84.4418059   10.93988239  86.25217587\n",
      "  53.94352749  53.5614953   25.85101909  58.81243146  17.130118\n",
      "  76.9750139   69.21215842 269.99287147  30.17413971  41.75188175\n",
      "  26.53448376  81.86163723   0.          81.86163723  27.09747022\n",
      " 241.85566175]\n",
      "22-th iteration, loss: 0.10258666522596901, 32 gd steps\n",
      "insert gradient: -0.015279234743430093\n",
      "22-th iteration, new layer inserted. now 33 layers\n",
      "[  1.31248161  69.67708958 124.33626544 102.43452923 131.81450851\n",
      "  75.84301198 115.7646562   65.65301417  92.94161257  42.00763166\n",
      "  85.47839282  52.61309428  86.79691032  12.5529726   81.55144445\n",
      "  55.35634242  63.31176108  24.08429595  61.08603916  15.17601758\n",
      "  69.07361178  69.24395906 190.30554259   0.          76.12221703\n",
      "  26.07568275  49.78409817  28.33548309  77.16966878   7.25029696\n",
      "  73.55676319  26.97228879 241.85566175]\n",
      "23-th iteration, loss: 0.09625336554855893, 23 gd steps\n",
      "insert gradient: -0.00920546439376299\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  1.30395458  70.10075042 124.929227   104.73659267 133.39816516\n",
      "  73.55711874 117.77159752  67.79007789  96.64816032  42.26264115\n",
      "  83.49037067  54.56823599  92.69001519  13.8855214   75.96865541\n",
      "  54.95551259  77.43542218  27.20745828  66.34658685  16.06658903\n",
      "  53.56188363  50.64215179 172.97193055  19.63287488  60.67156647\n",
      "  32.48979756  58.50853303  21.8412137   73.35191195  11.82950814\n",
      "  63.82855839  38.84311666 241.85566175]\n",
      "24-th iteration, loss: 0.09121925872697016, 22 gd steps\n",
      "insert gradient: -0.006917002611070278\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[  1.47073709  70.63006307 123.09048117 104.52851465 138.92201456\n",
      "  76.42050763 115.0246629   68.63631092 101.81747257  45.29069448\n",
      "  80.59199362  52.3700012   99.84090848  19.47915982  59.02758445\n",
      "  54.55923554  94.48662223  27.80775522  57.18845919  23.28994822\n",
      "  38.79093615  49.14540023 161.26523335  36.78411595  42.40032779\n",
      "  36.00377118  68.67707993  16.63723284  72.54660927  15.49062182\n",
      "  53.27770599  47.64703485 241.85566175]\n",
      "25-th iteration, loss: 0.0863062935018481, 23 gd steps\n",
      "insert gradient: -0.00898099208166344\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[  0.80002828  70.18003013 123.17754756 107.40800453 130.98105107\n",
      "  79.1610265  115.35969066  69.97578067 108.87528818  46.59438218\n",
      "  82.53697078  49.77256192  97.06532218  32.62897307  39.60840312\n",
      "  54.48117672  98.20598767  34.18516168  39.6142563   32.65623772\n",
      "  28.0158703   45.70528923 152.61016611  58.41864083  30.49457296\n",
      "  39.4410231   79.07597644  15.74420601  68.45533885  19.51783213\n",
      "  34.43595155  49.96929797 138.20323529   0.         103.65242647]\n",
      "26-th iteration, loss: 0.08038755090871008, 31 gd steps\n",
      "insert gradient: -0.004600318864067694\n",
      "26-th iteration, new layer inserted. now 37 layers\n",
      "[1.52229879e+00 7.24503596e+01 1.23675178e+02 0.00000000e+00\n",
      " 1.42108547e-14 1.07641300e+02 1.28822160e+02 8.05012299e+01\n",
      " 1.17233285e+02 6.98791986e+01 1.12721547e+02 5.01291230e+01\n",
      " 8.11124227e+01 4.94129167e+01 9.58975051e+01 4.52221934e+01\n",
      " 2.97042578e+01 5.01160996e+01 9.67742810e+01 4.22862294e+01\n",
      " 2.89472553e+01 4.17750044e+01 2.30838285e+01 4.12939207e+01\n",
      " 1.40724726e+02 7.64034693e+01 3.60370256e+01 3.40159146e+01\n",
      " 8.66399774e+01 1.96068275e+01 5.66204750e+01 2.87632379e+01\n",
      " 2.57393686e+01 3.97188548e+01 1.38234852e+02 1.48378918e+01\n",
      " 1.03652426e+02]\n",
      "27-th iteration, loss: 0.07442404333739233, 29 gd steps\n",
      "insert gradient: -0.007616931967768016\n",
      "27-th iteration, new layer inserted. now 35 layers\n",
      "[  0.9466665   75.22818361 121.85625087 110.07939705 127.09432669\n",
      "  81.24724518 118.27176154  69.47477743 116.56631945  54.78132817\n",
      "  79.53532514  49.03593827  95.89048116  55.02679184  30.2856672\n",
      "  41.79782975  95.87601013  57.68543669  28.98008265  46.2970429\n",
      "  30.08951063  23.64026731 109.73839369  91.39545831  58.29572257\n",
      "  24.90512863  88.76972442  35.45028586  40.14539397  37.22271256\n",
      "  27.30636207  17.9530585  137.2978711   22.53674767 103.65242647]\n",
      "28-th iteration, loss: 0.07330959092311815, 23 gd steps\n",
      "insert gradient: -0.0034774574884477295\n",
      "28-th iteration, new layer inserted. now 37 layers\n",
      "[4.82908087e-01 7.44341023e+01 1.21621213e+02 1.11196909e+02\n",
      " 1.27628384e+02 8.21535768e+01 1.18546294e+02 6.96761018e+01\n",
      " 1.16682654e+02 5.51445997e+01 7.96227984e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.92167928e+01 9.60041400e+01 5.60747457e+01\n",
      " 3.24956177e+01 4.01418972e+01 9.62692371e+01 5.85538277e+01\n",
      " 3.24190803e+01 4.37151884e+01 3.45567001e+01 2.27144602e+01\n",
      " 1.05009360e+02 9.17799972e+01 6.48558697e+01 2.43517326e+01\n",
      " 8.65493357e+01 3.78083893e+01 3.96619830e+01 3.61287462e+01\n",
      " 2.98629884e+01 1.60012714e+01 1.37328941e+02 2.31410590e+01\n",
      " 1.03652426e+02]\n",
      "29-th iteration, loss: 0.07034253021203464, 23 gd steps\n",
      "insert gradient: -0.009132996395244855\n",
      "29-th iteration, new layer inserted. now 35 layers\n",
      "[  1.7982088   77.08026022 122.5941719  111.90999402 126.77984365\n",
      "  83.73079016 120.89919283  68.05245444 120.58686032  56.3462287\n",
      "  81.72620701  49.83030365  97.25851557  56.73433144  47.50081917\n",
      "  30.1045844   97.66874076  55.9167881   54.51999372  24.99414557\n",
      "  59.40723364  21.33209671  77.87031557  90.8008007  106.59483269\n",
      "  21.01007998  64.93703596  52.69491969  40.89912154  25.28367761\n",
      "  46.00948747   5.15655553 138.84994677  33.9693971  103.65242647]\n",
      "30-th iteration, loss: 0.06880462264809595, 27 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "30-th iteration, new layer inserted. now 37 layers\n",
      "[9.71358618e-02 7.37947120e+01 1.22659955e+02 1.13785308e+02\n",
      " 1.25977761e+02 8.37458392e+01 1.21565898e+02 6.94935993e+01\n",
      " 1.20099457e+02 5.82912855e+01 8.31523098e+01 4.82313884e+01\n",
      " 9.54982797e+01 5.67232402e+01 5.01695830e+01 2.87989193e+01\n",
      " 9.70325326e+01 5.79999375e+01 5.60565171e+01 2.51967974e+01\n",
      " 6.16102841e+01 2.04738834e+01 7.40560140e+01 9.10426358e+01\n",
      " 1.10125534e+02 2.50391246e+01 5.76150391e+01 5.36986973e+01\n",
      " 4.48689074e+01 2.17906788e+01 4.91485103e+01 3.67489463e+00\n",
      " 4.67248643e+01 0.00000000e+00 9.34497286e+01 3.18854709e+01\n",
      " 1.03652426e+02]\n",
      "31-th iteration, loss: 0.06720370995670609, 22 gd steps\n",
      "insert gradient: -0.0062661003854263386\n",
      "31-th iteration, new layer inserted. now 35 layers\n",
      "[  1.41198496  75.50731117 123.31779081 114.93258424 124.33106213\n",
      "  84.22234967 120.95265425  70.55930341 121.62559977  58.57239789\n",
      "  85.81681269  49.12390438  94.95709337  56.33654513  59.98315901\n",
      "  24.54439014  94.69077902  58.8235919   60.43112005  20.84059552\n",
      "  67.73172218  24.10265535  62.12882447  89.43354476 119.04324238\n",
      "  34.98349019  38.81179764  53.98718939  56.10085171  13.95981487\n",
      "  93.77416742   7.14377179  90.1620394   41.48222521 103.65242647]\n",
      "32-th iteration, loss: 0.06609667896855667, 30 gd steps\n",
      "insert gradient: -0.00412845239148911\n",
      "32-th iteration, new layer inserted. now 37 layers\n",
      "[4.96973439e-01 7.23918167e+01 1.23058432e+02 0.00000000e+00\n",
      " 1.42108547e-14 1.15513911e+02 1.25204672e+02 8.43939313e+01\n",
      " 1.23575112e+02 7.03799455e+01 1.21852655e+02 6.00695160e+01\n",
      " 8.76390008e+01 4.82907882e+01 9.38872429e+01 5.58014496e+01\n",
      " 6.25341480e+01 2.40734445e+01 9.37156992e+01 5.85002466e+01\n",
      " 6.42845837e+01 1.92244535e+01 6.92393968e+01 2.76723749e+01\n",
      " 5.62540826e+01 8.81231396e+01 1.22181893e+02 3.99625170e+01\n",
      " 3.43332177e+01 5.47238121e+01 5.81716160e+01 1.32993892e+01\n",
      " 9.38420310e+01 9.71665087e+00 8.15803977e+01 3.98563503e+01\n",
      " 1.03652426e+02]\n",
      "33-th iteration, loss: 0.0631857008182703, 25 gd steps\n",
      "insert gradient: -0.008330989485427105\n",
      "33-th iteration, new layer inserted. now 37 layers\n",
      "[1.13384341e+00 0.00000000e+00 1.66533454e-16 6.76226410e+01\n",
      " 1.27615728e+02 1.16727200e+02 1.27228443e+02 8.22066201e+01\n",
      " 1.33271092e+02 6.80360799e+01 1.17594747e+02 6.36195390e+01\n",
      " 9.11523724e+01 4.98038498e+01 9.45112445e+01 5.33550887e+01\n",
      " 7.86902394e+01 2.13165582e+01 8.57012168e+01 5.70328945e+01\n",
      " 8.80192700e+01 2.02903510e+01 6.04112787e+01 3.41840207e+01\n",
      " 4.51238769e+01 6.30149359e+01 1.45554341e+02 6.39340558e+01\n",
      " 2.00405652e+01 5.05748137e+01 8.90338667e+01 1.24490805e+01\n",
      " 7.91712571e+01 2.46542174e+01 2.38726863e+01 5.58846812e+01\n",
      " 1.03652426e+02]\n",
      "34-th iteration, loss: 0.061356239185802894, 18 gd steps\n",
      "insert gradient: -0.0029848083475402095\n",
      "34-th iteration, new layer inserted. now 37 layers\n",
      "[1.34418821e+00 7.49229478e+01 1.28165182e+02 1.14679940e+02\n",
      " 1.28376873e+02 8.03774858e+01 1.30197399e+02 6.98224358e+01\n",
      " 1.19590506e+02 6.40561819e+01 9.07832947e+01 4.78898107e+01\n",
      " 9.39387947e+01 5.67876844e+01 8.08541735e+01 2.08275009e+01\n",
      " 8.47769762e+01 5.62620520e+01 8.79795627e+01 0.00000000e+00\n",
      " 3.55271368e-15 2.18870631e+01 5.99987829e+01 3.43382905e+01\n",
      " 4.44244535e+01 6.06321763e+01 1.46985354e+02 6.51949264e+01\n",
      " 2.05966884e+01 5.08931434e+01 9.05098311e+01 1.51901926e+01\n",
      " 7.65490593e+01 2.54034854e+01 2.25739971e+01 5.40931303e+01\n",
      " 1.03652426e+02]\n",
      "35-th iteration, loss: 0.04868436152944415, 390 gd steps\n",
      "insert gradient: -0.0011656048461094095\n",
      "35-th iteration, new layer inserted. now 35 layers\n",
      "[5.99611578e-02 8.02041443e+01 1.32249784e+02 1.12528609e+02\n",
      " 1.28451196e+02 7.78744557e+01 1.48377959e+02 7.25897211e+01\n",
      " 1.21201729e+02 7.41027983e+01 1.03405610e+02 5.17073326e+01\n",
      " 9.27641406e+01 5.65893057e+01 1.02277897e+02 5.79493270e+01\n",
      " 5.47188888e+01 3.68078958e+01 8.95489225e+01 4.31392417e+01\n",
      " 4.66590056e+01 5.26383603e+01 6.02002198e+01 2.20084796e+01\n",
      " 8.73493926e+01 1.00909050e+02 8.03057689e+01 3.49342605e+01\n",
      " 8.17867464e+01 4.17858618e+01 4.17364381e-01 2.62146631e+01\n",
      " 2.80444294e+01 1.10858635e+01 1.03652426e+02]\n",
      "36-th iteration, loss: 0.04760890562754279, 20 gd steps\n",
      "insert gradient: -0.002418023713684448\n",
      "36-th iteration, new layer inserted. now 35 layers\n",
      "[0.00000000e+00 7.89294514e+01 1.34741480e+02 1.13631358e+02\n",
      " 1.28039952e+02 7.44030018e+01 1.53755129e+02 7.19618186e+01\n",
      " 1.19336985e+02 7.50709626e+01 1.08625285e+02 5.14750623e+01\n",
      " 9.11732206e+01 5.75221391e+01 1.00982794e+02 0.00000000e+00\n",
      " 3.55271368e-15 5.65968615e+01 6.77035024e+01 3.00623382e+01\n",
      " 8.81953960e+01 4.92794501e+01 4.37344318e+01 4.61503719e+01\n",
      " 7.34035568e+01 1.97812322e+01 7.50719035e+01 1.02455740e+02\n",
      " 9.37596029e+01 3.78805911e+01 7.07652117e+01 5.80312384e+01\n",
      " 2.49955284e+01 9.70987861e+00 1.03652426e+02]\n",
      "37-th iteration, loss: 0.04671707464011104, 42 gd steps\n",
      "insert gradient: -0.0014263008789238071\n",
      "37-th iteration, new layer inserted. now 35 layers\n",
      "[1.45740931e-02 7.80636882e+01 1.37722787e+02 1.14974230e+02\n",
      " 1.28928772e+02 7.42196074e+01 1.54507744e+02 7.42379483e+01\n",
      " 1.20185658e+02 7.47257622e+01 1.09184534e+02 5.40702020e+01\n",
      " 9.18995839e+01 0.00000000e+00 2.13162821e-14 5.56819749e+01\n",
      " 1.02580698e+02 5.83593273e+01 6.96992396e+01 2.92256532e+01\n",
      " 8.76775980e+01 4.97751280e+01 4.56528794e+01 4.54966700e+01\n",
      " 7.66947252e+01 2.24335078e+01 6.97492084e+01 9.93670406e+01\n",
      " 1.01968406e+02 4.21395220e+01 6.40359377e+01 5.61149429e+01\n",
      " 2.47838300e+01 9.95616822e+00 1.03652426e+02]\n",
      "38-th iteration, loss: 0.04635232364537555, 17 gd steps\n",
      "insert gradient: -0.002137388703337181\n",
      "38-th iteration, new layer inserted. now 35 layers\n",
      "[  0.          77.11145461 138.98437318 115.56880623 128.91829111\n",
      "  74.18204791 154.87472479  74.51007688 120.04185934  73.97018649\n",
      " 112.15534083  55.37377404  90.62514536  55.87680338 102.53876211\n",
      "  58.21813199  72.01143023  28.23874876  86.77628238  50.68894651\n",
      "  49.38101811  40.95553288  79.26747343  25.28621644  61.2530593\n",
      "  70.5232447    0.          28.20929788 108.2443913   45.9848352\n",
      "  57.84349498  52.16984762  25.04504394   9.98499424 103.65242647]\n",
      "39-th iteration, loss: 0.040500094456225315, 141 gd steps\n",
      "insert gradient: -0.0011583181102890762\n",
      "39-th iteration, new layer inserted. now 33 layers\n",
      "[  0.63835446  76.70240992 141.08366098 113.99855777 139.53985454\n",
      "  76.56546888 151.37557607  79.27890351 121.02561962  74.49322587\n",
      " 116.63967016  59.93792672  92.93704016  55.36335331 103.27382907\n",
      "  61.84997141  88.73169874  28.30291262  81.67610965  53.16673715\n",
      "  62.39117338  38.84540521  79.31623815  35.51931377  56.6254679\n",
      "  46.74631352 120.32228578   0.          48.12891431  61.78146048\n",
      "  53.65282855  48.21351273 123.19718445]\n",
      "40-th iteration, loss: 0.035452697291707976, 294 gd steps\n",
      "insert gradient: -0.0014413903644382787\n",
      "40-th iteration, new layer inserted. now 32 layers\n",
      "[  1.87334354  74.72133231 132.17920756 102.4617587  164.971077\n",
      "  75.33516793 146.99838722  89.47663773 120.15459801  74.1700757\n",
      " 114.5471146   68.16299106  99.43122154  49.57162349 103.59949002\n",
      "  64.3605659  100.08893377  41.34050771  64.27648334  43.94292348\n",
      "  86.25558837  36.64828406  73.6896616   46.96188337  56.23070221\n",
      "  35.40312895  93.22371857  98.57295812  81.29962474  37.46514576\n",
      " 123.19718445   0.        ]\n",
      "41-th iteration, loss: 0.03237918066386828, 48 gd steps\n",
      "insert gradient: -0.0015534079726057472\n",
      "41-th iteration, new layer inserted. now 34 layers\n",
      "[1.90384925e+00 7.25375954e+01 1.33170715e+02 9.94975430e+01\n",
      " 1.70069861e+02 7.20095711e+01 1.40907198e+02 9.33666345e+01\n",
      " 1.17562195e+02 7.28664623e+01 1.14858205e+02 7.13002861e+01\n",
      " 9.81318821e+01 0.00000000e+00 1.06581410e-14 4.68628539e+01\n",
      " 1.02632165e+02 6.56639075e+01 1.01974842e+02 4.45721753e+01\n",
      " 6.89354425e+01 4.14037806e+01 8.66286691e+01 3.62916418e+01\n",
      " 7.15540682e+01 4.73644035e+01 6.94829661e+01 2.11007723e+01\n",
      " 8.88958246e+01 1.01464215e+02 9.55157622e+01 2.70226208e+01\n",
      " 1.26783291e+02 4.15364783e+01]\n",
      "42-th iteration, loss: 0.03202943084002987, 28 gd steps\n",
      "insert gradient: -0.0007753050583989927\n",
      "42-th iteration, new layer inserted. now 36 layers\n",
      "[1.52279268e+00 7.19786703e+01 1.32703130e+02 9.76069159e+01\n",
      " 1.69904327e+02 7.33640142e+01 1.40304661e+02 9.22714129e+01\n",
      " 1.18575229e+02 7.39760987e+01 1.15343100e+02 7.05922087e+01\n",
      " 9.77195872e+01 5.72773117e-01 2.39378039e-02 4.75202379e+01\n",
      " 1.02031965e+02 6.47320912e+01 1.01744178e+02 4.44629659e+01\n",
      " 6.86483692e+01 4.12166998e+01 8.66353794e+01 3.63455306e+01\n",
      " 7.13258049e+01 4.79060645e+01 6.97477812e+01 1.99501054e+01\n",
      " 8.87191948e+01 1.01539489e+02 9.60821941e+01 2.52296292e+01\n",
      " 6.33550723e+01 0.00000000e+00 6.33550723e+01 4.36015323e+01]\n",
      "43-th iteration, loss: 0.031667794024887654, 60 gd steps\n",
      "insert gradient: -0.0001056844263927522\n",
      "43-th iteration, new layer inserted. now 36 layers\n",
      "[1.11663229e+00 7.24223296e+01 1.32578497e+02 9.39747761e+01\n",
      " 1.70112426e+02 7.55123435e+01 1.38308012e+02 9.21186550e+01\n",
      " 1.19583926e+02 7.28440508e+01 1.16274470e+02 7.03600345e+01\n",
      " 9.61726101e+01 4.86352963e+01 1.01641635e+02 6.34159431e+01\n",
      " 1.01815275e+02 4.53841050e+01 6.77392470e+01 4.13366951e+01\n",
      " 8.60375357e+01 3.63730218e+01 6.98412340e+01 4.85153567e+01\n",
      " 6.92848069e+01 1.82429038e+01 8.86672125e+01 1.02088973e+02\n",
      " 9.91663779e+01 2.26143626e+01 5.97375523e+01 9.29238811e-01\n",
      " 5.98362701e+01 0.00000000e+00 1.42108547e-14 5.18538292e+01]\n",
      "44-th iteration, loss: 0.03153081959365348, 22 gd steps\n",
      "insert gradient: -0.000664235458989512\n",
      "44-th iteration, new layer inserted. now 34 layers\n",
      "[1.06895241e+00 7.24991967e+01 1.31660331e+02 9.23987273e+01\n",
      " 1.72406677e+02 7.55905021e+01 1.36726814e+02 9.25055111e+01\n",
      " 1.19295150e+02 7.39970414e+01 1.16048262e+02 7.03953205e+01\n",
      " 9.51202446e+01 0.00000000e+00 3.55271368e-15 4.85129836e+01\n",
      " 1.00944075e+02 6.32460334e+01 1.01957652e+02 4.61121181e+01\n",
      " 6.76926186e+01 4.08464669e+01 8.59410834e+01 3.70295815e+01\n",
      " 6.80764003e+01 4.85300543e+01 7.01092782e+01 1.74492052e+01\n",
      " 8.80731039e+01 1.02442917e+02 1.04241283e+02 2.34096681e+01\n",
      " 1.05882554e+02 6.14246627e+01]\n",
      "45-th iteration, loss: 0.031422724291182834, 90 gd steps\n",
      "insert gradient: -9.54989873899775e-05\n",
      "45-th iteration, new layer inserted. now 34 layers\n",
      "[  1.09313413  72.20510371 131.73008888  91.71671081 171.95999662\n",
      "  76.18068567 135.8697583   92.50644995 119.96337063  72.76468955\n",
      " 117.08768992  70.19650062  95.08702813  48.80742922 101.38103494\n",
      "  62.65260221 102.20735908  46.06076992  66.52227191  41.32678542\n",
      "  85.94126662  36.60435614  68.04084362  48.87219869  68.28358877\n",
      "  17.82533826  88.23278732  72.99303248   0.          29.19721299\n",
      " 104.03381365  25.35163847  98.06820364  64.18187145]\n",
      "46-th iteration, loss: 0.031299960776823725, 53 gd steps\n",
      "insert gradient: -0.00024090678532165505\n",
      "46-th iteration, new layer inserted. now 34 layers\n",
      "[  1.23449244  72.45121767 132.04080118  90.7498156  171.8740231\n",
      "  76.45476677 136.74114903  92.11192715 120.47730967  72.20356327\n",
      " 117.71793798  70.51520673  95.40888888  48.17083034 101.36840583\n",
      "  63.35783749 102.60379141  45.70938544  67.81553758  41.56083121\n",
      "  85.87727817  36.01802953  69.43924129  49.18811993  68.98789571\n",
      "  17.59796187  88.76864531  66.02866519   9.08071876  27.96335741\n",
      " 100.98581295  31.41198307  81.29053591  69.80978418]\n",
      "47-th iteration, loss: 0.03127440167333585, 44 gd steps\n",
      "insert gradient: -0.0006756125351953846\n",
      "47-th iteration, new layer inserted. now 34 layers\n",
      "[  1.17999581  72.89701522 132.54132649  90.52498511 171.59650417\n",
      "  76.38653596 137.11449315  91.70313032 120.18261317  73.0461439\n",
      " 117.18377283  69.92450614  95.16836744  48.52603186 101.39337717\n",
      "  63.07038023 102.37743771  45.64638145  68.27765627  41.33881013\n",
      "  85.79378094  36.14302467  69.6930192   49.17578278  69.43542343\n",
      "  17.60572931  89.2449147   64.6261079   11.0488745   27.34856364\n",
      " 100.70205666  32.08484145  79.36969092  70.14068584]\n",
      "48-th iteration, loss: 0.031271437720810515, 16 gd steps\n",
      "insert gradient: -4.167607155811342e-05\n",
      "48-th iteration, new layer inserted. now 36 layers\n",
      "[1.17356278e+00 7.28755624e+01 1.32532218e+02 9.05171703e+01\n",
      " 1.71596188e+02 7.63875933e+01 1.37121732e+02 9.17044970e+01\n",
      " 1.20182893e+02 7.30490426e+01 1.17194361e+02 6.99538628e+01\n",
      " 9.51926699e+01 4.85645930e+01 1.01418452e+02 6.31147339e+01\n",
      " 1.02390901e+02 4.56575996e+01 6.82860021e+01 4.13443691e+01\n",
      " 8.57914754e+01 3.61358172e+01 6.96955212e+01 4.91727904e+01\n",
      " 6.94343466e+01 1.75858149e+01 8.92456709e+01 6.46178217e+01\n",
      " 0.00000000e+00 1.06581410e-14 1.10753541e+01 2.73446631e+01\n",
      " 1.00697046e+02 3.20734366e+01 7.93444131e+01 7.01431590e+01]\n",
      "49-th iteration, loss: 0.03126077680006143, 43 gd steps\n",
      "insert gradient: -3.215510394570518e-05\n",
      "49-th iteration, new layer inserted. now 34 layers\n",
      "[  1.17487925  72.99595016 132.69284309  90.35629103 171.48541082\n",
      "  76.34082983 137.57479428  91.57877342 120.07333375  73.13497648\n",
      " 117.12397849  69.97899733  95.32532947  48.4833161  101.38385715\n",
      "  63.30058469 102.34763963  45.50848592  68.80640318  41.3343258\n",
      "  85.69312587  36.04235641  70.09864912  49.12267182  70.05536544\n",
      "  17.38162973  89.64946253  63.52925423  13.3505743   26.34866527\n",
      " 100.4735169   32.60503654  77.78203302  70.50604194]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5135986193249256\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  40.36245484    0.         1969.68779641]\n",
      "1-th iteration, loss: 0.7407460658493235, 11 gd steps\n",
      "insert gradient: -0.6111159889166502\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  39.97368704   62.24911224  240.20582883    0.         1729.48196758]\n",
      "2-th iteration, loss: 0.6091203376622952, 13 gd steps\n",
      "insert gradient: -0.7071067265748529\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   4.19962198   76.83427585  225.50935762   38.41838644  247.06885251\n",
      "    0.         1482.41311507]\n",
      "3-th iteration, loss: 0.43933993711729674, 28 gd steps\n",
      "insert gradient: -0.5504873474236252\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   2.64508482   41.88862827  110.40087431    0.          116.89504338\n",
      "   69.4279101   117.22803259   69.55528207 1482.41311507]\n",
      "4-th iteration, loss: 0.3632811486432218, 86 gd steps\n",
      "insert gradient: -0.35365667181747756\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   0.           44.42305156   55.19726394   48.89465034  106.90871175\n",
      "   76.7884542   123.11846739   80.14370081  219.61675779    0.\n",
      " 1262.79635728]\n",
      "5-th iteration, loss: 0.2414315627458609, 72 gd steps\n",
      "insert gradient: -0.13695170783771568\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[   1.93326133   47.97528529   75.43278034   41.89465672   94.50995768\n",
      "   79.18387516  118.74391762   82.27820474  129.391235     82.2313189\n",
      "  229.59933769    0.         1033.19701959]\n",
      "6-th iteration, loss: 0.18689509825099457, 60 gd steps\n",
      "insert gradient: -0.06799155371118473\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.96557379  47.32300714  83.27918903  42.88291081  86.91117485\n",
      "  81.81205874 110.00725519  83.76149407 123.72635077  82.55621056\n",
      " 153.19437934  81.39605771 918.39735075   0.         114.79966884]\n",
      "7-th iteration, loss: 0.18171594279263095, 33 gd steps\n",
      "insert gradient: -0.05606606800343628\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  2.60181285  47.44006331  84.30407209  44.0198093   85.37341615\n",
      "  82.68355849 108.96974662  83.81500194 122.36790202  84.66775308\n",
      " 152.15512496  82.31794504 172.10515716   0.         745.78901437\n",
      "  12.98023553 114.79966884]\n",
      "8-th iteration, loss: 0.17815297964755022, 15 gd steps\n",
      "insert gradient: -0.04419722178981726\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[1.56807790e+00 0.00000000e+00 6.10622664e-16 4.64330353e+01\n",
      " 8.55803716e+01 4.57039619e+01 8.36744264e+01 8.23593382e+01\n",
      " 1.11098839e+02 8.29894524e+01 1.23784290e+02 8.60225272e+01\n",
      " 1.48644082e+02 7.74950737e+01 1.71074299e+02 1.08954542e+01\n",
      " 7.40191168e+02 1.18183250e+01 1.14799669e+02]\n",
      "9-th iteration, loss: 0.17538490614672483, 471 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.3067657   49.6675504   85.99729312  44.37133104  83.38591979\n",
      "  83.78921612 109.69295593  83.02759314 122.73894108  86.80130913\n",
      " 149.54120437  76.25774251 175.37246997  19.44272925  52.36646031\n",
      "   0.         680.763984    11.99999887 114.79966884]\n",
      "10-th iteration, loss: 0.16423210316369805, 17 gd steps\n",
      "insert gradient: -0.04051955238523223\n",
      "10-th iteration, new layer inserted. now 19 layers\n",
      "[  4.11740854  52.46089375  85.86808964  42.82573971  83.48735448\n",
      "  85.3907431  108.99511091  83.13334376 120.75838328  85.45575828\n",
      " 148.3855799   77.58790495 165.86556547  44.54698556  22.12205737\n",
      "  23.11773165 642.64600717   7.54669754 114.79966884]\n",
      "11-th iteration, loss: 0.16089811627258283, 20 gd steps\n",
      "insert gradient: -0.01983537073977497\n",
      "11-th iteration, new layer inserted. now 19 layers\n",
      "[  1.76712545  47.31520567  86.16414407  46.66393559  84.10569374\n",
      "  84.74203556 107.04179754  83.53823912 120.82621226  85.28270615\n",
      " 146.91140494  76.44687528 164.90128057  46.76759353  18.44754557\n",
      "  25.21752094 640.69041401   6.57258752 114.79966884]\n",
      "12-th iteration, loss: 0.1597852866456278, 33 gd steps\n",
      "insert gradient: -0.014509804280677814\n",
      "12-th iteration, new layer inserted. now 21 layers\n",
      "[  1.83900582  47.61160542  86.13532735  46.31609401  84.03494358\n",
      "  84.87908673 105.94417736  83.6597325  121.16390827  84.99089809\n",
      " 145.92747835  76.93706871 162.85112731  51.22927853  12.39436863\n",
      "  27.84966301 212.34665297   0.         424.69330594   6.17618028\n",
      " 114.79966884]\n",
      "13-th iteration, loss: 0.1589334549105889, 14 gd steps\n",
      "insert gradient: -0.022583073041715508\n",
      "13-th iteration, new layer inserted. now 23 layers\n",
      "[1.77068312e+00 0.00000000e+00 2.22044605e-16 4.68872429e+01\n",
      " 8.61878904e+01 4.67596360e+01 8.41987902e+01 8.48090878e+01\n",
      " 1.05503686e+02 8.34790840e+01 1.21222529e+02 8.50980732e+01\n",
      " 1.45237390e+02 7.71121570e+01 1.61362737e+02 5.38189022e+01\n",
      " 9.62099447e+00 2.93818577e+01 2.09921085e+02 3.24591945e+00\n",
      " 4.21899522e+02 5.34717738e+00 1.14799669e+02]\n",
      "14-th iteration, loss: 0.15816897998427454, 21 gd steps\n",
      "insert gradient: -0.019259674137428528\n",
      "14-th iteration, new layer inserted. now 23 layers\n",
      "[  1.97136808  47.30610707  86.46344686  46.59466356  84.30620631\n",
      "  85.1223047  105.10720943  83.51541916 121.12093816  85.06296074\n",
      " 144.93499581  77.50924574 160.5512893   55.35144176   7.85075948\n",
      "  30.45713733 209.15449605   5.78317137 382.00120496   0.\n",
      "  38.2001205    4.05678341 114.79966884]\n",
      "15-th iteration, loss: 0.1567705368008897, 23 gd steps\n",
      "insert gradient: -0.014394758578212019\n",
      "15-th iteration, new layer inserted. now 25 layers\n",
      "[  2.00384786  47.95436768  86.36499667  46.74776781  84.66722185\n",
      "  85.5045861  104.17593188  83.57982335 120.82122427  85.03178381\n",
      " 144.3104467   78.47654921 159.97777357  57.7793754    5.56617299\n",
      "  31.58603084 208.89177448   8.20748065 380.9034994    5.81695936\n",
      "  37.74408511   2.30404706  45.91986754   0.          68.87980131]\n",
      "16-th iteration, loss: 0.15465743784861893, 111 gd steps\n",
      "insert gradient: -0.010463823241544356\n",
      "16-th iteration, new layer inserted. now 27 layers\n",
      "[  2.15367324  47.72393475  85.70462718  46.41170686  83.74917027\n",
      "  85.25245863 103.31469754  82.75925661 121.99870727  83.7974727\n",
      " 141.30125513  80.15572998 159.223292    65.27720507   3.57164956\n",
      "  27.97638837 202.58272933   8.38684533 119.86263007   0.\n",
      " 239.72526015  14.66509844  28.6366143   13.72975493  31.3094054\n",
      "  10.49888494  68.87980131]\n",
      "17-th iteration, loss: 0.15373898509267855, 18 gd steps\n",
      "insert gradient: -0.017140201769606168\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[1.80552340e+00 4.79161689e+01 8.44935355e+01 0.00000000e+00\n",
      " 7.10542736e-15 4.61343158e+01 8.42490403e+01 8.45905952e+01\n",
      " 1.03671396e+02 8.22968611e+01 1.22479713e+02 8.32745424e+01\n",
      " 1.40196369e+02 8.07565405e+01 1.60087894e+02 6.64602199e+01\n",
      " 4.34242438e+00 2.71771464e+01 2.05823962e+02 7.83852398e+00\n",
      " 1.10880045e+02 3.90327721e+00 2.30875921e+02 1.74231494e+01\n",
      " 2.71910916e+01 1.73247482e+01 2.77793780e+01 1.12980457e+01\n",
      " 6.88798013e+01]\n",
      "18-th iteration, loss: 0.15366936583677118, 8 gd steps\n",
      "insert gradient: -0.007493908130571963\n",
      "18-th iteration, new layer inserted. now 31 layers\n",
      "[1.82810115e+00 4.80086481e+01 8.46254676e+01 1.63487717e-01\n",
      " 1.29885528e-01 4.62993430e+01 8.42851300e+01 8.45801277e+01\n",
      " 1.03678546e+02 8.22546914e+01 1.22473330e+02 8.32317605e+01\n",
      " 1.40160447e+02 8.07427737e+01 1.60061225e+02 6.64269892e+01\n",
      " 4.33155336e+00 2.71316788e+01 2.05822956e+02 7.80338000e+00\n",
      " 4.15638493e+01 0.00000000e+00 6.92730822e+01 3.89548085e+00\n",
      " 2.30836302e+02 1.74306625e+01 2.71819480e+01 1.73448823e+01\n",
      " 2.77638873e+01 1.12889988e+01 6.88798013e+01]\n",
      "19-th iteration, loss: 0.1525631942086513, 30 gd steps\n",
      "insert gradient: -0.0062799144467806794\n",
      "19-th iteration, new layer inserted. now 33 layers\n",
      "[1.79646909e+00 4.73065449e+01 8.50815272e+01 5.69352755e-01\n",
      " 2.84176379e-02 4.59017567e+01 8.38246547e+01 8.40862101e+01\n",
      " 1.04314487e+02 8.15265541e+01 1.23218287e+02 8.25156378e+01\n",
      " 1.38843042e+02 8.11805844e+01 1.59438029e+02 6.57376154e+01\n",
      " 6.03595174e+00 2.54140132e+01 2.04387161e+02 6.81222575e+00\n",
      " 3.72178229e+01 5.32490185e+00 6.51439902e+01 7.43682570e+00\n",
      " 1.93787870e+02 0.00000000e+00 3.22979784e+01 1.88862589e+01\n",
      " 2.63653000e+01 2.01776893e+01 2.64058908e+01 1.10007692e+01\n",
      " 6.88798013e+01]\n",
      "20-th iteration, loss: 0.15154438220620486, 21 gd steps\n",
      "insert gradient: -0.004773649224565022\n",
      "20-th iteration, new layer inserted. now 33 layers\n",
      "[1.63028477e+00 4.70930596e+01 8.54035481e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.54936683e+01 8.39328744e+01 8.29940887e+01\n",
      " 1.05090766e+02 8.09399316e+01 1.24191165e+02 8.20511208e+01\n",
      " 1.38324758e+02 8.13119612e+01 1.59312816e+02 6.58385971e+01\n",
      " 7.38471581e+00 2.43264998e+01 2.02542065e+02 5.71413110e+00\n",
      " 3.44302963e+01 8.15492368e+00 6.23895059e+01 1.01001171e+01\n",
      " 1.90658820e+02 3.33089419e+00 2.90818262e+01 2.15904851e+01\n",
      " 2.34867655e+01 2.23429490e+01 2.61501173e+01 9.98549870e+00\n",
      " 6.88798013e+01]\n",
      "21-th iteration, loss: 0.15065562818840736, 23 gd steps\n",
      "insert gradient: -0.009726323813360834\n",
      "21-th iteration, new layer inserted. now 33 layers\n",
      "[1.72220062e+00 0.00000000e+00 5.55111512e-17 4.68791130e+01\n",
      " 8.39306276e+01 4.59437629e+01 8.30036484e+01 8.26914929e+01\n",
      " 1.05607108e+02 8.00715283e+01 1.24456307e+02 8.16136781e+01\n",
      " 1.36776067e+02 8.17818893e+01 1.59352068e+02 6.65394607e+01\n",
      " 9.46252101e+00 2.25117113e+01 2.00672846e+02 4.31925783e+00\n",
      " 3.27023378e+01 1.06399394e+01 5.96638928e+01 1.27743903e+01\n",
      " 1.87717219e+02 5.08913473e+00 2.54427582e+01 2.54544026e+01\n",
      " 1.91775693e+01 2.57619451e+01 2.56080226e+01 9.26640091e+00\n",
      " 6.88798013e+01]\n",
      "22-th iteration, loss: 0.1505425712640334, 15 gd steps\n",
      "insert gradient: -0.0026430408187109594\n",
      "22-th iteration, new layer inserted. now 35 layers\n",
      "[1.69502475e+00 1.26471155e-01 2.77457585e-02 4.69132613e+01\n",
      " 8.42295410e+01 4.59201481e+01 8.30337096e+01 8.24232934e+01\n",
      " 1.05698108e+02 7.99393930e+01 1.24593591e+02 8.16106339e+01\n",
      " 1.36623639e+02 8.19274047e+01 1.59290314e+02 6.66105216e+01\n",
      " 9.62974671e+00 2.22734261e+01 1.71875680e+02 0.00000000e+00\n",
      " 2.86459466e+01 4.20687192e+00 3.25908741e+01 1.08056336e+01\n",
      " 5.93866140e+01 1.30055284e+01 1.87427269e+02 5.19714208e+00\n",
      " 2.50358674e+01 2.58037668e+01 1.87143783e+01 2.60580436e+01\n",
      " 2.55786937e+01 9.20874188e+00 6.88798013e+01]\n",
      "23-th iteration, loss: 0.14979330901410334, 17 gd steps\n",
      "insert gradient: -0.0027486109553348705\n",
      "23-th iteration, new layer inserted. now 37 layers\n",
      "[1.61916508e+00 4.35979527e-01 1.31064420e-01 4.65441715e+01\n",
      " 8.45487618e+01 4.58445224e+01 8.30385130e+01 8.17633383e+01\n",
      " 1.06496397e+02 7.92933709e+01 1.25315954e+02 8.13488495e+01\n",
      " 1.35632665e+02 8.22731882e+01 1.59019352e+02 6.75128054e+01\n",
      " 1.07417736e+01 2.03290186e+01 1.70614910e+02 2.74320848e+00\n",
      " 2.74611499e+01 2.98831859e+00 3.15555835e+01 0.00000000e+00\n",
      " 3.55271368e-15 1.36852033e+01 5.65263426e+01 1.45210785e+01\n",
      " 1.85021535e+02 6.45429838e+00 2.16372205e+01 2.89819287e+01\n",
      " 1.46521530e+01 2.88664696e+01 2.52557225e+01 9.03625692e+00\n",
      " 6.88798013e+01]\n",
      "24-th iteration, loss: 0.14868493427588345, 30 gd steps\n",
      "insert gradient: -0.02879771125158522\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[  1.73133756  47.00211229  84.05853975  44.594688    82.24961727\n",
      "  81.71418511 106.56015128  78.67432581 125.49707477  80.77725574\n",
      " 134.5748281   82.01905126 158.15300629  68.81373577  11.95749366\n",
      "  17.19060014 167.72494849   7.56108887  25.14131708   1.33196906\n",
      "  29.55193474  19.6315824   50.07782906  17.26657342 181.74154323\n",
      "   8.35921149  16.82470378  33.69410959   8.34442214  32.71986618\n",
      "  24.38737432   9.22779719  68.87980131]\n",
      "25-th iteration, loss: 0.14850387099087364, 11 gd steps\n",
      "insert gradient: -0.016385191186159904\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[1.60526004e+00 0.00000000e+00 1.66533454e-16 4.65145485e+01\n",
      " 8.41476915e+01 4.52464738e+01 8.23497041e+01 8.15665589e+01\n",
      " 1.06592230e+02 7.87230141e+01 1.25636599e+02 8.09593299e+01\n",
      " 1.34601996e+02 8.22405512e+01 1.58010938e+02 6.88100739e+01\n",
      " 1.19402009e+01 1.69367502e+01 1.67576702e+02 7.70618868e+00\n",
      " 2.49881184e+01 1.31286686e+00 2.94031284e+01 1.97756660e+01\n",
      " 4.97514518e+01 1.73677046e+01 1.81555833e+02 8.40528106e+00\n",
      " 1.65555540e+01 3.38599694e+01 8.02539861e+00 3.28380133e+01\n",
      " 2.43418187e+01 9.21675127e+00 6.88798013e+01]\n",
      "26-th iteration, loss: 0.14667847240790433, 36 gd steps\n",
      "insert gradient: -0.016536376086524245\n",
      "26-th iteration, new layer inserted. now 33 layers\n",
      "[1.57931332e+00 4.70594077e+01 8.29774245e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.52725623e+01 8.11066615e+01 8.09893435e+01\n",
      " 1.06514798e+02 7.78948387e+01 1.25865317e+02 8.04162479e+01\n",
      " 1.33782497e+02 8.20664233e+01 1.55491247e+02 7.16691163e+01\n",
      " 1.25263287e+01 1.26281245e+01 1.62786958e+02 1.33494441e+01\n",
      " 1.96895563e+01 2.14418104e+00 2.41421337e+01 2.96443090e+01\n",
      " 3.74683400e+01 2.21496888e+01 1.77243277e+02 1.04207720e+01\n",
      " 9.42787072e+00 7.93033439e+01 2.10673316e+01 9.88343675e+00\n",
      " 6.88798013e+01]\n",
      "27-th iteration, loss: 0.14599860149565957, 38 gd steps\n",
      "insert gradient: -0.002832748871235067\n",
      "27-th iteration, new layer inserted. now 35 layers\n",
      "[1.69238404e+00 4.64828537e+01 8.45197445e+01 2.18744970e-01\n",
      " 6.54868492e-01 4.48016592e+01 8.13511172e+01 8.08431934e+01\n",
      " 1.06602978e+02 7.73405239e+01 1.26271800e+02 8.00413691e+01\n",
      " 1.33396278e+02 8.19836776e+01 1.53968144e+02 7.33071247e+01\n",
      " 1.29871692e+01 1.05157323e+01 1.61216008e+02 1.58796694e+01\n",
      " 1.69438685e+01 2.72596913e+00 2.13491426e+01 0.00000000e+00\n",
      " 2.66453526e-15 3.57442223e+01 3.10522829e+01 2.49702702e+01\n",
      " 1.75726622e+02 9.25874846e+00 7.71566234e+00 7.97935738e+01\n",
      " 2.14867879e+01 9.23041657e+00 6.88798013e+01]\n",
      "28-th iteration, loss: 0.14585197208061787, 16 gd steps\n",
      "insert gradient: -0.002564827509182152\n",
      "28-th iteration, new layer inserted. now 35 layers\n",
      "[1.71306495e+00 4.63902820e+01 8.40836476e+01 1.00919384e-01\n",
      " 9.72346977e-02 4.54076723e+01 8.15153634e+01 8.05075317e+01\n",
      " 1.06569387e+02 7.73108179e+01 1.26380573e+02 8.01123363e+01\n",
      " 1.33430388e+02 8.20648061e+01 1.31795899e+02 0.00000000e+00\n",
      " 2.19659831e+01 7.34569926e+01 1.30002086e+01 1.02798407e+01\n",
      " 1.61024441e+02 1.60563120e+01 1.65579496e+01 2.68357281e+00\n",
      " 2.09605710e+01 3.67793337e+01 3.02517772e+01 2.51932039e+01\n",
      " 1.75547926e+02 9.18221129e+00 7.52074630e+00 7.98660085e+01\n",
      " 2.15219313e+01 9.18523802e+00 6.88798013e+01]\n",
      "29-th iteration, loss: 0.14521609344987071, 35 gd steps\n",
      "insert gradient: -0.02110008202878093\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[  1.69711859  46.30744511  84.21213413  44.30421527  81.62053665\n",
      "  80.27430807 106.92166932  76.89298369 126.71451287  79.83334517\n",
      " 133.39341533  81.75987477 130.63601201   1.27271044  20.91450614\n",
      "  74.39926935  14.41328545   8.59063604 158.5635961   18.42189702\n",
      "  13.39262705   2.26833988  17.5924036   45.78661732  22.06767497\n",
      "  28.85104563 173.51921243   8.54752499   5.37118507  80.94405176\n",
      "  21.78603326   8.65629127  68.87980131]\n",
      "30-th iteration, loss: 0.1450955782826998, 16 gd steps\n",
      "insert gradient: -0.002479189754199677\n",
      "30-th iteration, new layer inserted. now 33 layers\n",
      "[  1.70511975  46.28018883  84.28219075  45.13370233  81.58430913\n",
      "  80.03646527 106.88628818  76.81528313 126.75052019  79.84955166\n",
      " 133.36706707  81.88145318 130.49291201   1.23009487  20.77513012\n",
      "  74.37157194  14.42385722   8.47541003 158.40811362  18.4349408\n",
      "  13.29881721   2.15369948  17.48619382  46.15915546  21.74313938\n",
      "  28.93735358 173.41441791   8.51291751   5.25751085  80.97017162\n",
      "  21.8119962    8.61444732  68.87980131]\n",
      "31-th iteration, loss: 0.14506534831586562, 13 gd steps\n",
      "insert gradient: -0.008576757296825685\n",
      "31-th iteration, new layer inserted. now 35 layers\n",
      "[1.69905291e+00 4.61240160e+01 8.40118601e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.49143931e+01 8.13530865e+01 8.00407662e+01\n",
      " 1.06912715e+02 7.67888098e+01 1.26802021e+02 7.98543221e+01\n",
      " 1.33312579e+02 8.19881200e+01 1.30380587e+02 1.26472493e+00\n",
      " 2.06739040e+01 7.43819607e+01 1.44849930e+01 8.39347634e+00\n",
      " 1.58218019e+02 1.85485262e+01 1.31894679e+01 2.06486013e+00\n",
      " 1.73647284e+01 4.67152119e+01 2.13483371e+01 2.91058457e+01\n",
      " 1.73297862e+02 8.47898193e+00 5.13307780e+00 8.10188176e+01\n",
      " 2.18445313e+01 8.56941615e+00 6.88798013e+01]\n",
      "32-th iteration, loss: 0.14468014313800825, 45 gd steps\n",
      "insert gradient: -0.002007695646468114\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[1.72195860e+00 4.61041388e+01 8.40529234e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.49642020e+01 8.11018980e+01 7.97531636e+01\n",
      " 1.07201170e+02 7.63961374e+01 1.27093815e+02 7.96477678e+01\n",
      " 1.33093509e+02 8.19931207e+01 1.29848179e+02 1.55078685e+00\n",
      " 2.03079211e+01 7.49649573e+01 1.56832980e+01 7.68607641e+00\n",
      " 1.55863558e+02 1.96692430e+01 1.16776754e+01 5.90093482e-01\n",
      " 1.56842212e+01 5.42281514e+01 1.59602894e+01 3.10195567e+01\n",
      " 1.71734038e+02 8.18046592e+00 3.61521368e+00 8.17133208e+01\n",
      " 2.24616597e+01 8.19744053e+00 6.88798013e+01]\n",
      "33-th iteration, loss: 0.144592973972279, 17 gd steps\n",
      "insert gradient: -0.005510756178123922\n",
      "33-th iteration, new layer inserted. now 33 layers\n",
      "[1.74945832e+00 4.63381066e+01 8.42467208e+01 0.00000000e+00\n",
      " 1.77635684e-14 4.45349993e+01 8.10400282e+01 7.96146181e+01\n",
      " 1.07275287e+02 7.61850702e+01 1.27247338e+02 7.94947161e+01\n",
      " 1.32978355e+02 8.19527413e+01 1.29578100e+02 1.77866004e+00\n",
      " 2.01460414e+01 7.52101417e+01 1.65507262e+01 7.51954420e+00\n",
      " 1.54735660e+02 1.99922638e+01 2.66480969e+01 5.72013947e+01\n",
      " 1.38728091e+01 3.16564827e+01 1.71010849e+02 8.11793001e+00\n",
      " 3.00190797e+00 8.19864775e+01 2.29215758e+01 8.12525969e+00\n",
      " 6.88798013e+01]\n",
      "34-th iteration, loss: 0.14456307157909026, 14 gd steps\n",
      "insert gradient: -0.0014982575978689675\n",
      "34-th iteration, new layer inserted. now 33 layers\n",
      "[  1.70600043  46.07947823  84.07708704  44.98953734  81.05108411\n",
      "  79.60071233 107.30705812  76.22125751 127.30113149  79.55173017\n",
      " 132.98355075  82.02408797 129.52395093   1.74932635  20.09880189\n",
      "  75.18072223  16.5694742    7.43754626 154.61240582  19.89708313\n",
      "  26.59681403  57.37701414  13.74148977  31.69548719 146.52381651\n",
      "   0.          24.42063609   8.09904177   2.94591579  81.98938902\n",
      "  22.95041954   8.06256886  68.87980131]\n",
      "35-th iteration, loss: 0.1444934259489228, 24 gd steps\n",
      "insert gradient: -0.005993022600624211\n",
      "35-th iteration, new layer inserted. now 33 layers\n",
      "[  1.54485095  45.64210827  84.26312454  45.10413975  81.01645505\n",
      "  79.42652486 107.48057813  76.08955251 127.43527658  79.51243511\n",
      " 132.89371558  82.09968797 129.32342369   1.88615769  20.0369179\n",
      "  75.32446406  17.4175983    7.30267703 153.1736241   19.59441261\n",
      "  26.22652664  60.09954985  12.04840664  31.78037929 146.07710539\n",
      "   0.95277215  24.00726258   7.66508544   1.54660327  82.15473465\n",
      "  23.23218531   8.01097464  68.87980131]\n",
      "36-th iteration, loss: 0.1443846177635904, 45 gd steps\n",
      "insert gradient: -0.0016560848769590361\n",
      "36-th iteration, new layer inserted. now 33 layers\n",
      "[1.78889755e+00 4.62364919e+01 8.37065429e+01 4.49434288e+01\n",
      " 8.07877750e+01 0.00000000e+00 1.42108547e-14 7.92533606e+01\n",
      " 1.07648255e+02 7.59072895e+01 1.27524745e+02 7.94130997e+01\n",
      " 1.32845246e+02 8.20758332e+01 1.29092313e+02 1.99450807e+00\n",
      " 2.00436700e+01 7.56289684e+01 1.88255968e+01 7.15903044e+00\n",
      " 1.50673980e+02 1.90860106e+01 2.59013694e+01 6.38942251e+01\n",
      " 1.00747008e+01 3.14964875e+01 1.45967163e+02 1.55992528e+00\n",
      " 2.40542762e+01 8.97125484e+01 2.35946432e+01 7.87818301e+00\n",
      " 6.88798013e+01]\n",
      "37-th iteration, loss: 0.14437842811673285, 12 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "37-th iteration, new layer inserted. now 35 layers\n",
      "[1.75282484e+00 4.60959153e+01 8.37260496e+01 4.49982089e+01\n",
      " 8.08319522e+01 2.47800174e-02 2.88288446e-02 7.92810285e+01\n",
      " 1.07662340e+02 7.59098742e+01 1.27533189e+02 7.94074805e+01\n",
      " 1.32839695e+02 8.20802725e+01 1.29078138e+02 2.00855902e+00\n",
      " 2.00343803e+01 7.56438943e+01 1.88678583e+01 7.16961116e+00\n",
      " 2.15179008e+01 0.00000000e+00 1.29107405e+02 1.90907700e+01\n",
      " 2.59079358e+01 6.39505612e+01 1.00487264e+01 3.14807929e+01\n",
      " 1.45939144e+02 1.53779761e+00 2.40277615e+01 8.97077557e+01\n",
      " 2.36085961e+01 7.87295629e+00 6.88798013e+01]\n",
      "38-th iteration, loss: 0.14430797788953037, 35 gd steps\n",
      "insert gradient: -0.002102966014893909\n",
      "38-th iteration, new layer inserted. now 35 layers\n",
      "[1.68213698e+00 4.60361365e+01 8.39081022e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.47390627e+01 8.10901056e+01 7.91634120e+01\n",
      " 1.07863557e+02 7.57813458e+01 1.27722341e+02 7.93471693e+01\n",
      " 1.32896305e+02 8.21256188e+01 1.28738391e+02 2.33507126e+00\n",
      " 1.98676467e+01 7.64027653e+01 1.94722658e+01 5.62700948e+00\n",
      " 1.90610519e+01 1.61971827e+00 1.26769226e+02 1.87499287e+01\n",
      " 2.63999316e+01 6.66291571e+01 8.98084257e+00 3.14123752e+01\n",
      " 1.45610442e+02 1.38350726e+00 2.38386606e+01 8.99848403e+01\n",
      " 2.39685304e+01 7.80384094e+00 6.88798013e+01]\n",
      "39-th iteration, loss: 0.14430400543213348, 15 gd steps\n",
      "insert gradient: -0.00046117683535319167\n",
      "39-th iteration, new layer inserted. now 37 layers\n",
      "[1.71062553e+00 4.60555962e+01 8.38332540e+01 7.02723433e-02\n",
      " 4.80362363e-03 4.48108666e+01 8.10050567e+01 7.91304099e+01\n",
      " 1.07878169e+02 7.57662861e+01 1.27742163e+02 7.93378553e+01\n",
      " 1.32888624e+02 8.21443358e+01 1.28706073e+02 2.34116946e+00\n",
      " 1.98394857e+01 7.64289263e+01 1.94942362e+01 5.53780723e+00\n",
      " 1.89873457e+01 1.70008298e+00 1.26708037e+02 1.87491478e+01\n",
      " 2.64331603e+01 0.00000000e+00 5.32907052e-15 6.67070908e+01\n",
      " 8.93456682e+00 3.14088828e+01 1.45597599e+02 1.37720359e+00\n",
      " 2.38314608e+01 8.99896751e+01 2.39878301e+01 7.79299497e+00\n",
      " 6.88798013e+01]\n",
      "40-th iteration, loss: 0.14428626102555894, 35 gd steps\n",
      "insert gradient: -0.00043928117191085716\n",
      "40-th iteration, new layer inserted. now 35 layers\n",
      "[1.74421518e+00 4.60786570e+01 8.37728694e+01 4.49080452e+01\n",
      " 8.08565380e+01 7.90971672e+01 1.08014059e+02 7.56857474e+01\n",
      " 1.27853076e+02 7.93029081e+01 1.32846280e+02 8.21894049e+01\n",
      " 1.28560231e+02 2.44429202e+00 1.97244649e+01 7.66343977e+01\n",
      " 1.95453024e+01 4.96225694e+00 1.84491191e+01 2.24329067e+00\n",
      " 1.26269549e+02 1.86312710e+01 2.65920710e+01 3.99125190e-01\n",
      " 1.15949891e-01 6.71201140e+01 8.55629935e+00 3.13526148e+01\n",
      " 1.45491807e+02 1.32138066e+00 2.37626882e+01 9.00363163e+01\n",
      " 2.40978723e+01 7.74748493e+00 6.88798013e+01]\n",
      "41-th iteration, loss: 0.1442828589605246, 15 gd steps\n",
      "insert gradient: -0.0006276736078970301\n",
      "41-th iteration, new layer inserted. now 37 layers\n",
      "[1.73952854e+00 4.60554596e+01 8.37449832e+01 0.00000000e+00\n",
      " 3.55271368e-15 4.48745161e+01 8.08402419e+01 7.90925134e+01\n",
      " 1.08033349e+02 7.56805777e+01 1.27873692e+02 7.93075726e+01\n",
      " 1.32841710e+02 8.22028676e+01 1.28537916e+02 2.46137143e+00\n",
      " 1.97077214e+01 7.66697506e+01 1.95435657e+01 4.85144527e+00\n",
      " 1.83488876e+01 2.34625384e+00 1.26190817e+02 1.86205191e+01\n",
      " 2.66271034e+01 4.63988828e-01 1.29740052e-01 6.71912507e+01\n",
      " 8.49673872e+00 3.13543463e+01 1.45476053e+02 1.31306361e+00\n",
      " 2.37539472e+01 9.00474322e+01 2.41154802e+01 7.73761496e+00\n",
      " 6.88798013e+01]\n",
      "42-th iteration, loss: 0.14424556597897584, 17 gd steps\n",
      "insert gradient: -0.0013277951086730696\n",
      "42-th iteration, new layer inserted. now 35 layers\n",
      "[1.75690307e+00 4.61639405e+01 8.37041395e+01 4.47882042e+01\n",
      " 8.07764290e+01 7.90176762e+01 1.08214122e+02 7.55617287e+01\n",
      " 1.27993699e+02 7.92501054e+01 1.32820874e+02 8.22212179e+01\n",
      " 1.28370377e+02 2.61422833e+00 1.95906199e+01 7.73528135e+01\n",
      " 1.93781642e+01 3.22102504e+00 1.68524856e+01 3.85034612e+00\n",
      " 1.25054210e+02 1.84119202e+01 2.70505758e+01 1.31370710e+00\n",
      " 1.21701059e-01 6.81539376e+01 7.73460613e+00 3.12511171e+01\n",
      " 1.45300577e+02 1.16902075e+00 2.36733159e+01 9.03203917e+01\n",
      " 2.43874810e+01 7.64179816e+00 6.88798013e+01]\n",
      "43-th iteration, loss: 0.14423075718489467, 35 gd steps\n",
      "insert gradient: -0.0004745556484586989\n",
      "43-th iteration, new layer inserted. now 35 layers\n",
      "[1.74050785e+00 4.60714579e+01 8.37289088e+01 4.48558488e+01\n",
      " 8.07932098e+01 7.89285069e+01 1.08284000e+02 7.55129411e+01\n",
      " 1.28090960e+02 7.92427751e+01 1.32822156e+02 8.22655668e+01\n",
      " 1.28261058e+02 2.68591756e+00 1.94764950e+01 7.75964605e+01\n",
      " 1.93041481e+01 2.53886217e+00 1.65374736e+01 4.43797352e+00\n",
      " 1.24843306e+02 1.83678535e+01 2.73472341e+01 1.46888271e+00\n",
      " 4.24415795e-02 6.83368541e+01 7.50807897e+00 3.12474241e+01\n",
      " 1.45232282e+02 1.13113788e+00 2.36339078e+01 9.03775718e+01\n",
      " 2.45062978e+01 7.60571363e+00 6.88798013e+01]\n",
      "44-th iteration, loss: 0.14420416562496421, 16 gd steps\n",
      "insert gradient: -0.0028030986556940325\n",
      "44-th iteration, new layer inserted. now 35 layers\n",
      "[1.64886400e+00 0.00000000e+00 3.33066907e-16 4.58230180e+01\n",
      " 8.37530851e+01 4.50228772e+01 8.08862981e+01 7.88432419e+01\n",
      " 1.08412683e+02 7.54288110e+01 1.28191848e+02 7.92067984e+01\n",
      " 1.32831457e+02 8.22854095e+01 1.28160377e+02 2.79415846e+00\n",
      " 1.93052209e+01 7.85105806e+01 1.88869714e+01 5.08772187e-01\n",
      " 1.56178351e+01 6.09926923e+00 1.24261857e+02 1.81278522e+01\n",
      " 2.81067971e+01 7.07839885e+01 6.89324288e+00 3.12978838e+01\n",
      " 1.45094293e+02 9.83120573e-01 2.35740837e+01 9.06409796e+01\n",
      " 2.48120450e+01 7.49023398e+00 6.88798013e+01]\n",
      "45-th iteration, loss: 0.14419292258716185, 14 gd steps\n",
      "insert gradient: -0.0005178578747577066\n",
      "45-th iteration, new layer inserted. now 33 layers\n",
      "[  1.69970616  46.08008952  83.7083579   44.82132391  80.81392795\n",
      "  78.84745707 108.42284597  75.41125691 128.19855113  79.18939293\n",
      " 132.82289903  82.27776073 128.14647905   2.79229334  19.28833644\n",
      "  78.53022853  18.87481148   0.4176981   15.60158999   6.18865927\n",
      " 124.24634457  18.10900829  28.12044525  70.79203125   6.86371587\n",
      "  31.30027235 145.0927963    0.97839008  23.57553561  90.65245479\n",
      "  24.82832252   7.49491173  68.87980131]\n",
      "46-th iteration, loss: 0.14418465357851826, 597 gd steps\n",
      "insert gradient: -0.00016305473678350575\n",
      "46-th iteration, new layer inserted. now 31 layers\n",
      "[  1.76392866  46.05509938  83.66672117  44.83731897  80.7185977\n",
      "  78.82255158 108.51369657  75.35473643 128.28026277  79.16306784\n",
      " 132.81091251  82.29952084 128.07277924   2.85523163  19.18883196\n",
      "  78.72483943  34.17922363   6.6226188  124.1238799   18.02130463\n",
      "  28.31233636  70.89748859   6.70863562  31.37761591 145.06190566\n",
      "   0.92838321  23.56425391  90.72474928  24.94058697   7.46619516\n",
      "  68.87980131]\n",
      "47-th iteration, loss: 0.14417959092837085, 40 gd steps\n",
      "insert gradient: -0.0015371927699072128\n",
      "47-th iteration, new layer inserted. now 33 layers\n",
      "[1.71625195e+00 0.00000000e+00 1.66533454e-16 4.59674549e+01\n",
      " 8.36360809e+01 4.48261688e+01 8.07266740e+01 7.86894032e+01\n",
      " 1.08697345e+02 7.52371201e+01 1.28398210e+02 7.91045794e+01\n",
      " 1.32797708e+02 8.23216971e+01 1.28016061e+02 2.93809775e+00\n",
      " 1.91349270e+01 7.88120634e+01 3.41349233e+01 6.85969858e+00\n",
      " 1.23757434e+02 1.76334184e+01 2.90108266e+01 7.12507692e+01\n",
      " 6.07350741e+00 3.15793183e+01 1.44875836e+02 7.86899746e-01\n",
      " 2.34373230e+01 9.10362927e+01 2.54458488e+01 7.38188854e+00\n",
      " 6.88798013e+01]\n",
      "48-th iteration, loss: 0.14417892594177237, 12 gd steps\n",
      "insert gradient: -0.0005598877459899348\n",
      "48-th iteration, new layer inserted. now 31 layers\n",
      "[  1.72305912  46.01484807  83.6332076   44.80068891  80.72051137\n",
      "  78.68317627 108.69652147  75.23353158 128.39819683  79.10266961\n",
      " 132.79646952  82.32092639 128.01412211   2.93783926  19.13301945\n",
      "  78.81136401  34.13501022   6.86133595 123.75263838  17.63074491\n",
      "  29.01754276  71.25420899   6.06712519  31.5808928  144.87386591\n",
      "   0.78567309  23.43596391  91.03989938  25.45254956   7.38123611\n",
      "  68.87980131]\n",
      "49-th iteration, loss: 0.14417864913588085, 15 gd steps\n",
      "insert gradient: -0.00011959669064419687\n",
      "49-th iteration, new layer inserted. now 31 layers\n",
      "[  1.73243858  46.03800747  83.63401     44.79283782  80.72433933\n",
      "  78.68007235 108.70390164  75.22510551 128.40341474  79.09626252\n",
      " 132.79359426  82.31947805 128.00670255   2.94285707  19.12555662\n",
      "  78.81280747  34.13973952   6.87033958 123.73551642  17.61740704\n",
      "  29.04502948  71.26696843   6.03844977  31.58560047 144.86325495\n",
      "   0.778469    23.42770533  91.05229293  25.48022853   7.37745797\n",
      "  68.87980131]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5192175030792874\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  40.86698553    0.         1994.30889387]\n",
      "1-th iteration, loss: 0.7418874016344715, 11 gd steps\n",
      "insert gradient: -0.5957039068126859\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  40.34032568   62.29260792  243.20840169    0.         1751.10049218]\n",
      "2-th iteration, loss: 0.6113841832379334, 13 gd steps\n",
      "insert gradient: -0.7245327625679476\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   4.09440643   76.38294784  228.27576717   36.8456236   250.15721317\n",
      "    0.         1500.94327901]\n",
      "3-th iteration, loss: 0.449865927092373, 22 gd steps\n",
      "insert gradient: -0.5268401274904079\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   2.89497928   50.05791064  228.4867492    80.05344677   95.33574656\n",
      "   73.78708623  171.53637474    0.         1329.40690426]\n",
      "4-th iteration, loss: 0.38547404232801336, 13 gd steps\n",
      "insert gradient: -0.46672636901191905\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   4.16038141   54.68008035  107.18738788    0.          115.43257156\n",
      "   73.11798243  112.29974811   67.05273511  147.86748145   36.38642248\n",
      " 1329.40690426]\n",
      "5-th iteration, loss: 0.30912807290511496, 13 gd steps\n",
      "insert gradient: -0.2716360313931469\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[   7.4973451    60.37713921   77.60146997   25.96526713   82.97823488\n",
      "   70.28293406  127.78271026   71.6390767   125.74247058   65.7049309\n",
      " 1329.40690426]\n",
      "6-th iteration, loss: 0.2414100277102874, 39 gd steps\n",
      "insert gradient: -0.16632653037958453\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[   1.94300479   47.8390929    75.52644213   41.89316132   94.58234285\n",
      "   79.17557358  118.83014454   82.04816881  128.6736035    82.5629809\n",
      "  181.28275967    0.         1148.12414459]\n",
      "7-th iteration, loss: 0.186033653901114, 40 gd steps\n",
      "insert gradient: -0.04583501885460277\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  3.97651127  46.6327019   83.6181459   44.03590278  88.59089226\n",
      "  82.03094136 109.22963753  84.65551847 125.62761205  81.55436293\n",
      " 156.55481091  84.92929845 191.3540241    0.         956.77012049]\n",
      "8-th iteration, loss: 0.18378780046541954, 16 gd steps\n",
      "insert gradient: -0.0510790101747283\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  2.62518206  45.75301867  85.53983996  44.48309346  86.38457605\n",
      "  83.81813907 107.44732257  84.43840496 126.74685473  82.4023273\n",
      " 154.84364353  88.69680068 189.79302449   7.52071554 717.57759037\n",
      "   0.         239.19253012]\n",
      "9-th iteration, loss: 0.17976878374241845, 22 gd steps\n",
      "insert gradient: -0.05097884232551002\n",
      "9-th iteration, new layer inserted. now 18 layers\n",
      "[  1.41989695  45.36819224  85.98640097  45.84547657  84.6929881\n",
      "  84.33601532 105.88572221  84.35322533 124.25155607  84.54029888\n",
      " 152.71846059  84.37289561 183.17807319   7.72549648 711.70707223\n",
      "  11.6528243  239.19253012   0.        ]\n",
      "10-th iteration, loss: 0.17121333499631936, 53 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "10-th iteration, new layer inserted. now 20 layers\n",
      "[  1.62783826  49.0231199   86.02382613  45.4046373   82.49153944\n",
      "  85.11897162 107.25506285  83.07287952 122.46560291  86.0655902\n",
      " 144.57660671  79.6351127  180.29525852  24.03001982  54.79390574\n",
      "   0.         657.52686883  12.59621646 232.66740733  13.42277719]\n",
      "11-th iteration, loss: 0.16013153607249025, 42 gd steps\n",
      "insert gradient: -0.014804900785241892\n",
      "11-th iteration, new layer inserted. now 20 layers\n",
      "[  1.8699413   49.98561096  85.94400747  45.42880063  83.3176875\n",
      "  85.43717457 105.92319038  83.49216638 122.70749022  85.23176419\n",
      " 142.06397263  79.80095637 172.32343761  41.52335506  28.25493308\n",
      "  20.94125551 643.27628638   8.5625853  229.13535866  10.63078755]\n",
      "12-th iteration, loss: 0.15713589695891017, 52 gd steps\n",
      "insert gradient: -0.015118108545609974\n",
      "12-th iteration, new layer inserted. now 22 layers\n",
      "[  1.98575815  48.74215423  86.43149291  45.50575005  83.49530142\n",
      "  85.51690607 103.20643013  83.39578449 122.49882491  84.23645293\n",
      " 140.6189781   79.7525474  161.83523451  67.2183959    8.24820341\n",
      "  22.22208286 211.0308613    0.         422.06172261   8.08645474\n",
      " 228.32231215   9.09927163]\n",
      "13-th iteration, loss: 0.15644650276161717, 13 gd steps\n",
      "insert gradient: -0.009287772236374647\n",
      "13-th iteration, new layer inserted. now 24 layers\n",
      "[  2.06481546  48.68784001  86.01971128  45.83915398  83.48061001\n",
      "  85.27321564 103.55271844  83.14318003 122.33796881  84.79228169\n",
      " 140.13590094  79.95875086 160.90125202  66.86731965   7.55534669\n",
      "  22.30523286 209.26753055   3.40391743 377.96670813   0.\n",
      "  41.9963009    7.13622568 228.11999536   8.87917476]\n",
      "14-th iteration, loss: 0.15585090782174932, 14 gd steps\n",
      "insert gradient: -0.00751238140631164\n",
      "14-th iteration, new layer inserted. now 26 layers\n",
      "[  2.08878937  48.53982977  86.32181608  45.63740033  83.58985781\n",
      "  85.29500578 103.53287837  83.1929707  121.74252322  84.55875375\n",
      " 140.13697429  80.13488977 160.51952021  67.25413896   6.75032008\n",
      "  22.86828326 208.47469339   5.30117165 334.23426978   0.\n",
      "  41.77928372   3.18132292  40.0798778    6.39849171 226.9875236\n",
      "   7.82746268]\n",
      "15-th iteration, loss: 0.15551388577137654, 13 gd steps\n",
      "insert gradient: -0.005410194241679276\n",
      "15-th iteration, new layer inserted. now 28 layers\n",
      "[  2.13475048  48.5546109   86.05205264  45.58251225  83.55753701\n",
      "  85.18474213 103.43691979  83.18085845 121.44954409  84.40676945\n",
      " 139.9771518   80.07360835 160.09874317  67.38772042   6.19493781\n",
      "  23.04092306 208.04399989   6.02737106 332.97908294   1.58289522\n",
      "  40.53553287   4.44863252  38.79268236   6.54619709 201.02919377\n",
      "   0.          25.12864922   7.2868212 ]\n",
      "16-th iteration, loss: 0.15472188693139197, 27 gd steps\n",
      "insert gradient: -0.0047798844000795085\n",
      "16-th iteration, new layer inserted. now 30 layers\n",
      "[  1.97812432  47.75958942  85.79267337  45.74293837  83.52589927\n",
      "  84.86673234 103.49353678  82.76338683 121.20658633  83.97122352\n",
      " 139.57219269  80.15076802 159.53431308  67.738154     5.67766061\n",
      "  23.23752677 207.59443918   6.65897794 331.10727279   3.62842887\n",
      "  38.825301     7.66092923  36.65133734   7.82880218 175.01187863\n",
      "   0.          25.00169695   4.11790499  24.43078688   5.46493757]\n",
      "17-th iteration, loss: 0.14931810474209245, 42 gd steps\n",
      "insert gradient: -0.015185164916368097\n",
      "17-th iteration, new layer inserted. now 30 layers\n",
      "[  2.20286776  48.88472977  84.89449784  44.52299538  82.47765944\n",
      "  83.65971521 103.63909608  80.81460033 123.25509027  81.9241031\n",
      " 137.95037467  81.0265273  157.75287652  67.0325565    8.35426337\n",
      "  20.70930565 204.1511976    3.17028096 325.73798045  12.66685858\n",
      "  38.05207891  22.47372139  29.37517616   8.74789932 175.31299152\n",
      "  17.45797883  24.84745091  13.07593249  26.8759053   12.93572979]\n",
      "18-th iteration, loss: 0.14882563260753215, 11 gd steps\n",
      "insert gradient: -0.012811254862138159\n",
      "18-th iteration, new layer inserted. now 32 layers\n",
      "[1.57229317e+00 0.00000000e+00 1.66533454e-16 4.65393089e+01\n",
      " 8.48291043e+01 4.62856567e+01 8.28734225e+01 8.35350610e+01\n",
      " 1.03594985e+02 8.05057430e+01 1.23194754e+02 8.17043377e+01\n",
      " 1.37651241e+02 8.10456935e+01 1.57654737e+02 6.69325566e+01\n",
      " 8.62479849e+00 2.04313143e+01 2.03922667e+02 2.99321165e+00\n",
      " 3.25556192e+02 1.31181271e+01 3.80564242e+01 2.27441112e+01\n",
      " 2.90315967e+01 8.75914643e+00 1.75249919e+02 1.75842814e+01\n",
      " 2.46372991e+01 1.34560140e+01 2.66821221e+01 1.29004107e+01]\n",
      "19-th iteration, loss: 0.1480580489285741, 27 gd steps\n",
      "insert gradient: -0.011386872316187135\n",
      "19-th iteration, new layer inserted. now 32 layers\n",
      "[1.75717656e+00 3.79348479e-01 6.23177994e-02 4.70345835e+01\n",
      " 8.46900738e+01 4.53340466e+01 8.22790691e+01 8.28259396e+01\n",
      " 1.04157423e+02 7.99601425e+01 1.24107468e+02 8.16161747e+01\n",
      " 1.36439954e+02 8.21393551e+01 1.57489537e+02 6.70309775e+01\n",
      " 1.04890085e+01 1.87661231e+01 2.01858421e+02 2.26688118e+00\n",
      " 3.23941880e+02 1.53123019e+01 3.67187280e+01 2.47978669e+01\n",
      " 2.61502006e+01 8.25189533e+00 1.74448251e+02 1.94113401e+01\n",
      " 2.31379430e+01 1.66865241e+01 2.52401212e+01 1.33443557e+01]\n",
      "20-th iteration, loss: 0.14786714613324203, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "20-th iteration, new layer inserted. now 34 layers\n",
      "[1.70532127e+00 3.31301596e-01 5.63277503e-02 4.68683914e+01\n",
      " 8.47501840e+01 4.56664775e+01 8.23257917e+01 8.26469526e+01\n",
      " 1.04318701e+02 7.97472846e+01 1.24279846e+02 8.14825336e+01\n",
      " 1.36077132e+02 8.23041737e+01 1.57448636e+02 6.71890856e+01\n",
      " 1.09333171e+01 1.83969717e+01 2.01408652e+02 2.21663919e+00\n",
      " 9.24640075e+01 0.00000000e+00 2.31160019e+02 1.57634997e+01\n",
      " 3.62044991e+01 2.52094222e+01 2.54605076e+01 8.18791388e+00\n",
      " 1.74193437e+02 1.98479094e+01 2.27738715e+01 1.74579703e+01\n",
      " 2.49066269e+01 1.33762759e+01]\n",
      "21-th iteration, loss: 0.14655847671123864, 28 gd steps\n",
      "insert gradient: -0.002674058907986088\n",
      "21-th iteration, new layer inserted. now 36 layers\n",
      "[1.72416248e+00 1.58423743e-01 3.52056828e-02 4.65909732e+01\n",
      " 8.45938707e+01 4.53947135e+01 8.18690267e+01 8.17325996e+01\n",
      " 1.04797003e+02 7.86853150e+01 1.24878110e+02 8.07921931e+01\n",
      " 1.34725119e+02 8.18291941e+01 1.56742071e+02 6.79057153e+01\n",
      " 1.14503014e+01 1.59286813e+01 1.97742073e+02 4.01227228e+00\n",
      " 5.95079912e+01 0.00000000e+00 2.97539956e+01 3.85737808e+00\n",
      " 2.28318565e+02 1.96520518e+01 3.23828083e+01 2.81244565e+01\n",
      " 2.05462579e+01 6.28374614e+00 1.72452866e+02 2.31843178e+01\n",
      " 2.02229571e+01 2.20008029e+01 2.28500992e+01 1.36451805e+01]\n",
      "22-th iteration, loss: 0.14478319607435355, 16 gd steps\n",
      "insert gradient: -0.05468371327792875\n",
      "22-th iteration, new layer inserted. now 38 layers\n",
      "[1.63207760e+00 1.41307931e+00 3.67283732e-01 4.59495933e+01\n",
      " 8.45090052e+01 0.00000000e+00 1.06581410e-14 4.27567458e+01\n",
      " 8.04848034e+01 8.16246217e+01 1.04623492e+02 7.74643434e+01\n",
      " 1.24421405e+02 7.96770381e+01 1.32662849e+02 8.07220404e+01\n",
      " 1.54445848e+02 6.98515413e+01 9.67630270e+00 1.22236441e+01\n",
      " 1.85010061e+02 1.04164186e+01 5.30412177e+01 3.33971906e+00\n",
      " 2.29338128e+01 8.29688047e+00 2.23260396e+02 2.52004243e+01\n",
      " 2.25774065e+01 3.77364363e+01 1.34275414e+01 6.85904957e+00\n",
      " 1.69462126e+02 3.20084922e+01 1.28393629e+01 3.29295448e+01\n",
      " 1.71989142e+01 1.24235585e+01]\n",
      "23-th iteration, loss: 0.14189697270299714, 746 gd steps\n",
      "insert gradient: -0.0038351374743878946\n",
      "23-th iteration, new layer inserted. now 36 layers\n",
      "[  1.62188824  45.89564115  84.07649755  45.32363379  80.4915108\n",
      "  79.65505444 105.30068618  75.47222374 127.07647231  79.11685082\n",
      " 132.27813464  82.49207661 124.84590565   0.          24.96918113\n",
      "  70.23144525   5.34921989   9.57531034 182.52589674  16.13567823\n",
      "  49.60433184   6.41011865  19.88891164   8.93991821 221.56647809\n",
      "  27.25091192  19.4259584   39.77195634  10.51137261   6.41107164\n",
      " 168.39229878  33.83391576   9.145028    34.88974536  17.26550844\n",
      "  11.83103154]\n",
      "24-th iteration, loss: 0.08694637100735413, 568 gd steps\n",
      "insert gradient: -0.003971135619090514\n",
      "24-th iteration, new layer inserted. now 28 layers\n",
      "[  1.94311709  43.27491578  77.61399283  41.19299557  81.37154919\n",
      "  44.22058916 106.12525823  69.93576182  93.95704316  73.97701429\n",
      " 112.68121807  75.15463929 119.159009    79.8350671  142.62851905\n",
      "   0.          17.82856488  53.22522525   8.8317441   16.76380143\n",
      "   1.83610447  13.17756896 172.78568269  85.66360107 151.97854216\n",
      "  82.83536317  20.93452809   5.71664095]\n",
      "25-th iteration, loss: 0.08057258041215651, 256 gd steps\n",
      "insert gradient: -0.00040664982307487543\n",
      "25-th iteration, new layer inserted. now 24 layers\n",
      "[2.58933005e+00 4.49608126e+01 8.05128995e+01 4.16687894e+01\n",
      " 8.29889478e+01 4.62986130e+01 8.64843993e+01 8.14438022e+01\n",
      " 8.24988190e+01 7.45008910e+01 1.16137662e+02 7.26426009e+01\n",
      " 1.28414175e+02 7.62242644e+01 1.34447571e+02 9.04992539e+01\n",
      " 1.80354630e+02 0.00000000e+00 2.13162821e-14 9.53905915e+01\n",
      " 1.56186735e+02 7.92398974e+01 1.59543416e+01 7.35516817e+00]\n",
      "26-th iteration, loss: 0.08053924074063157, 56 gd steps\n",
      "insert gradient: -0.00048216896542953145\n",
      "26-th iteration, new layer inserted. now 22 layers\n",
      "[  2.616561    44.87089047  80.63575404  41.75564674  83.1845111\n",
      "  46.29580107  86.3200443   81.73802686  82.06373035  74.44925202\n",
      " 116.79502167  72.38234954 128.96770227  76.07489417 134.20226385\n",
      "  90.23912707 180.06818799  96.13652858 156.77320247  77.95953322\n",
      "  12.27762708   8.71581256]\n",
      "27-th iteration, loss: 0.08053859375751388, 18 gd steps\n",
      "insert gradient: -0.00014563203355847697\n",
      "27-th iteration, new layer inserted. now 24 layers\n",
      "[2.62702486e+00 4.49076375e+01 8.06536277e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.17735528e+01 8.31817953e+01 4.62636179e+01\n",
      " 8.63178639e+01 8.17551494e+01 8.20736820e+01 7.44606859e+01\n",
      " 1.16790826e+02 7.23896477e+01 1.28961370e+02 7.60796474e+01\n",
      " 1.34203999e+02 9.02298741e+01 1.80054858e+02 9.61453270e+01\n",
      " 1.56799841e+02 7.79328334e+01 1.21850842e+01 8.75430159e+00]\n",
      "28-th iteration, loss: 0.08053563191548094, 30 gd steps\n",
      "insert gradient: -0.00014007105154918579\n",
      "28-th iteration, new layer inserted. now 22 layers\n",
      "[  2.62929036  44.91199054  80.66331088  41.79899269  83.21668036\n",
      "  46.27473265  86.2839894   81.7914008   82.0727707   74.51420594\n",
      " 116.74745836  72.44005975 128.92695699  76.09807125 134.17947268\n",
      "  90.14898027 179.99010084  96.25855836 157.00361622  77.45497239\n",
      "  11.27801254   9.22029949]\n",
      "29-th iteration, loss: 0.0805351761775083, 16 gd steps\n",
      "insert gradient: -0.00013935473062892136\n",
      "29-th iteration, new layer inserted. now 22 layers\n",
      "[  2.62778669  44.93087814  80.6595898   41.80373182  83.23940858\n",
      "  46.27409443  86.27209     81.80482703  82.07935394  74.53847361\n",
      " 116.71212337  72.47400314 128.89881456  76.11794954 134.15673874\n",
      "  90.0996689  179.9718678   96.30526547 157.0731224   77.38893347\n",
      "  11.06447326   9.30755765]\n",
      "30-th iteration, loss: 0.08053489033505189, 19 gd steps\n",
      "insert gradient: -0.0002865296421255418\n",
      "30-th iteration, new layer inserted. now 24 layers\n",
      "[2.61596089e+00 4.48838447e+01 8.06573505e+01 0.00000000e+00\n",
      " 1.42108547e-14 4.18306977e+01 8.32517837e+01 4.62804207e+01\n",
      " 8.62631724e+01 8.18091342e+01 8.20789311e+01 7.45705772e+01\n",
      " 1.16689689e+02 7.24939438e+01 1.28880183e+02 7.61286184e+01\n",
      " 1.34151941e+02 9.00826407e+01 1.79965024e+02 9.63427419e+01\n",
      " 1.57114558e+02 7.73694129e+01 1.09429456e+01 9.36437682e+00]\n",
      "31-th iteration, loss: 0.08053470226738073, 17 gd steps\n",
      "insert gradient: -8.312094845798093e-05\n",
      "31-th iteration, new layer inserted. now 24 layers\n",
      "[2.61198876e+00 4.48740567e+01 8.06589042e+01 9.15596185e-03\n",
      " 4.12614282e-04 4.18493585e+01 8.32665197e+01 4.62888204e+01\n",
      " 8.62623184e+01 8.18244924e+01 8.20800791e+01 7.45865649e+01\n",
      " 1.16660915e+02 7.25107525e+01 1.28861732e+02 7.61358020e+01\n",
      " 1.34142476e+02 9.00581953e+01 1.79953317e+02 9.63657760e+01\n",
      " 1.57150279e+02 7.73374932e+01 1.08275843e+01 9.40666048e+00]\n",
      "32-th iteration, loss: 0.08053421961134066, 45 gd steps\n",
      "insert gradient: -0.0001519805410248732\n",
      "32-th iteration, new layer inserted. now 22 layers\n",
      "[  2.61267125  44.89348963  80.66607074  41.85668551  83.27981749\n",
      "  46.28505575  86.2403584   81.84667329  82.07124299  74.64308445\n",
      " 116.60325489  72.55425799 128.82364164  76.15462942 134.12360629\n",
      "  90.00862726 179.92272992  96.42626097 157.23751101  77.25287895\n",
      "  10.54991177   9.52605421]\n",
      "33-th iteration, loss: 0.0805340590972182, 35 gd steps\n",
      "insert gradient: -2.9459605527662226e-05\n",
      "33-th iteration, new layer inserted. now 24 layers\n",
      "[2.61172567e+00 4.48995522e+01 8.06654936e+01 4.18595787e+01\n",
      " 8.32914579e+01 4.62998463e+01 8.62331270e+01 0.00000000e+00\n",
      " 1.06581410e-14 8.18583767e+01 8.20665944e+01 7.46680396e+01\n",
      " 1.16575318e+02 7.25759503e+01 1.28804868e+02 7.61644570e+01\n",
      " 1.34114322e+02 8.99849482e+01 1.79909607e+02 9.64590282e+01\n",
      " 1.57279524e+02 7.72144303e+01 1.04195883e+01 9.58504863e+00]\n",
      "34-th iteration, loss: 0.08053398387870456, 23 gd steps\n",
      "insert gradient: -2.819310827282693e-05\n",
      "34-th iteration, new layer inserted. now 22 layers\n",
      "[  2.61026308  44.90018209  80.66660817  41.86443363  83.29777234\n",
      "  46.30044096  86.22723158  81.8676329   82.06286964  74.68523066\n",
      " 116.55731913  72.58807394 128.79351915  76.16885397 134.108769\n",
      "  89.96945825 179.8988824   96.47590352 157.30539673  77.17932264\n",
      "  10.32842491   9.62734505]\n",
      "35-th iteration, loss: 0.08053393853180979, 14 gd steps\n",
      "insert gradient: -4.6446906450276745e-05\n",
      "35-th iteration, new layer inserted. now 22 layers\n",
      "[  2.60826529  44.90234448  80.65961365  41.8591688   83.30789074\n",
      "  46.31780582  86.21562561  81.87889411  82.05349704  74.69663475\n",
      " 116.5399031   72.6003563  128.78442952  76.17192087 134.10009111\n",
      "  89.95127198 179.8859614   96.49282646 157.33526436  77.1110407\n",
      "  10.21176537   9.68883264]\n",
      "36-th iteration, loss: 0.08053389273487736, 31 gd steps\n",
      "insert gradient: -2.8946989208726078e-05\n",
      "36-th iteration, new layer inserted. now 24 layers\n",
      "[2.60813281e+00 4.48997340e+01 8.06675374e+01 0.00000000e+00\n",
      " 1.06581410e-14 4.18749863e+01 8.33083296e+01 4.63023004e+01\n",
      " 8.62188280e+01 8.18818694e+01 8.20596292e+01 7.47101108e+01\n",
      " 1.16529762e+02 7.26097351e+01 1.28775164e+02 7.61787797e+01\n",
      " 1.34099416e+02 8.99461109e+01 1.79883928e+02 9.65102468e+01\n",
      " 1.57351575e+02 7.71066454e+01 1.01677173e+01 9.71285231e+00]\n",
      "37-th iteration, loss: 0.0805338894156435, 13 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.660312726606179e-06\n",
      "37-th iteration, new layer inserted. now 22 layers\n",
      "[  2.60818778  44.90033475  80.66757129  41.87588977  83.30905684\n",
      "  46.30292016  86.21905378  81.88378264  82.06032567  74.71190177\n",
      " 116.52811618  72.61060004 128.77370577  76.17884782 134.09872679\n",
      "  89.94482649 179.88340341  96.51175203 157.35361907  77.10587116\n",
      "  10.16224286   9.71518473]\n",
      "38-th iteration, loss: 0.08053388941070212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.661375144460488e-06\n",
      "38-th iteration, new layer inserted. now 22 layers\n",
      "[  2.60818791  44.90033604  80.66757233  41.87589229  83.30905934\n",
      "  46.30292375  86.21905486  81.88378596  82.06032726  74.71190641\n",
      " 116.52811444  72.61060378 128.77370476  76.17885012 134.09872679\n",
      "  89.94482586 179.88340287  96.51175427 157.35362214  77.10587033\n",
      "  10.16223391   9.71518861]\n",
      "39-th iteration, loss: 0.08053388940580708, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.661514605150582e-06\n",
      "39-th iteration, new layer inserted. now 24 layers\n",
      "[2.60818803e+00 4.49003373e+01 8.06675733e+01 4.18758947e+01\n",
      " 8.33090618e+01 4.63029272e+01 8.62190559e+01 8.18837893e+01\n",
      " 8.20603288e+01 0.00000000e+00 1.06581410e-14 7.47119111e+01\n",
      " 1.16528113e+02 7.26106075e+01 1.28773704e+02 7.61788524e+01\n",
      " 1.34098727e+02 8.99448252e+01 1.79883402e+02 9.65117565e+01\n",
      " 1.57353625e+02 7.71058695e+01 1.01622250e+01 9.71519250e+00]\n",
      "40-th iteration, loss: 0.08053388940036185, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.64136668589736e-06\n",
      "40-th iteration, new layer inserted. now 24 layers\n",
      "[2.60818814e+00 4.49003384e+01 8.06675743e+01 4.18758970e+01\n",
      " 8.33090641e+01 4.63029306e+01 8.62190569e+01 8.18837925e+01\n",
      " 8.20603304e+01 4.63105248e-06 1.56639431e-06 7.47119157e+01\n",
      " 1.16528111e+02 7.26106112e+01 1.28773703e+02 7.61788547e+01\n",
      " 1.34098727e+02 8.99448246e+01 1.79883402e+02 9.65117588e+01\n",
      " 1.57353628e+02 7.71058687e+01 1.01622160e+01 9.71519640e+00]\n",
      "41-th iteration, loss: 0.08053388939496454, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.620801314818448e-06\n",
      "41-th iteration, new layer inserted. now 24 layers\n",
      "[2.60818824e+00 4.49003396e+01 8.06675752e+01 4.18758992e+01\n",
      " 8.33090665e+01 4.63029339e+01 8.62190579e+01 8.18837958e+01\n",
      " 8.20603320e+01 9.23983452e-06 3.11149169e-06 7.47119203e+01\n",
      " 1.16528109e+02 7.26106150e+01 1.28773702e+02 7.61788570e+01\n",
      " 1.34098727e+02 8.99448239e+01 1.79883401e+02 9.65117611e+01\n",
      " 1.57353631e+02 7.71058679e+01 1.01622071e+01 9.71520031e+00]\n",
      "42-th iteration, loss: 0.08053388938961281, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.599858603794141e-06\n",
      "42-th iteration, new layer inserted. now 26 layers\n",
      "[2.60818833e+00 4.49003407e+01 8.06675760e+01 4.18759014e+01\n",
      " 8.33090688e+01 4.63029372e+01 8.62190588e+01 8.18837990e+01\n",
      " 8.20603335e+01 1.38259674e-05 4.63544855e-06 0.00000000e+00\n",
      " 6.35274710e-22 7.47119249e+01 1.16528107e+02 7.26106186e+01\n",
      " 1.28773701e+02 7.61788592e+01 1.34098727e+02 8.99448232e+01\n",
      " 1.79883401e+02 9.65117635e+01 1.57353635e+02 7.71058671e+01\n",
      " 1.01621981e+01 9.71520424e+00]\n",
      "43-th iteration, loss: 0.08053388938373596, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.559558014036517e-06\n",
      "43-th iteration, new layer inserted. now 28 layers\n",
      "[2.60818841e+00 4.49003417e+01 8.06675769e+01 4.18759035e+01\n",
      " 8.33090710e+01 4.63029403e+01 8.62190598e+01 8.18838022e+01\n",
      " 8.20603350e+01 1.83769486e-05 6.13422976e-06 4.55756268e-06\n",
      " 1.49878120e-06 0.00000000e+00 3.70576914e-22 7.47119294e+01\n",
      " 1.16528106e+02 7.26106223e+01 1.28773699e+02 7.61788615e+01\n",
      " 1.34098727e+02 8.99448225e+01 1.79883400e+02 9.65117658e+01\n",
      " 1.57353638e+02 7.71058663e+01 1.01621892e+01 9.71520818e+00]\n",
      "44-th iteration, loss: 0.0805338893773672, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.500786122850776e-06\n",
      "44-th iteration, new layer inserted. now 30 layers\n",
      "[2.60818849e+00 4.49003428e+01 8.06675777e+01 4.18759055e+01\n",
      " 8.33090732e+01 4.63029434e+01 8.62190607e+01 8.18838053e+01\n",
      " 8.20603365e+01 2.28718166e-05 7.60190101e-06 9.06114010e-06\n",
      " 2.95998158e-06 4.50570541e-06 1.46120038e-06 0.00000000e+00\n",
      " 1.58818678e-22 7.47119340e+01 1.16528104e+02 7.26106259e+01\n",
      " 1.28773698e+02 7.61788637e+01 1.34098726e+02 8.99448218e+01\n",
      " 1.79883400e+02 9.65117682e+01 1.57353641e+02 7.71058655e+01\n",
      " 1.01621802e+01 9.71521212e+00]\n",
      "45-th iteration, loss: 0.08053388937055307, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.4246473793546645e-06\n",
      "45-th iteration, new layer inserted. now 30 layers\n",
      "[2.60818856e+00 4.49003438e+01 8.06675785e+01 4.18759076e+01\n",
      " 8.33090754e+01 4.63029464e+01 8.62190615e+01 8.18838084e+01\n",
      " 8.20603380e+01 2.72907586e-05 9.03289246e-06 1.34908754e-05\n",
      " 4.37810791e-06 8.93964333e-06 2.87292945e-06 4.43601256e-06\n",
      " 1.41172908e-06 7.47119384e+01 1.16528102e+02 7.26106295e+01\n",
      " 1.28773697e+02 7.61788659e+01 1.34098726e+02 8.99448211e+01\n",
      " 1.79883399e+02 9.65117706e+01 1.57353644e+02 7.71058647e+01\n",
      " 1.01621713e+01 9.71521608e+00]\n",
      "46-th iteration, loss: 0.08053388936385644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.3503637479193405e-06\n",
      "46-th iteration, new layer inserted. now 32 layers\n",
      "[2.60818863e+00 4.49003448e+01 8.06675793e+01 4.18759096e+01\n",
      " 8.33090776e+01 4.63029494e+01 8.62190624e+01 8.18838114e+01\n",
      " 8.20603394e+01 3.16268081e-05 1.04260115e-05 1.78397499e-05\n",
      " 5.75207241e-06 1.32947339e-05 4.23420134e-06 8.79518217e-06\n",
      " 2.76670266e-06 0.00000000e+00 5.29395592e-22 7.47119427e+01\n",
      " 1.16528100e+02 7.26106331e+01 1.28773696e+02 7.61788681e+01\n",
      " 1.34098726e+02 8.99448204e+01 1.79883399e+02 9.65117730e+01\n",
      " 1.57353647e+02 7.71058639e+01 1.01621624e+01 9.71522006e+00]\n",
      "47-th iteration, loss: 0.08053388935678539, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.260288468928382e-06\n",
      "47-th iteration, new layer inserted. now 34 layers\n",
      "[2.60818871e+00 4.49003458e+01 8.06675801e+01 4.18759116e+01\n",
      " 8.33090798e+01 4.63029523e+01 8.62190632e+01 8.18838144e+01\n",
      " 8.20603408e+01 3.58708180e-05 1.17783014e-05 2.20985627e-05\n",
      " 7.07903319e-06 1.75617136e-05 5.54228608e-06 1.30681736e-05\n",
      " 4.06229989e-06 4.27691967e-06 1.29559724e-06 0.00000000e+00\n",
      " 5.29395592e-23 7.47119470e+01 1.16528098e+02 7.26106366e+01\n",
      " 1.28773695e+02 7.61788702e+01 1.34098726e+02 8.99448197e+01\n",
      " 1.79883398e+02 9.65117754e+01 1.57353650e+02 7.71058631e+01\n",
      " 1.01621535e+01 9.71522404e+00]\n",
      "48-th iteration, loss: 0.08053388934940212, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.155849641778169e-06\n",
      "48-th iteration, new layer inserted. now 36 layers\n",
      "[2.60818878e+00 4.49003469e+01 8.06675809e+01 4.18759136e+01\n",
      " 8.33090819e+01 4.63029552e+01 8.62190639e+01 8.18838173e+01\n",
      " 8.20603422e+01 4.00063017e-05 1.30852422e-05 2.62507694e-05\n",
      " 8.35459819e-06 2.17239712e-05 6.79291674e-06 1.72383002e-05\n",
      " 5.29437619e-06 8.45281395e-06 2.52160110e-06 4.17773378e-06\n",
      " 1.22600387e-06 0.00000000e+00 1.05879118e-22 7.47119512e+01\n",
      " 1.16528096e+02 7.26106401e+01 1.28773694e+02 7.61788724e+01\n",
      " 1.34098726e+02 8.99448190e+01 1.79883398e+02 9.65117779e+01\n",
      " 1.57353654e+02 7.71058624e+01 1.01621445e+01 9.71522804e+00]\n",
      "49-th iteration, loss: 0.08053388934177338, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.038775952825634e-06\n",
      "49-th iteration, new layer inserted. now 38 layers\n",
      "[2.60818886e+00 4.49003479e+01 8.06675818e+01 4.18759156e+01\n",
      " 8.33090841e+01 4.63029581e+01 8.62190647e+01 8.18838202e+01\n",
      " 8.20603435e+01 4.40185130e-05 1.43428630e-05 3.02815594e-05\n",
      " 9.57494783e-06 2.57666231e-05 7.98242240e-06 2.12905968e-05\n",
      " 6.45940669e-06 1.25126276e-05 3.67463020e-06 8.24112762e-06\n",
      " 2.37310136e-06 4.06513453e-06 1.14709749e-06 7.47119553e+01\n",
      " 1.16528094e+02 7.26106435e+01 1.28773692e+02 7.61788745e+01\n",
      " 1.34098726e+02 8.99448183e+01 1.79883397e+02 9.65117804e+01\n",
      " 1.57353657e+02 7.71058616e+01 1.01621356e+01 0.00000000e+00\n",
      " 2.22044605e-15 9.71523205e+00]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5240593199350068\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.37151622    0.         2018.92999132]\n",
      "1-th iteration, loss: 0.7430007564978636, 11 gd steps\n",
      "insert gradient: -0.5863915841270686\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  40.70624204   62.3263319   221.5898771     0.         1797.34011423]\n",
      "2-th iteration, loss: 0.5301091342064516, 26 gd steps\n",
      "insert gradient: -0.39866230802114533\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   0.           54.44061896  182.17806978   52.42632491  330.12369445\n",
      "    0.         1467.21641978]\n",
      "3-th iteration, loss: 0.4415280124931446, 56 gd steps\n",
      "insert gradient: -0.48637412787142503\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[1.06169559e+00 5.41076833e+01 1.64133043e+02 5.30717452e+01\n",
      " 2.55033305e+02 5.48158600e+01 1.25761407e+02 0.00000000e+00\n",
      " 1.34145501e+03]\n",
      "4-th iteration, loss: 0.3704961772358009, 28 gd steps\n",
      "insert gradient: -0.3897827607295532\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  3.35021537  55.6732544  175.477885    48.53243011 225.31407355\n",
      "  66.3253214  101.93494586  55.80177779 645.8857467    0.\n",
      " 695.56926567]\n",
      "5-th iteration, loss: 0.26975723286416736, 62 gd steps\n",
      "insert gradient: -0.24379068240874335\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  2.24771797  48.81186228 188.50006155  45.34232667 225.92743616\n",
      "  64.60393803 105.73323867  59.54934273 572.31366147  71.12230456\n",
      " 632.33569607   0.          63.23356961]\n",
      "6-th iteration, loss: 0.23446593326578302, 15 gd steps\n",
      "insert gradient: -0.20699745901991762\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  0.          47.35613256 184.79214052  47.6136529  114.88049532\n",
      "   0.         114.88049532  62.70121273 106.86618172  60.09819742\n",
      " 580.97467828  67.09879745 582.87169323  45.25564834  63.23356961]\n",
      "7-th iteration, loss: 0.20749739847456647, 16 gd steps\n",
      "insert gradient: -0.08851735877455336\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[2.17404780e+00 0.00000000e+00 2.77555756e-16 5.33670485e+01\n",
      " 1.77531645e+02 5.13208765e+01 8.65426615e+01 1.91072322e+01\n",
      " 9.12201642e+01 6.42365058e+01 1.11311686e+02 5.99311845e+01\n",
      " 5.71852640e+02 7.20505715e+01 5.76598801e+02 5.66385357e+01\n",
      " 6.32335696e+01]\n",
      "8-th iteration, loss: 0.20203351391493712, 364 gd steps\n",
      "insert gradient: -0.0710653796260784\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  2.29228436  55.88818865 171.8495415   54.33075352  81.48955082\n",
      "  19.47360398  89.02785206  61.41083143 114.12976009  58.60833571\n",
      " 563.80867437  69.92117447 170.51696667   0.         375.13732667\n",
      "  78.35447202  63.23356961]\n",
      "9-th iteration, loss: 0.19265205342335628, 27 gd steps\n",
      "insert gradient: -0.07757407893226842\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.34379062  61.06850122 169.94079819  55.77942571  86.89195783\n",
      "  20.79166683  82.78683378  60.73779992 117.53942617  59.9923538\n",
      " 122.35000574   0.         448.61668773  60.49991308 156.10665942\n",
      "  19.65957917 358.57250068  84.96036166  63.23356961]\n",
      "10-th iteration, loss: 0.18534344176422904, 26 gd steps\n",
      "insert gradient: -0.09079085361084253\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.0740567   60.16345957 171.6809041   60.64865608  89.80093629\n",
      "  21.71256859  78.65282561  59.55697626 118.72622888  63.05169597\n",
      " 112.65296537  12.20980744 221.10931891   0.         221.10931891\n",
      "  56.80981377 155.72950921  28.52097931 340.91532106  90.14747383\n",
      "  63.23356961]\n",
      "11-th iteration, loss: 0.17320423479704827, 45 gd steps\n",
      "insert gradient: -0.04861062374891402\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  1.39222701  61.78164675 177.00554448  61.04334243  96.16023182\n",
      "  20.33122191  76.63350857  60.32715327 115.92668683  67.95694009\n",
      " 104.64925925  26.87296304 194.11140105  14.51625585 133.48515447\n",
      "   0.          76.27723112  50.03857115 154.19943996  42.70003657\n",
      " 317.57658626  99.55379828  63.23356961]\n",
      "12-th iteration, loss: 0.14311760814130303, 40 gd steps\n",
      "insert gradient: -0.05747552291331693\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  1.1589121   64.13734139 190.86103645  58.87718292 108.30798154\n",
      "  31.48986045  50.26921266  59.04201428 115.14924652  75.55113211\n",
      " 100.75373119  48.68888739  68.15858774   0.         102.23788161\n",
      "  22.63814243  91.11500394  44.59401615   4.37721447  53.18135509\n",
      " 171.36885375  59.24149783 296.70383207  79.99307085  63.23356961]\n",
      "13-th iteration, loss: 0.13735074667826255, 64 gd steps\n",
      "insert gradient: -0.01599159864754609\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  0.98167988  62.82644409 194.22240112  59.88101173 103.47935123\n",
      "  41.24590006  39.97820534  59.51658539 112.52591279  75.89139996\n",
      " 105.58028595  57.10674523  56.79882583  12.17543871  63.64047937\n",
      "  35.34066999  75.61400123 103.65372986 122.01937687   0.\n",
      "  52.29401866  62.38008589 299.49851737  81.19174611  63.23356961]\n",
      "14-th iteration, loss: 0.1350196526904351, 22 gd steps\n",
      "insert gradient: -0.01328765923344995\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  1.3382917   61.19037509 196.5106885   60.35623587 103.55181227\n",
      "  43.09219528  34.4866466   60.40929937 114.00336024  75.90243001\n",
      " 106.21305907  56.41575681  65.7965099   12.61452095  54.41350844\n",
      "  35.43845104  83.61815536  99.11386672 110.45595273  12.7709269\n",
      "  39.5371182   61.0451015  199.93340839   0.          99.96670419\n",
      "  80.00530466  63.23356961]\n",
      "15-th iteration, loss: 0.13356771483408958, 25 gd steps\n",
      "insert gradient: -0.013732096552416806\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[  1.2690017   62.01399464 197.12088815  59.94617341 103.30694768\n",
      "  43.69640826  34.37103076  59.45265    115.0426177   75.07906296\n",
      " 107.09603439  57.70620215  67.38286568  14.44540248  47.64046677\n",
      "  36.7786599   82.35735362  99.41843719 109.12881286  16.7056927\n",
      "  34.17544047  59.29983361  72.94361304   0.         121.5726884\n",
      "   6.84646707  87.99433795  81.61708032  63.23356961]\n",
      "16-th iteration, loss: 0.13081545802309932, 21 gd steps\n",
      "insert gradient: -0.025148773987471992\n",
      "16-th iteration, new layer inserted. now 31 layers\n",
      "[  1.9335686   59.81202635 101.6253938    0.         101.6253938\n",
      "  58.93753511 103.3746983   44.43610984  32.5536736   57.82297174\n",
      " 119.2749667   75.72824991 105.2213106   59.32976272  66.97211766\n",
      "  12.77862701  45.02878339  42.21462847  78.88412111  92.0678918\n",
      " 115.13851395  26.65247799  17.06545137  68.88534667  62.81396255\n",
      "   7.66156363 111.2589505   14.31130744  61.42661496  88.85879741\n",
      "  63.23356961]\n",
      "17-th iteration, loss: 0.11778827042591596, 98 gd steps\n",
      "insert gradient: -0.016798919136018192\n",
      "17-th iteration, new layer inserted. now 29 layers\n",
      "[  1.95847288  60.14081017 101.47983548  15.06685283  88.00531727\n",
      "  58.63386482 105.28704602  48.33037518  16.72162909  64.31229647\n",
      " 129.75994288  76.52675219 109.48572823  73.84623642 131.45845849\n",
      "  40.0223238   77.48846989  90.26239163 109.88735759 109.70115381\n",
      "  53.8393683   20.79398545 105.15560935  29.72958638  25.98525329\n",
      "  71.25946474   0.          42.75567884  63.23356961]\n",
      "18-th iteration, loss: 0.10963965540535439, 20 gd steps\n",
      "insert gradient: -0.0266871405764874\n",
      "18-th iteration, new layer inserted. now 29 layers\n",
      "[3.79794046e+00 6.41814764e+01 8.88981979e+01 2.88782613e+01\n",
      " 8.02161154e+01 6.14474960e+01 1.04724101e+02 0.00000000e+00\n",
      " 1.77635684e-14 1.14011770e+02 1.33588736e+02 8.00383458e+01\n",
      " 1.15830790e+02 8.03366643e+01 1.16773868e+02 4.89784368e+01\n",
      " 7.13226804e+01 9.19501897e+01 9.75597916e+01 1.01789095e+02\n",
      " 7.77177393e+01 2.64701455e+01 9.75995207e+01 2.26238557e+01\n",
      " 1.62561073e+01 7.39054629e+01 3.86107446e+01 2.63434877e+01\n",
      " 6.32335696e+01]\n",
      "19-th iteration, loss: 0.10078942321260459, 62 gd steps\n",
      "insert gradient: -0.0057781810724241295\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[1.09264825e+00 5.96503940e+01 8.97915322e+01 2.62724486e+01\n",
      " 8.31989734e+01 5.91458460e+01 1.05664468e+02 1.20151183e+02\n",
      " 1.34720115e+02 7.89701908e+01 1.13064266e+02 7.94163226e+01\n",
      " 1.22429627e+02 4.87498433e+01 6.17206943e+01 9.60550256e+01\n",
      " 9.75324922e+01 9.79849411e+01 9.39547317e+01 2.56820908e+01\n",
      " 8.99811194e+01 2.01202213e+01 2.04692097e+01 7.10685659e+01\n",
      " 6.26063931e+01 1.08667087e+01 6.32335696e+01 0.00000000e+00\n",
      " 5.32907052e-15]\n",
      "20-th iteration, loss: 0.08985886558693956, 42 gd steps\n",
      "insert gradient: -0.009468748799641139\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[1.05066579e+00 5.95597419e+01 9.08776015e+01 2.82465520e+01\n",
      " 8.06937504e+01 5.83484857e+01 1.03668834e+02 1.18263965e+02\n",
      " 1.42186002e+02 7.57241784e+01 1.17747125e+02 7.57049762e+01\n",
      " 1.24276715e+02 6.13874759e+01 4.20434860e+01 5.02298684e+01\n",
      " 0.00000000e+00 5.02298684e+01 9.02345857e+01 8.65492856e+01\n",
      " 1.25065070e+02 4.18879644e+01 5.18820895e+01 3.02582462e+01\n",
      " 3.58071685e+01 4.44627009e+01 8.80131546e+01 2.10530740e+01\n",
      " 4.52885483e+01 3.23684314e+01 1.26482659e-13]\n",
      "21-th iteration, loss: 0.08671406609606544, 35 gd steps\n",
      "insert gradient: -0.0025259078340355992\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[6.99081093e-01 5.94319091e+01 9.04555389e+01 3.01663921e+01\n",
      " 7.89223616e+01 5.82742428e+01 1.05348362e+02 1.15481544e+02\n",
      " 1.45277721e+02 7.81212227e+01 1.12716791e+02 7.68442523e+01\n",
      " 1.23130380e+02 6.58960141e+01 4.69050220e+01 3.87534525e+01\n",
      " 1.53821452e+01 4.55694052e+01 8.76996797e+01 8.66251084e+01\n",
      " 1.20648317e+02 5.28844343e+01 3.88037846e+01 2.90883387e+01\n",
      " 4.89535610e+01 3.70681648e+01 8.62116844e+01 2.75604817e+01\n",
      " 2.60732041e+01 4.61732810e+01 1.96526968e-13]\n",
      "22-th iteration, loss: 0.08410205164211637, 48 gd steps\n",
      "insert gradient: -0.003048800994003213\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[9.69230185e-01 5.89674734e+01 9.06042996e+01 3.20639713e+01\n",
      " 7.81884639e+01 5.82147447e+01 1.05839847e+02 1.13638367e+02\n",
      " 1.52379206e+02 7.63941298e+01 1.15425751e+02 7.64687212e+01\n",
      " 1.25300718e+02 6.18853779e+01 6.15249344e+01 2.08939583e+01\n",
      " 3.89666965e+01 4.09958467e+01 8.81383171e+01 9.08025147e+01\n",
      " 1.09654475e+02 6.13882774e+01 3.10483382e+01 2.30385415e+01\n",
      " 7.03163935e+01 3.02264044e+01 7.99544832e+01 2.87980043e+01\n",
      " 1.77439644e+01 5.93792897e+01 2.27396770e-13]\n",
      "23-th iteration, loss: 0.08205848666662645, 31 gd steps\n",
      "insert gradient: -0.005701311683295442\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[8.23941236e-01 5.89627567e+01 8.99561253e+01 3.42442406e+01\n",
      " 7.81240713e+01 5.83395536e+01 1.05748813e+02 1.13137628e+02\n",
      " 1.58676717e+02 7.39860170e+01 1.17217634e+02 0.00000000e+00\n",
      " 1.42108547e-14 7.83553301e+01 1.20827925e+02 6.29280109e+01\n",
      " 7.41344850e+01 7.93405443e+00 6.33483750e+01 3.44353641e+01\n",
      " 8.45005369e+01 9.47832177e+01 1.03114531e+02 6.43334341e+01\n",
      " 2.24924421e+01 1.96182436e+01 9.58680131e+01 2.55324579e+01\n",
      " 7.14068634e+01 3.15216202e+01 1.66079002e+01 6.14529537e+01\n",
      " 3.92000570e-13]\n",
      "24-th iteration, loss: 0.07878905614548169, 108 gd steps\n",
      "insert gradient: -0.0016002820567885437\n",
      "24-th iteration, new layer inserted. now 31 layers\n",
      "[7.12305277e-01 5.85408383e+01 8.87966961e+01 3.74174104e+01\n",
      " 8.23473341e+01 5.86532927e+01 1.05280868e+02 1.17757392e+02\n",
      " 1.66608388e+02 7.44257668e+01 1.18399939e+02 8.10343843e+01\n",
      " 1.20116158e+02 6.91258255e+01 1.63027267e+02 3.70716836e+01\n",
      " 6.94236823e+01 5.09733051e+01 0.00000000e+00 5.09733051e+01\n",
      " 9.79772093e+01 6.47561370e+01 1.04346672e+01 2.58032198e+01\n",
      " 1.07425768e+02 3.09581273e+01 5.36022270e+01 3.65906832e+01\n",
      " 2.86942437e+01 5.30895643e+01 4.15981968e-13]\n",
      "25-th iteration, loss: 0.07676684427691072, 24 gd steps\n",
      "insert gradient: -0.003764562871638469\n",
      "25-th iteration, new layer inserted. now 33 layers\n",
      "[5.47461260e-01 5.85977534e+01 9.21230023e+01 3.85427684e+01\n",
      " 8.01579979e+01 5.77380236e+01 1.07513254e+02 0.00000000e+00\n",
      " 3.55271368e-15 1.20115245e+02 1.68085900e+02 7.20555820e+01\n",
      " 1.20608950e+02 8.11181977e+01 1.19590698e+02 7.06262563e+01\n",
      " 1.47433847e+02 5.97448107e+01 4.80216496e+01 4.80391149e+01\n",
      " 1.20839400e+01 4.58234027e+01 9.21534500e+01 6.53743686e+01\n",
      " 5.69687137e+00 2.73322004e+01 1.12124984e+02 3.97702846e+01\n",
      " 3.05051200e+01 4.66648275e+01 3.70225359e+01 3.77675032e+01\n",
      " 4.49439989e-13]\n",
      "26-th iteration, loss: 0.07563834595949212, 25 gd steps\n",
      "insert gradient: -0.006295590927649779\n",
      "26-th iteration, new layer inserted. now 31 layers\n",
      "[7.38403507e-01 5.93853560e+01 8.83292824e+01 3.94142073e+01\n",
      " 8.48149146e+01 5.89268706e+01 1.04519171e+02 1.22428160e+02\n",
      " 1.70228531e+02 7.22402417e+01 1.20427873e+02 8.16196328e+01\n",
      " 1.19974141e+02 7.25962506e+01 1.46307077e+02 6.05967999e+01\n",
      " 5.03055125e+01 4.47493327e+01 1.70683699e+01 4.50661665e+01\n",
      " 9.14860386e+01 6.60031143e+01 4.93358550e+00 2.89667850e+01\n",
      " 1.10806786e+02 4.33239162e+01 2.57266706e+01 4.73972383e+01\n",
      " 4.18713048e+01 3.63914791e+01 4.42119381e-13]\n",
      "27-th iteration, loss: 0.07473774669841085, 22 gd steps\n",
      "insert gradient: -0.007927545708157811\n",
      "27-th iteration, new layer inserted. now 33 layers\n",
      "[3.69891824e-01 5.94322451e+01 8.84366760e+01 0.00000000e+00\n",
      " 1.77635684e-14 3.91462441e+01 8.50752521e+01 5.84732152e+01\n",
      " 1.05561202e+02 1.23847121e+02 1.71365471e+02 7.18307063e+01\n",
      " 1.21434014e+02 8.24856889e+01 1.20365002e+02 7.31834099e+01\n",
      " 1.45082332e+02 6.43454881e+01 5.26112805e+01 3.97206090e+01\n",
      " 2.35326218e+01 4.33960591e+01 8.90883437e+01 6.56674673e+01\n",
      " 5.38760239e+00 2.95732705e+01 1.09760501e+02 4.76725704e+01\n",
      " 2.11759760e+01 4.73393991e+01 4.68273599e+01 3.30666962e+01\n",
      " 4.28128014e-13]\n",
      "28-th iteration, loss: 0.07359728864801242, 30 gd steps\n",
      "insert gradient: -0.005876179308686824\n",
      "28-th iteration, new layer inserted. now 33 layers\n",
      "[6.71956706e-01 5.99391996e+01 8.81073320e+01 4.06196819e+01\n",
      " 8.63837683e+01 5.88517229e+01 1.04922140e+02 7.13877981e+01\n",
      " 0.00000000e+00 5.35408485e+01 1.72382091e+02 7.16716729e+01\n",
      " 1.21807291e+02 8.27586578e+01 1.20911695e+02 7.45142660e+01\n",
      " 1.44175709e+02 6.67448942e+01 5.78926819e+01 3.33997799e+01\n",
      " 3.13501148e+01 4.20915098e+01 8.58635795e+01 6.48668484e+01\n",
      " 6.71532727e+00 3.03141103e+01 1.08357110e+02 5.35222869e+01\n",
      " 1.71921071e+01 4.60709317e+01 5.38453728e+01 3.02832180e+01\n",
      " 4.28241249e-13]\n",
      "29-th iteration, loss: 0.06747713748984108, 71 gd steps\n",
      "insert gradient: -0.004890521479260791\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[7.33693986e-01 6.09937630e+01 8.99035039e+01 4.18132361e+01\n",
      " 8.77288267e+01 5.86181320e+01 1.06493261e+02 6.79797050e+01\n",
      " 2.70894554e+01 3.57610758e+01 1.82460080e+02 7.38406503e+01\n",
      " 1.21277521e+02 8.53579128e+01 1.22124608e+02 7.65266977e+01\n",
      " 1.45202505e+02 6.63012578e+01 9.42158262e+01 1.11138068e+01\n",
      " 5.83020924e+01 4.37707622e+01 6.75353475e+01 5.44452780e+01\n",
      " 2.15324220e+01 3.35154506e+01 9.80604080e+01 6.69018642e+01\n",
      " 6.16777077e+00 3.71885220e+01 9.70524018e+01 1.73222132e+01\n",
      " 5.59572432e-13]\n",
      "30-th iteration, loss: 0.06606416202920787, 29 gd steps\n",
      "insert gradient: -0.005940284100000486\n",
      "30-th iteration, new layer inserted. now 33 layers\n",
      "[7.47410148e-01 6.08440340e+01 9.02336086e+01 4.27107196e+01\n",
      " 8.92616911e+01 5.89629286e+01 1.05846461e+02 6.81260399e+01\n",
      " 3.02508966e+01 3.50856509e+01 1.83118829e+02 7.44730792e+01\n",
      " 1.21278547e+02 8.61211612e+01 1.22796624e+02 7.65351985e+01\n",
      " 1.47362146e+02 6.72461780e+01 1.00014985e+02 8.21391483e+00\n",
      " 5.93948304e+01 4.88222568e+01 6.45072187e+01 5.06857376e+01\n",
      " 2.77215846e+01 3.22136968e+01 9.58485193e+01 6.67529039e+01\n",
      " 7.00627854e+00 3.70063221e+01 1.03661387e+02 1.78857101e+01\n",
      " 5.43952604e-13]\n",
      "31-th iteration, loss: 0.06450752652502567, 36 gd steps\n",
      "insert gradient: -0.005573190151999527\n",
      "31-th iteration, new layer inserted. now 33 layers\n",
      "[5.90476075e-01 6.02919157e+01 8.95521520e+01 4.35360784e+01\n",
      " 9.04445391e+01 5.90184929e+01 1.06264850e+02 6.82053102e+01\n",
      " 3.40548838e+01 3.60615644e+01 1.83849166e+02 7.56856500e+01\n",
      " 1.21600813e+02 8.72677121e+01 1.23338436e+02 7.65180871e+01\n",
      " 1.49357618e+02 6.80005963e+01 1.04520374e+02 5.58590528e+00\n",
      " 5.91146437e+01 5.47621916e+01 6.35024540e+01 4.71988376e+01\n",
      " 3.40328393e+01 3.12003079e+01 9.30601554e+01 6.57180000e+01\n",
      " 9.92571146e+00 3.56257291e+01 1.07884753e+02 2.00817429e+01\n",
      " 5.07368952e-13]\n",
      "32-th iteration, loss: 0.063049467312765, 29 gd steps\n",
      "insert gradient: -0.0038625495295505395\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[4.72913926e-01 6.02725296e+01 8.91328793e+01 4.47000707e+01\n",
      " 9.19685701e+01 5.89154697e+01 1.06665002e+02 6.68635613e+01\n",
      " 3.75945723e+01 3.58210482e+01 1.84048058e+02 7.60838000e+01\n",
      " 1.21383593e+02 8.75845391e+01 1.23552591e+02 7.63331276e+01\n",
      " 1.51503450e+02 6.73347603e+01 1.09277072e+02 0.00000000e+00\n",
      " 1.77635684e-14 3.77059069e+00 5.63319056e+01 6.17633469e+01\n",
      " 6.47719343e+01 4.12566451e+01 4.28505364e+01 3.01903646e+01\n",
      " 8.82448527e+01 6.27599521e+01 1.55908944e+01 3.31414298e+01\n",
      " 1.12411048e+02 2.20723369e+01 4.34702328e-13]\n",
      "33-th iteration, loss: 0.06150869943912883, 27 gd steps\n",
      "insert gradient: -0.004044926866957613\n",
      "33-th iteration, new layer inserted. now 35 layers\n",
      "[9.15355428e-01 5.98509348e+01 8.81717881e+01 4.60145423e+01\n",
      " 9.37381840e+01 5.91808963e+01 1.08307008e+02 6.50513301e+01\n",
      " 4.04996707e+01 3.75596345e+01 1.84242990e+02 7.45940876e+01\n",
      " 1.22430834e+02 8.82044984e+01 1.21944400e+02 7.81664589e+01\n",
      " 1.51526267e+02 6.62959542e+01 1.14150533e+02 1.70746375e-01\n",
      " 4.73700951e+00 2.51032174e+00 5.22766897e+01 6.06328895e+01\n",
      " 7.91611051e+01 3.08073641e+01 5.35152364e+01 3.20139321e+01\n",
      " 8.17137564e+01 5.57723222e+01 2.39088880e+01 3.09747536e+01\n",
      " 1.18956459e+02 2.32685442e+01 3.28083880e-13]\n",
      "34-th iteration, loss: 0.06003039667759162, 23 gd steps\n",
      "insert gradient: -0.0034957268813363912\n",
      "34-th iteration, new layer inserted. now 37 layers\n",
      "[9.39550722e-01 6.04458244e+01 8.84473210e+01 4.68698707e+01\n",
      " 9.48135832e+01 5.89719878e+01 1.07589651e+02 6.71183095e+01\n",
      " 4.09149889e+01 3.98710504e+01 1.85112151e+02 7.30789251e+01\n",
      " 1.23925130e+02 0.00000000e+00 1.42108547e-14 8.92560626e+01\n",
      " 1.21517702e+02 8.00361065e+01 1.51602567e+02 6.70910353e+01\n",
      " 1.16325427e+02 4.60574991e-01 6.88302749e+00 2.92508230e-01\n",
      " 5.41072506e+01 5.73593256e+01 8.74543066e+01 2.39073438e+01\n",
      " 5.80047214e+01 3.73336963e+01 7.73877493e+01 5.07051077e+01\n",
      " 2.95419504e+01 3.19255121e+01 1.18285424e+02 2.00797240e+01\n",
      " 2.56103802e-13]\n",
      "35-th iteration, loss: 0.05721951385695559, 172 gd steps\n",
      "insert gradient: -0.001797130020830904\n",
      "35-th iteration, new layer inserted. now 33 layers\n",
      "[5.86611826e-01 5.79758862e+01 8.82303498e+01 4.91620694e+01\n",
      " 9.69276777e+01 5.96663922e+01 1.07574916e+02 6.66142040e+01\n",
      " 4.48365458e+01 4.41226603e+01 1.85088925e+02 7.04040663e+01\n",
      " 1.23475577e+02 9.25337877e+01 1.20288804e+02 8.12591253e+01\n",
      " 1.51367430e+02 6.63853686e+01 1.83509917e+02 5.67173944e+01\n",
      " 9.00624742e+01 1.76790451e+01 6.35608034e+01 4.42124137e+01\n",
      " 7.23192762e+01 4.44037255e+01 0.00000000e+00 1.77635684e-15\n",
      " 3.64791735e+01 3.29053069e+01 1.14572665e+02 1.79473822e+01\n",
      " 2.19630899e-13]\n",
      "36-th iteration, loss: 0.04694661748468744, 37 gd steps\n",
      "insert gradient: -0.013085076940822456\n",
      "36-th iteration, new layer inserted. now 33 layers\n",
      "[8.95441114e-01 5.42027164e+01 8.74548972e+01 5.09947909e+01\n",
      " 1.00064933e+02 5.95100330e+01 1.05535801e+02 6.10616809e+01\n",
      " 6.26878365e+01 5.60456822e+01 1.79099002e+02 6.38338290e+01\n",
      " 1.23581806e+02 9.62097107e+01 1.16661236e+02 8.60723137e+01\n",
      " 1.03748624e+02 0.00000000e+00 4.14994494e+01 6.75523356e+01\n",
      " 1.84119739e+02 5.46375163e+01 9.84365294e+01 6.58142348e+00\n",
      " 8.12248055e+01 5.49361849e+01 5.47970102e+01 2.82092796e+01\n",
      " 8.70595698e+01 2.20965732e+01 9.28600779e+01 2.64100771e+01\n",
      " 1.28069686e-13]\n",
      "37-th iteration, loss: 0.04691985498042906, 32 gd steps\n",
      "insert gradient: -0.0012219895217719298\n",
      "37-th iteration, new layer inserted. now 31 layers\n",
      "[8.96480344e-01 5.42034513e+01 8.74574957e+01 5.10015847e+01\n",
      " 1.00069150e+02 5.95101061e+01 1.05531048e+02 6.10533277e+01\n",
      " 6.26995838e+01 5.60480050e+01 1.79100274e+02 6.38482251e+01\n",
      " 1.23592286e+02 9.62190072e+01 1.16672503e+02 8.60943282e+01\n",
      " 1.45271136e+02 6.75638034e+01 1.84126360e+02 5.46367803e+01\n",
      " 9.84348606e+01 6.58029608e+00 8.12301948e+01 5.49254455e+01\n",
      " 5.47847856e+01 2.82021981e+01 8.70642451e+01 2.20933696e+01\n",
      " 9.28437613e+01 2.64208837e+01 1.28043402e-13]\n",
      "38-th iteration, loss: 0.03670866564344324, 119 gd steps\n",
      "insert gradient: -6.053419392443561e-05\n",
      "38-th iteration, new layer inserted. now 32 layers\n",
      "[1.00974940e+00 5.84407034e+01 9.57030109e+01 5.22517617e+01\n",
      " 9.19806286e+01 5.37356600e+01 9.59038328e+01 5.33718185e+01\n",
      " 9.36428060e+01 6.05424672e+01 1.52947569e+02 7.49605643e+01\n",
      " 1.17342252e+02 9.22117311e+01 0.00000000e+00 3.55271368e-15\n",
      " 1.21820481e+02 7.62807594e+01 1.65599984e+02 6.03180084e+01\n",
      " 1.95753793e+02 5.21084105e+01 4.63034368e+01 3.32517613e+01\n",
      " 9.40381692e+01 3.44436130e+01 3.10099472e+01 5.66453126e+01\n",
      " 1.14096661e+02 2.99731124e+00 6.37276381e+01 6.63566571e+01]\n",
      "39-th iteration, loss: 0.03669298040463496, 59 gd steps\n",
      "insert gradient: -2.4543355542599013e-05\n",
      "39-th iteration, new layer inserted. now 32 layers\n",
      "[9.88644871e-01 5.85982097e+01 9.58650359e+01 0.00000000e+00\n",
      " 2.13162821e-14 5.21686782e+01 9.18327342e+01 5.37898738e+01\n",
      " 9.57861185e+01 5.32104674e+01 9.35622623e+01 6.06408830e+01\n",
      " 1.52781605e+02 7.49679875e+01 1.17053311e+02 9.22949320e+01\n",
      " 1.21956191e+02 7.61621768e+01 1.65299322e+02 6.03668351e+01\n",
      " 1.95411607e+02 5.24276899e+01 4.47410043e+01 3.37914861e+01\n",
      " 9.43985305e+01 3.36125925e+01 3.20667226e+01 5.65054270e+01\n",
      " 1.14667662e+02 1.72000392e+00 6.77501209e+01 6.48498641e+01]\n",
      "40-th iteration, loss: 0.0366917353865001, 25 gd steps\n",
      "insert gradient: -5.3754008415077155e-05\n",
      "40-th iteration, new layer inserted. now 30 layers\n",
      "[  0.97880254  58.59602253  95.92604672  52.19883968  91.80000954\n",
      "  53.74554669  95.78059937  53.20773148  93.53083139  60.54588185\n",
      " 152.71366503  74.91807072 117.0326712   92.30477899 121.98754364\n",
      "  76.1042532  165.23236921  60.43092231 195.36866633  52.51906786\n",
      "  44.39380787  33.94980752  94.36401336  33.40128915  32.27837405\n",
      "  56.49826775 114.43201128   1.55137663  68.54346139  64.6444002 ]\n",
      "41-th iteration, loss: 0.03669005667054451, 222 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.854714737925541e-07\n",
      "41-th iteration, new layer inserted. now 30 layers\n",
      "[  0.97323414  58.62331092  95.94776555  52.13495717  91.75019668\n",
      "  53.7572572   95.74795232  53.1466853   93.52518515  60.61927398\n",
      " 152.70480661  74.87838794 116.99615745  92.25788159 121.9837471\n",
      "  76.09966584 165.07108773  60.50734106 195.23227817  52.61658371\n",
      "  43.87401315  34.16888268  94.41769907  33.18408303  32.49472037\n",
      "  56.56467262 111.85416485   1.41435887  71.54202094  64.37233188]\n",
      "42-th iteration, loss: 0.03669005667046078, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.82939821805425e-07\n",
      "42-th iteration, new layer inserted. now 30 layers\n",
      "[  0.97323414  58.62331089  95.94776542  52.13495675  91.75019649\n",
      "  53.7572572   95.74795234  53.146685    93.52518499  60.61927416\n",
      " 152.70480676  74.8783883  116.9961576   92.25788207 121.98374754\n",
      "  76.09966633 165.07108789  60.50734112 195.23227805  52.61658372\n",
      "  43.874013    34.1688827   94.41769907  33.184083    32.49472043\n",
      "  56.56467267 111.85416466   1.41435906  71.54202136  64.37233182]\n",
      "43-th iteration, loss: 0.0366900566703779, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.804294660777527e-07\n",
      "43-th iteration, new layer inserted. now 30 layers\n",
      "[  0.97323415  58.62331086  95.94776529  52.13495633  91.7501963\n",
      "  53.75725721  95.74795236  53.14668471  93.52518484  60.61927435\n",
      " 152.70480691  74.87838867 116.99615775  92.25788255 121.98374797\n",
      "  76.09966681 165.07108805  60.50734118 195.23227794  52.61658373\n",
      "  43.87401285  34.16888272  94.41769907  33.18408298  32.49472049\n",
      "  56.56467273 111.85416446   1.41435924  71.54202177  64.37233175]\n",
      "44-th iteration, loss: 0.0366900566702958, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.779402097104151e-07\n",
      "44-th iteration, new layer inserted. now 30 layers\n",
      "[  0.97323416  58.62331083  95.94776516  52.13495592  91.75019611\n",
      "  53.75725722  95.74795239  53.14668442  93.52518468  60.61927453\n",
      " 152.70480706  74.87838903 116.9961579   92.25788303 121.9837484\n",
      "  76.09966729 165.0710882   60.50734124 195.23227782  52.61658373\n",
      "  43.8740127   34.16888274  94.41769907  33.18408295  32.49472055\n",
      "  56.56467279 111.85416427   1.41435942  71.54202219  64.37233169]\n",
      "45-th iteration, loss: 0.03669005667021449, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.754718581633483e-07\n",
      "45-th iteration, new layer inserted. now 32 layers\n",
      "[9.73234165e-01 5.86233108e+01 9.59477650e+01 5.21349555e+01\n",
      " 9.17501959e+01 5.37572572e+01 9.57479524e+01 5.31466841e+01\n",
      " 9.35251845e+01 6.06192747e+01 1.52704807e+02 7.48783894e+01\n",
      " 1.16996158e+02 9.22578835e+01 1.21983749e+02 0.00000000e+00\n",
      " 2.13162821e-14 7.60996678e+01 1.65071088e+02 6.05073413e+01\n",
      " 1.95232278e+02 5.26165837e+01 4.38740125e+01 3.41688828e+01\n",
      " 9.44176991e+01 3.31840829e+01 3.24947206e+01 5.65646728e+01\n",
      " 1.11854164e+02 1.41435960e+00 7.15420226e+01 6.43723316e+01]\n",
      "46-th iteration, loss: 0.036690056670111784, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -4.7200189505016794e-07\n",
      "46-th iteration, new layer inserted. now 34 layers\n",
      "[9.73234172e-01 5.86233108e+01 9.59477649e+01 5.21349551e+01\n",
      " 9.17501957e+01 5.37572573e+01 9.57479524e+01 5.31466838e+01\n",
      " 9.35251844e+01 6.06192749e+01 1.52704807e+02 7.48783897e+01\n",
      " 1.16996158e+02 9.22578840e+01 1.21983749e+02 4.74149195e-07\n",
      " 4.28504503e-07 0.00000000e+00 1.32348898e-22 7.60996682e+01\n",
      " 1.65071089e+02 6.05073414e+01 1.95232278e+02 5.26165837e+01\n",
      " 4.38740124e+01 3.41688828e+01 9.44176991e+01 3.31840829e+01\n",
      " 3.24947207e+01 5.65646729e+01 1.11854164e+02 1.41435978e+00\n",
      " 7.15420230e+01 6.43723316e+01]\n",
      "47-th iteration, loss: 0.036690056669988584, 0 gd steps\n",
      "insert gradient: -0.00021183615245310444\n",
      "47-th iteration, new layer inserted. now 36 layers\n",
      "[9.73234180e-01 5.86233107e+01 9.59477648e+01 5.21349547e+01\n",
      " 9.17501955e+01 5.37572573e+01 9.57479525e+01 5.31466836e+01\n",
      " 9.35251842e+01 6.06192751e+01 1.52704807e+02 7.48783901e+01\n",
      " 1.16996158e+02 9.22578844e+01 1.21983750e+02 9.44309744e-07\n",
      " 8.54256135e-07 4.70291877e-07 4.25751632e-07 7.60996687e+01\n",
      " 1.65071089e+02 6.05073414e+01 9.76161387e+01 0.00000000e+00\n",
      " 9.76161387e+01 5.26165837e+01 4.38740122e+01 3.41688828e+01\n",
      " 9.44176991e+01 3.31840829e+01 3.24947207e+01 5.65646730e+01\n",
      " 1.11854164e+02 1.41435997e+00 7.15420234e+01 6.43723315e+01]\n",
      "48-th iteration, loss: 0.036682191245558174, 110 gd steps\n",
      "insert gradient: -1.0541334257647143e-05\n",
      "48-th iteration, new layer inserted. now 34 layers\n",
      "[9.57865120e-01 5.87113046e+01 9.59545868e+01 5.21957132e+01\n",
      " 9.17842899e+01 5.38025967e+01 9.56634345e+01 5.31436260e+01\n",
      " 9.36824415e+01 6.07930414e+01 1.52576151e+02 7.49484889e+01\n",
      " 1.17006604e+02 9.22615615e+01 1.22350017e+02 7.60510329e+01\n",
      " 1.65123514e+02 6.06780030e+01 9.61348477e+01 0.00000000e+00\n",
      " 1.77635684e-14 1.01258187e+00 9.63771900e+01 5.28025136e+01\n",
      " 4.36054532e+01 3.43116454e+01 9.45687815e+01 3.30204380e+01\n",
      " 3.25254331e+01 5.68820008e+01 1.12030905e+02 1.16697763e+00\n",
      " 7.16831182e+01 6.48676154e+01]\n",
      "49-th iteration, loss: 0.03667894297252006, 145 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.594073861013047e-07\n",
      "49-th iteration, new layer inserted. now 32 layers\n",
      "[  0.95148966  58.83571185  95.98603615  52.24287407  91.84196905\n",
      "  53.83346675  95.46020914  53.10523557  93.92146155  60.94894234\n",
      " 152.51352536  74.93680208 116.95576742  92.29796939 122.63631033\n",
      "  75.89337038 165.34704166  60.84490145  93.36353095   1.77876036\n",
      "  96.87127887  53.02905716  42.71158906  34.86863688  94.50909666\n",
      "  32.48428173  33.24666845  56.97430955 112.96803549   0.55913242\n",
      "  72.40906749  65.06328595]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5281248093431112\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  41.8760469     0.         2043.55108878]\n",
      "1-th iteration, loss: 0.7440863825287543, 11 gd steps\n",
      "insert gradient: -0.5999386503538285\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.07154436   62.35023901  224.29219267    0.         1819.25889611]\n",
      "2-th iteration, loss: 0.5323289434356375, 25 gd steps\n",
      "insert gradient: -0.33683845504209287\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   0.           49.94432769  182.11575852   56.65147727  334.14959316\n",
      "    0.         1485.10930294]\n",
      "3-th iteration, loss: 0.4811046253925413, 13 gd steps\n",
      "insert gradient: -0.46709734015143556\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           52.59229166  179.59638188   59.97134731  306.15830853\n",
      "   30.41011254  169.72677748    0.         1315.38252546]\n",
      "4-th iteration, loss: 0.41772885432267615, 13 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "4-th iteration, new layer inserted. now 9 layers\n",
      "[   0.           47.34255176  186.79499055   61.49686089  283.73551728\n",
      "   45.35173558  143.28182423   30.92577309 1315.38252546]\n",
      "5-th iteration, loss: 0.38058243245225076, 20 gd steps\n",
      "insert gradient: -0.3395094517028138\n",
      "5-th iteration, new layer inserted. now 11 layers\n",
      "[  1.29777294  51.82441846 190.74452218  51.37838373 277.74937574\n",
      "  50.13800899 119.4086589   64.28384453 682.05019839   0.\n",
      " 633.33232708]\n",
      "6-th iteration, loss: 0.300825501970842, 17 gd steps\n",
      "insert gradient: -0.2473470162532527\n",
      "6-th iteration, new layer inserted. now 13 layers\n",
      "[  6.1411304   50.8270885  211.09346195  42.27077849 104.60488268\n",
      "   0.         151.09594165  67.36454273 107.18615069  71.82902697\n",
      " 627.04377065  72.37984378 633.33232708]\n",
      "7-th iteration, loss: 0.2547211762757582, 25 gd steps\n",
      "insert gradient: -0.1801943422834447\n",
      "7-th iteration, new layer inserted. now 15 layers\n",
      "[  3.47357116  54.15206397 205.66244506  48.1800524   58.95367687\n",
      "  31.18524243 103.51386963  69.71484706 105.91287329  71.92100764\n",
      " 606.35550513  67.87405277 175.92564641   0.         457.40668067]\n",
      "8-th iteration, loss: 0.23034398495307237, 15 gd steps\n",
      "insert gradient: -0.1476910916623675\n",
      "8-th iteration, new layer inserted. now 17 layers\n",
      "[  3.18834267  61.21673     88.31729821   0.         113.55081198\n",
      "  43.58362506  73.90664492  29.5900511   89.19865297  72.17695819\n",
      " 103.5889089   72.71402352 610.89993904  57.16226571 155.59642973\n",
      "  40.48729787 457.40668067]\n",
      "9-th iteration, loss: 0.21818509355425308, 20 gd steps\n",
      "insert gradient: -0.08428906550207503\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.3421594   55.46052697  77.57265309   6.93103703  96.50114701\n",
      "  56.97014894  76.37342868  24.85985497  93.76251809  72.8118666\n",
      " 108.25721699  69.24157925 174.88234946   0.         437.20587365\n",
      "  55.27950242 136.92375378  62.83308148 457.40668067]\n",
      "10-th iteration, loss: 0.19577146093790632, 27 gd steps\n",
      "insert gradient: -0.10440084462506431\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.45393814  58.53306072  88.47257437   4.97876191  97.06815969\n",
      "  56.16420194  79.66856557  23.88872243  92.72941008  72.46143141\n",
      " 112.76636688  62.61132597 153.47859181  26.66630747 358.89081644\n",
      "   0.          71.77816329  45.40578073 134.69938254  69.77932289\n",
      " 457.40668067]\n",
      "11-th iteration, loss: 0.17608223696332323, 27 gd steps\n",
      "insert gradient: -0.07056706619865245\n",
      "11-th iteration, new layer inserted. now 21 layers\n",
      "[  1.31043061  57.97442874  95.53828198   4.58772872  95.42240258\n",
      "  57.78423461  89.19132099  22.84500148  83.53073888  75.95370143\n",
      " 113.74962328  62.41331166 130.58700745  57.62087061 337.03501612\n",
      "  17.87181861  53.02858625  34.80555678 143.53430292  69.89552153\n",
      " 457.40668067]\n",
      "12-th iteration, loss: 0.17299540199591582, 27 gd steps\n",
      "insert gradient: -0.06121987719356901\n",
      "12-th iteration, new layer inserted. now 23 layers\n",
      "[  1.26799852  57.48043284  94.6680648    7.99405904  92.59828663\n",
      "  58.34097863  88.55319457  22.71015632  81.22577682  76.27832707\n",
      " 113.30871161  64.44833879 130.53906684  59.77086378 318.93884124\n",
      "  27.92371894  44.73163832  38.92492595 149.10361749  65.02161282\n",
      " 207.91212758   0.         249.49455309]\n",
      "13-th iteration, loss: 0.15758129786719807, 50 gd steps\n",
      "insert gradient: -0.0513986533682263\n",
      "13-th iteration, new layer inserted. now 25 layers\n",
      "[  2.18565886  60.53814621  92.91928848  13.70555217  84.84044292\n",
      "  56.13634098  93.6530117   23.91155477  77.90848345  75.11744103\n",
      " 120.14873684  65.09951462 129.00086903  66.57120969 220.46461356\n",
      "   0.          94.48483438  29.43618801  55.71633283  27.95365444\n",
      " 165.72056429  57.58695125 168.05815132  35.83500223 249.49455309]\n",
      "14-th iteration, loss: 0.13918491519514076, 55 gd steps\n",
      "insert gradient: -0.025388650713319385\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  2.14687499  61.40621867  96.10366674  11.74325406  84.43319931\n",
      "  57.1378959   93.88781015  33.18206453  76.40811578  70.38148965\n",
      " 123.61513581  66.5071983  125.41314734  73.74245539 201.60287575\n",
      "  32.05470743  37.2138145   32.54872635  87.17904723  20.41618123\n",
      "  96.68949028   0.          77.35159222  52.48164768 163.25983504\n",
      "  53.13699709 249.49455309]\n",
      "15-th iteration, loss: 0.12923926728768187, 19 gd steps\n",
      "insert gradient: -0.034581475743797774\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[  0.          55.55564479  96.99733509  12.10441857  84.01681473\n",
      "  59.67148443  91.43915844  36.56392378  78.65765938  68.55414432\n",
      " 128.60844559  63.08975794 126.99783554  76.2279711  187.27618557\n",
      "  65.99832113   6.84916678  27.76057861  99.20105803  26.62364884\n",
      "  83.54427654   8.77586258  64.31952442  39.40244478 161.92053634\n",
      "  64.16451462 218.30773395   0.          31.18681914]\n",
      "16-th iteration, loss: 0.09610310444362799, 104 gd steps\n",
      "insert gradient: -0.006621580004265903\n",
      "16-th iteration, new layer inserted. now 29 layers\n",
      "[  0.87333074  58.32804726  97.938418    13.83587838  72.00950231\n",
      "  58.0547666   92.60468755  43.94374503  83.76063047  61.70227724\n",
      " 123.16068806  68.15497132 116.73395019  88.35881574 184.98955802\n",
      "  67.69673761 155.45407393  59.95759064  60.15596378  20.71891626\n",
      "  67.35262915   6.34962604 133.81614215  51.58988835   0.\n",
      "  51.58988835 192.1925326   46.06801448  31.18681914]\n",
      "17-th iteration, loss: 0.08793445010120995, 185 gd steps\n",
      "insert gradient: -0.004061317582038065\n",
      "17-th iteration, new layer inserted. now 31 layers\n",
      "[1.30773320e+00 5.85959289e+01 1.04470340e+02 0.00000000e+00\n",
      " 2.13162821e-14 2.52045142e+01 4.44696487e+01 6.01912512e+01\n",
      " 9.34854710e+01 4.69043484e+01 8.64230728e+01 6.13823861e+01\n",
      " 1.24828751e+02 6.89751336e+01 1.13432638e+02 9.67396434e+01\n",
      " 1.88166009e+02 6.13213372e+01 1.54437541e+02 6.07883489e+01\n",
      " 9.67805683e+01 3.26058246e+00 9.34561606e+01 3.30402321e+01\n",
      " 2.89670453e+01 5.77182454e+01 4.15610876e+01 2.86408453e+01\n",
      " 1.89094625e+02 7.59341637e+01 3.11868191e+01]\n",
      "18-th iteration, loss: 0.08584496260060694, 205 gd steps\n",
      "insert gradient: -0.004701365638680406\n",
      "18-th iteration, new layer inserted. now 31 layers\n",
      "[  1.60685356  57.2506578  103.66912596  42.74586091  24.52975553\n",
      "  58.88920438  92.56558178  46.71868706  85.3815425   61.36370833\n",
      " 123.86327093  69.98176113 112.23569934  98.69875183 189.33761068\n",
      "  60.13337513 154.35747867  60.27994927  97.63106478   2.156535\n",
      "  91.5296943   40.73356558  17.98952941  55.21508405  62.8180152\n",
      "  18.5724619   98.243788     0.          98.243788    75.31074961\n",
      "  31.18681914]\n",
      "19-th iteration, loss: 0.07901064754870425, 50 gd steps\n",
      "insert gradient: -0.012940610306815037\n",
      "19-th iteration, new layer inserted. now 31 layers\n",
      "[  2.45272399  57.86252964 101.82924975  51.17384392  23.79552863\n",
      "  55.04271334  94.31094122  48.97958378  88.88584888  60.15116662\n",
      " 128.92403248  68.64147215 113.09950686  95.94045098 124.48760559\n",
      "   0.          74.69256335  61.00425514 159.1061195   56.86890461\n",
      " 186.85105491  52.53558249   4.23553248  55.73011881  96.24734238\n",
      "  12.47194471  69.47631534  26.33148117  69.80316482  90.66885697\n",
      "  31.18681914]\n",
      "20-th iteration, loss: 0.06914646532895867, 25 gd steps\n",
      "insert gradient: -0.014228856355443754\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  1.19537552  57.29411552 102.19657649  47.06464514  35.42484437\n",
      "  49.15939478  96.77618005  53.19179754  94.06474725  62.68523958\n",
      " 132.09816428  63.83346967 118.40791407  91.24640249 207.01744625\n",
      "  67.82020953 154.3684619   62.70826925  86.50478668   0.\n",
      "  86.50478668  57.62027266   5.62437446  51.93049852 104.57823988\n",
      "  17.51431673  41.71463951  47.88218289  58.98563993 109.15053161\n",
      "  31.18681914]\n",
      "21-th iteration, loss: 0.06146267719132513, 56 gd steps\n",
      "insert gradient: -0.006056173553017901\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[  1.50415274  57.36947772  96.56514286  50.44511197  44.41246317\n",
      "  45.13651251  94.65190572  56.81992234 100.03688772  71.84664093\n",
      " 121.62549536  64.74555349 115.49636705  90.80776976 133.85971539\n",
      "   0.          80.31582924  73.20747833 159.1244666   58.01045062\n",
      " 185.62923974  54.31610392  43.9395105   25.57966767 104.22056173\n",
      "  33.74188694  19.42905901  52.88857194  85.48110011 105.25242382\n",
      "  31.18681914]\n",
      "22-th iteration, loss: 0.05141363197420505, 32 gd steps\n",
      "insert gradient: -0.0065441594754388965\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[  1.20485134  55.66278544  95.94025396  52.16908021  48.59534765\n",
      "  47.17109036  93.24678771  54.52137329  95.86947242  71.15066775\n",
      " 122.47322519  69.53245608 108.73721383  85.62163534 121.61974975\n",
      " 124.25709719 173.54217633  61.93454025 106.1951507    0.\n",
      "  63.71709042  53.43836294  88.7683476    8.38810708  93.56554829\n",
      "  48.29024683  26.41433872  40.34472973  91.07829029 104.83025585\n",
      "  31.18681914]\n",
      "23-th iteration, loss: 0.050672403508671034, 23 gd steps\n",
      "insert gradient: -0.001415354313669292\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  1.84181299  55.68598668  97.13656921  50.46138077  50.36116167\n",
      "  46.97400306  95.74072983  53.76629021  99.314536    68.57687774\n",
      " 127.38302571  67.39777298 111.59361026  86.17833621 119.03200719\n",
      " 124.17203251 178.63384442  58.2869869  107.39260952   1.41475383\n",
      "  63.72740704  53.68936557  93.72972948   7.05589174  90.79903126\n",
      "  45.01081163  30.48254229  39.12963462  93.81462561  44.77868813\n",
      "   0.          59.70491751  31.18681914]\n",
      "24-th iteration, loss: 0.04935217286677213, 26 gd steps\n",
      "insert gradient: -0.0014245812866358922\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[1.05890708e+00 5.62015108e+01 9.67525856e+01 4.98246231e+01\n",
      " 5.43762010e+01 0.00000000e+00 1.77635684e-15 4.69981508e+01\n",
      " 9.33758014e+01 5.61199526e+01 9.89473778e+01 7.07828447e+01\n",
      " 1.26638906e+02 6.74347082e+01 1.14802364e+02 8.48472003e+01\n",
      " 1.15383816e+02 1.22982904e+02 1.84414096e+02 6.25257430e+01\n",
      " 1.75991996e+02 5.18524435e+01 9.47130176e+01 7.48479136e+00\n",
      " 8.54671949e+01 4.85664667e+01 2.67167780e+01 4.14598788e+01\n",
      " 1.00968171e+02 2.47497283e+01 2.38703641e+01 5.73199609e+01\n",
      " 3.11868191e+01]\n",
      "25-th iteration, loss: 0.049087369197204564, 254 gd steps\n",
      "insert gradient: -0.00029575487269289735\n",
      "25-th iteration, new layer inserted. now 33 layers\n",
      "[  1.38936105  57.21793915  96.00652231  49.78789871  57.01497359\n",
      "  45.81147158  93.90202423  55.17678942 100.00873202  70.43942104\n",
      " 126.72432697  68.3316551  114.04748501  84.82526504 116.73458397\n",
      " 104.02985636   0.          17.33830939 187.54258343  61.99113935\n",
      " 171.70566236  55.08234825  92.35946365   7.28464227  83.97167006\n",
      "  48.4458375   29.92655068  39.98019704  98.78552375  22.63414484\n",
      "  29.65452813  57.05377706  31.18681914]\n",
      "26-th iteration, loss: 0.04885925634767159, 124 gd steps\n",
      "insert gradient: -0.00016206064644092315\n",
      "26-th iteration, new layer inserted. now 33 layers\n",
      "[  1.22210584  56.80064445  95.33336734  48.83133736  62.71586729\n",
      "  45.76497056  92.21595962  55.68635728 101.5055069   71.44131878\n",
      " 127.93742997  66.62808724 119.25582549  83.54127926 116.52486797\n",
      "  97.39723316   4.40336982  11.37894265 200.65125813  66.93321464\n",
      " 170.13840924  56.42285224  91.5380525    5.49267179  85.3850716\n",
      "  49.65346809  35.09860925  36.76543342  96.07110501  22.06384854\n",
      "  37.260275    54.66004036  31.18681914]\n",
      "27-th iteration, loss: 0.04882666761920491, 37 gd steps\n",
      "insert gradient: -0.00010877056421450592\n",
      "27-th iteration, new layer inserted. now 33 layers\n",
      "[  1.17462275  56.80836802  95.13506294  48.59645465  64.00145181\n",
      "  45.73552472  91.87663706  55.79792854 101.7646174   71.70562593\n",
      " 128.03765802  66.35138044 120.41773057  83.15185026 116.94932688\n",
      "  96.6468908    2.76420792   9.96509276 205.49366903  67.30965314\n",
      " 170.47919288  56.36554015  92.3485647    4.75853906  86.14779166\n",
      "  49.71538758  36.6900668   35.69811373  95.43446761  22.46201531\n",
      "  38.0736472   54.35457612  31.18681914]\n",
      "28-th iteration, loss: 0.04880080439055452, 372 gd steps\n",
      "insert gradient: -0.0001079923331256995\n",
      "28-th iteration, new layer inserted. now 33 layers\n",
      "[  1.15258078  56.87082542  94.87174118  48.2618601   65.4868262\n",
      "  45.82576145  91.52344529  55.80287047 102.00700239  72.19619777\n",
      " 128.07650526  66.08236899 121.81794847  82.62546618 117.40026171\n",
      " 104.47489096 151.21767769   0.          60.48707108  67.49403037\n",
      " 171.10653391  56.30726415  92.91239923   4.13597543  86.74394965\n",
      "  49.96316169  38.25735186  34.5631754   94.86332851  22.97415519\n",
      "  38.82126511  54.0462433   31.18681914]\n",
      "29-th iteration, loss: 0.04767059737150149, 72 gd steps\n",
      "insert gradient: -0.004220792267484486\n",
      "29-th iteration, new layer inserted. now 33 layers\n",
      "[1.27333205e+00 5.72670424e+01 9.44655932e+01 4.77072516e+01\n",
      " 7.03015155e+01 4.81304962e+01 9.20438689e+01 0.00000000e+00\n",
      " 1.06581410e-14 5.46537503e+01 9.93840825e+01 7.44498922e+01\n",
      " 1.29553489e+02 6.58834395e+01 1.25459772e+02 8.24706935e+01\n",
      " 1.15372126e+02 9.28528204e+01 1.54844294e+02 1.95634492e+01\n",
      " 3.82440218e+01 6.50297579e+01 1.78847055e+02 5.70639299e+01\n",
      " 1.86925940e+02 5.54740447e+01 4.97022226e+01 2.36291352e+01\n",
      " 9.52031058e+01 2.98337857e+01 3.41723397e+01 5.45845636e+01\n",
      " 3.11868191e+01]\n",
      "30-th iteration, loss: 0.04511383799045574, 22 gd steps\n",
      "insert gradient: -0.002505854685180392\n",
      "30-th iteration, new layer inserted. now 33 layers\n",
      "[  0.94988217  57.87077319  94.5518074   46.02578569  74.97307653\n",
      "  49.05907469  94.16823605   0.64842772   1.56760909  53.6553356\n",
      "  94.36998603  73.51006334 132.68186948  65.23254488 123.39532901\n",
      "  85.75536085 112.92804359  86.95858348 146.6460895   46.10941517\n",
      "  14.94030418  60.08646521 192.02398247  51.05920437 183.41175848\n",
      "  55.82446551  73.08112984   9.02106594  99.41258225  45.67389685\n",
      "  20.97598581  53.32932313  31.18681914]\n",
      "31-th iteration, loss: 0.04375580801014753, 26 gd steps\n",
      "insert gradient: -0.004129518462465107\n",
      "31-th iteration, new layer inserted. now 35 layers\n",
      "[  0.94051848  58.20180532  94.21642297  47.61755347  75.99853497\n",
      "  50.71158635  93.19755965   0.35467791   0.44510244  53.89684623\n",
      "  97.88560265  70.54172248 138.10988743  66.19185132 120.32372462\n",
      "  87.04585921 114.60156326  87.77390108 141.29362643  56.82287908\n",
      "  11.03822958  52.79045629 112.41728517   0.          84.31296388\n",
      "  52.14155199 186.78243622  55.00278252  76.33740805   6.52721279\n",
      "  96.08706301  50.94060683  28.77242785  41.05988533  31.18681914]\n",
      "32-th iteration, loss: 0.0396780267707142, 63 gd steps\n",
      "insert gradient: -0.0018736259320263876\n",
      "32-th iteration, new layer inserted. now 35 layers\n",
      "[9.31075736e-01 5.97676268e+01 9.59773962e+01 5.02811270e+01\n",
      " 8.60447665e+01 5.29260248e+01 9.31377065e+01 8.95971596e-02\n",
      " 2.33737770e-02 5.29623578e+01 9.55246564e+01 6.41483582e+01\n",
      " 1.52802975e+02 6.94381511e+01 1.18843519e+02 9.06495909e+01\n",
      " 1.18181833e+02 8.45981922e+01 1.45281281e+02 6.64397024e+01\n",
      " 3.67908889e+01 2.11005491e+01 1.02092389e+02 4.01146377e+01\n",
      " 3.47103873e+01 5.01342516e+01 1.13358883e+02 0.00000000e+00\n",
      " 8.50191623e+01 5.33404082e+01 1.88908388e+02 5.96357492e+01\n",
      " 4.19657306e+01 1.50824565e+01 3.11868191e+01]\n",
      "33-th iteration, loss: 0.03703106709723803, 141 gd steps\n",
      "insert gradient: -0.00019862258739583073\n",
      "33-th iteration, new layer inserted. now 33 layers\n",
      "[1.05222900e+00 5.90143063e+01 9.71241993e+01 5.21068889e+01\n",
      " 8.98602470e+01 5.32951048e+01 9.51231539e+01 5.27975708e+01\n",
      " 9.44203776e+01 6.15456240e+01 1.52406228e+02 7.40238436e+01\n",
      " 1.16148646e+02 9.21734210e+01 1.23425553e+02 7.67614860e+01\n",
      " 1.60728499e+02 6.38169963e+01 7.47118602e+01 5.97294120e+00\n",
      " 1.02802423e+02 5.32065744e+01 3.11528650e+01 4.26511840e+01\n",
      " 9.78615809e+01 2.24684837e+01 4.71438103e+01 5.58743279e+01\n",
      " 1.88297445e+02 0.00000000e+00 2.13162821e-14 6.14566405e+01\n",
      " 9.04548426e+01]\n",
      "34-th iteration, loss: 0.03669008348436963, 251 gd steps\n",
      "insert gradient: -3.65095526428679e-05\n",
      "34-th iteration, new layer inserted. now 31 layers\n",
      "[  0.91686082  58.94267117  96.14764737  52.21324355  91.62394502\n",
      "  53.80469387  95.23081475  53.08357461  94.0620564   61.15564095\n",
      " 152.53425823  74.68980446 116.90534322  92.37562749 122.5126501\n",
      "  76.02928152 164.89920313  61.10984828  87.66867873   2.19534922\n",
      " 101.40845608  53.1453848   40.34628705  36.32165513  94.74368635\n",
      "  30.87664678  35.44032108  56.76858407 187.28108697  64.52685704\n",
      "  90.45484262]\n",
      "35-th iteration, loss: 0.03667931385413253, 95 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6519455599363377e-06\n",
      "35-th iteration, new layer inserted. now 33 layers\n",
      "[9.48416896e-01 5.89035652e+01 9.60385222e+01 5.22560072e+01\n",
      " 9.18502845e+01 5.38424652e+01 9.53684001e+01 5.30696101e+01\n",
      " 9.39916728e+01 0.00000000e+00 1.06581410e-14 6.10089253e+01\n",
      " 1.52517956e+02 7.48962382e+01 1.16900758e+02 9.23310940e+01\n",
      " 1.22715359e+02 7.58183969e+01 1.65394714e+02 6.08862093e+01\n",
      " 9.18922608e+01 1.98126138e+00 9.77749353e+01 5.30503634e+01\n",
      " 4.22331587e+01 3.51595259e+01 9.43870462e+01 3.22219344e+01\n",
      " 3.36801205e+01 5.69580293e+01 1.87146879e+02 6.48177709e+01\n",
      " 9.04548426e+01]\n",
      "36-th iteration, loss: 0.036679313849090955, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6442852544243026e-06\n",
      "36-th iteration, new layer inserted. now 35 layers\n",
      "[9.48417085e-01 5.89035659e+01 9.60385230e+01 5.22560092e+01\n",
      " 9.18502855e+01 5.38424660e+01 9.53684009e+01 5.30696130e+01\n",
      " 9.39916751e+01 3.64460940e-06 2.35048536e-06 0.00000000e+00\n",
      " 4.23516474e-22 6.10089290e+01 1.52517958e+02 7.48962404e+01\n",
      " 1.16900758e+02 9.23310930e+01 1.22715358e+02 7.58183965e+01\n",
      " 1.65394714e+02 6.08862096e+01 9.18922633e+01 1.98126372e+00\n",
      " 9.77749338e+01 5.30503651e+01 4.22331603e+01 3.51595273e+01\n",
      " 9.43870468e+01 3.22219363e+01 3.36801216e+01 5.69580307e+01\n",
      " 1.87146880e+02 6.48177722e+01 9.04548426e+01]\n",
      "37-th iteration, loss: 0.03667931384305644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6322371575865705e-06\n",
      "37-th iteration, new layer inserted. now 37 layers\n",
      "[9.48417273e-01 5.89035666e+01 9.60385238e+01 5.22560111e+01\n",
      " 9.18502864e+01 5.38424668e+01 9.53684017e+01 5.30696158e+01\n",
      " 9.39916775e+01 7.27882053e-06 4.69259521e-06 3.63489860e-06\n",
      " 2.34210985e-06 0.00000000e+00 3.17637355e-22 6.10089326e+01\n",
      " 1.52517959e+02 7.48962425e+01 1.16900759e+02 9.23310921e+01\n",
      " 1.22715358e+02 7.58183960e+01 1.65394715e+02 6.08862099e+01\n",
      " 9.18922657e+01 1.98126606e+00 9.77749322e+01 5.30503668e+01\n",
      " 4.22331620e+01 3.51595288e+01 9.43870474e+01 3.22219382e+01\n",
      " 3.36801226e+01 5.69580321e+01 1.87146880e+02 6.48177735e+01\n",
      " 9.04548426e+01]\n",
      "38-th iteration, loss: 0.036679313836051136, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.6158573602185424e-06\n",
      "38-th iteration, new layer inserted. now 37 layers\n",
      "[9.48417459e-01 5.89035673e+01 9.60385246e+01 5.22560130e+01\n",
      " 9.18502873e+01 5.38424675e+01 9.53684024e+01 5.30696187e+01\n",
      " 9.39916798e+01 1.08975884e-05 7.02395821e-06 7.25503891e-06\n",
      " 4.67240971e-06 3.62082534e-06 2.33029986e-06 6.10089362e+01\n",
      " 1.52517961e+02 7.48962446e+01 1.16900759e+02 9.23310911e+01\n",
      " 1.22715358e+02 7.58183955e+01 1.65394715e+02 6.08862103e+01\n",
      " 9.18922681e+01 1.98126840e+00 9.77749307e+01 5.30503685e+01\n",
      " 4.22331636e+01 3.51595303e+01 9.43870481e+01 3.22219401e+01\n",
      " 3.36801237e+01 5.69580335e+01 1.87146881e+02 6.48177747e+01\n",
      " 9.04548426e+01]\n",
      "39-th iteration, loss: 0.036679313829099294, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.599575302108244e-06\n",
      "39-th iteration, new layer inserted. now 37 layers\n",
      "[9.48417642e-01 5.89035680e+01 9.60385254e+01 5.22560149e+01\n",
      " 9.18502882e+01 5.38424683e+01 9.53684032e+01 5.30696215e+01\n",
      " 9.39916821e+01 1.44979853e-05 9.34353194e-06 1.08574902e-05\n",
      " 6.98986148e-06 7.22464323e-06 4.64669261e-06 6.10089398e+01\n",
      " 1.52517962e+02 7.48962467e+01 1.16900759e+02 9.23310902e+01\n",
      " 1.22715357e+02 7.58183951e+01 1.65394715e+02 6.08862106e+01\n",
      " 9.18922706e+01 1.98127074e+00 9.77749292e+01 5.30503702e+01\n",
      " 4.22331653e+01 3.51595318e+01 9.43870487e+01 3.22219420e+01\n",
      " 3.36801248e+01 5.69580350e+01 1.87146882e+02 6.48177760e+01\n",
      " 9.04548426e+01]\n",
      "40-th iteration, loss: 0.03667931382220037, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.583390241554889e-06\n",
      "40-th iteration, new layer inserted. now 39 layers\n",
      "[9.48417824e-01 5.89035687e+01 9.60385262e+01 5.22560167e+01\n",
      " 9.18502891e+01 5.38424690e+01 9.53684040e+01 5.30696243e+01\n",
      " 9.39916845e+01 1.80801200e-05 1.16514000e-05 1.44423577e-05\n",
      " 9.29455393e-06 1.08115551e-05 6.94927199e-06 0.00000000e+00\n",
      " 1.27054942e-21 6.10089434e+01 1.52517964e+02 7.48962489e+01\n",
      " 1.16900760e+02 9.23310893e+01 1.22715357e+02 7.58183947e+01\n",
      " 1.65394715e+02 6.08862109e+01 9.18922730e+01 1.98127308e+00\n",
      " 9.77749277e+01 5.30503719e+01 4.22331669e+01 3.51595332e+01\n",
      " 9.43870493e+01 3.22219439e+01 3.36801259e+01 5.69580364e+01\n",
      " 1.87146883e+02 6.48177773e+01 9.04548426e+01]\n",
      "41-th iteration, loss: 0.03667931381437836, 0 gd steps\n",
      "insert gradient: -3.5763059639188746e-05\n",
      "41-th iteration, new layer inserted. now 41 layers\n",
      "[9.48418003e-01 5.89035693e+01 9.60385270e+01 5.22560186e+01\n",
      " 9.18502900e+01 5.38424698e+01 9.53684047e+01 5.30696271e+01\n",
      " 9.39916868e+01 2.16420741e-05 1.39463693e-05 1.80077197e-05\n",
      " 1.15852991e-05 1.43796356e-05 9.23685496e-06 3.57011302e-06\n",
      " 2.28758297e-06 6.10089470e+01 1.52517965e+02 7.48962510e+01\n",
      " 1.16900760e+02 9.23310884e+01 1.22715357e+02 7.58183942e+01\n",
      " 1.65394716e+02 6.08862112e+01 9.18922754e+01 1.98127542e+00\n",
      " 9.77749262e+01 5.30503736e+01 4.22331685e+01 3.51595347e+01\n",
      " 9.43870499e+01 3.22219458e+01 3.36801270e+01 5.69580378e+01\n",
      " 1.12288130e+02 0.00000000e+00 7.48587535e+01 6.48177786e+01\n",
      " 9.04548426e+01]\n",
      "42-th iteration, loss: 0.036678967364025455, 34 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.173504309391138e-06\n",
      "42-th iteration, new layer inserted. now 35 layers\n",
      "[9.58140709e-01 5.88650607e+01 9.60047601e+01 5.22599785e+01\n",
      " 9.18555194e+01 0.00000000e+00 3.55271368e-15 5.38281660e+01\n",
      " 9.53948044e+01 5.30967665e+01 9.39874348e+01 6.09941541e+01\n",
      " 1.52526183e+02 7.49075869e+01 1.16940547e+02 9.23192194e+01\n",
      " 1.22671412e+02 7.58670113e+01 1.65369075e+02 6.08775291e+01\n",
      " 9.21934141e+01 1.91657534e+00 9.76613479e+01 5.30418263e+01\n",
      " 4.23564482e+01 3.50933205e+01 9.44991947e+01 3.22412516e+01\n",
      " 3.35980766e+01 5.69691381e+01 1.11853633e+02 2.89339302e-01\n",
      " 7.43870795e+01 6.49549167e+01 9.04548426e+01]\n",
      "43-th iteration, loss: 0.0366789673498741, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.123136710562686e-06\n",
      "43-th iteration, new layer inserted. now 37 layers\n",
      "[9.58141004e-01 5.88650639e+01 9.60047618e+01 5.22599812e+01\n",
      " 9.18555219e+01 6.15419408e-06 2.49965087e-06 0.00000000e+00\n",
      " 5.29395592e-22 5.38281721e+01 9.53948054e+01 5.30967637e+01\n",
      " 9.39874327e+01 6.09941517e+01 1.52526181e+02 7.49075832e+01\n",
      " 1.16940547e+02 9.23192188e+01 1.22671414e+02 7.58670132e+01\n",
      " 1.65369075e+02 6.08775336e+01 9.21934163e+01 1.91657390e+00\n",
      " 9.76613475e+01 5.30418270e+01 4.23564464e+01 3.50933154e+01\n",
      " 9.44991928e+01 3.22412500e+01 3.35980712e+01 5.69691356e+01\n",
      " 1.11853632e+02 2.89337727e-01 7.43870768e+01 6.49549163e+01\n",
      " 9.04548426e+01]\n",
      "44-th iteration, loss: 0.036678967333532625, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -6.054478418872155e-06\n",
      "44-th iteration, new layer inserted. now 39 layers\n",
      "[9.58141293e-01 5.88650671e+01 9.60047634e+01 5.22599838e+01\n",
      " 9.18555244e+01 1.22489993e-05 4.96378626e-06 6.09560141e-06\n",
      " 2.46413539e-06 0.00000000e+00 2.11758237e-22 5.38281782e+01\n",
      " 9.53948063e+01 5.30967610e+01 9.39874306e+01 6.09941493e+01\n",
      " 1.52526178e+02 7.49075796e+01 1.16940546e+02 9.23192183e+01\n",
      " 1.22671415e+02 7.58670151e+01 1.65369075e+02 6.08775380e+01\n",
      " 9.21934184e+01 1.91657249e+00 9.76613472e+01 5.30418278e+01\n",
      " 4.23564446e+01 3.50933104e+01 9.44991910e+01 3.22412484e+01\n",
      " 3.35980659e+01 5.69691332e+01 1.11853631e+02 2.89336169e-01\n",
      " 7.43870741e+01 6.49549159e+01 9.04548426e+01]\n",
      "45-th iteration, loss: 0.036678967315164755, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.968250371923895e-06\n",
      "45-th iteration, new layer inserted. now 39 layers\n",
      "[9.58141576e-01 5.88650702e+01 9.60047651e+01 5.22599864e+01\n",
      " 9.18555268e+01 1.82656714e-05 7.38281007e-06 1.21138547e-05\n",
      " 4.88121756e-06 6.01903820e-06 2.41708217e-06 5.38281842e+01\n",
      " 9.53948072e+01 5.30967581e+01 9.39874285e+01 6.09941469e+01\n",
      " 1.52526176e+02 7.49075759e+01 1.16940545e+02 9.23192177e+01\n",
      " 1.22671416e+02 7.58670171e+01 1.65369075e+02 6.08775425e+01\n",
      " 9.21934205e+01 1.91657110e+00 9.76613468e+01 5.30418286e+01\n",
      " 4.23564428e+01 3.50933054e+01 9.44991891e+01 3.22412468e+01\n",
      " 3.35980605e+01 5.69691308e+01 1.11853629e+02 2.89334629e-01\n",
      " 7.43870714e+01 6.49549156e+01 9.04548426e+01]\n",
      "46-th iteration, loss: 0.03667896729715295, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.8835847939010305e-06\n",
      "46-th iteration, new layer inserted. now 39 layers\n",
      "[9.58141851e-01 5.88650733e+01 9.60047667e+01 5.22599889e+01\n",
      " 9.18555292e+01 2.41944849e-05 9.75214432e-06 1.80450198e-05\n",
      " 7.24669317e-06 1.19517581e-05 4.78064053e-06 5.38281902e+01\n",
      " 9.53948080e+01 5.30967553e+01 9.39874264e+01 6.09941445e+01\n",
      " 1.52526174e+02 7.49075723e+01 1.16940545e+02 9.23192172e+01\n",
      " 1.22671417e+02 7.58670190e+01 1.65369075e+02 6.08775469e+01\n",
      " 9.21934226e+01 1.91656974e+00 9.76613465e+01 5.30418294e+01\n",
      " 4.23564410e+01 3.50933004e+01 9.44991873e+01 3.22412452e+01\n",
      " 3.35980552e+01 5.69691284e+01 1.11853628e+02 2.89333108e-01\n",
      " 7.43870687e+01 6.49549152e+01 9.04548426e+01]\n",
      "47-th iteration, loss: 0.03667896727948512, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.8004522760584375e-06\n",
      "47-th iteration, new layer inserted. now 39 layers\n",
      "[9.58142118e-01 5.88650764e+01 9.60047683e+01 5.22599913e+01\n",
      " 9.18555315e+01 3.00370387e-05 1.20727016e-05 2.38906800e-05\n",
      " 9.56150254e-06 1.77997266e-05 7.09164288e-06 5.38281960e+01\n",
      " 9.53948088e+01 5.30967524e+01 9.39874242e+01 6.09941421e+01\n",
      " 1.52526172e+02 7.49075686e+01 1.16940544e+02 9.23192167e+01\n",
      " 1.22671419e+02 7.58670209e+01 1.65369075e+02 6.08775514e+01\n",
      " 9.21934248e+01 1.91656840e+00 9.76613462e+01 5.30418302e+01\n",
      " 4.23564393e+01 3.50932954e+01 9.44991854e+01 3.22412437e+01\n",
      " 3.35980499e+01 5.69691260e+01 1.11853627e+02 2.89331606e-01\n",
      " 7.43870661e+01 6.49549149e+01 9.04548426e+01]\n",
      "48-th iteration, loss: 0.03667896726214972, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.718823967602371e-06\n",
      "48-th iteration, new layer inserted. now 39 layers\n",
      "[9.58142378e-01 5.88650794e+01 9.60047698e+01 5.22599937e+01\n",
      " 9.18555338e+01 3.57949020e-05 1.43453772e-05 2.96523889e-05\n",
      " 1.18265682e-05 2.35644811e-05 9.35103876e-06 5.38282018e+01\n",
      " 9.53948096e+01 5.30967494e+01 9.39874221e+01 6.09941397e+01\n",
      " 1.52526170e+02 7.49075650e+01 1.16940544e+02 9.23192162e+01\n",
      " 1.22671420e+02 7.58670229e+01 1.65369076e+02 6.08775559e+01\n",
      " 9.21934269e+01 1.91656709e+00 9.76613459e+01 5.30418310e+01\n",
      " 4.23564376e+01 3.50932904e+01 9.44991836e+01 3.22412422e+01\n",
      " 3.35980446e+01 5.69691237e+01 1.11853626e+02 2.89330123e-01\n",
      " 7.43870634e+01 6.49549145e+01 9.04548426e+01]\n",
      "49-th iteration, loss: 0.03667896724513554, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -5.63867156538454e-06\n",
      "49-th iteration, new layer inserted. now 39 layers\n",
      "[9.58142630e-01 5.88650825e+01 9.60047714e+01 5.22599960e+01\n",
      " 9.18555360e+01 4.14696143e-05 1.65710494e-05 3.53316706e-05\n",
      " 1.40427952e-05 2.92475300e-05 1.15597598e-05 5.38282075e+01\n",
      " 9.53948103e+01 5.30967465e+01 9.39874199e+01 6.09941372e+01\n",
      " 1.52526168e+02 7.49075614e+01 1.16940543e+02 9.23192157e+01\n",
      " 1.22671421e+02 7.58670249e+01 1.65369076e+02 6.08775604e+01\n",
      " 9.21934291e+01 1.91656581e+00 9.76613456e+01 5.30418319e+01\n",
      " 4.23564359e+01 3.50932855e+01 9.44991818e+01 3.22412408e+01\n",
      " 3.35980394e+01 5.69691214e+01 1.11853625e+02 2.89328659e-01\n",
      " 7.43870607e+01 6.49549142e+01 9.04548426e+01]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5314150056394777\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.38057759    0.         2068.17218623]\n",
      "1-th iteration, loss: 0.745144539899422, 11 gd steps\n",
      "insert gradient: -0.6117233507397899\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.43634346   62.36427952  226.99450825    0.         1841.17767799]\n",
      "2-th iteration, loss: 0.5296849013738686, 28 gd steps\n",
      "insert gradient: -0.3590179534008256\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   2.51948759   57.54509391  174.70266879   56.57112516  338.17549188\n",
      "    0.         1503.00218611]\n",
      "3-th iteration, loss: 0.45238353741853465, 36 gd steps\n",
      "insert gradient: -0.4834811629180582\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   1.50193718   58.3424816   173.29546402   55.32733546  273.46525323\n",
      "   49.57800578  128.82875881    0.         1374.1734273 ]\n",
      "4-th iteration, loss: 0.3865969729113001, 14 gd steps\n",
      "insert gradient: -0.3918527403819869\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  9.90240252  53.0096485  172.26180619  53.99779858 268.68231277\n",
      "  53.92291766  89.12130769  65.6666669  661.63905759   0.\n",
      " 712.53436971]\n",
      "5-th iteration, loss: 0.2767193119989811, 45 gd steps\n",
      "insert gradient: -0.22722858505938281\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.54614546  47.17374204 198.70275148  45.34350247 237.08866423\n",
      "  65.93669873 101.90391795  67.1507893  592.457954    65.23612435\n",
      " 161.93962948   0.         550.59474023]\n",
      "6-th iteration, loss: 0.23554778080407857, 89 gd steps\n",
      "insert gradient: -0.17678009329443925\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  1.33976716  46.51827724  99.72968951   0.          99.72968951\n",
      "  46.32739469 227.94826084  69.37454998 100.88785774  67.5559141\n",
      " 590.52691629  61.8437024  114.01513841  67.21719506 550.59474023]\n",
      "7-th iteration, loss: 0.22994236073746513, 14 gd steps\n",
      "insert gradient: -0.12551362339299632\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  1.29113709  48.49024088  82.98986866  11.17647573  80.6337236\n",
      "  49.81296137 113.42953297   0.         113.42953297  69.19543697\n",
      " 104.63929667  67.27713215 589.7059921   63.25876716 111.34397274\n",
      "  67.66589261 550.59474023]\n",
      "8-th iteration, loss: 0.2143159922889177, 67 gd steps\n",
      "insert gradient: -0.10087778021939145\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  2.62291144  54.7217822   91.62425194   3.78316242  90.23926513\n",
      "  51.92613603  78.92109764  22.29928861  91.28771741  71.79541734\n",
      " 107.47866595  67.12337013 169.13915555   0.         422.84788888\n",
      "  62.31189428 114.94899813  73.53650739 550.59474023]\n",
      "9-th iteration, loss: 0.20051385426361668, 25 gd steps\n",
      "insert gradient: -0.08592798720432755\n",
      "9-th iteration, new layer inserted. now 21 layers\n",
      "[  1.86935156  54.41926392  94.58340045   2.0154162   92.55459959\n",
      "  55.30674438  83.28847418  20.91952238  91.30689281  70.51710043\n",
      " 111.91112148  61.91283616 147.23627049  21.49325201 103.48689262\n",
      "   0.         310.46067786  61.65306943 116.42685362  75.3039969\n",
      " 550.59474023]\n",
      "10-th iteration, loss: 0.18570126745976243, 31 gd steps\n",
      "insert gradient: -0.06350647741636535\n",
      "10-th iteration, new layer inserted. now 23 layers\n",
      "[  2.40506253  61.35933813  96.52927497   2.62045694  92.22470631\n",
      "  53.11719859  85.62668722  23.47353423  84.85154561  70.1346499\n",
      " 115.18920307  62.43830731 141.34556024  43.05960692  76.75888459\n",
      "  13.7367855  239.72729581   0.          53.2727324   56.95823469\n",
      " 122.56176643  76.71069654 550.59474023]\n",
      "11-th iteration, loss: 0.17455441716374537, 26 gd steps\n",
      "insert gradient: -0.029024866253004617\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  1.67872123  60.00922156  98.71509962   3.88023718  92.73547888\n",
      "  55.09381483  86.8171947   23.79038008  81.07787495  71.79785021\n",
      " 117.66420564  59.9185855  132.27366852  64.02283856  59.43262967\n",
      "  14.89548071 231.22130748  14.37338345  41.04575787  52.46668268\n",
      " 130.23533102  72.01037083 550.59474023]\n",
      "12-th iteration, loss: 0.17232044683630762, 52 gd steps\n",
      "insert gradient: -0.03622778547453111\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  1.41582724  58.23249579  97.01428397   5.28291137  92.94418085\n",
      "  57.24912429  87.07482052  23.42003729  81.88928489  70.24220404\n",
      " 120.7019481   60.51870713 130.22747629  66.83205805  66.70187525\n",
      "  12.61776858 224.93656233  23.21689557  33.87456388  48.27044367\n",
      " 143.45825808  64.78667424 165.17842207   0.         385.41631816]\n",
      "13-th iteration, loss: 0.16070601790053227, 30 gd steps\n",
      "insert gradient: -0.04085567588337344\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "[  1.76079239  56.92697814  94.21881317   9.71734409  89.08250038\n",
      "  57.05415979  94.9185907   23.91014666  74.6083441   75.40975428\n",
      " 116.06083575  64.71039441 128.20610203  66.7865215   77.81429542\n",
      "   4.40795024 145.79986216   0.          72.89993108  33.01097159\n",
      "  37.1502308   41.61273088 154.48877593  61.22688585 169.04719945\n",
      "  25.93749835 385.41631816]\n",
      "14-th iteration, loss: 0.14916715310539727, 23 gd steps\n",
      "insert gradient: -0.06787629648545306\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  1.93352864  60.53723694  93.64071436  13.8479984   86.11754626\n",
      "  56.3209037   95.11776847  28.606661    75.20989277  73.84035291\n",
      " 122.06086378  67.01242041 122.44912749  72.10744875 209.7896399\n",
      "  20.62013187  48.64804206  48.82960113  53.18717392  26.86862373\n",
      "  88.32052992   0.          70.65642393  63.80845522 162.85717876\n",
      "  43.68938331 385.41631816]\n",
      "15-th iteration, loss: 0.12872034909504879, 35 gd steps\n",
      "insert gradient: -0.025696297824143215\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[  0.99834014  58.63518385  95.84614475  12.36847829  83.05716574\n",
      "  56.35324823  91.39081984  37.66963069  79.99548405  68.36228075\n",
      " 124.47202756  64.22063236 127.2542215   78.15366371 188.69431532\n",
      "  57.22575682   6.34714827  37.74021989  88.15593436  30.14706865\n",
      "  78.27366645   8.33592418  61.23514132  47.94089136 160.73226569\n",
      "  61.99693558 240.88519885   0.         144.53111931]\n",
      "16-th iteration, loss: 0.11537964106089105, 23 gd steps\n",
      "insert gradient: -0.03708834599585877\n",
      "16-th iteration, new layer inserted. now 31 layers\n",
      "[  1.06932967  59.48568837  94.10188274  14.99123957  78.50043319\n",
      "  55.08640313  89.94319339  43.55908112  78.31665631  69.95162609\n",
      " 119.97344077  66.3315351  125.06137966  79.99310508 176.39639551\n",
      "  69.95647818   1.95132631  23.72629483 117.14613755  44.9968258\n",
      "  63.35053287  22.1108941   50.12165204  24.70144735 127.87220121\n",
      "   0.          42.62406707  62.80092476 223.39764909  27.88926222\n",
      " 144.53111931]\n",
      "17-th iteration, loss: 0.10129670110012669, 25 gd steps\n",
      "insert gradient: -0.015394089463607341\n",
      "17-th iteration, new layer inserted. now 33 layers\n",
      "[5.50393815e-01 5.63333856e+01 9.63489557e+01 1.55859495e+01\n",
      " 7.25043646e+01 5.60240973e+01 9.15132188e+01 4.44661004e+01\n",
      " 8.39471080e+01 5.82972148e+01 1.28266138e+02 6.79017806e+01\n",
      " 1.20747210e+02 8.46433894e+01 1.83582018e+02 6.44980697e+01\n",
      " 7.33929738e+00 8.92538784e+00 1.30999820e+02 5.95458510e+01\n",
      " 6.14394946e+01 2.65663920e+01 5.83598061e+01 1.48790603e+01\n",
      " 1.13917581e+02 0.00000000e+00 2.13162821e-14 2.22920017e+01\n",
      " 2.69326073e+01 4.47954021e+01 2.11454824e+02 4.89216580e+01\n",
      " 1.44531119e+02]\n",
      "18-th iteration, loss: 0.09362366789724949, 34 gd steps\n",
      "insert gradient: -0.0055568736831708085\n",
      "18-th iteration, new layer inserted. now 31 layers\n",
      "[1.04821532e+00 5.82090787e+01 1.01894884e+02 0.00000000e+00\n",
      " 1.77635684e-14 1.47579638e+01 6.57932342e+01 5.78036753e+01\n",
      " 9.35066946e+01 4.50217483e+01 8.31159209e+01 6.13134975e+01\n",
      " 1.23451716e+02 6.84400697e+01 1.15443116e+02 9.12856854e+01\n",
      " 1.83139519e+02 6.72231615e+01 1.49795534e+02 6.66363918e+01\n",
      " 6.08000382e+01 2.30098570e+01 7.17009341e+01 1.19878589e+01\n",
      " 1.07947942e+02 3.32010164e+01 2.75663344e+01 4.09613672e+01\n",
      " 1.91473945e+02 5.84199561e+01 1.44531119e+02]\n",
      "19-th iteration, loss: 0.08868245446056572, 113 gd steps\n",
      "insert gradient: -0.00397338831762111\n",
      "19-th iteration, new layer inserted. now 29 layers\n",
      "[  0.92442719  59.01943379 103.12346558  20.16410261  54.54844159\n",
      "  59.2702131   93.39658687  47.01937529  86.6703887   61.59535913\n",
      " 125.25996208  68.38094364 114.01271836  96.13248698 187.10629431\n",
      "  62.65322341 153.10801392  62.87262201  87.30453683   5.16823262\n",
      " 100.57958852  26.89916552  43.08350854  49.55254616  40.88915454\n",
      "  30.80638321 188.38928423  76.53567466 144.53111931]\n",
      "20-th iteration, loss: 0.08733686465144201, 30 gd steps\n",
      "insert gradient: -0.0036967458633469547\n",
      "20-th iteration, new layer inserted. now 31 layers\n",
      "[  0.94949405  58.70493949 105.17087319  28.4370733   39.64423775\n",
      "  59.79936879  93.94856226  47.05274989  85.24051883  62.54234499\n",
      " 123.66595845  68.93633405 113.37152692  97.9416374  187.59811222\n",
      "  60.86387323 154.843539    60.19938426  96.83933005   2.78610734\n",
      "  91.71423418  38.11600247  22.17321246  57.35530381  49.73877276\n",
      "  23.88011823  96.79948582   0.          96.79948582  74.8818289\n",
      " 144.53111931]\n",
      "21-th iteration, loss: 0.07627902233803813, 53 gd steps\n",
      "insert gradient: -0.03932128135942231\n",
      "21-th iteration, new layer inserted. now 31 layers\n",
      "[  2.74682323  61.51713714  99.35370794  45.4364045   32.47708354\n",
      "  53.31784276  93.67187014  51.95138877  88.19741157  59.72849588\n",
      " 131.17257851  70.8035114  119.7446334   83.86419079 128.35323316\n",
      "   0.          77.0119399   69.76264012 152.55496133  56.9886103\n",
      " 181.67122726  51.39555419   9.13139958  52.72367874 103.29109951\n",
      "  13.74441195  41.90640958  55.45293102  42.81284115 109.44908753\n",
      " 144.53111931]\n",
      "22-th iteration, loss: 0.06852661782394737, 17 gd steps\n",
      "insert gradient: -0.015292029948269635\n",
      "22-th iteration, new layer inserted. now 31 layers\n",
      "[  0.94678991  53.41562495  97.70078774  49.01117737  36.42786512\n",
      "  49.57105052  96.27355669  54.16741275  95.27693018  62.43015626\n",
      " 133.27503369  63.7978357  117.92237687  88.52225037 129.7712422\n",
      "   0.70856369  77.84253309  69.26727778 154.98274846  58.80590837\n",
      " 177.86715566  56.559848    13.4901827   44.8062484  104.17504801\n",
      "  17.8521417   32.18409814  59.28038511  53.36375615 110.85001255\n",
      " 144.53111931]\n",
      "23-th iteration, loss: 0.06442172288577258, 24 gd steps\n",
      "insert gradient: -0.007011551696345661\n",
      "23-th iteration, new layer inserted. now 33 layers\n",
      "[  1.35821079  55.70781309  98.66687489  50.56741582  38.33372039\n",
      "  47.08210038  94.48365748  54.8403827   98.19406234  64.35693048\n",
      " 131.92918471  62.82464703 117.19192726  89.40592159 129.44403674\n",
      "   2.66960648  32.46491989   0.          43.28655986  69.83183804\n",
      " 156.72733529  59.22951106 178.12191643  58.60643473  21.76628682\n",
      "  37.71704996 104.95340604  23.05791973  25.77421459  58.47560384\n",
      "  63.42134103 109.76780272 144.53111931]\n",
      "24-th iteration, loss: 0.05079926724457109, 694 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "24-th iteration, new layer inserted. now 33 layers\n",
      "[  1.2871334   57.20352877  98.7841779   51.55313945  53.71766242\n",
      "  41.8708965   92.20146686  55.95506852  99.90437342  64.02535196\n",
      " 135.57304953  63.30554538 115.79373438  82.11009247 132.39023726\n",
      "  49.22193411  14.5900745   54.91598732 182.48881205  55.03959773\n",
      " 173.63890894  56.86081582  74.15996473   7.02125413 108.61221626\n",
      "  42.21708285  15.52447745  54.3960833   94.9997623   29.14573048\n",
      "   0.          72.86432619 144.53111931]\n",
      "25-th iteration, loss: 0.048204736762968416, 115 gd steps\n",
      "insert gradient: -0.0005941490667928413\n",
      "25-th iteration, new layer inserted. now 35 layers\n",
      "[  1.37247357  57.39547681  98.8439491   51.34529089  60.95837273\n",
      "  41.17176972  91.19461299  55.41524409  99.80612331  64.45794146\n",
      " 136.6141691   63.89152389 118.37762244  82.12285781 122.8860702\n",
      "  65.55587888   8.6173372   40.01468291 114.06029782   0.\n",
      "  85.54522336  54.10312968 173.3313415   54.47219811  91.49533383\n",
      "   1.36951396  96.16596535  51.72971446  22.18588033  42.43870499\n",
      " 108.09333777  14.30803301  33.79978632  61.79899696 144.53111931]\n",
      "26-th iteration, loss: 0.04815095762962684, 36 gd steps\n",
      "insert gradient: -0.00028174124311988003\n",
      "26-th iteration, new layer inserted. now 35 layers\n",
      "[  1.30988389  57.44286606  98.88296251  51.53906601  61.27769347\n",
      "  41.225935    91.08376851  55.46223173  99.83936661  64.22743091\n",
      " 137.09086659  64.14359445 118.27247344  82.14023167 123.55277909\n",
      "  65.8611044   10.54820969  37.75709118 108.42130167   3.04436976\n",
      "  83.1802946   55.29529688 173.16889958  54.50205729  92.58143798\n",
      "   0.59473115  96.41456169  52.85706184  21.92961247  41.8471729\n",
      " 108.83949566  13.85756676  33.79947474  62.86445441 144.53111931]\n",
      "27-th iteration, loss: 0.048135099850117234, 30 gd steps\n",
      "insert gradient: -0.00016511865127494032\n",
      "27-th iteration, new layer inserted. now 35 layers\n",
      "[  1.33354525  57.58486394  99.00252879  51.55157969  61.75591567\n",
      "  41.13748904  90.94209674  55.52537518  99.90155429  64.25553274\n",
      " 137.23946352  64.27213057 118.27797442  82.14691281 123.8501702\n",
      "  65.76321632  11.31805131  37.36703542 105.46509428   3.91637309\n",
      "  83.93462524  55.14648668 173.40134565  54.64334444  93.07910818\n",
      "   0.28432735  96.71846287  53.03196734  22.21964968  41.44735498\n",
      " 109.4112995   13.7180003   33.62954129  63.3528052  144.53111931]\n",
      "28-th iteration, loss: 0.04812996399923923, 34 gd steps\n",
      "insert gradient: -5.720017145345133e-05\n",
      "28-th iteration, new layer inserted. now 35 layers\n",
      "[1.32596003e+00 5.76030098e+01 9.90395769e+01 5.16056075e+01\n",
      " 6.19979231e+01 4.10642646e+01 9.08300670e+01 5.55221635e+01\n",
      " 9.99962453e+01 6.42692473e+01 1.37321249e+02 6.43348495e+01\n",
      " 1.18313532e+02 8.22041738e+01 1.24028668e+02 6.57562081e+01\n",
      " 1.15190781e+01 3.72966343e+01 1.03562180e+02 4.32495850e+00\n",
      " 8.45803377e+01 5.51739371e+01 1.73595508e+02 5.45458174e+01\n",
      " 9.33523475e+01 1.34814117e-01 9.69115765e+01 5.30940065e+01\n",
      " 2.24337929e+01 4.12027521e+01 1.09834157e+02 1.36418017e+01\n",
      " 3.34747904e+01 6.36199722e+01 1.44531119e+02]\n",
      "29-th iteration, loss: 0.04812848737497815, 52 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.992466140890363e-06\n",
      "29-th iteration, new layer inserted. now 35 layers\n",
      "[1.32066081e+00 5.76146522e+01 9.90598270e+01 5.16210673e+01\n",
      " 6.21344362e+01 4.10174950e+01 9.07668899e+01 5.55361051e+01\n",
      " 1.00053760e+02 6.42600890e+01 1.37361842e+02 6.43472497e+01\n",
      " 1.18345981e+02 8.22040244e+01 1.24123305e+02 6.57452701e+01\n",
      " 1.15246914e+01 3.74541684e+01 1.01795720e+02 4.51152368e+00\n",
      " 8.56154801e+01 5.51363653e+01 1.73693922e+02 5.45046379e+01\n",
      " 9.35381260e+01 5.60698288e-02 9.70252331e+01 5.30606577e+01\n",
      " 2.25797270e+01 4.10507764e+01 1.10246378e+02 1.36072148e+01\n",
      " 3.32650427e+01 6.37542105e+01 1.44531119e+02]\n",
      "30-th iteration, loss: 0.04812848735815501, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.933095811335433e-06\n",
      "30-th iteration, new layer inserted. now 37 layers\n",
      "[1.32066038e+00 5.76146504e+01 9.90598264e+01 5.16210691e+01\n",
      " 6.21344392e+01 4.10174996e+01 9.07668908e+01 5.55361061e+01\n",
      " 1.00053762e+02 6.42600944e+01 1.37361844e+02 6.43472550e+01\n",
      " 1.18345982e+02 8.22040260e+01 1.24123304e+02 6.57452687e+01\n",
      " 1.15246886e+01 3.74541673e+01 1.01795719e+02 4.51151637e+00\n",
      " 8.56154829e+01 5.51363723e+01 1.73693927e+02 0.00000000e+00\n",
      " 1.42108547e-14 5.45046479e+01 9.35381279e+01 5.60725942e-02\n",
      " 9.70252346e+01 5.30606619e+01 2.25797267e+01 4.10507775e+01\n",
      " 1.10246378e+02 1.36072127e+01 3.32650413e+01 6.37542131e+01\n",
      " 1.44531119e+02]\n",
      "31-th iteration, loss: 0.048128487336403285, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.840748472928834e-06\n",
      "31-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065996e+00 5.76146487e+01 9.90598258e+01 5.16210710e+01\n",
      " 6.21344422e+01 4.10175042e+01 9.07668917e+01 5.55361071e+01\n",
      " 1.00053764e+02 6.42600999e+01 1.37361846e+02 6.43472602e+01\n",
      " 1.18345983e+02 8.22040275e+01 1.24123304e+02 6.57452674e+01\n",
      " 1.15246858e+01 3.74541663e+01 1.01795719e+02 4.51150911e+00\n",
      " 8.56154858e+01 5.51363794e+01 1.73693932e+02 9.90275063e-06\n",
      " 5.17314603e-06 0.00000000e+00 2.11758237e-22 5.45046578e+01\n",
      " 9.35381297e+01 5.60753175e-02 9.70252362e+01 5.30606660e+01\n",
      " 2.25797263e+01 4.10507786e+01 1.10246378e+02 1.36072106e+01\n",
      " 3.32650398e+01 6.37542158e+01 1.44531119e+02]\n",
      "32-th iteration, loss: 0.048128487309980504, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.716390658240813e-06\n",
      "32-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065953e+00 5.76146469e+01 9.90598252e+01 5.16210728e+01\n",
      " 6.21344451e+01 4.10175087e+01 9.07668925e+01 5.55361080e+01\n",
      " 1.00053766e+02 6.42601052e+01 1.37361848e+02 6.43472654e+01\n",
      " 1.18345984e+02 8.22040290e+01 1.24123303e+02 6.57452661e+01\n",
      " 1.15246830e+01 3.74541652e+01 1.01795718e+02 4.51150190e+00\n",
      " 8.56154886e+01 5.51363863e+01 1.73693937e+02 1.97016930e-05\n",
      " 1.03073139e-05 9.80053543e-06 5.13416785e-06 5.45046676e+01\n",
      " 9.35381315e+01 5.60779833e-02 9.70252376e+01 5.30606701e+01\n",
      " 2.25797259e+01 4.10507797e+01 1.10246378e+02 1.36072085e+01\n",
      " 3.32650383e+01 6.37542185e+01 1.44531119e+02]\n",
      "33-th iteration, loss: 0.04812848728412667, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.593748717659079e-06\n",
      "33-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065911e+00 5.76146452e+01 9.90598246e+01 5.16210746e+01\n",
      " 6.21344480e+01 4.10175131e+01 9.07668934e+01 5.55361089e+01\n",
      " 1.00053768e+02 6.42601106e+01 1.37361850e+02 6.43472706e+01\n",
      " 1.18345985e+02 8.22040305e+01 1.24123303e+02 6.57452648e+01\n",
      " 1.15246802e+01 3.74541642e+01 1.01795717e+02 4.51149473e+00\n",
      " 8.56154915e+01 5.51363931e+01 1.73693943e+02 2.93737134e-05\n",
      " 1.53950576e-05 1.94757299e-05 1.02188935e-05 5.45046773e+01\n",
      " 9.35381333e+01 5.60805875e-02 9.70252391e+01 5.30606741e+01\n",
      " 2.25797256e+01 4.10507807e+01 1.10246378e+02 1.36072065e+01\n",
      " 3.32650369e+01 6.37542211e+01 1.44531119e+02]\n",
      "34-th iteration, loss: 0.048128487258827644, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.47279684783049e-06\n",
      "34-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065868e+00 5.76146434e+01 9.90598240e+01 5.16210763e+01\n",
      " 6.21344509e+01 4.10175176e+01 9.07668942e+01 5.55361098e+01\n",
      " 1.00053770e+02 6.42601159e+01 1.37361852e+02 6.43472757e+01\n",
      " 1.18345986e+02 8.22040320e+01 1.24123302e+02 6.57452635e+01\n",
      " 1.15246775e+01 3.74541632e+01 1.01795716e+02 4.51148759e+00\n",
      " 8.56154943e+01 5.51363999e+01 1.73693948e+02 3.89205478e-05\n",
      " 2.04370446e-05 2.90273051e-05 1.52548830e-05 5.45046868e+01\n",
      " 9.35381350e+01 5.60831310e-02 9.70252405e+01 5.30606781e+01\n",
      " 2.25797252e+01 4.10507817e+01 1.10246378e+02 1.36072044e+01\n",
      " 3.32650354e+01 6.37542237e+01 1.44531119e+02]\n",
      "35-th iteration, loss: 0.048128487234069685, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.35350966809782e-06\n",
      "35-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065825e+00 5.76146416e+01 9.90598234e+01 5.16210781e+01\n",
      " 6.21344537e+01 4.10175219e+01 9.07668950e+01 5.55361107e+01\n",
      " 1.00053772e+02 6.42601212e+01 1.37361855e+02 6.43472807e+01\n",
      " 1.18345987e+02 8.22040334e+01 1.24123302e+02 6.57452622e+01\n",
      " 1.15246747e+01 3.74541621e+01 1.01795715e+02 4.51148050e+00\n",
      " 8.56154972e+01 5.51364066e+01 1.73693953e+02 4.83439061e-05\n",
      " 2.54339316e-05 3.84569570e-05 2.02428311e-05 5.45046963e+01\n",
      " 9.35381366e+01 5.60856150e-02 9.70252418e+01 5.30606820e+01\n",
      " 2.25797247e+01 4.10507827e+01 1.10246378e+02 1.36072023e+01\n",
      " 3.32650339e+01 6.37542263e+01 1.44531119e+02]\n",
      "36-th iteration, loss: 0.0481284872098394, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.235862213564069e-06\n",
      "36-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065781e+00 5.76146399e+01 9.90598228e+01 5.16210798e+01\n",
      " 6.21344565e+01 4.10175263e+01 9.07668957e+01 5.55361116e+01\n",
      " 1.00053774e+02 6.42601264e+01 1.37361857e+02 6.43472858e+01\n",
      " 1.18345988e+02 8.22040348e+01 1.24123301e+02 6.57452609e+01\n",
      " 1.15246720e+01 3.74541611e+01 1.01795714e+02 4.51147344e+00\n",
      " 8.56155000e+01 5.51364132e+01 1.73693958e+02 5.76454727e-05\n",
      " 3.03863645e-05 4.77663559e-05 2.51834212e-05 5.45047056e+01\n",
      " 9.35381383e+01 5.60880405e-02 9.70252431e+01 5.30606859e+01\n",
      " 2.25797243e+01 4.10507836e+01 1.10246378e+02 1.36072002e+01\n",
      " 3.32650324e+01 6.37542289e+01 1.44531119e+02]\n",
      "37-th iteration, loss: 0.048128487186123746, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.119829927218757e-06\n",
      "37-th iteration, new layer inserted. now 39 layers\n",
      "[1.32065738e+00 5.76146381e+01 9.90598221e+01 5.16210815e+01\n",
      " 6.21344594e+01 4.10175306e+01 9.07668965e+01 5.55361124e+01\n",
      " 1.00053776e+02 6.42601316e+01 1.37361859e+02 6.43472907e+01\n",
      " 1.18345989e+02 8.22040362e+01 1.24123301e+02 6.57452596e+01\n",
      " 1.15246694e+01 3.74541601e+01 1.01795713e+02 4.51146642e+00\n",
      " 8.56155028e+01 5.51364197e+01 1.73693963e+02 6.68269066e-05\n",
      " 3.52949787e-05 5.69571473e-05 3.00773257e-05 5.45047148e+01\n",
      " 9.35381398e+01 5.60904085e-02 9.70252444e+01 5.30606897e+01\n",
      " 2.25797239e+01 4.10507845e+01 1.10246378e+02 1.36071981e+01\n",
      " 3.32650310e+01 6.37542314e+01 1.44531119e+02]\n",
      "38-th iteration, loss: 0.04812848716291003, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -9.00538865289942e-06\n",
      "38-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065694e+00 5.76146363e+01 9.90598215e+01 5.16210832e+01\n",
      " 6.21344622e+01 4.10175348e+01 9.07668972e+01 5.55361132e+01\n",
      " 1.00053778e+02 6.42601368e+01 1.37361861e+02 6.43472956e+01\n",
      " 1.18345990e+02 8.22040375e+01 1.24123300e+02 6.57452582e+01\n",
      " 1.15246667e+01 3.74541590e+01 1.01795712e+02 4.51145945e+00\n",
      " 8.56155056e+01 5.51364261e+01 1.73693967e+02 7.58898421e-05\n",
      " 4.01603993e-05 6.60309521e-05 3.49252061e-05 0.00000000e+00\n",
      " 1.69406589e-21 5.45047239e+01 9.35381414e+01 5.60927200e-02\n",
      " 9.70252456e+01 5.30606934e+01 2.25797234e+01 4.10507854e+01\n",
      " 1.10246378e+02 1.36071960e+01 3.32650295e+01 6.37542339e+01\n",
      " 1.44531119e+02]\n",
      "39-th iteration, loss: 0.048128487135947313, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.862337910770992e-06\n",
      "39-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065651e+00 5.76146345e+01 9.90598209e+01 5.16210849e+01\n",
      " 6.21344649e+01 4.10175390e+01 9.07668979e+01 5.55361140e+01\n",
      " 1.00053780e+02 6.42601419e+01 1.37361863e+02 6.43473005e+01\n",
      " 1.18345991e+02 8.22040389e+01 1.24123299e+02 6.57452569e+01\n",
      " 1.15246641e+01 3.74541580e+01 1.01795711e+02 4.51145250e+00\n",
      " 8.56155084e+01 5.51364324e+01 1.73693972e+02 8.48265352e-05\n",
      " 4.49774968e-05 7.49800127e-05 3.97219691e-05 8.95981611e-06\n",
      " 4.79676297e-06 5.45047328e+01 9.35381429e+01 5.60949662e-02\n",
      " 9.70252468e+01 5.30606971e+01 2.25797229e+01 4.10507862e+01\n",
      " 1.10246377e+02 1.36071940e+01 3.32650280e+01 6.37542364e+01\n",
      " 1.44531119e+02]\n",
      "40-th iteration, loss: 0.048128487109689554, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.721778459069843e-06\n",
      "40-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065607e+00 5.76146327e+01 9.90598202e+01 5.16210865e+01\n",
      " 6.21344677e+01 4.10175432e+01 9.07668986e+01 5.55361147e+01\n",
      " 1.00053782e+02 6.42601470e+01 1.37361865e+02 6.43473053e+01\n",
      " 1.18345992e+02 8.22040402e+01 1.24123299e+02 6.57452555e+01\n",
      " 1.15246614e+01 3.74541570e+01 1.01795710e+02 4.51144558e+00\n",
      " 8.56155112e+01 5.51364386e+01 1.73693977e+02 9.36165909e-05\n",
      " 4.97397986e-05 8.37839196e-05 4.44611802e-05 1.77759556e-05\n",
      " 9.53321488e-06 5.45047416e+01 9.35381443e+01 5.60971437e-02\n",
      " 9.70252479e+01 5.30607007e+01 2.25797224e+01 4.10507870e+01\n",
      " 1.10246377e+02 1.36071919e+01 3.32650265e+01 6.37542389e+01\n",
      " 1.44531119e+02]\n",
      "41-th iteration, loss: 0.04812848708411489, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.583663131915941e-06\n",
      "41-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065563e+00 5.76146309e+01 9.90598196e+01 5.16210882e+01\n",
      " 6.21344705e+01 4.10175473e+01 9.07668992e+01 5.55361154e+01\n",
      " 1.00053784e+02 6.42601521e+01 1.37361867e+02 6.43473101e+01\n",
      " 1.18345993e+02 8.22040414e+01 1.24123298e+02 6.57452541e+01\n",
      " 1.15246588e+01 3.74541559e+01 1.01795709e+02 4.51143870e+00\n",
      " 8.56155140e+01 5.51364447e+01 1.73693982e+02 1.02262537e-04\n",
      " 5.44483198e-05 9.24451834e-05 4.91438993e-05 2.64509117e-05\n",
      " 1.42104597e-05 5.45047503e+01 9.35381458e+01 5.60992539e-02\n",
      " 9.70252490e+01 5.30607042e+01 2.25797219e+01 4.10507877e+01\n",
      " 1.10246377e+02 1.36071898e+01 3.32650250e+01 6.37542413e+01\n",
      " 1.44531119e+02]\n",
      "42-th iteration, loss: 0.048128487059202175, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.447945722740218e-06\n",
      "42-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065519e+00 5.76146291e+01 9.90598190e+01 5.16210898e+01\n",
      " 6.21344732e+01 4.10175514e+01 9.07668999e+01 5.55361162e+01\n",
      " 1.00053786e+02 6.42601571e+01 1.37361869e+02 6.43473148e+01\n",
      " 1.18345994e+02 8.22040427e+01 1.24123298e+02 6.57452528e+01\n",
      " 1.15246562e+01 3.74541549e+01 1.01795708e+02 4.51143184e+00\n",
      " 8.56155167e+01 5.51364507e+01 1.73693986e+02 1.10766853e-04\n",
      " 5.91040544e-05 1.00966268e-04 5.37711644e-05 3.49871303e-05\n",
      " 1.88295789e-05 5.45047589e+01 9.35381471e+01 5.61012983e-02\n",
      " 9.70252500e+01 5.30607077e+01 2.25797214e+01 4.10507885e+01\n",
      " 1.10246377e+02 1.36071877e+01 3.32650235e+01 6.37542438e+01\n",
      " 1.44531119e+02]\n",
      "43-th iteration, loss: 0.04812848703493095, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.314580963201983e-06\n",
      "43-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065475e+00 5.76146273e+01 9.90598183e+01 5.16210914e+01\n",
      " 6.21344759e+01 4.10175554e+01 9.07669005e+01 5.55361169e+01\n",
      " 1.00053788e+02 6.42601621e+01 1.37361871e+02 6.43473195e+01\n",
      " 1.18345995e+02 8.22040439e+01 1.24123297e+02 6.57452514e+01\n",
      " 1.15246537e+01 3.74541538e+01 1.01795707e+02 4.51142501e+00\n",
      " 8.56155195e+01 5.51364566e+01 1.73693991e+02 1.19131973e-04\n",
      " 6.37079763e-05 1.09349589e-04 5.83439925e-05 4.33870112e-05\n",
      " 2.33916321e-05 5.45047673e+01 9.35381484e+01 5.61032782e-02\n",
      " 9.70252510e+01 5.30607111e+01 2.25797208e+01 4.10507891e+01\n",
      " 1.10246376e+02 1.36071856e+01 3.32650220e+01 6.37542461e+01\n",
      " 1.44531119e+02]\n",
      "44-th iteration, loss: 0.0481284870112815, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.183524503240214e-06\n",
      "44-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065430e+00 5.76146255e+01 9.90598177e+01 5.16210930e+01\n",
      " 6.21344786e+01 4.10175594e+01 9.07669010e+01 5.55361175e+01\n",
      " 1.00053790e+02 6.42601671e+01 1.37361873e+02 6.43473242e+01\n",
      " 1.18345996e+02 8.22040450e+01 1.24123297e+02 6.57452500e+01\n",
      " 1.15246511e+01 3.74541527e+01 1.01795706e+02 4.51141822e+00\n",
      " 8.56155222e+01 5.51364624e+01 1.73693996e+02 1.27360283e-04\n",
      " 6.82610393e-05 1.17597519e-04 6.28633796e-05 5.16529089e-05\n",
      " 2.78976575e-05 5.45047755e+01 9.35381497e+01 5.61051951e-02\n",
      " 9.70252520e+01 5.30607145e+01 2.25797203e+01 4.10507898e+01\n",
      " 1.10246376e+02 1.36071835e+01 3.32650205e+01 6.37542485e+01\n",
      " 1.44531119e+02]\n",
      "45-th iteration, loss: 0.04812848698823473, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -8.05473289160368e-06\n",
      "45-th iteration, new layer inserted. now 41 layers\n",
      "[1.32065386e+00 5.76146237e+01 9.90598171e+01 5.16210946e+01\n",
      " 6.21344813e+01 4.10175634e+01 9.07669016e+01 5.55361182e+01\n",
      " 1.00053792e+02 6.42601720e+01 1.37361875e+02 6.43473288e+01\n",
      " 1.18345997e+02 8.22040462e+01 1.24123296e+02 6.57452485e+01\n",
      " 1.15246486e+01 3.74541517e+01 1.01795705e+02 4.51141145e+00\n",
      " 8.56155250e+01 5.51364681e+01 1.73694000e+02 1.35454125e-04\n",
      " 7.27641776e-05 1.25712383e-04 6.73303017e-05 5.97871337e-05\n",
      " 3.23486722e-05 5.45047837e+01 9.35381509e+01 5.61070504e-02\n",
      " 9.70252529e+01 5.30607178e+01 2.25797197e+01 4.10507904e+01\n",
      " 1.10246376e+02 1.36071814e+01 3.32650191e+01 6.37542508e+01\n",
      " 1.44531119e+02]\n",
      "46-th iteration, loss: 0.04812848696577219, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.928163556095236e-06\n",
      "46-th iteration, new layer inserted. now 43 layers\n",
      "[1.32065341e+00 5.76146219e+01 9.90598164e+01 5.16210961e+01\n",
      " 6.21344840e+01 4.10175673e+01 9.07669022e+01 5.55361188e+01\n",
      " 1.00053794e+02 6.42601769e+01 1.37361877e+02 6.43473333e+01\n",
      " 1.18345998e+02 8.22040473e+01 1.24123296e+02 6.57452471e+01\n",
      " 1.15246461e+01 3.74541506e+01 1.01795704e+02 4.51140472e+00\n",
      " 8.56155277e+01 5.51364737e+01 1.73694005e+02 1.43415799e-04\n",
      " 7.72183062e-05 1.33696466e-04 7.17457149e-05 6.77919521e-05\n",
      " 3.67456727e-05 0.00000000e+00 1.69406589e-21 5.45047917e+01\n",
      " 9.35381521e+01 5.61088452e-02 9.70252537e+01 5.30607210e+01\n",
      " 2.25797191e+01 4.10507910e+01 1.10246376e+02 1.36071793e+01\n",
      " 3.32650176e+01 6.37542532e+01 1.44531119e+02]\n",
      "47-th iteration, loss: 0.04812848694056505, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.777128163231892e-06\n",
      "47-th iteration, new layer inserted. now 43 layers\n",
      "[1.32065297e+00 5.76146201e+01 9.90598158e+01 5.16210977e+01\n",
      " 6.21344866e+01 4.10175712e+01 9.07669027e+01 5.55361194e+01\n",
      " 1.00053796e+02 6.42601817e+01 1.37361879e+02 6.43473378e+01\n",
      " 1.18345999e+02 8.22040484e+01 1.24123295e+02 6.57452457e+01\n",
      " 1.15246436e+01 3.74541495e+01 1.01795703e+02 4.51139801e+00\n",
      " 8.56155304e+01 5.51364792e+01 1.73694009e+02 1.51239286e-04\n",
      " 8.16192307e-05 1.41543734e-04 7.61054647e-05 7.56613156e-05\n",
      " 4.10845445e-05 7.88068009e-06 4.33887181e-06 5.45047996e+01\n",
      " 9.35381532e+01 5.61105722e-02 9.70252545e+01 5.30607241e+01\n",
      " 2.25797185e+01 4.10507915e+01 1.10246375e+02 1.36071772e+01\n",
      " 3.32650161e+01 6.37542555e+01 1.44531119e+02]\n",
      "48-th iteration, loss: 0.04812848691612373, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.6292408515611684e-06\n",
      "48-th iteration, new layer inserted. now 45 layers\n",
      "[1.32065252e+00 5.76146183e+01 9.90598152e+01 5.16210992e+01\n",
      " 6.21344893e+01 4.10175751e+01 9.07669032e+01 5.55361200e+01\n",
      " 1.00053798e+02 6.42601866e+01 1.37361881e+02 6.43473422e+01\n",
      " 1.18346000e+02 8.22040494e+01 1.24123295e+02 6.57452442e+01\n",
      " 1.15246411e+01 3.74541485e+01 1.01795702e+02 4.51139132e+00\n",
      " 8.56155331e+01 5.51364845e+01 1.73694014e+02 1.58907425e-04\n",
      " 8.59615979e-05 1.49237010e-04 8.04042403e-05 8.33780296e-05\n",
      " 4.53600184e-05 1.56100471e-05 8.61191864e-06 0.00000000e+00\n",
      " 4.23516474e-22 5.45048073e+01 9.35381543e+01 5.61122289e-02\n",
      " 9.70252553e+01 5.30607272e+01 2.25797179e+01 4.10507920e+01\n",
      " 1.10246375e+02 1.36071751e+01 3.32650145e+01 6.37542577e+01\n",
      " 1.44531119e+02]\n",
      "49-th iteration, loss: 0.048128486889358675, 0 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.458806166564958e-06\n",
      "49-th iteration, new layer inserted. now 47 layers\n",
      "[1.32065207e+00 5.76146165e+01 9.90598145e+01 5.16211008e+01\n",
      " 6.21344919e+01 4.10175789e+01 9.07669037e+01 5.55361206e+01\n",
      " 1.00053800e+02 6.42601914e+01 1.37361883e+02 6.43473466e+01\n",
      " 1.18346001e+02 8.22040505e+01 1.24123294e+02 6.57452427e+01\n",
      " 1.15246386e+01 3.74541474e+01 1.01795701e+02 4.51138465e+00\n",
      " 8.56155357e+01 5.51364898e+01 1.73694018e+02 1.66415457e-04\n",
      " 9.02418093e-05 1.56771516e-04 8.46384905e-05 9.09372981e-05\n",
      " 4.95685901e-05 2.31832855e-05 1.28156828e-05 7.57589072e-06\n",
      " 4.20376418e-06 0.00000000e+00 2.11758237e-22 5.45048149e+01\n",
      " 9.35381553e+01 5.61138087e-02 9.70252560e+01 5.30607302e+01\n",
      " 2.25797172e+01 4.10507925e+01 1.10246375e+02 1.36071729e+01\n",
      " 3.32650130e+01 6.37542599e+01 1.44531119e+02]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.5339312369628484\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  42.88510827    0.         2092.79328369]\n",
      "1-th iteration, loss: 0.7461754952690046, 11 gd steps\n",
      "insert gradient: -0.6212822648909352\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  41.80075262   62.36839932  229.69682382    0.         1863.09645987]\n",
      "2-th iteration, loss: 0.5280349808628184, 36 gd steps\n",
      "insert gradient: -0.3794523591301806\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[9.97451540e-01 5.33621227e+01 1.77541597e+02 5.64334571e+01\n",
      " 3.04179014e+02 0.00000000e+00 1.55891745e+03]\n",
      "3-th iteration, loss: 0.45088718484002976, 22 gd steps\n",
      "insert gradient: -0.5057375709024684\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   2.09582517   60.16410309  168.41805804   53.30730785  271.71820803\n",
      "   47.91187581  133.62149537    0.         1425.29595064]\n",
      "4-th iteration, loss: 0.37033800105112297, 58 gd steps\n",
      "insert gradient: -0.45062714813515714\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[  2.50323621  51.05810586 183.24882334  50.05684494 232.69157905\n",
      "  65.7860747  100.88832144  62.56281434 633.46486695   0.\n",
      " 791.83108369]\n",
      "5-th iteration, loss: 0.27639823134357683, 19 gd steps\n",
      "insert gradient: -0.24392913775837455\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  1.26694242  48.97066419 195.99329851  45.92182498 229.52220003\n",
      "  66.50089707 103.71205317  58.64351123 160.88932326   0.\n",
      " 429.03819536  65.93432417 791.83108369]\n",
      "6-th iteration, loss: 0.23917063323554347, 38 gd steps\n",
      "insert gradient: -0.12840914689866398\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.36630461  48.96902973 197.17248138  49.17330079  97.57954114\n",
      "   0.         121.97442643  71.68977035 107.54701007  60.23350925\n",
      " 122.00045233  39.68452061 399.01970191  66.41196629 791.83108369]\n",
      "7-th iteration, loss: 0.22993596519076917, 20 gd steps\n",
      "insert gradient: -0.106505150939818\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  3.56809296  53.16946705 195.14965005  52.34273511  82.33737201\n",
      "  12.20109392 105.48801289  72.11968021 107.36679149  62.84946514\n",
      " 116.59682909  40.56456155 396.4491535   68.59738112 148.46832819\n",
      "   0.         643.3627555 ]\n",
      "8-th iteration, loss: 0.2038783710513411, 28 gd steps\n",
      "insert gradient: -0.0891911227733697\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[4.58280633e+00 5.99150121e+01 2.01175994e+02 4.82222860e+01\n",
      " 8.52042245e+01 0.00000000e+00 1.24344979e-14 1.89152574e+01\n",
      " 9.29056663e+01 7.25871497e+01 1.09107114e+02 6.93054247e+01\n",
      " 1.19606018e+02 3.83754421e+01 3.97238542e+02 7.14422356e+01\n",
      " 1.50142933e+02 4.08074981e+01 6.43362755e+02]\n",
      "9-th iteration, loss: 0.19583124964169807, 25 gd steps\n",
      "insert gradient: -0.08395721430856855\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  2.59351962  53.99829116 202.34959077  51.95393829  85.2082986\n",
      "  22.79324908  86.69604149  73.62208964 109.92707032  67.77298\n",
      " 123.49721986  40.65097407 337.04907336   0.          56.17484556\n",
      "  70.20599143 146.7594943   50.97056789 643.3627555 ]\n",
      "10-th iteration, loss: 0.17803090840201244, 27 gd steps\n",
      "insert gradient: -0.0822381203583338\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[  1.84265759  53.54038881 102.96299775   0.         102.96299775\n",
      "  54.410496    90.47718191  24.96021772  75.25972838  75.57283575\n",
      " 111.24497407  63.23800146 129.82400449  55.97881936 317.28932994\n",
      "  24.50257798  25.63159568  61.62047562 139.59219248  64.05521114\n",
      " 643.3627555 ]\n",
      "11-th iteration, loss: 0.17256590314173825, 41 gd steps\n",
      "insert gradient: -0.056666468555554475\n",
      "11-th iteration, new layer inserted. now 23 layers\n",
      "[  1.37392005  58.45185542  94.44991179   8.11715316  92.61374722\n",
      "  55.92621085  88.2933936   22.29258649  80.05254607  75.90827986\n",
      " 114.22871171  62.68997358 130.99741734  59.34478243 307.46599639\n",
      "  37.95734124  24.07586189  52.48877485 145.27784974  62.8127591\n",
      " 175.46256968   0.         467.90018582]\n",
      "12-th iteration, loss: 0.15936755254817023, 45 gd steps\n",
      "insert gradient: -0.037909519648290015\n",
      "12-th iteration, new layer inserted. now 25 layers\n",
      "[  1.52100147  57.57622344  92.05109892  12.5735648   86.60893518\n",
      "  57.57518967  93.99130912  22.45965483  76.53392793  75.42853326\n",
      " 117.26797321  65.30085147 128.6206547   64.41024246 243.17726568\n",
      "   0.          60.79431642  42.12683821  31.904241    39.16491561\n",
      " 156.77868142  60.466304   167.54383287  29.24535551 467.90018582]\n",
      "13-th iteration, loss: 0.1445506562328772, 23 gd steps\n",
      "insert gradient: -0.03048651542106251\n",
      "13-th iteration, new layer inserted. now 27 layers\n",
      "[  2.08234804  58.24124726 105.32328678   9.27670006  83.0374738\n",
      "  52.38081315  96.03997996  30.47396537  76.70588922  67.71802709\n",
      " 124.43965872  66.26623127 120.76208256  76.2351762  108.76762149\n",
      "   0.          87.01409719  29.30151062  30.25486858  45.27878337\n",
      "  57.58858423  27.94257853 160.38895078  62.7941338  159.61227545\n",
      "  42.15814827 467.90018582]\n",
      "14-th iteration, loss: 0.13362925622703856, 20 gd steps\n",
      "insert gradient: -0.0636106938462982\n",
      "14-th iteration, new layer inserted. now 27 layers\n",
      "[  1.5957738   58.30742639  94.15528214  12.29757205  81.36930175\n",
      "  57.79533891  90.99134643  39.20381866  73.79461044  66.53684514\n",
      " 127.50522861  64.64058272 128.19273342  72.70593612 192.8968774\n",
      "  55.56742754   0.98498769  46.52819989  74.17158     25.77838766\n",
      "  90.3940147    0.          72.31521176  54.65037575 158.63236985\n",
      "  57.08846878 467.90018582]\n",
      "15-th iteration, loss: 0.12519213197180912, 47 gd steps\n",
      "insert gradient: -0.03326267891530848\n",
      "15-th iteration, new layer inserted. now 29 layers\n",
      "[1.17901262e+00 5.97308418e+01 9.74295852e+01 1.40757571e+01\n",
      " 7.92369953e+01 5.68554089e+01 9.15071569e+01 3.92663902e+01\n",
      " 8.13984234e+01 7.06696577e+01 1.23546853e+02 6.60026190e+01\n",
      " 1.30133115e+02 7.54988227e+01 1.82943655e+02 5.59757092e+01\n",
      " 1.50077300e-01 4.65804100e+01 9.93111174e+01 3.32787948e+01\n",
      " 7.94767810e+01 9.60361067e+00 6.37663045e+01 3.98592272e+01\n",
      " 1.64542070e+02 6.07584067e+01 2.33950093e+02 0.00000000e+00\n",
      " 2.33950093e+02]\n",
      "16-th iteration, loss: 0.11469605538920566, 21 gd steps\n",
      "insert gradient: -0.03231499633546574\n",
      "16-th iteration, new layer inserted. now 29 layers\n",
      "[7.12884827e-01 5.86407268e+01 9.36970010e+01 1.63220170e+01\n",
      " 7.99954233e+01 5.71981862e+01 8.99804134e+01 4.39464691e+01\n",
      " 8.21001149e+01 7.10099176e+01 1.25433227e+02 6.74045105e+01\n",
      " 1.23214784e+02 7.88413630e+01 1.81315297e+02 1.01791278e+02\n",
      " 1.13310920e+02 4.60037089e+01 6.13058156e+01 1.78344788e+01\n",
      " 6.57760375e+01 2.79777945e+01 1.66472499e+02 5.80537643e+01\n",
      " 2.28576050e+02 2.79155636e+01 2.33950093e+02 0.00000000e+00\n",
      " 4.26325641e-14]\n",
      "17-th iteration, loss: 0.10300213532294385, 42 gd steps\n",
      "insert gradient: -0.010490093374543548\n",
      "17-th iteration, new layer inserted. now 30 layers\n",
      "[  1.5933049   57.47987541  92.67723823  21.78976851  72.45281839\n",
      "  55.99688845  93.25552077  43.85697262  79.87349615  75.06453267\n",
      " 116.14795068  67.40454859 124.94455288  77.41799211 173.96932348\n",
      " 100.96917001 115.10233014  63.45875318  27.60547168  20.14227144\n",
      "  81.83577333  22.12857207 177.83287213  50.8488761   87.05210613\n",
      "   0.         145.08684354  20.22786732 224.1186683   31.20020144]\n",
      "18-th iteration, loss: 0.08160204416024903, 52 gd steps\n",
      "insert gradient: -0.03699016841509582\n",
      "18-th iteration, new layer inserted. now 30 layers\n",
      "[  1.96824795  63.46126514 107.05775846  16.05072477  63.87032749\n",
      "  60.86748621  98.21515266  49.60622383  82.5843765   57.46238842\n",
      " 116.93420675  77.66053129 111.68132639  76.81755883 166.12746988\n",
      " 107.88544011 111.37192008  97.9898315   97.55842876  23.04991777\n",
      " 105.06371372   0.         105.06371372  24.35544249  91.22092\n",
      "   7.86767125 129.63869522  20.3613323  223.54946881  54.18752033]\n",
      "19-th iteration, loss: 0.0724187130721394, 26 gd steps\n",
      "insert gradient: -0.0063942968982659935\n",
      "19-th iteration, new layer inserted. now 32 layers\n",
      "[1.28126256e+00 6.16140110e+01 1.07296873e+02 2.27503066e+01\n",
      " 5.13852950e+01 6.14224584e+01 9.82933063e+01 4.99501212e+01\n",
      " 8.49033866e+01 5.70270546e+01 1.12994452e+02 0.00000000e+00\n",
      " 2.13162821e-14 7.75017038e+01 1.15474864e+02 7.14980374e+01\n",
      " 1.64252935e+02 1.11052028e+02 1.12094415e+02 9.29362709e+01\n",
      " 1.21228675e+02 2.96400242e+01 8.23034747e+01 1.40726637e+01\n",
      " 8.94411101e+01 2.02789806e+01 9.59731181e+01 1.27982668e+01\n",
      " 1.08463686e+02 1.91931895e+01 2.11173736e+02 8.30068725e+01]\n",
      "20-th iteration, loss: 0.06217833251423074, 56 gd steps\n",
      "insert gradient: -0.009338317386739224\n",
      "20-th iteration, new layer inserted. now 32 layers\n",
      "[  0.          57.55630187 102.91594365  45.11678118  31.93819079\n",
      "  59.4173713   99.41639656  49.68154151  88.25873817  60.86597655\n",
      " 118.67417768  75.37684527 111.43593075  74.52503664 150.75806798\n",
      " 118.55864408 122.95759233  89.20892867 121.25901775  64.83548155\n",
      "  15.32013228  25.27338442  98.167951    22.30271584  63.27336742\n",
      "  49.44544208  57.45087951  17.67457218 135.67039774   0.\n",
      " 101.75279831 103.94958951]\n",
      "21-th iteration, loss: 0.05701515852542402, 22 gd steps\n",
      "insert gradient: -0.005111379325970232\n",
      "21-th iteration, new layer inserted. now 32 layers\n",
      "[  1.21517621  58.03593189 100.20730512  49.1581859   35.18089684\n",
      "  54.23365943  96.79632087  52.65743295  89.82626235  61.39037752\n",
      " 123.73855882  73.40650452 114.81570414  73.10346553 148.38324256\n",
      " 121.95468313 127.37715733  88.96344837 116.92538186  70.43286735\n",
      "   0.16985942  33.73483277 102.69495292  18.16080882  60.65890816\n",
      "  55.13227558  53.65028022  12.5224119  119.94057896  19.02845756\n",
      "  85.80737477 109.25483634]\n",
      "22-th iteration, loss: 0.05591707026260122, 19 gd steps\n",
      "insert gradient: -0.003831821721506892\n",
      "22-th iteration, new layer inserted. now 32 layers\n",
      "[  1.61729686  61.41824172 101.42463687  50.31560811  37.595309\n",
      "  51.42383055  96.12398833  53.41814396  92.58972571  59.26314917\n",
      " 126.0291082   75.01076268 114.39760202  75.50902236 147.36443529\n",
      " 121.61059819 129.71824267  90.00333372 115.53466287 104.63039984\n",
      " 105.10428874  16.85414642  60.98142361  56.80091098  55.91852263\n",
      "  10.55655184 118.77454629  24.47205495  75.45640703  48.14086707\n",
      "   0.          64.18782276]\n",
      "23-th iteration, loss: 0.04799558619341332, 38 gd steps\n",
      "insert gradient: -0.003536864031493576\n",
      "23-th iteration, new layer inserted. now 34 layers\n",
      "[  0.92569172  57.7585554  100.03670986  53.168372    45.70652695\n",
      "  45.41936861  95.18015707  55.79951743  97.30229051  61.07711887\n",
      " 127.45201771  72.62411974 112.4739629   76.41980799 143.2999351\n",
      "  71.77697578   0.          53.83273183 132.68463733  90.4969623\n",
      " 110.6920623  100.26006362 111.31939646  24.50033875  44.70902647\n",
      "  55.13617317  71.20199019   0.53893447 122.22972662  54.23579447\n",
      "  49.20372722  30.38503509  49.58918372  40.66469095]\n",
      "24-th iteration, loss: 0.042163252523816155, 320 gd steps\n",
      "insert gradient: -0.002668587682286918\n",
      "24-th iteration, new layer inserted. now 34 layers\n",
      "[  1.55401724  59.21134666 101.00160729  53.94455625  63.96703269\n",
      "  39.83637084  92.12760852  56.94179973  99.00021062  60.32900174\n",
      " 135.2721929   71.55329882 118.96482381  71.42102655 145.98112025\n",
      "  67.22132224  40.46659494   5.71678558 166.21638826 108.54009864\n",
      " 103.157113   102.42021565  95.91956995  43.55867816  31.90301167\n",
      "  41.1766082  125.39910169   0.          94.04932627  43.80225678\n",
      "  73.47749745   9.93471829  95.9031767   41.97364239]\n",
      "25-th iteration, loss: 0.040709066861650864, 209 gd steps\n",
      "insert gradient: -0.00048748089737738094\n",
      "25-th iteration, new layer inserted. now 34 layers\n",
      "[  1.45119067  58.35165267 102.14651295  55.08326465  65.28624152\n",
      "  40.28780814  92.29436592  55.25979692  98.82349311  60.07791066\n",
      " 137.42213106  69.61874117 122.43829997  69.17841779 148.71344354\n",
      "  67.83715031 206.49271023 114.19112143 105.38142255  57.93974398\n",
      "   0.          43.45480799 100.08790355  47.16637204  29.46106924\n",
      "  38.3565863  100.68975891  16.46450573  76.01962506  44.18289273\n",
      "  76.61529373   5.48365013 107.28375198  48.06954286]\n",
      "26-th iteration, loss: 0.0403152047364642, 59 gd steps\n",
      "insert gradient: -0.0007243277307258908\n",
      "26-th iteration, new layer inserted. now 32 layers\n",
      "[  1.74827925  58.11072439 103.38111123  55.83441819  71.13268866\n",
      "  39.1566729   87.89321283  53.04229749 100.68631227  62.14549312\n",
      " 128.24967351  75.29332045 114.68430262  74.98164312 143.6541259\n",
      "  68.44550008 204.39200667 114.51521886 109.36729723 100.93969149\n",
      " 101.72090155  49.12059547  27.97999959  38.31798642  99.96004553\n",
      "  18.3752568   71.95942198  45.36140964  74.94766319   6.88561955\n",
      " 105.10175256  44.75587677]\n",
      "27-th iteration, loss: 0.040273194166554574, 18 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "27-th iteration, new layer inserted. now 34 layers\n",
      "[  1.77530881  57.89376262 103.2721061   55.23749111  72.07101212\n",
      "  39.61413118  87.18272448  52.24244469 100.86934496  62.86620308\n",
      " 127.42213945  75.89675359 113.58999337  75.33258468 142.71892194\n",
      "  68.96172542 204.75097971 114.21728347  31.27789315   0.\n",
      "  78.19473288 100.76734405 102.46335482  49.84027284  28.34942274\n",
      "  37.73984099  99.84106913  18.55837901  71.50125269  45.30601246\n",
      "  74.60466107   6.97270022 104.8813185   44.78811679]\n",
      "28-th iteration, loss: 0.04026706515106903, 24 gd steps\n",
      "insert gradient: -0.0005029214089269415\n",
      "28-th iteration, new layer inserted. now 32 layers\n",
      "[  1.77804572  57.90572126 103.2815836   55.24904965  72.07389122\n",
      "  39.61366715  87.18435074  52.25161861 100.87480293  62.87261668\n",
      " 127.42277276  75.90277515 113.59298349  75.33817987 142.71821467\n",
      "  68.96508118 204.75231113 114.22318614 109.48562403 100.77774891\n",
      " 102.46944627  49.84760603  28.35489444  37.74638641  99.84403969\n",
      "  18.56604026  71.50319743  45.30977059  74.6066633    6.97875786\n",
      " 104.88280641  44.78915444]\n",
      "29-th iteration, loss: 0.04014238351207162, 52 gd steps\n",
      "insert gradient: -2.5358193240606148e-05\n",
      "29-th iteration, new layer inserted. now 34 layers\n",
      "[1.71129216e+00 5.74779483e+01 1.04838368e+02 5.62262673e+01\n",
      " 7.39842541e+01 3.89546847e+01 8.53732171e+01 5.15082756e+01\n",
      " 1.01014530e+02 6.35545234e+01 1.24925542e+02 7.77543146e+01\n",
      " 1.11174643e+02 7.70697668e+01 1.39898185e+02 7.05101878e+01\n",
      " 2.05476812e+02 1.13497285e+02 1.09070667e+02 1.00685228e+02\n",
      " 1.04188227e+02 5.07046551e+01 2.89333222e+01 3.70619919e+01\n",
      " 9.98356798e+01 1.87365478e+01 7.04262091e+01 4.57775169e+01\n",
      " 7.42808714e+01 6.86619642e+00 1.04575741e+02 0.00000000e+00\n",
      " 1.06581410e-14 4.50587355e+01]\n",
      "30-th iteration, loss: 0.04013877535237347, 13 gd steps\n",
      "insert gradient: -0.00044703376920788353\n",
      "30-th iteration, new layer inserted. now 34 layers\n",
      "[  1.70284148  57.43912765 104.81791936  56.2024068   73.98868091\n",
      "  38.95916665  85.36516479  51.4906075  101.00788232  63.55534835\n",
      " 124.91749214  77.76430006 111.16527382  77.08023737 139.8898666\n",
      "  70.52527628 117.42226829   0.          88.06670122 113.49739632\n",
      " 109.05660666 100.6820175  104.18760101  50.70805463  28.94034667\n",
      "  37.06351314  99.8385377   18.74074441  70.42559008  45.77981238\n",
      "  74.27940932   6.85806127 104.57539427  45.06480638]\n",
      "31-th iteration, loss: 0.039953180410820195, 25 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -7.509590429173674e-19\n",
      "31-th iteration, new layer inserted. now 36 layers\n",
      "[1.76053619e+00 5.64084903e+01 1.05435361e+02 5.69464396e+01\n",
      " 7.66000114e+01 3.84317065e+01 8.35331598e+01 5.08812787e+01\n",
      " 1.00660668e+02 6.44979417e+01 1.21406945e+02 8.02312463e+01\n",
      " 1.08395392e+02 7.90812002e+01 1.36091822e+02 7.25357685e+01\n",
      " 1.16051827e+02 1.42084333e+00 8.69197540e+01 1.12787126e+02\n",
      " 1.06634070e+02 1.00658643e+02 1.07000532e+02 5.25130613e+01\n",
      " 2.87917771e+01 3.60412100e+01 1.00612171e+02 1.86372760e+01\n",
      " 6.88483545e+01 4.66605305e+01 7.48666301e+01 6.03216992e+00\n",
      " 1.04961834e+02 4.60270409e+01 0.00000000e+00 1.77635684e-15]\n",
      "32-th iteration, loss: 0.03992625462354558, 7 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "32-th iteration, new layer inserted. now 37 layers\n",
      "[1.76046357e+00 5.64078172e+01 1.05434899e+02 5.69461115e+01\n",
      " 7.65999151e+01 3.84305835e+01 8.35316290e+01 5.08764581e+01\n",
      " 1.00658325e+02 6.44957199e+01 1.21404466e+02 8.02282034e+01\n",
      " 1.08392242e+02 7.90753146e+01 1.36086859e+02 7.25307838e+01\n",
      " 1.16046759e+02 1.41332585e+00 8.69143764e+01 1.12767748e+02\n",
      " 1.06615530e+02 1.00632307e+02 3.56606015e+01 0.00000000e+00\n",
      " 7.13212030e+01 5.24925791e+01 2.87837496e+01 3.60323202e+01\n",
      " 1.00609285e+02 1.86335004e+01 6.88471024e+01 4.66592706e+01\n",
      " 7.48663999e+01 6.03180187e+00 1.04961684e+02 4.60269161e+01\n",
      " 7.24297663e-18]\n",
      "33-th iteration, loss: 0.03992453311491433, 5 gd steps\n",
      "insert gradient: -0.000544527248457397\n",
      "33-th iteration, new layer inserted. now 37 layers\n",
      "[1.76051685e+00 5.64079969e+01 1.05434921e+02 5.69460659e+01\n",
      " 7.66000126e+01 3.84308052e+01 8.35317500e+01 5.08768341e+01\n",
      " 1.00658532e+02 6.44959265e+01 1.21404515e+02 8.02285797e+01\n",
      " 1.08392511e+02 7.90762301e+01 1.36087369e+02 7.25317118e+01\n",
      " 1.16047407e+02 1.41509118e+00 8.69151204e+01 1.12770400e+02\n",
      " 1.06617758e+02 1.00635540e+02 3.56630970e+01 3.58188363e-03\n",
      " 7.13236985e+01 5.24953235e+01 2.87848416e+01 3.60334005e+01\n",
      " 1.00609680e+02 1.86338019e+01 6.88471366e+01 4.66594111e+01\n",
      " 7.48664591e+01 6.03175626e+00 1.04961734e+02 4.60269553e+01\n",
      " 7.99044626e-18]\n",
      "34-th iteration, loss: 0.03992365231982139, 6 gd steps\n",
      "insert gradient: -0.0003109457569253442\n",
      "34-th iteration, new layer inserted. now 37 layers\n",
      "[1.76066663e+00 5.64083672e+01 1.05434844e+02 5.69458157e+01\n",
      " 7.66002978e+01 3.84311674e+01 8.35316588e+01 5.08765205e+01\n",
      " 1.00658448e+02 6.44958650e+01 1.21403880e+02 8.02288079e+01\n",
      " 1.08392366e+02 7.90772652e+01 1.36087413e+02 7.25330710e+01\n",
      " 1.16047845e+02 0.00000000e+00 1.42108547e-14 1.41829294e+00\n",
      " 8.69157673e+01 1.12772700e+02 1.06619009e+02 1.00637519e+02\n",
      " 1.06990939e+02 5.24976259e+01 2.87857641e+01 3.60340052e+01\n",
      " 1.00610019e+02 1.86335139e+01 6.88468306e+01 4.66594549e+01\n",
      " 7.48665701e+01 6.03147011e+00 1.04961846e+02 4.60270482e+01\n",
      " 9.81841101e-18]\n",
      "35-th iteration, loss: 0.03987833975019586, 17 gd steps\n",
      "WARNING: insert gradient close to zero, min grad: -3.184010378924785e-18\n",
      "35-th iteration, new layer inserted. now 37 layers\n",
      "[1.89366177e+00 5.66740313e+01 1.05357129e+02 5.68344725e+01\n",
      " 7.70216539e+01 3.85714209e+01 8.32825445e+01 5.05063265e+01\n",
      " 1.00611835e+02 6.46927315e+01 1.20727037e+02 8.06459747e+01\n",
      " 1.08031757e+02 7.94005228e+01 1.35420142e+02 7.29450461e+01\n",
      " 1.15170232e+02 2.08501490e+00 8.63078267e+01 1.12796379e+02\n",
      " 1.05891193e+02 1.00540117e+02 1.07464552e+02 5.29960619e+01\n",
      " 2.89094239e+01 3.58145058e+01 1.00883974e+02 1.84448126e+01\n",
      " 6.85422640e+01 4.68147527e+01 7.50838778e+01 5.79234803e+00\n",
      " 1.05183600e+02 4.62418192e+01 0.00000000e+00 1.77635684e-15\n",
      " 4.75562450e-15]\n",
      "36-th iteration, loss: 0.03987153182053509, 6 gd steps\n",
      "insert gradient: -0.002319950420411731\n",
      "36-th iteration, new layer inserted. now 39 layers\n",
      "[1.89362528e+00 5.66737458e+01 1.05357043e+02 5.68344779e+01\n",
      " 7.70216094e+01 3.85705273e+01 8.32817292e+01 5.05044019e+01\n",
      " 1.00610862e+02 6.46918526e+01 1.20725847e+02 8.06448224e+01\n",
      " 1.08030343e+02 7.93978166e+01 1.35417651e+02 7.29426808e+01\n",
      " 1.15167441e+02 2.08103638e+00 8.63048972e+01 1.12786753e+02\n",
      " 1.05881867e+02 0.00000000e+00 1.42108547e-14 1.00526662e+02\n",
      " 1.07454923e+02 5.29853548e+01 2.89051151e+01 3.58097105e+01\n",
      " 1.00882558e+02 1.84428608e+01 6.85415837e+01 4.68142186e+01\n",
      " 7.50838650e+01 5.79217210e+00 1.05183621e+02 4.62418403e+01\n",
      " 4.69541830e-18 3.45079081e-05 4.75919537e-15]\n",
      "37-th iteration, loss: 0.03979198221690978, 54 gd steps\n",
      "insert gradient: -0.00011690675875972316\n",
      "37-th iteration, new layer inserted. now 37 layers\n",
      "[1.94140949e+00 5.66286021e+01 1.05392197e+02 5.70101346e+01\n",
      " 7.76668569e+01 3.83154242e+01 8.28843695e+01 5.04997434e+01\n",
      " 1.00445765e+02 6.47186616e+01 1.19652278e+02 8.13228597e+01\n",
      " 1.07550184e+02 7.97914213e+01 1.34402211e+02 7.36051202e+01\n",
      " 1.13302731e+02 3.07162734e+00 8.52572718e+01 1.13098904e+02\n",
      " 1.04552070e+02 1.46854204e-03 6.95239759e-04 1.00407335e+02\n",
      " 1.08278488e+02 5.35974025e+01 2.88566482e+01 3.53917482e+01\n",
      " 1.01568949e+02 1.82137200e+01 6.78907391e+01 4.71348381e+01\n",
      " 7.56035637e+01 5.23020904e+00 1.05671570e+02 4.68461500e+01\n",
      " 1.16814107e-14]\n",
      "38-th iteration, loss: 0.03976420787025918, 23 gd steps\n",
      "WARNING: inserted layer is on the edge of a layer which may indicate the termination of needle optimization\n",
      "38-th iteration, new layer inserted. now 37 layers\n",
      "[1.96568474e+00 5.66261136e+01 1.05401226e+02 5.70387790e+01\n",
      " 7.78956468e+01 3.82341644e+01 8.27687489e+01 5.05215231e+01\n",
      " 1.00386742e+02 6.46993616e+01 1.19277088e+02 8.15795104e+01\n",
      " 1.07389092e+02 7.99079808e+01 1.34095568e+02 7.38712134e+01\n",
      " 1.12414343e+02 3.50678917e+00 8.48315699e+01 1.13230354e+02\n",
      " 1.04000692e+02 1.00359683e+02 3.61931962e+01 0.00000000e+00\n",
      " 7.23863924e+01 5.38067447e+01 2.87873999e+01 3.52372447e+01\n",
      " 1.01924080e+02 1.81541069e+01 6.76041412e+01 4.72839899e+01\n",
      " 7.58683453e+01 5.01366345e+00 1.05924053e+02 4.69619177e+01\n",
      " 1.95051726e-14]\n",
      "39-th iteration, loss: 0.039762771977397506, 5 gd steps\n",
      "insert gradient: -2.8331380703780675e-05\n",
      "39-th iteration, new layer inserted. now 37 layers\n",
      "[1.96569869e+00 5.66261680e+01 1.05401252e+02 5.70387784e+01\n",
      " 7.78957029e+01 3.82342319e+01 8.27688260e+01 5.05218646e+01\n",
      " 1.00386906e+02 6.46995078e+01 1.19277160e+02 8.15798370e+01\n",
      " 1.07389315e+02 7.99085782e+01 1.34095959e+02 7.38718155e+01\n",
      " 1.12414646e+02 3.50792852e+00 8.48321024e+01 1.13232654e+02\n",
      " 1.04002721e+02 1.00362961e+02 3.61959132e+01 4.34127055e-03\n",
      " 7.23891094e+01 5.38097394e+01 2.87885237e+01 3.52383862e+01\n",
      " 1.01924539e+02 1.81544772e+01 6.76041727e+01 4.72841410e+01\n",
      " 7.58684173e+01 5.01358200e+00 1.05924114e+02 4.69619433e+01\n",
      " 1.95081129e-14]\n",
      "40-th iteration, loss: 0.03976174813136333, 6 gd steps\n",
      "insert gradient: -5.4789570075693994e-05\n",
      "40-th iteration, new layer inserted. now 35 layers\n",
      "[1.96571095e+00 5.66261462e+01 1.05401241e+02 5.70387334e+01\n",
      " 7.78958571e+01 3.82341421e+01 8.27687383e+01 5.05219655e+01\n",
      " 1.00386911e+02 6.46994665e+01 1.19276867e+02 8.15801439e+01\n",
      " 1.07389259e+02 7.99088381e+01 1.34095893e+02 7.38722094e+01\n",
      " 1.12414021e+02 3.50878209e+00 8.48319821e+01 1.13233794e+02\n",
      " 1.04003352e+02 1.00364401e+02 1.08588337e+02 5.38113467e+01\n",
      " 2.87888495e+01 3.52385190e+01 1.01924880e+02 1.81541367e+01\n",
      " 6.76037643e+01 4.72840966e+01 7.58685488e+01 5.01312914e+00\n",
      " 1.05924244e+02 4.69619648e+01 1.95226734e-14]\n",
      "41-th iteration, loss: 0.039397639310354414, 37 gd steps\n",
      "insert gradient: -0.001135060334123555\n",
      "41-th iteration, new layer inserted. now 35 layers\n",
      "[1.85647528e+00 5.64600504e+01 1.06390800e+02 5.75746563e+01\n",
      " 7.95193610e+01 3.71746279e+01 8.21670184e+01 5.02649295e+01\n",
      " 1.01484969e+02 6.39700681e+01 1.15255807e+02 8.37472227e+01\n",
      " 1.06509646e+02 7.98200440e+01 1.33334269e+02 7.84360085e+01\n",
      " 9.16335070e+01 1.14170685e+01 7.95154837e+01 6.62840055e+01\n",
      " 0.00000000e+00 4.97130041e+01 9.58278383e+01 1.01936482e+02\n",
      " 1.07776614e+02 5.70813501e+01 2.81828565e+01 3.13905007e+01\n",
      " 1.12151476e+02 1.82778577e+01 6.01833578e+01 4.81549671e+01\n",
      " 1.94938923e+02 5.07728524e+01 4.08292344e-13]\n",
      "42-th iteration, loss: 0.037602224900342165, 29 gd steps\n",
      "insert gradient: -0.0013931807784499726\n",
      "42-th iteration, new layer inserted. now 37 layers\n",
      "[2.45278886e+00 5.85886751e+01 1.03648529e+02 5.63679225e+01\n",
      " 7.96040008e+01 3.66306028e+01 8.44783994e+01 5.11008659e+01\n",
      " 1.02648661e+02 6.26272330e+01 1.13905372e+02 0.00000000e+00\n",
      " 1.42108547e-14 8.31827865e+01 1.08008249e+02 7.70503764e+01\n",
      " 1.37386998e+02 7.79322798e+01 9.69237509e+01 1.17519832e+01\n",
      " 7.59289038e+01 5.61344579e+01 1.99749153e+01 3.93703994e+01\n",
      " 1.01083988e+02 1.01430527e+02 1.07580886e+02 6.50182399e+01\n",
      " 2.16864152e+01 2.77621774e+01 1.22926691e+02 1.87223875e+01\n",
      " 4.91988697e+01 5.47436393e+01 1.85589288e+02 6.58867552e+01\n",
      " 5.63360155e-13]\n",
      "43-th iteration, loss: 0.03713480422215721, 17 gd steps\n",
      "insert gradient: -0.0017699309439683543\n",
      "43-th iteration, new layer inserted. now 37 layers\n",
      "[2.76392416e+00 5.85653434e+01 1.02890687e+02 5.70585992e+01\n",
      " 8.13125027e+01 0.00000000e+00 1.77635684e-14 3.59947673e+01\n",
      " 8.37380527e+01 5.21241170e+01 1.02378575e+02 6.13652194e+01\n",
      " 1.14741546e+02 8.61155639e+01 1.07669072e+02 7.56334743e+01\n",
      " 1.36924724e+02 7.75865808e+01 9.68820514e+01 1.39907318e+01\n",
      " 7.08672834e+01 5.61525110e+01 2.33080051e+01 3.78684351e+01\n",
      " 9.91343617e+01 1.01536518e+02 1.10140496e+02 6.72268886e+01\n",
      " 2.16953001e+01 2.44096279e+01 1.23302113e+02 2.25739516e+01\n",
      " 4.41428402e+01 5.37218890e+01 1.84362724e+02 6.93346553e+01\n",
      " 6.06472990e-13]\n",
      "44-th iteration, loss: 0.03661109921978658, 30 gd steps\n",
      "insert gradient: -0.0008912687426888759\n",
      "44-th iteration, new layer inserted. now 35 layers\n",
      "[2.44727445e+00 5.88422735e+01 1.02858335e+02 5.75437737e+01\n",
      " 8.26846638e+01 3.61131988e+01 8.43508832e+01 5.28229205e+01\n",
      " 1.02146863e+02 6.12366856e+01 1.13939066e+02 8.63160445e+01\n",
      " 1.07926525e+02 7.61023274e+01 1.36314551e+02 7.90458452e+01\n",
      " 9.81804605e+01 1.55457673e+01 6.50666296e+01 5.69144579e+01\n",
      " 2.75260744e+01 3.55876810e+01 9.88566929e+01 1.01497917e+02\n",
      " 1.11750427e+02 6.79322104e+01 2.31870581e+01 2.17141546e+01\n",
      " 1.22730265e+02 2.82457336e+01 3.83205373e+01 5.33950917e+01\n",
      " 1.85563380e+02 7.02620456e+01 6.40818670e-13]\n",
      "45-th iteration, loss: 0.03580911208520345, 26 gd steps\n",
      "insert gradient: -0.001297352274485891\n",
      "45-th iteration, new layer inserted. now 35 layers\n",
      "[1.53438701e+00 5.89513350e+01 1.03618569e+02 5.78905552e+01\n",
      " 7.99730494e+01 3.74435655e+01 8.64261225e+01 5.47695524e+01\n",
      " 1.00597643e+02 5.98407657e+01 1.16112388e+02 8.54020684e+01\n",
      " 1.09895585e+02 7.64557477e+01 1.37339113e+02 7.95425509e+01\n",
      " 1.01471362e+02 1.93735320e+01 5.16917088e+01 5.72074841e+01\n",
      " 3.96923450e+01 2.96089550e+01 9.72795317e+01 1.02708342e+02\n",
      " 1.12757653e+02 6.81475035e+01 2.73833074e+01 1.79993765e+01\n",
      " 1.17220036e+02 4.03974565e+01 2.44832000e+01 5.47254675e+01\n",
      " 1.87286996e+02 7.15737698e+01 7.04484394e-13]\n",
      "46-th iteration, loss: 0.03535573608262828, 30 gd steps\n",
      "insert gradient: -0.0008564334745418648\n",
      "46-th iteration, new layer inserted. now 35 layers\n",
      "[1.87994218e+00 6.08083756e+01 1.03060430e+02 5.72926447e+01\n",
      " 8.18137069e+01 3.84438667e+01 8.50509299e+01 5.51043202e+01\n",
      " 1.00674081e+02 6.06059398e+01 1.17394786e+02 8.68995225e+01\n",
      " 1.10045505e+02 7.55937179e+01 1.37411309e+02 7.90822521e+01\n",
      " 1.02548815e+02 2.16629558e+01 4.67370290e+01 5.73392367e+01\n",
      " 4.70080096e+01 2.73059473e+01 9.52703188e+01 1.04176515e+02\n",
      " 1.12237013e+02 6.89039746e+01 2.89337685e+01 1.70561712e+01\n",
      " 1.14978054e+02 4.63787575e+01 2.04111993e+01 5.29751377e+01\n",
      " 1.90599273e+02 7.11343030e+01 7.33385034e-13]\n",
      "47-th iteration, loss: 0.03518593670877717, 25 gd steps\n",
      "insert gradient: -0.0006327668310578842\n",
      "47-th iteration, new layer inserted. now 35 layers\n",
      "[1.48395386e+00 5.99551377e+01 1.02935620e+02 5.78006443e+01\n",
      " 8.28746169e+01 3.88416745e+01 8.43114383e+01 5.53961487e+01\n",
      " 1.01059924e+02 6.09828323e+01 1.17340527e+02 8.68913117e+01\n",
      " 1.09988463e+02 7.63698598e+01 1.37441174e+02 7.92056083e+01\n",
      " 1.03167007e+02 2.28734717e+01 4.31419874e+01 5.73901795e+01\n",
      " 5.13129079e+01 2.57665755e+01 9.43249310e+01 1.05082718e+02\n",
      " 1.12310734e+02 6.95023467e+01 2.94312426e+01 1.67457193e+01\n",
      " 1.13570492e+02 4.92148801e+01 1.90204242e+01 5.19780986e+01\n",
      " 1.92590523e+02 7.08816055e+01 7.00374332e-13]\n",
      "48-th iteration, loss: 0.03509768550758931, 27 gd steps\n",
      "insert gradient: -0.00040943843488876224\n",
      "48-th iteration, new layer inserted. now 35 layers\n",
      "[1.57325811e+00 6.02763783e+01 1.02608980e+02 5.71063138e+01\n",
      " 8.39989954e+01 3.99994746e+01 8.36566581e+01 5.50567005e+01\n",
      " 1.01200528e+02 6.15403122e+01 1.17719546e+02 8.74301961e+01\n",
      " 1.10065332e+02 7.64866838e+01 1.37557788e+02 7.91542034e+01\n",
      " 1.03477617e+02 2.39962908e+01 4.09599819e+01 5.74639423e+01\n",
      " 5.43437017e+01 2.48340198e+01 9.33470545e+01 1.05885967e+02\n",
      " 1.12077891e+02 6.97476099e+01 3.00495396e+01 1.66364489e+01\n",
      " 1.12621943e+02 5.08237518e+01 1.85756658e+01 5.12330097e+01\n",
      " 1.94262154e+02 7.06207472e+01 7.63734503e-13]\n",
      "49-th iteration, loss: 0.035059939963245045, 26 gd steps\n",
      "insert gradient: -0.00029910145118604357\n",
      "49-th iteration, new layer inserted. now 37 layers\n",
      "[1.51196212e+00 6.06440431e+01 1.02832945e+02 5.74308842e+01\n",
      " 8.46850176e+01 0.00000000e+00 3.55271368e-15 4.00941582e+01\n",
      " 8.28558322e+01 5.52911472e+01 1.01396962e+02 6.15090254e+01\n",
      " 1.17893561e+02 8.74021011e+01 1.10024060e+02 7.69053809e+01\n",
      " 1.37652442e+02 7.92460768e+01 1.03756925e+02 2.47121398e+01\n",
      " 3.96019192e+01 5.76160091e+01 5.63943675e+01 2.43422013e+01\n",
      " 9.27207580e+01 1.06388934e+02 1.11949739e+02 6.99474315e+01\n",
      " 3.01853519e+01 1.65752571e+01 1.12096986e+02 5.16811853e+01\n",
      " 1.83517058e+01 5.05412351e+01 1.95394011e+02 7.06567750e+01\n",
      " 8.75157084e-13]\n",
      "0-th iteration, loss: 0.9545036783633585, 0 gd steps\n",
      "insert gradient: -2.535675124045147\n",
      "0-th iteration, new layer inserted. now 3 layers\n",
      "[  43.38963896    0.         2117.41438114]\n",
      "1-th iteration, loss: 0.747179520681565, 11 gd steps\n",
      "insert gradient: -0.6281321970379575\n",
      "1-th iteration, new layer inserted. now 5 layers\n",
      "[  42.16488755   62.36254011  232.39913939    0.         1885.01524175]\n",
      "2-th iteration, loss: 0.6041405613107446, 13 gd steps\n",
      "insert gradient: -0.6014785576544475\n",
      "2-th iteration, new layer inserted. now 7 layers\n",
      "[   3.69006917   77.83236047  218.24828251   42.05801977  115.40909643\n",
      "    0.         1769.60614532]\n",
      "3-th iteration, loss: 0.4658363378800264, 30 gd steps\n",
      "insert gradient: -0.7059103151774996\n",
      "3-th iteration, new layer inserted. now 9 layers\n",
      "[   0.          103.62586213  178.07709572   55.19849555  100.79415561\n",
      "   55.15704198  404.48140464    0.         1365.12474067]\n",
      "4-th iteration, loss: 0.3808417137143428, 19 gd steps\n",
      "insert gradient: -0.3040156158671362\n",
      "4-th iteration, new layer inserted. now 11 layers\n",
      "[   0.          103.05910582  180.85154262   59.89406138  102.42345064\n",
      "   56.38450194  252.41279859    0.          106.27907309   44.35478338\n",
      " 1365.12474067]\n",
      "5-th iteration, loss: 0.30332326634902657, 20 gd steps\n",
      "insert gradient: -0.2735544119973575\n",
      "5-th iteration, new layer inserted. now 13 layers\n",
      "[  0.          75.6896453  205.02109784  64.82171747 116.82489863\n",
      "  55.24758781 174.52513011  55.7746208   69.30588823  41.92237032\n",
      " 868.71574406   0.         496.40899661]\n",
      "6-th iteration, loss: 0.22990386554838407, 70 gd steps\n",
      "insert gradient: -0.18062567943000607\n",
      "6-th iteration, new layer inserted. now 15 layers\n",
      "[  2.28582007  61.78208924 197.99721552  66.75742007 158.52755814\n",
      "  65.77646134 135.35408259  50.00240859  81.32362573  44.69292247\n",
      " 824.9655297   44.6546166  110.31311036   0.         386.09588625]\n",
      "7-th iteration, loss: 0.20032017984849762, 88 gd steps\n",
      "insert gradient: -0.12272211052413433\n",
      "7-th iteration, new layer inserted. now 17 layers\n",
      "[  0.84130581  61.93242812 183.8386078   82.30892457 134.13174779\n",
      "  79.47523284 119.14283898  48.85174941  81.71092803  47.15977518\n",
      " 205.6245125    0.         616.87353751  45.99763251  82.91350563\n",
      "  34.84507629 386.09588625]\n",
      "8-th iteration, loss: 0.19172581324710994, 18 gd steps\n",
      "insert gradient: -0.057787205384353994\n",
      "8-th iteration, new layer inserted. now 19 layers\n",
      "[  0.73540979  61.75044046 180.3364805   82.49106472 134.49597742\n",
      "  81.70321904 115.21693188  51.74622844  79.78257894  40.48036365\n",
      " 142.08285603   0.          56.83314241  11.51632006 601.70221533\n",
      "  48.22454992  85.48836299  32.20140126 386.09588625]\n",
      "9-th iteration, loss: 0.1522224725709742, 47 gd steps\n",
      "insert gradient: -0.032357148049894785\n",
      "9-th iteration, new layer inserted. now 19 layers\n",
      "[  0.43986279  61.85575655 177.19837924  84.034214   130.42398795\n",
      "  85.14631423 114.59312797  51.92637145  86.95653955  44.41475128\n",
      "  80.8058464   94.80997288 247.14588041   0.         329.52784054\n",
      "  47.46479297  83.402668    33.39801536 386.09588625]\n",
      "10-th iteration, loss: 0.1503301467487213, 19 gd steps\n",
      "insert gradient: -0.03875713752008141\n",
      "10-th iteration, new layer inserted. now 21 layers\n",
      "[2.25320586e-01 5.99216684e+01 1.84089485e+02 8.10147746e+01\n",
      " 1.32964933e+02 8.22705060e+01 1.17509290e+02 5.22459791e+01\n",
      " 8.76955658e+01 4.46971032e+01 8.22515235e+01 9.72960980e+01\n",
      " 1.18864605e+02 0.00000000e+00 1.18864605e+02 5.79876067e+00\n",
      " 3.18199270e+02 4.95120010e+01 8.46242553e+01 3.08241208e+01\n",
      " 3.86095886e+02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./../../designer/script/')\n",
    "from design import Design\n",
    "from film import FilmSimple\n",
    "from spectrum import Spectrum\n",
    "\n",
    "\n",
    "def make_reflection_design(init_film: FilmSimple):\n",
    "    inc_ang = 0.\n",
    "    wls = np.linspace(400, 1000, 500) # when wls = 50, ~100 min\n",
    "    target_spec = [Spectrum(inc_ang, wls, np.ones(wls.shape[0], dtype='float'))]\n",
    "    \n",
    "    design = Design(target_spec, init_film)\n",
    "    return design\n",
    "\n",
    "result_design_SiO2_ls = []\n",
    "for run_num, init_ot in enumerate(np.linspace(0, 5000, 200)):\n",
    "    \n",
    "    d_init = np.array([init_ot], dtype='float')\n",
    "    film = FilmSimple('SiO2', 'TiO2', 'SiO2', d_init)\n",
    "    design = make_reflection_design(film)\n",
    "    result_design_SiO2_ls.append(design) # find a way tosave this obj later\n",
    "    try:\n",
    "        design.TFNN_train(epoch=50)\n",
    "    except Exception as e:\n",
    "        print(e.args[0])\n",
    "\n",
    "    np.savetxt(\n",
    "        f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final', \n",
    "        design.film.get_d() / 1000 # in \\mu m\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_design_TiO2_ls = []\n",
    "for run_num, init_ot in enumerate(np.linspace(0, 5000, 200)):\n",
    "    \n",
    "    d_init = np.array([init_ot], dtype='float')\n",
    "    film = FilmSimple('TiO2', 'SiO2', 'SiO2', d_init)\n",
    "    design = make_reflection_design(film)\n",
    "    result_design_TiO2_ls.append(design) # find a way tosave this obj later\n",
    "    try:\n",
    "        design.TFNN_train(epoch=50)\n",
    "    except Exception as e:\n",
    "        print(e.args[0])\n",
    "\n",
    "    np.savetxt(\n",
    "        f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_TiO2/{run_num}_final', \n",
    "        design.film.get_d() / 1000 # in \\mu m\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfeng6\\AppData\\Local\\Temp\\ipykernel_13436\\3791610427.py:4: UserWarning: loadtxt: input contained no data: \"./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/0_final\"\n",
      "  d = np.loadtxt(f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final') * 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n",
      "WARNING: spec not in this film's spec list. New spec added to film!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "design_ls: list[Design] = []\n",
    "for run_num, init_gt in enumerate(np.linspace(0, 5000, 200)):\n",
    "    d = np.loadtxt(f'./raw_result/single_inc/0_inc-400to1000wls-init_single_layer_SiO2/{run_num}_final') * 1000.\n",
    "    if d.shape == (0, ):\n",
    "        continue\n",
    "    f = FilmSimple('SiO2', 'TiO2', 'SiO2', d)\n",
    "    \n",
    "    inc_ang = 0.\n",
    "    wls = np.linspace(400, 1000, 500)\n",
    "    target_spec = [Spectrum(inc_ang, wls, np.ones(wls.shape[0], dtype='float'))]\n",
    "    \n",
    "    design_ls.append(Design(target_spec, FilmSimple('SiO2', 'TiO2', 'SiO2', np.array([init_gt])), f))\n",
    "\n",
    "init_ot, loss = [], []\n",
    "for d in design_ls:\n",
    "    init_ot.append(d.get_init_ot())\n",
    "    loss.append(d.calculate_loss())\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(init_ot, np.log10(loss))\n",
    "ax.set_xlabel('init ot / nm')\n",
    "ax.set_ylabel('log 10 loss')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(wls, design_ls[198].film.get_spec().get_R())\n",
    "ax.set_ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
